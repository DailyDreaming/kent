#!/bin/csh -f # set emacs mode
exit; # don't actually run this like a script :)

# This file describes how we made the browser database on 
# NCBI build 36 (October 2005 freeze)

# HOW TO BUILD AN ASSEMBLY FROM NCBI FILES 
# ---------------------------------------
# 10/06/2005
# Make gs.19 directory, gs.19/build36 directory, and gs.19/ffa directory.
    ssh kkstore02
    mkdir /cluster/store11/gs.19
    mkdir /cluster/store11/gs.19/build36
    mkdir /cluster/store11/gs.19/agp
    mkdir /cluster/store11/gs.19/ffa

#    Make a symbolic link from /cluster/store1 to this location
#	(I assume there is some use for this later ?)
	
    cd /cluster/store1
    ln -s /cluster/store11/gs.19 ./gs.19
    ln -s /cluster/store11/gs.19/build36 /cluster/data/hg18

#    Make a symbolic link from your home directory to the build dir:
#	(Investigate what this is used for, may no longer be necessary)

    cd
    ln -s /cluster/store11/gs.19/build36 ~/oo

# NCBI download site, fetch everything into this one directory:

#	with the machine and password in your $HOME/.netrc file, this
#	wget command will require no login.  Your $HOME/.netrc file
#	is set to 'chmod 600 .netrc' to prevent anyone from finding
#	the data.  (There were some early files that later moved
#		into an OLD subdirectory.  They were broken.)

# 11/16/2005
# Received answer from Greg to go ahead with the new build.

    ssh kkstore02
    mkdir /cluster/store11/gs.19/ncbi
    cd /cluster/store11/gs.19/ncbi
    bash
    wget --timestamp ftp://ftp-private.ncbi.nih.gov/build_36/*

#	New to this build is the sequence: NC_001807 which is the
#	mitochondria sequence.  This prefix NC_ is new to the process
#	and will have to be accounted for below.  The other two special
#	prefixes are similar to what was seen before:
#	from DR52.agp NG_002392
#	Homo sapiens major histocompatibility complex, class II,
#		DR52 haplotype (DR52) on chromosome 6
#	and from DR53.agp NG_002433
#	Homo sapiens major histocompatibility complex, class II,
#		DR53 haplotype (DR53) on chromosome 6

#	Fixup seq_contig.md
#
#	It has a bunch of stuff belonging to the Celera
#	genome assembly.  Filter those out.  I don't know what the
#	NT_07959[0-7] items are, but there are no definitions for them
#	in the agp files and no sequence in any fa.gz file.
#	Fixup the names for the NG_ items, and change chrom MT to be M

# get the seq_contig.md file Craig just made for us on 11/28/05.
cd /cluster/store11/gs.19/ncbi
wget --timestamp ftp://ftp-private.ncbi.nih.gov/build_36/seq_contig.md

# remove Celera and Toronto entries
# and replace chrom number for those haplotypes

ssh hgwdev
cd /cluster/store11/gs.19/build36
egrep -v "Celera|NT_07959[0-7]" ../ncbi/seq_contig.md |grep -v CRA_TCA >seq_contig0.tab

hgsql hg18 -e 'drop table seq_contig0'
hgsql hg18 <~/src/hg/lib/seq_contig0.sql
hgsql hg18 -e 'load data local infile "seq_contig0.tab" into table seq_contig0'

#     fix seq_contig and
#	get the randoms sorted in proper order.  The createNcbiLifts
#	does not work correctly if the randoms are not grouped together
#	by chromosome
fixMd0 hg18 |sed -e "s/6_qbl_hap1/6_qbl_hap2/"| sed -e "s/MT/M/" | grep -v "|" >seq_contig1.tab

hgsql hg18 -e 'drop table seq_contig1'
hgsql hg18 <~/src/hg/lib/seq_contig1.sql
hgsql hg18 -e 'load data local infile "seq_contig1.tab" into table seq_contig1'
fixMd hg18 seq_contig1 >seq_contig.md

#	This pulls out all the randoms and groups them within the
#	same chrom but leaving them in the same order as they orginally
#	were  (warning this is BASH code ...)
bash
    grep "|" seq_contig0.tab | awk -F"|" '{print $1}' | \
        awk '{print $2}' | sort -n -u | while read CHR
do
        grep "[^0-9]${CHR}|" seq_contig0.tab
done >> seq_contig.md
exit

hgsql hg18 -e 'drop table seq_contig'
hgsql hg18 <~/src/hg/lib/seq_contig.sql
hgsql hg18 -e 'load data local infile "seq_contig.md" into table seq_contig'

# FYI: agp file format documented at:
#	http://www.ncbi.nlm.nih.gov/Genbank/WGS.agpformat.html# fixup a couple of names for our own purposes here
cd /cluster/store11/gs.19/agp
ln -s ../ncbi/chr*.agp ../ncbi/chr*.fa.gz .

sed -e "s#MT/NC_001807#NC_001807#" ../ncbi/chrMT.agp > chrM.agp

cat ../ncbi/c22_H2.agp > chr22_h2_hap1.agp
cat ../ncbi/c5_H2.agp  > chr5_h2_hap1.agp
cat ../ncbi/c6_COX.agp > chr6_cox_hap1.agp
cat ../ncbi/c6_QBL.agp > chr6_qbl_hap2.agp

cp -p ../ncbi/c22_H2.fa.gz chr22_h2_hap1.fa.gz
cp -p ../ncbi/c5_H2.fa.gz  chr5_h2_hap1.fa.gz
cp -p ../ncbi/c6_COX.fa.gz chr6_cox_hap1.fa.gz
cp -p ../ncbi/c6_QBL.fa.gz chr6_qbl_hap2.fa.gz

mkdir sav
cp -p *hap*.agp sav

# fix hap type agp files that have multiple contigs.
 
fixAgp hg18 sav/chr5_h2_hap1.agp chr5_h2_hap1.agp
fixAgp hg18 sav/chr6_qbl_hap2.agp chr6_qbl_hap2.agp

# PLEASE NOTE THAT THESE TWO CORRECTED .agp FILES ABOVE ARE USED LATER, 
# NOT BY THE NEXT STEP IMMEDIATELY.
    
#  Put all the agp files together into one.

#	The chrM sequence now has its own agp, remove it from
#	ref_placed.agp
# sed -e "/^NC_001807/d" ../ncbi/ref_placed.agp > ref_placed.agp

# PLEASE NOTE THAT THE ORIGINAL NCBI .agp FILES FOR THOSE
# SPECIAL HAP TYPE SEQUENCES ARE USED, NOT THE CORRECTED ONES.

cd /cluster/store11/gs.19/build36
cat ../ncbi/ref_placed.agp ../agp/chrM.agp ../ncbi/ref_unplaced.agp \
../ncbi/c22_H2.agp \
../ncbi/c5_H2.agp \
../ncbi/c6_COX.agp \
../ncbi/c6_QBL.agp \
../ncbi/PAR.agp > ncbi_build36.agp

# cat ../ncbi/ref_placed.agp ../agp/chrM.agp ../ncbi/ref_unplaced.agp \
# ../agp/chr22_h2_hap1.agp ../agp/chr5_h2_hap1.agp \
# ../agp/chr6_cox_hap1.agp ../agp/chr6_qbl_hap2.agp \
# ../ncbi/PAR.agp > ncbi_build36.agp

    zcat ../ncbi/chrMT.fa.gz | \
	sed -e "s/gi|17981852|ref|NC_001807.4/ref|NC_001807/" | \
	gzip > chrM.fa.gz

#	and into ffa
    cd /cluster/store11/gs.19/ffa
# NO LONGER TRUE FOR GS19!
# There is a single bogus line at the end of ref_placed.fa.gz
#	declaring the NC_001807 MT sequence, this was later replaced by
#	chrMT.fa.gz, so remove that one line:
    zcat ../ncbi/ref_placed.fa.gz | sed -e "/^>ref|NC_001807/d" | \
    gzip > ref_placed.fa.gz
#	(That's a 40 minute job)

#	sequence.inf is usually here, symlink it
#ln -s ../ncbi/sequence.inf
    ln -s ../ncbi/chromosome_extents.inf
#	put all the fa.gz files together in one big fa.gz
#   time zcat ref_placed.fa.gz ../agp/chrM.fa.gz ../ncbi/ref_unplaced.fa.gz \
time zcat ../ncbi/ref_placed.fa.gz ../ncbi/ref_unplaced.fa.gz \
../agp/*hap?.fa.gz ../ncbi/PAR.fa.gz | gzip \
> ncbi_build36.fa.gz

#	Make a listing of all the fasta record headers, just FYI:
    cd /cluster/store11/gs.19
    zcat ffa/ncbi_build36.fa.gz | grep "^>" > ncbi.fa.headers

# Sanity check, checkYbr was updated to handle the NC_ identifier
cd /cluster/store11/gs.19/build36
zcat ../ffa/ncbi_build36.fa.gz | $HOME/bin/i386/checkYbr ncbi_build36.agp stdin seq_contig.md >check.seq_contig
#	result should be clean:
cat check.seq_contig
# Read 378 contigs from ncbi_build36.agp
# Verifying sequence sizes in stdin
# 0 problems detected

# Convert fa files into UCSC style fa files and place in "contigs"
# directory inside the gs.19/build36 directory 
#	(a check that can be done here is make a list of the contigs
#	in this ./contigs directory before and compare it with the
#	list of distributed contigs created after they have been
#	disbursed.)
#	faNcbiToUcsc was fixed to handle the NC_ identifier

cd /cluster/store11/gs.19/build36

# We've been through this often

# mv contigs contigs.0
zcat ../ffa/ncbi_build36.fa.gz | $HOME/bin/i386/faNcbiToUcsc \
-split -ntLast stdin contigs

#	If you want to compare anything to previous work, check now, then:
#     rm -fr contigs.0

# Determine the chromosome sizes from agps
#	Watch carefully how chrY gets constructed.  I'm not sure
#	this chrom_sizes represents the whole length of chrY with
#	the PAR added.  We will see about that.
#	Script updated to handle new chrom names:
#	my @chroms = (1 .. 22, 'X', 'Y', 'M', '6_hla_hap1', '6_hla_hap2');

cd /cluster/store11/gs.19/build36
/cluster/bin/scripts/getChromSizes ../agp

#	Create chrom.lst list for use in foreach() loops
awk '{print $1}' chrom_sizes | sed -e "s/chr//" > chrom.lst

# Create lift files (this will create chromosome directory structure) and
#	inserts file
  
/cluster/bin/scripts/createNcbiLifts -s chrom_sizes seq_contig.md .

# Create contig agp files (will create contig directory structure)
	
/cluster/bin/scripts/createNcbiCtgAgp seq_contig.md ncbi_build36.agp .

# Create chromsome random agp files.

/cluster/bin/scripts/createNcbiChrAgp -randomonly .

# Copy the original chrN.agp files from the gs.19/agp directory 
#    into each of the chromosome directories since they contain better 
#    gap information. Delete the comments at top from these.
cd /cluster/store11/gs.19/build36
foreach c ( `cat chrom.lst` )
	sed -e "/^#.*/d" ../agp/chr${c}.agp > ./${c}/chr${c}.agp
end
#	chrM needs a name fixup
sed -e "s#NC_001807#chrM#" ../agp/chrM.agp > M/chrM.agp

# Distribute contig .fa to appropriate directory (assumes all files
# are in "contigs" directory).

# Create inserts file from agp and lift files (new - added by Terry, 2004-07-12)
/cluster/bin/scripts/createInserts /cluster/data/hg18 > /cluster/data/hg18/inserts

# create global data link for everyone.  No more home directory
# links required.
ln -s /cluster/store11/gs.19/build36 /cluster/data/hg18
cd /cluster/data/hg18
/cluster/bin/scripts/distNcbiCtgFa contigs .
#	Verify that everything was moved properly, the contigs directory
#	should be empty:
ls contigs
#	Nothing there, then remove it
rmdir  contigs

#	Make a list of the contigs for use later
    rm contig.lst
    touch contig.lst
    foreach chrom ( `cat chrom.lst` )
	foreach c ( $chrom/N{C,G,T}_?????? )
	    set contig = $c:t
	    echo "${chrom}/${contig}/${contig}.fa" >> contig.lst
	end
    end
#   For later comparisons, this is how many contigs we have:
    wc -l contig.lst
# 378 contig.lst 

#	Note 2004-06-30 - there are some clone numbers left in some of
#	the NCBI files that are incorrect.  Due to version number
#	changes, more than one version is listed.  Namely for accession
#	numbers: AC004491 AC004921 AC004983 AC005088 AC006014 AC099654
#	The AGP files are correct, the sequence.inf file lists these
#	twice: AC004491.1 AC004491.2
#	AC004921.1 AC004921.2 AC004983.2 AC004983.3
#	AC005088.2 AC005088.3 AC006014.2 AC006014.3
#	AC099654.4 AC099654.5

# for hg18, NCBI did not provide the seq.inf file.

# FILES ARE NOW READY FOR REPEAT MASKING - start that process as
#	other steps here can proceed in parallel.

#	Previous practice used to copy everything over for jkStuff from a
#	previous build.  Rather than do that, pick up whatever is needed
#	at the time it is needed and verify that it is going to do what
#	you expect.

    cd /cluster/data/hg18
    mkdir jkStuff

# Create the contig.gl files - XXX - NCBI doesn't deliver
# contig_overlaps.agp - 2004-06-18 - this is beginning to come
# together and there is now a contig_overlaps.agp file

#	This is properly done below with a combination of psLayout
#	alignments to create the contig_overlaps.agp file
# /cluster/bin/i386/agpToGl contig_overlaps.agp . -md=seq_contig.md
# Create chromosome gl files
# jkStuff/liftGl.csh contig.gl

# CREATING DATABASE  (DONE - 2005-11-30 - Fan)

    ssh hgwdev

# Make sure there is at least 5 gig free on hgwdev:/var/lib/mysql
    df -h /var/lib/mysql
# Filesystem            Size  Used Avail Use% Mounted on
# /dev/sdc1             1.8T  1.3T  356G  79% /var/lib/mysql

# Create the database.
    hgsql -e 'create database hg18' mysql
# Copy over grp table (for track grouping) from another database:
    hgsql -e "create table grp (PRIMARY KEY(NAME)) select * from hg17.grp" hg18

# The DB updates to grp below are not needed since we copied from hg17.
# ENCODE groups
# Added 2005-08016 kate
    echo 'UPDATE grp SET priority=7 WHERE name="varRep"'| hgsql hg18
    echo 'UPDATE grp SET priority=8 WHERE name="encode"'| hgsql hg18
    echo 'INSERT INTO grp (name, label, priority) VALUES ("encodeGenes", "ENCODE Regions and Genes", 8.1)' | hgsql hg18
    echo 'INSERT INTO grp (name, label, priority) VALUES ("encodeTxLevels", "ENCODE Transcript Levels", 8.2)' | hgsql hg18
    echo 'INSERT INTO grp (name, label, priority) VALUES ("encodeChip", "ENCODE Chromatin Immunoprecipitation", 8.3)' | hgsql hg18
    echo 'INSERT INTO grp (name, label, priority) VALUES ("encodeChrom", "ENCODE Chromosome, Chromatin and DNA Structure", 8.4)' | hgsql hg18
    echo 'INSERT INTO grp (name, label, priority) VALUES ("encodeCompGeno", "ENCODE Comparative Genomics", 8.5)' | hgsql hg18
    echo 'INSERT INTO grp (name, label, priority) VALUES ("encodeVariation", "ENCODE Variation", 8.6)' | hgsql hg18
    echo 'INSERT INTO grp (name, label, priority) VALUES ("encodeAnalysis", "ENCODE Analysis", 8.9)' | hgsql hg18
    
# MAKE CHROMINFO TABLE WITH (TEMPORARILY UNMASKED) NIBS
#	(DONE - 2005-12-02 - Fan)

# Make nib/, unmasked until RepeatMasker and TRF steps are done.
# Do this now so that the chromInfo table will exist and thus the
#	trackDb tables can be built in the next step.
#	These unmasked nibs will be replaced by the masked nibs after
#	repeat mask and trf are done.
    ssh kkstore02
    cd /cluster/data/hg18
    cp /cluster/data/hg17/jkStuff/chrFa.csh jkStuff -p

# Make chr*.fa from contig .fa
#  Copied chrFa.sh from hg17/jkStuff, renamed it to chrFa.csh
bash
time ./jkStuff/chrFa.csh
# real    2m34.406s
# user    1m17.405s
# sys     0m16.730s
exit

    mkdir nib
    foreach c (`cat chrom.lst`)
      foreach f ($c/chr${c}{,_random}.fa)
        if (-e $f) then
          echo "nibbing $f"
          /cluster/bin/i386/faToNib $f nib/$f:t:r.nib
        endif
      end
    end

# Make symbolic links from /gbdb/hg18/nib to the real nibs.
    ssh hgwdev
    mkdir -p /gbdb/hg18/nib
    ln -s /cluster/data/hg18/nib/chr*.nib /gbdb/hg18/nib
# Load /gbdb/hg18/nib paths into database and save size info.
    cd /cluster/data/hg18
    hgsql hg18  < $HOME/kent/src/hg/lib/chromInfo.sql
    hgNibSeq -preMadeNib hg18 /gbdb/hg18/nib */chr*.fa
    hgsql -N -e "select chrom,size from chromInfo order by chrom" hg18 \
	> chrom.sizes
# You can compare this chrom.sizes with the previously created
# chrom_sizes.  Should be no difference
    sort chrom_sizes > s0
    sort chrom.sizes | grep -v random > s1
    diff s0 s1
    rm s0 s1

# MAKE HGCENTRALTEST ENTRY AND TRACKDB TABLE (DONE - 2005-12-06 - Fan)
#	dbDb orderKey updated 2005-12-06 - Fan
    ssh hgwdev
#	reset dbDb orderKey - these have never been ordered properly
#	before, this will get them on the program.
    hgsql -e 'update dbDb set orderKey=11 where name = "hg17";' \
	-h genome-testdb hgcentraltest
    hgsql -e 'update dbDb set orderKey=12 where name = "hg16";' \
	-h genome-testdb hgcentraltest
    hgsql -e 'update dbDb set orderKey=13 where name = "hg15";' \
	-h genome-testdb hgcentraltest
    hgsql -e 'update dbDb set orderKey=14 where name = "hg13";' \
	-h genome-testdb hgcentraltest

# Enter hg18 into hgcentraltest.dbDb so test browser knows about it:
    hgsql -e 'INSERT INTO dbDb (name, description, nibPath, organism, \
	defaultPos, active, orderKey, genome, scientificName, \
	htmlPath, hgNearOk, hgPbOk, sourceName) \
	VALUES("hg18", "Feb. 2006", "/gbdb/hg18/nib", "Human", \
	"chr4:56214201-56291736", 1, 10, "Human", "Homo sapiens", \
	"/gbdb/hg18/html/description.html", 0, 0, "NCBI Build 36.1");' \
	-h genome-testdb hgcentraltest
# Make trackDb table so browser knows what tracks to expect:
    cd ~/kent/src/hg/makeDb/trackDb
    cvs up -d -P .
# Edit the makefile to add hg18 in all the right places and do
    make update
    make alpha
    cvs commit makefile

# MAKE LIFTALL.LFT, NCBI.LFT (DONE - 2005-12-07 Fan)
    cd /cluster/data/hg18
    mkdir -p jkStuff
    cat */lift/{ordered,random}.lft > jkStuff/liftAll.lft
# Create jkStuff/ncbi.lft for lifting stuff built with the NCBI assembly.
# Note: this ncbi.lift will not lift floating contigs to chr_random coords,
# but it will show the strand orientation of the floating contigs 
# (grep for '|').
#   mdToNcbiLift seq_contig.md jkStuff/ncbi.lft 
#	XXXX - appears to be unused, not done - Hiram

# REPEAT MASKING (DONE - 2005-12-09 - Fan)

#	Record the RM version here:
ls -l /cluster/bluearc/RepeatMasker
# lrwxrwxrwx    1 angie    protein        18 Nov  3 10:40 /cluster/bluearc/RepeatMasker -> RepeatMasker051101

/cluster/bluearc/RepeatMasker/RepeatMasker | head -1
# RepeatMasker version development-$Id: makeHg18.doc,v 1.30 2006/02/01 23:55:26 fanhsu Exp $

#	RepBase Update 8.12, RM database version 20040130  ??? don't know where this came from. Fan???
#	as this changes over time and there is no record in the results

# Split contigs, run RepeatMasker, lift results
#	This split takes a few minutes
    ssh kkstore02
    cd /cluster/data/hg18
    foreach chrom ( `cat chrom.lst` )
	foreach c ( $chrom/N{C,G,T}_?????? )
	    set contig = $c:t
	    echo "splitting ${chrom}/${contig}/${contig}.fa"
	    faSplit size ${chrom}/${contig}/$contig.fa 500000 \
		${chrom}/${contig}/${contig}_ \
		-lift=${chrom}/${contig}/$contig.lft -maxN=500000
	end
    end

#- Make the run directory and job list:
    cd /cluster/data/hg18
    mkdir -p jkStuff
#  According to RepeatMasker help file, no arguments are required to
#	specify species because its default is set for primate (human)
#  This run script saves the .tbl file to be sent to Arian.  He uses
# those for his analysis.  Sometimes he needs the .cat and .align files for
# checking problems.  Krish needs the .align files, they are large.

    cat << '_EOF_' > jkStuff/RMHuman
#!/bin/csh -fe

cd $1
pushd .
/bin/mkdir -p /tmp/hg18/$2
/bin/cp $2 /tmp/hg18/$2/
cd /tmp/hg18/$2
/cluster/bluearc/RepeatMasker/RepeatMasker -ali -s $2
popd
/bin/cp /tmp/hg18/$2/$2.out ./
if (-e /tmp/hg18/$2/$2.align) /bin/cp /tmp/hg18/$2/$2.align ./
if (-e /tmp/hg18/$2/$2.tbl) /bin/cp /tmp/hg18/$2/$2.tbl ./
# if (-e /tmp/hg18/$2/$2.cat) /bin/cp /tmp/hg18/$2/$2.cat ./
/bin/rm -fr /tmp/hg18/$2/*
/bin/rmdir --ignore-fail-on-non-empty /tmp/hg18/$2
/bin/rmdir --ignore-fail-on-non-empty /tmp/hg18
'_EOF_'
# << this line makes emacs coloring happy
    chmod +x jkStuff/RMHuman

    ssh kkstore02
    cd /cluster/data/hg18
    mkdir RMRun
    rm -f RMRun/RMJobs
    touch RMRun/RMJobs
    foreach d ( `cat chrom.lst` )
     foreach c ( ${d}/N{C,G,T}_*/N{C,G,T}_*_*.fa )
        set f = $c:t
        set cc = $c:h
        set contig = $cc:t
        echo /cluster/store11/gs.19/build36/jkStuff/RMHuman \
   		/cluster/store11/gs.19/build36/${d}/${contig} $f \
   '{'check out line+ /cluster/store11/gs.19/build36/${d}/${contig}/$f.out'}' \
          >> RMRun/RMJobs
      end
    end

# We have 5990 jobs in RMJobs:
    wc RMRun/RMJobs
#	5990   41930 1127992 RMRun/RMJobs

#- Do the run
    ssh pk
    cd /cluster/data/hg18/RMRun
    para create RMJobs
    para try, para check, para check, para push, para check,...

#- While that is running, you can run TRF (simpleRepeat) on the small
# cluster.  See SIMPLE REPEAT section below
# Completed: 5990 of 5990 jobs
# CPU time in finished jobs:   30661460s  511024.34m  8517.07h  354.88d  0.972 y
# IO & Wait Time:                 38038s     633.96m    10.57h    0.44d  0.001 y
# Average job time:                5125s      85.42m     1.42h    0.06d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            6693s     111.55m     1.86h    0.08d
# Submission to last job:         86532s    1442.20m    24.04h    1.00d

#	Lift up the split-contig .out's to contig-level .out's
#
#	If a mistake is made in the following it would be possible to
#	destroy all the RM output.  So, just to be paranoid, save all
#	the RM output in bluearc for the time being:
    ssh kkstore02

    cd /cluster/data/hg18
    mkdir /cluster/bluearc/hg18/RMOutput
    foreach c ( `cat chrom.lst` )
     foreach d ( ${c}/N{C,G,T}_* )
	set T = /cluster/bluearc/hg18/RMOutput/${d}
	mkdir -p ${T}
        cd ${d}
        set contig = $d:t
        cp -p ${contig}_?{,?,??}.fa.out ${T}
        cd ../..
	echo "${d} done"
     end
    end
#	Make sure we got them all:
#	(this doesn't work later since there are more *.fa.out files
#	after the lifting.  More explicitly to find just these:
#		find . -name "N?_*_*.fa.out" -print | wc -l
    find . -name "*.fa.out" -print | wc -l
#	5990
    find /cluster/bluearc/hg18/RMOutput -type f | wc -l
#	5990
#	same count

#	OK, now you can try this operation, do it in a script like this
#	and save the output of the script for a record of what happened.

    cat << '_EOF_' > jkStuff/liftRM.csh
#!/bin/csh -fe
foreach c ( `cat chrom.lst` )
 foreach d ( ${c}/N{C,G,T}_* )
    cd $d
    set contig = $d:t
    liftUp $contig.fa.out $contig.lft warn ${contig}_?{,?,??}.fa.out 
    cd ../..
 end
end
'_EOF_'
    chmod +x jkStuff/liftRM.csh
    mkdir scriptsOutput
    
    script lift.log
    bash
    time jkStuff/liftRM.csh > scriptsOutput/liftRM.1 2>&1
    exit
    exit

#	Check that they all were done:
    grep "fa.out" scriptsOutput/liftRM.1 | wc -l
#	5990
#	same count as above

#- Lift up RepeatMask .out files to chromosome coordinates via
# picked up jkStuff/liftOut2.sh from the hg17 build.  Renamed to
# liftOut2.csh, changed the line that does the chrom listing
    bash
    time ./jkStuff/liftOut2.csh > scriptsOutput/liftOut2 2>&1
# real    0m30.488s
# user    0m24.670s
# sys     0m2.797s
# seems much faster than hg17 ???
    
# hg17 numbers:
#	real    9m46.780s
#	user    1m18.900s
#	sys     7m33.990s

#- By this point, the database should have been created (above):
    ssh hgwdev
    cd /cluster/data/hg18
    bash
    time hgLoadOut hg18 ?/*.fa.out ??/*.fa.out *hap*/*.fa.out > \
	scriptsOutput/hgLoadOut 2>&1
# real    9m9.045s
# user    2m19.500s
# sys     0m24.440s

# errors during this load:  (there are always a couple of these)
# Strange perc. field -1.2 line 153851 of 2/chr2.fa.out
# Strange perc. field -10423.3 line 174747 of 3/chr3.fa.out
# Strange perc. field -5635.9 line 174747 of 3/chr3.fa.out
# Strange perc. field -259.3 line 174747 of 3/chr3.fa.out
# Strange perc. field -1.4 line 205545 of 4/chr4.fa.out
# Strange perc. field -0.1 line 167690 of 7/chr7.fa.out
# Strange perc. field -1331.2 line 198656 of 7/chr7.fa.out
# Strange perc. field -1460.4 line 198656 of 7/chr7.fa.out
# Strange perc. field -4.2 line 223183 of 7/chr7.fa.out
# Strange perc. field -3192.0 line 60424 of 8/chr8.fa.out
# Strange perc. field -423.4 line 60424 of 8/chr8.fa.out
# Strange perc. field -784.0 line 60424 of 8/chr8.fa.out
# Strange perc. field -0.1 line 52020 of X/chrX.fa.out
# Strange perc. field -4526.7 line 190254 of X/chrX.fa.out
# Strange perc. field -3757.2 line 190254 of X/chrX.fa.out
# Strange perc. field -597.2 line 190254 of X/chrX.fa.out
# Strange perc. field -13030.4 line 137624 of 16/chr16.fa.out
# Strange perc. field -1359.8 line 137624 of 16/chr16.fa.out
# Strange perc. field -2223.5 line 137624 of 16/chr16.fa.out
# Strange perc. field -1.3 line 11573 of 22/chr22.fa.out
# Strange perc. field -12.7 line 69873 of 22/chr22.fa.out


#	Verify we have similar results to previous assembly:
#	featureBits hg18 rmsk
# 	1406290513 bases of 3107677273 (45.252%) in intersection
#	featureBits -countGaps hg17 rmsk
#	1390952984 bases of 3095016460 (44.942%) in intersection
#	featureBits hg17 rmsk
#	1391378842 bases of 2867328468 (48.525%) in intersection
#	featureBits hg16 rmsk
#	1388770568 bases of 2865248791 (48.469%) in intersection
#	Now proceed to MASK SEQUENCE BOTH REPEATMASKER AND SIMPLE REPEAT/TRF
#	following the SIMPLE REPEAT sections below

# let Rachel know that RepeatMask is done.

# SIMPLE REPEAT [TRF] TRACK (DONE - 2005-12-07 - Fan)
#	Copy the contigs, first to the bluearc, then to /iscratch/i
    ssh kkstore02
    mkdir /cluster/bluearc/hg18
    mkdir /cluster/bluearc/hg18/contigs

    cd /cluster/data/hg18
    foreach ctg ( `cat contig.lst` )
	set c = $ctg:t
 	echo "$ctg > /cluster/bluearc/hg18/contigs/$c"
	cp -p $ctg /cluster/bluearc/hg18/contigs/$c
    end
#	Check how much is there:
#	du -hsc /cluster/bluearc/hg18/contigs
#	2.8G    /cluster/bluearc/hg18/contigs
    exit

# Distribute contigs to /iscratch/i
    ssh pk
    mkdir -p /san/sanvol1/scratch/hg18/unmaskedContigs
    cd /san/sanvol1/scratch/hg18/unmaskedContigs
    cp -p /cluster/bluearc/hg18/contigs/* .
    ls .

# Verify same amount made it there:
#	du -hsc /san/sanvol1/scratch/hg18/unmaskedContigs
#	2.9G    /san/sanvol1/scratch/hg18/unmaskedContigs
#	Then send them to the other 7 Iservers
#    /cluster/bin/iSync

#	Go to the small cluster for this business:
    ssh pk

    mkdir -p /cluster/data/hg18/bed/simpleRepeat
    cd /cluster/data/hg18/bed/simpleRepeat
    mkdir trf
    cat << '_EOF_' > runTrf
#!/bin/csh -fe
#
set path1 = $1
set inputFN = $1:t
set outpath = $2
set outputFN = $2:t
mkdir -p /tmp/$outputFN
cp $path1 /tmp/$outputFN
pushd .
cd /tmp/$outputFN
/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $inputFN /dev/null -bedAt=$outputFN -tempDir=/tmp
popd
rm -f $outpath
cp -p /tmp/$outputFN/$outputFN $outpath
rm -fr /tmp/$outputFN/*
rmdir --ignore-fail-on-non-empty /tmp/$outputFN
'_EOF_'
# << this line makes emacs coloring happy
    chmod +x runTrf

    cat << '_EOF_' > gsub
#LOOP
./runTrf {check in line+ $(path1)}  {check out line trf/$(root1).bed}
#ENDLOOP
'_EOF_'
# << this line makes emacs coloring happy

    ls -1S /san/sanvol1/scratch/hg18/unmaskedContigs/*.fa > genome.lst
    gensub2 genome.lst single gsub jobList
    para create jobList
    para try
    para check
    para push
    para check
# Completed: 378 of 378 jobs
# CPU time in finished jobs:      18956s     315.93m     5.27h    0.22d  0.001 y
# IO & Wait Time:                  2519s      41.98m     0.70h    0.03d  0.000 y
# Average job time:                  57s       0.95m     0.02h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            2345s      39.08m     0.65h    0.03d
# Submission to last job:          2427s      40.45m     0.67h    0.03d

bash
liftUp simpleRepeat.bed /cluster/data/hg18/jkStuff/liftAll.lft \
warn trf/*.bed  > lu.out 2>&1

# Load into the database:
    ssh hgwdev
    cd /cluster/data/hg18/bed/simpleRepeat
    /cluster/bin/i386/hgLoadBed hg18 simpleRepeat simpleRepeat.bed \
      -sqlTable=$HOME/kent/src/hg/lib/simpleRepeat.sql
#	Loaded 629076 elements of size 16
#	Compare with previous assembly
    featureBits hg18 simpleRepeat
# 56164158 bases of 3107677273 (1.807%) in intersection
# 	featureBits hg17 simpleRepeat
#	54952425 bases of 2866216770 (1.917%) in intersection
#     featureBits hg16 simpleRepeat
#	54320136 bases of 2865248791 (1.896%) in intersection
#	GAPS weren't in hg18 yet at this point, after gaps added:
#	featureBits hg18 simpleRepeat
#	54964044 bases of 2867328468 (1.917%) in intersection
#	featureBits -countGaps hg18 simpleRepeat
#	54964044 bases of 3096628158 (1.775%) in intersection

# PROCESS SIMPLE REPEATS INTO MASK (DONE - 2005-12-09 - Fan)
# After the simpleRepeats track has been built, make a filtered version 
# of the trf output: keep trf's with period <= 12:
    ssh kkstore02
    mkdir -p cd /cluster/data/hg18/bed/simpleRepeat
    cd /cluster/data/hg18/bed/simpleRepeat
    mkdir -p trfMask
    foreach f (trf/*.bed)
      awk '{if ($5 <= 12) print;}' $f > trfMask/$f:t
    end

#	The 4 lines below were left over from makeHg17.doc.
#	EXPERIMENT, at a filter of <= 12, we have coverage:
#	20904399 bases of 2867328468 (0.729%) in intersection
#	at a filter of <= 9, we have coverage:
#	19271270 bases of 2867328468 (0.672%) in intersection

# Lift up filtered trf output to chrom coords as well:
    cd /cluster/data/hg18
    mkdir bed/simpleRepeat/trfMaskChrom
    foreach c ( `cat chrom.lst` )
      if (-e $c/lift/ordered.lst) then
        perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
          $c/lift/ordered.lst > $c/lift/oTrf.lst
        liftUp bed/simpleRepeat/trfMaskChrom/chr$c.bed \
          jkStuff/liftAll.lft warn `cat $c/lift/oTrf.lst`
      endif
      if (-e $c/lift/random.lst) then
        perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
           $c/lift/random.lst > $c/lift/rTrf.lst
        liftUp bed/simpleRepeat/trfMaskChrom/chr${c}_random.bed \
          jkStuff/liftAll.lft warn `cat $c/lift/rTrf.lst`
      endif
    end

# MASK SEQUENCE BOTH REPEATMASKER AND SIMPLE REPEAT/TRF (DONE - 2005-12-09, Fan)
# This used to be done right after RepeatMasking.  Now, we mask with 
# TRF as well, so do this after the "PROCESS SIMPLE REPEATS" step above,
#	and after Repeat Masker is complete.
    ssh kkstore02
    cd /cluster/data/hg18

# Make chr*.fa from contig .fa
#  chrFa.csh was already copied from hg17/jkStuff
    bash    
    time ./jkStuff/chrFa.csh > scriptsOutput/chrFa.out 2>&1 &
# real    2m35.734s
# user    1m18.351s
# sys     0m16.596s
# much faster than hg17 numbers as shown below.  ???

# old hg17 numbers:
#	real    13m18.512s
#	user    9m1.670s
#	sys     1m7.290s

#- Soft-mask (lower-case) the contig and chr .fa's
    time ./jkStuff/makeFaMasked.csh > scriptsOutput/maFaMasked.out 2>&1
# real    8m47.289s
# user    3m45.698s
# sys     1m44.416s

#	old hg17 numbers:	
#	real    29m31.623s
#	user    13m49.700s
#	sys     5m58.750s

#- Make hard-masked .fa.masked files as well:
    time ./jkStuff/makeHardMasked.csh > scriptsOutput/maHardMasked.out 2>&1
# real    5m48.833s
# user    1m41.926s
# sys     0m52.084s

#- Create the bothMasksNib/ directory
    time ./jkStuff/makeNib.csh > scriptsOutput/maNib.out 2>&1
# real    2m23.280s
# user    1m6.462s
# sys     0m19.795s

# old hg17 numbers:
#	real    14m41.694s
#	user    6m28.000s
#	sys     1m42.500s

# Make symbolic links from /gbdb/hg18/nib to the real nibs.
    ssh hgwdev
    cd /cluster/store11/gs.19/build36
    mv nib nib.raw
    mv bothMasksNib nib
    rm /gbdb/hg18/nib/*.nib
    ln -s `pwd`/nib/* /gbdb/hg18/nib

# Load /gbdb/hg18/nib paths into database and save size info.

    cd /cluster/data/hg18
    hgNibSeq -preMadeNib hg18 /gbdb/hg18/nib */chr*.fa
# 3107677273 total bases

#	Should be the same size as before
    hgsql -N -e "select chrom,size from chromInfo order by chrom" hg18 \
	> chrom.sizes.masked
    diff chrom.sizes chrom.sizes.masked
#	should be no output at all, thus:
    rm chrom.sizes.masked

# Copy the masked contig fa to /scratch and /iscratch
#	And everything else we will need for blastz runs, etc ...
#	Best to do this sequence first to /cluster/bluearc/scratch,
#	which is going to be the source for the /scratch copy.
#	And then from there to the /iscratch
#	Make sure you are on the fileserver for the original source:
    ssh kkstore02
    mkdir -p /cluster/bluearc/scratch/hg/gs.19/build36
    cd /cluster/bluearc/scratch/hg/gs.19/build36

#	these copies take less than 2 minutes each
    mkdir bothMaskedNibs
    cp -p /cluster/data/hg18/nib/*.nib ./bothMaskedNibs
    mkdir maskedContigs
    foreach chrom ( `cat /cluster/data/hg18/chrom.lst` )
	cp -p /cluster/data/hg18/${chrom}/N{C,G,T}_*/N{C,G,T}_??????.fa \
		./maskedContigs
	echo "done ${chrom}"
    end
#	make sure you have them all:
    ls maskedContigs | wc -l
#	378
    wc -l /cluster/data/hg18/contig.lst
#	378
    mkdir rmsk
    foreach chrom ( `cat /cluster/data/hg18/chrom.lst` )
	cp -p /cluster/data/hg18/${chrom}/*.out ./rmsk
	echo "done ${chrom}"
    end

#	Now, go to the destination for /iscratch and copy from the
#	bluearc
    ssh kkr1u00
    mkdir -p /iscratch/i/gs.19/build36
    cd /iscratch/i/gs.19/build36
#	This takes about 5 minutes
    rsync -arlv /cluster/bluearc/scratch/hg/gs.19/build36/ .
    
    bash
    time /cluster/bin/iSync
#	real    7m27.649s

# request rsync of /cluster/bluearc/scratch to the KiloKluster /scratch

# Ask sysadmin to bring up BLAT server.

# update central dbDb table to add the new blat server entry

    echo 'INSERT INTO blatServers (db, host, port, isTrans) \
                VALUES ("hg18", "blat19", "17778", "1"); \
          INSERT INTO blatServers (db, host, port, isTrans) \
                VALUES ("hg18", "blat19", "17779", "0");' \
    | hgsql -h genome-testdb hgcentraltest

# LOAD ctgPos table - Contig position track 
#	After fixing up hgCtgPos to accept the -chromLst argument, simply:
    cd /cluster/data/hg18
    hgCtgPos -chromLst=chrom.lst hg18 .

# GOLD AND GAP TRACKS (DONE - 2005-12-10 - Fan)

    ssh hgwdev
    cd /cluster/data/hg18
    hgGoldGapGl -noGl -chromLst=chrom.lst hg18 /cluster/data/hg18 .
    #	Disappointing to see this create so many tables ...
    #	_gap and _gold for each chrom

# contig.gl ... section skipped for the time being.  (Fan 2005-12-13).

#############################################################################
# GC5BASE (DONE - 2005-12-13 - Fan)
    ssh kkstore02
    mkdir -p /cluster/data/hg18/bed/gc5Base
    cd /cluster/data/hg18/bed/gc5Base
    hgGcPercent -wigOut -doGaps -file=stdout -win=5 hg18 \
        /cluster/data/hg18/nib | wigEncode stdin gc5Base.wig gc5Base.wib

    #   runs for about 17 minutes

    #   load database
    ssh hgwdev
    cd /cluster/data/hg18/bed/gc5Base
    mkdir /gbdb/hg18/wib
    ln -s `pwd`/gc5Base.wib /gbdb/hg18/wib
    hgLoadWiggle -pathPrefix=/gbdb/hg18/wib hg18 gc5Base gc5Base.wig

    #   verify index is correct:
    hgsql hg18 -e "show index from gc5Base;"
    #   should see good numbers in Cardinality column

#########################################################################
# GENBANK auto update (DONE 2005-12-13 Fan)
    # align with revised genbank process. drop xeno ESTs.
    cd ~/kent/src/hg/makeDb/genbank
    cvs update -d etc
    # edit etc/genbank.conf to add hg18

# hg18
hg18.serverGenome = /cluster/data/hg18/nib/chr*.nib
hg18.clusterGenome = /scratch/hg/gs.18/build36/bothMaskedNibs/chr*.nib
hg18.ooc = /scratch/hg/h/11.ooc
hg18.lift = /cluster/store11/gs.19/build36/jkStuff/liftAll.lft
hg18.refseq.mrna.native.pslCDnaFilter  = ${finished.refseq.mrna.native.pslCDnaFilter}
hg18.refseq.mrna.xeno.pslCDnaFilter    = ${finished.refseq.mrna.xeno.pslCDnaFilter}
hg18.genbank.mrna.native.pslCDnaFilter = ${finished.genbank.mrna.native.pslCDnaFilter}
hg18.genbank.mrna.xeno.pslCDnaFilter   = ${finished.genbank.mrna.xeno.pslCDnaFilter}
#hg18.genbank.est.native.pslCDnaFilter  = ${finished.genbank.est.native.pslCDnaFilter}
#hg18.genbank.est.xeno.pslCDnaFilter    = ${finished.genbank.est.xeno.pslCDnaFilter}
#hg18.genbank.est.xeno.load = yes
hg18.refseq.mrna.xeno.load  = yes
hg18.refseq.mrna.xeno.loadDesc = yes
hg18.mgcTables.default = full
hg18.mgcTables.mgc = all
hg18.downloadDir = hg18

    # update /cluster/data/genbank/
    make etc-update

    ssh kkstore02
    cd /cluster/data/genbank
    nice bin/gbAlignStep -initial hg18 &

    # load database when finished
    ssh hgwdev
    cd /cluster/data/genbank
    nice ./bin/gbDbLoadStep -drop -initialLoad  hg18&

# PRODUCING GENSCAN PREDICTIONS (DONE - 2005-12-16 - Fan)
    
    ssh hgwdev
    mkdir /cluster/data/hg18/bed/genscan
    cd /cluster/data/hg18/bed/genscan
    cvs co hg3rdParty/genscanlinux

    ssh kkstore02
    cd /cluster/data/hg18/bed/genscan
    # Make 3 subdirectories for genscan to put their output files in
    mkdir gtf pep subopt
    # Generate a list file, genome.list, of all the contigs
    # *that do not have pure Ns* (due to heterochromatin, unsequencable 
    # stuff) which would cause genscan to run forever.
    rm -f genome.list
    bash
    for f in `cat /cluster/data/hg18/contig.lst`
    do
      egrep '[ACGT]' /cluster/data/hg18/$f.masked > /dev/null
	if [ $? = 0 ]; then
	    echo /cluster/data/hg18/$f.masked >> genome.list
	fi
    done
    # exit your bash shell if you are [t]csh ...
    #	This egrep matched all the contigs in hg18.  I guess none of
    #	them are complete Ns* at this point.

    # Log into kki (not kk !).  kki is the driver node for the small
    # cluster (kkr2u00 -kkr8u00. Genscan has problem running on the
    # big cluster, due to limitation of memory and swap space on each
    # processing node).
    ssh kki
    cd /cluster/data/hg18/bed/genscan
    # Create template file, gsub, for gensub2.  For example (3-line file):
    cat << '_EOF_' > gsub
#LOOP
/cluster/bin/x86_64/gsBig {check in line+ $(path1)} {check out line gtf/$(root1).gtf} -trans={check out line pep/$(root1).pep} -subopt={check out line subopt/$(root1).bed} -exe=hg3rdParty/genscanlinux/genscan -par=hg3rdParty/genscanlinux/HumanIso.smat -tmp=/tmp -window=2400000
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
    gensub2 genome.list single gsub jobList
    para create jobList
    para try
    para check
    para push ... etc ...
# Completed: 377 of 378 jobs
# Crashed: 1 jobs
# CPU time in finished jobs:      78976s    1316.27m    21.94h    0.91d  0.003 y
# IO & Wait Time:                  4961s      82.68m     1.38h    0.06d  0.000 y
# Average job time:                 223s       3.71m     0.06h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            3491s      58.18m     0.97h    0.04d
# Submission to last job:          7541s     125.68m     2.09h    0.09d

    #	Running the single failed job on kolossus with a smaller window:

ssh kkr7u00.kilokluster.ucsc.edu
/cluster/bin/x86_64/gsBig /cluster/data/hg18/5/NT_006576/NT_006576.fa.masked \
        gtf/NT_006576.fa.gtf -trans=pep/NT_006576.fa.pep \
        -subopt=subopt/NT_006576.fa.bed -exe=hg3rdParty/genscanlinux/genscan \
        -par=hg3rdParty/genscanlinux/HumanIso.smat -tmp=/tmp -window=2000000

    # If there were out-of-memory problems (run "para problems"), then 
    # re-run those jobs by hand but change the -window arg from 2400000
    # something lower.  In build33, this was 22/NT_011519
    #  In build34 there were NO failures !

    # Convert these to chromosome level files as so:     
    ssh kkstore02
    cd /cluster/data/hg18/bed/genscan
    $HOME/bin/i386/liftUp genscan.gtf ../../jkStuff/liftAll.lft warn gtf/N*.gtf
    $HOME/bin/i386/liftUp genscanSubopt.bed ../../jkStuff/liftAll.lft \
	warn subopt/N*.bed
    cat pep/*.pep > genscan.pep

    # Load into the database as so:
    ssh hgwdev
    cd /cluster/data/hg18/bed/genscan
    ldHgGene hg18 genscan genscan.gtf
# Reading genscan.gtf
# Read 43122 transcripts in 329799 lines in 1 files
# 43122 groups 49 seqs 1 sources 1 feature types
# 43122 gene predictions

    hgPepPred hg18 generic genscanPep genscan.pep
    #	Processing genscan.pep
    hgLoadBed hg18 genscanSubopt genscanSubopt.bed
    # Reading genscanSubopt.bed
    # Loaded 514065 elements of size 6
    #	Sorted
    #	Creating table definition for 
    #	Saving bed.tab
    #	Loading hg18

    # featureBits hg18 genscan
    # 56039161 bases of 2881515245 (1.945%) in intersection 
    #	featureBits hg17 genscan
    #	55323340 bases of 2866216770 (1.930%) in intersection
    #	featureBits hg16 genscan
    #	55333689 bases of 2865248791 (1.931%) in intersection

    #	featureBits hg18 genscanSubopt
    # 55685959 bases of 2881515245 (1.933%) in intersection 
    # featureBits hg17 genscanSubopt
    #	55986178 bases of 2866216770 (1.953%) in intersection
    #	featureBits hg16 genscanSubopt
    #	56082952 bases of 2865248791 (1.957%) in intersection

    #	Should be zero intersection with rmsk
    #	featureBits -chrom=chr1 hg18 genscan rmsk

# CREATE 2 BIT FILE (DONE 12/20/05, Fan)
   
   ssh kkstore02
   cd /cluster/data/hg18
   faToTwoBit */chr*.fa hg18.2bit
	  
# BLASTZ, CHAIN, NET, MAFNET, AXTNET AND ALIGNMENT DOWNLOADS FOR
# ZEBRAFISH (danRer3) (DONE, 2005-12-23, hartera)
    ssh pk
    # Blastz uses lineage-specific repeats. There are none for mouse
    # and fish so use all repeats for each species as lineage-specific.
    mkdir -p /san/sanvol1/scratch/hg18/linSpecRep.notInOthers
   foreach f (/cluster/bluearc/hg18/linSpecRep/notInOthers/chr*.out.spec)
     cp -p $f /san/sanvol1/scratch/hg18/linSpecRep.notInOthers/
   end

    # get only lineage specific repeats for chr1-25 and chrM
    mkdir -p /san/sanvol1/scratch/danRer3/linSpecRep.notInOthers
    foreach f (/cluster/data/danRer3/*/chr[0-9M]*.fa.out)
      cp -p $f \
          /san/sanvol1/scratch/danRer3/linSpecRep.notInOthers/$f:t:r:r.out.spec
    end
    # make a nib dir without random chroms
    mkdir -p /san/sanvol1/scratch/hg18/chromNib
    cp -p /cluster/data/hg18/nib/chr*.nib \
       /san/sanvol1/scratch/hg18/chromNib
    rm -r chr*_random.nib
    # make a nib dir that is also just chr1-25 and chrM
    mkdir -p /san/sanvol1/scratch/danRer3/chromNib
    cp /cluster/data/danRer3/nib/chr[0-9M]*.nib \
       /san/sanvol1/scratch/danRer3/chromNib
    
    ssh kkstore02
    mkdir /cluster/data/hg18/bed/blastz.danRer3.2005-12-17
    cd /cluster/data/hg18/bed
    ln -s blastz.danRer3.2005-12-17 blastz.danRer3
    # Three separate runs done to create chains. Runs 1 and 3 could be 
    # combined into one.
    # RUN 1: hg18 chroms (no randoms) vs danRer3 chr1-25 and chrM using 
    # lineage-specific repeats.
    ssh hgwdev
    cd /cluster/data/hg18/bed/blastz.danRer3
    # make run dir
    mkdir -p /san/sanvol1/scratch/hg18/blastzDanRer3/chromsRun
    ln -s /san/sanvol1/scratch/hg18/blastzDanRer3/chromsRun
    # make out dir
    mkdir -p /san/sanvol1/scratch/hg18/blastzDanRer3/chromsOut
    ln -s /san/sanvol1/scratch/hg18/blastzDanRer3/chromsOut
    cd chromsRun 
    # use parameters as for hg17 vs danRer2 - see makeHg17.doc
    cat << '_EOF_' > DEF
# human (hg18) vs zebrafish (danRer3)
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin

ALIGN=blastz-run
BLASTZ=blastz.v7.x86_64

# Reuse parameters from hg16-fr1, danRer-hg17 and mm5-danRer
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q
BLASTZ_ABRIDGE_REPEATS=1

# TARGET: Human (hg18)
SEQ1_DIR=/san/sanvol1/scratch/hg18/chromNib
SEQ1_RMSK=
SEQ1_FLAG=
SEQ1_SMSK=/san/sanvol1/scratch/hg18/linSpecRep.notInOthers
SEQ1_LIMIT=30
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY: Zebrafish (danRer3)
# just chroms 1-25 and chrM
SEQ2_DIR=/san/sanvol1/scratch/danRer3/chromNib
SEQ2_RMSK=
SEQ2_FLAG=
SEQ2_SMSK=/san/sanvol1/scratch/danRer3/linSpecRep.notInOthers
SEQ2_LIMIT=30
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/san/sanvol1/scratch/hg18/blastzDanRer3/chromsRun

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1chroms.len
SEQ2_LEN=$BASE/S2chroms.len
TMPDIR=/scratch/tmp
'_EOF_'
    # << this line keeps emacs coloring happy
    chmod +x DEF

    grep -v random /cluster/data/hg18/chrom.sizes > S1chroms.len
    grep -v chrUn /cluster/data/danRer3/chrom.sizes \
            | grep -v chrNA > S2chroms.len
    # do blastz and create chains for danRer3 chr1-25 and chrM using 
    # all repeats as lineage-specific repeats. 
    # chickenHumanTuned.gap scoring matrix is now used by axtChain if the 
    # linearGap parameter is set to "loose".
    nohup nice /cluster/bin/scripts/doBlastzChainNet.pl \
       -bigClusterHub=pk \
       -smallClusterHub=pk \
       -workhorse=pk \
       -blastzOutRoot /san/sanvol1/scratch/hg18/blastzDanRer3/chromsOut \ 
       -chainMinScore=5000 \
       -chainLinearGap loose \
       -stop chainRun `pwd`/DEF >& doChains.log &
    # Took 2 hours 45 minutes to run.
    # Then run the human hg18 chroms and randoms vs danRer3 chrUn and chrNA
    ssh hgwdev
    # get file of scaffolds for hg18 randoms. Use the Table Browser to
    # select sequence from the whole genome for the ctgPos table of contigs
    # restricting to chrom like "%_random" in the Free-form query box of 
    # the filter. hg18RandomContigs.fa
    cd /cluster/data/hg18/bed/blastz.danRer3
    # get the position and contig name from the ctgPos table
    hgsql -N -e 'select chrom, chromStart, chromEnd, contig from ctgPos \
          where chromlike "%_random";' hg18 > contigPosAndNames.txt
    ssh kkstore02
    cd /cluster/data/hg18/bed/blastz.danRer3
    # change header to just the position
    perl -pi.bak -e 's/>.+range=(chr[0-9XY]+_random:[0-9]+\-[0-9]+).+/>$1/' \
         hg18RandomContigs.fa
awk '{print "perl -pi.bak -e s/"$1":"$2+1"-"$3"/"$4"/ hg18RandomContigs.fa"}' \ 
       contigPosAndNames.txt > addContigNames.csh
    chmod +x addContigNames.csh
    # run script
    addContigNames.csh 
    ssh hgwdev
    # make a 2 bit file of the chroms and random scaffolds
    cd /cluster/data/hg18
    set dir=/san/sanvol1/scratch/hg18
    faToTwoBit [1-9]/chr[1-9].fa [12][0-9]/chr[12][0-9].fa M/chrM.fa \
               X/chrX.fa Y/chrY.fa *hap[12]/chr*.fa \
               /cluster/data/hg18/bed/blastz.danRer3/hg18RandomContigs.fa \
               $dir/chromsAndRandoms.2bit    
    twoBitInfo $dir/chromsAndRandoms.2bit $dir/chromsAndRandoms.len
    # make a 2 bit file for just the random scaffolds
    faToTwoBit /cluster/data/hg18/bed/blastz.danRer3/hg18RandomContigs.fa \
               $dir/randoms.2bit    
    twoBitInfo $dir/randoms.2bit $dir/randoms.len
    # make sure all the random chroms contigs are included - should be 88.
    # make a 2 bit file for all the chroms and random chroms, make sure to 
    # get the haplotype chrom sequences.
    faToTwoBit [1-9MXY]/chr*.fa [12][0-9]/chr*.fa *hap[12]/chr*.fa \
               $dir/hg18.2bit
    twoBitInfo $dir/hg18.2bit $dir/hg18Chroms.len
    twoBitInfo /san/sanvol1/scratch/danRer3/danRer3.2bit \
               /san/sanvol1/danRer3/danRer3Chroms.len
    # make file of scaffolds lengths for NA and Un scaffolds
    twoBitInfo \
       /san/sanvol1/scratch/danRer3/scaffoldsNAandUn/danRer3NAandUnScaf.2bit \
       /san/sanvol1/scratch/danRer3/scaffoldsNAandUn/NAandUnScafs.len
    cd /cluster/data/hg18/bed/blastz.danRer3
    # make a lift file for the hg18 randoms contigs
    cat /cluster/data/hg18/*/lift/random.lft >> $dir/randomContigs.lft
    # RUN 2: hg18 chroms and random chroms contigs vs danRer3 chrNA and 
    # chrUn scaffolds with no lineage-specific repeats as there are too 
    # many scaffolds in chrNA and chrUn. Use the dynamic masking function 
    # of Blastz instead.
    # make run dir
    mkdir -p /san/sanvol1/scratch/hg18/blastzDanRer3/chromsAndRandomsRun
    ln -s /san/sanvol1/scratch/hg18/blastzDanRer3/chromsAndRandomsRun
    # make out dir
    mkdir -p /san/sanvol1/scratch/hg18/blastzDanRer3/chromsAndRandomsOut
    ln -s /san/sanvol1/scratch/hg18/blastzDanRer3/chromsAndRandomsOut
    cd chromsAndRandomsRun 
    # use parameters similar to hg17 vs danRer2 - see makeHg17.doc
    # As lineage-specific repeats can not be used with chrUn and chrNA
    # scaffolds, then use dynamic masking, M=50.
    cat << '_EOF_' > DEF
# human (hg18) vs zebrafish (danRer3)
# human chroms and random chrom contigs vs zebrafish chrNA and chrUn scaffolds
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin

ALIGN=blastz-run
BLASTZ=blastz.v7.x86_64

# Reuse some parameters from hg16-fr1, danRer-hg17 and mm5-danRer
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_M=50
BLASTZ_Q=/cluster/data/blastz/HoxD55.q
BLASTZ_ABRIDGE_REPEATS=0

# TARGET: Human (hg18)
SEQ1_DIR=/san/sanvol1/scratch/hg18/hg18.2bit
SEQ1_CTGDIR=/san/sanvol1/scratch/hg18/chromsAndRandoms.2bit
SEQ1_LIFT=/san/sanvol1/scratch/hg18/randomContigs.lft
SEQ1_RMSK=
SEQ1_FLAG=
SEQ1_SMSK=
SEQ1_LIMIT=30
SEQ1_IN_CONTIGS=0
# 500 kb target with 5 kb overlap
SEQ1_CHUNK=500000
SEQ1_LAP=5000

# QUERY: Zebrafish (danRer3)
# just scaffolds for chrUn and chrNA
SEQ2_DIR=/san/sanvol1/scratch/danRer3/danRer3.2bit
SEQ2_CTGDIR=/san/sanvol1/scratch/danRer3/scaffoldsNAandUn/danRer3NAandUnScaf.2bit
SEQ2_LIFT=/san/sanvol1/scratch/danRer3/liftNAandUnScaffoldsToChrom.lft
SEQ2_RMSK=
SEQ2_FLAG=
SEQ2_SMSK=
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=1000000000
SEQ2_LAP=0

BASE=/san/sanvol1/scratch/hg18/blastzDanRer3/chromsAndRandomsRun

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=/san/sanvol1/scratch/hg18/hg18Chroms.len
SEQ1_CTGLEN=/san/sanvol1/scratch/hg18/chromsAndRandoms.len
SEQ2_LEN=/san/sanvol1/scratch/danRer3/danRer3Chroms.len
SEQ2_CTGLEN=/san/sanvol1/scratch/danRer3/scaffoldsNAandUn/NAandUnScafs.len
TMPDIR=/scratch/tmp
'_EOF_'
    # << this line keeps emacs coloring happy
    chmod +x DEF

    # do blastz and create chains for human chroms and random chroms in contigs
    # vs zebrafish danRer3 chrNA and chrUn in scaffolds without
    # lineage-specific repeats but using blastz's dynamic masking.
    # chickenHumanTuned.gap scoring matrix is now used by axtChain if the 
    # linearGap parameter is set to "loose".
  nohup nice /cluster/bin/scripts/doBlastzChainNet.pl \       
   -bigClusterHub=pk \
   -smallClusterHub=pk \
   -workhorse=pk \
   -blastzOutRoot /san/sanvol1/scratch/hg18/blastzDanRer3/chromsAndRandomsOut \ 
   -chainMinScore=5000 \
   -chainLinearGap loose \
   -stop chainRun `pwd`/DEF >& doChains.log &
    # Took about 15 hours to finish.
    ssh hgwdev
    # Try running hg18 random chroms in contigs vs danRer3 chroms 1-25 and chrM
    # with lineage-specific repeats.
    # make directory of human contigs repeats to serve as lineage-specific 
    # repeats for the random chroms contigs. 
    mkdir -p /san/sanvol1/scratch/hg18/linSpecRepRandoms.notInOthers
    cd /cluster/data/hg18/bed/blastz.danRer3
    awk '{print $4}' contigPosAndNames.txt > contigNames.txt
    foreach c (`cat contigNames.txt`)
      foreach f (/cluster/data/hg18/*/${c}/${c}.fa.out)
      cp -p $f \
      /san/sanvol1/scratch/hg18/linSpecRepRandoms.notInOthers/$f:t:r:r.out.spec
      end
    end
    # RUN 3: hg18 random chroms contigs vs danRer3 chr1-25 and chrM using
    # lineage-specific repeats.
    # make run dir
    mkdir -p /san/sanvol1/scratch/hg18/blastzDanRer3/randomsRun 
    ln -s /san/sanvol1/scratch/hg18/blastzDanRer3/randomsRun 
    # make out dir
    mkdir -p /san/sanvol1/scratch/hg18/blastzDanRer3/randomsOut 
    ln -s /san/sanvol1/scratch/hg18/blastzDanRer3/randomsOut 
    set dir=/san/sanvol1/scratch
    cp $dir/hg18/blastzDanRer3/chromsRun/S2chroms.len \
       $dir/danRer3/chr1to25andM.len
    # make nib dir for random contigs for hg18
    mkdir -p $dir/hg18/randomContigsNib
    foreach c (`cat contigNames.txt`)
      foreach f (/cluster/data/hg18/*/${c}/${c}.fa)
      faToNib -softMask $f $dir/hg18/randomContigsNib/$f:t:r.nib
      end
    end
    cd randomsRun
    cat << '_EOF_' > DEF
# human (hg18) vs zebrafish (danRer3)
# human random chrom contigs vs zebrafish chr1-15 and chrM
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin

ALIGN=blastz-run
BLASTZ=blastz.v7.x86_64

# Reuse parameters from hg16-fr1, danRer-hg17 and mm5-danRer
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q
BLASTZ_ABRIDGE_REPEATS=1

# TARGET: Human (hg18)
SEQ1_DIR=/san/sanvol1/scratch/hg18/hg18.2bit
SEQ1_CTGDIR=/san/sanvol1/scratch/hg18/randomContigsNib
SEQ1_LIFT=/san/sanvol1/scratch/hg18/randomContigs.lft
SEQ1_RMSK=
SEQ1_FLAG=
SEQ1_SMSK=/san/sanvol1/scratch/hg18/linSpecRepRandoms.notInOthers
SEQ1_LIMIT=30
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY: Zebrafish (danRer3)
# just chr1-25 and chrM
SEQ2_DIR=/san/sanvol1/scratch/danRer3/chromNib
SEQ2_RMSK=
SEQ2_FLAG=
SEQ2_LIMIT=30
SEQ2_SMSK=/san/sanvol1/scratch/danRer3/linSpecRep.notInOthers
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/san/sanvol1/scratch/hg18/blastzDanRer3/randomsRun

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=/san/sanvol1/scratch/hg18/hg18Chroms.len
SEQ1_CTGLEN=/san/sanvol1/scratch/hg18/randoms.len
SEQ2_LEN=/san/sanvol1/scratch/danRer3/chr1to25andM.len
TMPDIR=/scratch/tmp
'_EOF_'
    # << this line keeps emacs coloring happy
    chmod +x DEF

    # do blastz and create chains for human random chroms in contigs
    # vs zebrafish danRer3 chroms 1 to 25 and chrM using all repeats
    # as lineage-specific repeats. 
    # chickenHumanTuned.gap scoring matrix is now used by axtChain if the 
    # linearGap parameter is set to "loose".
  nohup nice /cluster/bin/scripts/doBlastzChainNet.pl \       
   -bigClusterHub=pk \
   -smallClusterHub=pk \
   -workhorse=pk \
   -blastzOutRoot /san/sanvol1/scratch/hg18/blastzDanRer3/randomsOut \ 
   -chainMinScore=5000 \
   -chainLinearGap loose \
   -stop chainRun `pwd`/DEF >& doChains.log &
   # Took 15 minutes.
   # chains are sorted by score so move into one directory and use
   # chainMergeSort
   ssh kolossus
   set blastzDir=/cluster/data/hg18/bed/blastz.danRer3
   cd $blastzDir/chromsRun/axtChain
   mkdir -p chainsNotMerged
   foreach r (chromsRun chromsAndRandomsRun randomsRun)
     nice cp -p ${blastzDir}/${r}/axtChain/run/chain/*.chain \
          ${blastzDir}/chromsRun/axtChain/chainsNotMerged/
   end
   nice chainMergeSort ./chainsNotMerged/*.chain | nice gzip -c \
        > hg18.danRer3.all.chain.gz
   # split into chains by chrom 
   nice zcat hg18.danRer3.all.chain.gz | chainSplit chain stdin
   # check chains, there are 48 should be 49. Chains for chr11_random
   # are missing. These sequences have a lot of repeats in the regions that
   # hits danRer3 with BLAT.
   # carry on with doBlastzChainNet.pl starting from net step
   ssh hgwdev
   cd /cluster/data/hg18/bed/blastz.danRer3/chromsRun
   mv DEF DEF.chroms
   # edit DEF to give hg18.2bit as the SEQ1_DIR and danRer3.2bit as SEQ2_DIR 
   # and remove lineage-specfic repeats.
   nohup nice /cluster/bin/scripts/doBlastzChainNet.pl \       
   -bigClusterHub=pk \
   -smallClusterHub=pk \
   -workhorse=pk \
   -blastzOutRoot /san/sanvol1/scratch/hg18/blastzDanRer3/chromsOut \
   -chainMinScore=5000 \
   -chainLinearGap loose \
   -continue net `pwd`/DEF >& doNetAndDownloads.log &
   # Took about 25 minutes.
   # crashed on ssh -X sanhead1 for cleanup so re-run script
   cleanUp.csh
   # copy chainDanRer3.html and netDanRer3.html to 
   # kent/src/hg/makeDb/trackDb/human/hg18/ and edit to describe method used.
   # Add tracks to trackDb.ra there. Edit README.txt in the downloads 
   # directory to describe method used for alignments.
# featureBits -chrom=chr1 hg18 refGene:cds chainDanRer3Link -enrichment
# refGene:cds 1.378%, chainDanRer3Link 2.601%, both 0.927%, cover 67.26%,
# enrich 25.86x 
# featureBits -chrom=chr1 hg17 refGene:cds chainDanRer2Link -enrichment
# refGene:cds 1.386%, chainDanRer3Link 2.742%, both 0.909%, cover 65.58%,
# enrich 23.91x 
# So similar coverage and enrichment to hg17 vs danRer2 chains.

#########################################################################
# BLASTZ MOUSE Mm7 second time (DONE - 2005-12-24 - 2005-12-25 Fan)
    #	After fixing a bug in the lineage specific repeat snip business
    #	in blastz-run-ucsc script
    ssh pk
    mkdir /cluster/data/hg18/bed/blastzMm7.2005-12-24
    cd /cluster/data/hg18/bed
    rm blastz.mm7
    ln -s blastzMm7.2005-12-24 blastz.mm7
    cd blastzMm7.2005-12-24

    cat << '_EOF_' > DEF
# human vs mouse
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin:/parasol/bin

BLASTZ=blastz.v7.x86_64
BLASTZ_ABRIDGE_REPEATS=1

# TARGET: Human Hg18
SEQ1_DIR=/scratch/hg/hg18/nib
SEQ1_SMSK=/scratch/hg/hg18/linSpecRep/notInMouse
SEQ1_CHUNK=10000000
SEQ1_LAP=10000
SEQ1_LEN=/scratch/hg/hg18/chrom.sizes

# QUERY: Mouse Mm7 - single chunk big enough to run entire genome
SEQ2_DIR=/scratch/hg/mm7/nib
SEQ2_SMSK=/scratch/hg/mm7/linSpecRep/notInHumanDogCow
SEQ2_LEN=/cluster/bluearc/mm7/chrom.sizes
SEQ2_CHUNK=3000000000
SEQ2_LAP=0

BASE=/cluster/data/hg18/bed/blastzMm7.2005-12-24
TMPDIR=/scratch/tmp
'_EOF_'
    # << happy emacs

    #	establish a screen to control this job
    screen
    bash
    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
	-stop=load \
	`pwd`/DEF > to-load.out 2>&1 &
    #	Started 2005-12-24 06:15

    mv to-load.out to-load.out.1

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
    -bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
    -continue=chainMerge -stop=load \
    `pwd`/DEF > to-load.out 2>&1 &

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
	-swap -stop=load \
	`pwd`/DEF > swap-load.out 2>&1 &

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
    -bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
    -continue=download \
    `pwd`/DEF > download.out 2>&1 &

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
    -bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
    -swap -continue=download \
    `pwd`/DEF > swap-download.out 2>&1 &

# PLEASE NOTE THAT SOME .OUT FILES MIGHT HAVE BEEN OVERWRITTEN
# DUE TO RETRIES AND/OR NEXT STEP COMMAND NOT FULLY EDITED CORRECTLY. 

    #	Measurements:

    ssh hgwdev

    featureBits mm7 chainHg18Link 
    # 990285408 bases of 2583394090 (38.333%) in intersection

    featureBits hg18 chainMm7Link 
    # 991769039 bases of 2881515245 (34.418%) in intersection

    # each of above took about half hour.

#########################################################################
# BLASTZ CHICKEN GalGal2 second time (DONE - 2005-12-28 Fan)

    ssh pk
    mkdir /cluster/data/hg18/bed/blastzGalGal2.2005-12-28
    cd /cluster/data/hg18/bed
    rm blastz.galGal2
    ln -s blastzGalGal2.2005-12-28 blastz.galGal2
    cd blastzGalGal2.2005-12-28

    cat << '_EOF_' > DEF
# human vs chicken
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin:/parasol/bin

BLASTZ=blastz.v7.x86_64

# Specific settings for chicken (per Webb email to Brian Raney)
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=10000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q
BLASTZ_ABRIDGE_REPEATS=1

# TARGET: Human Hg18
SEQ1_DIR=/scratch/hg/hg18/nib
SEQ1_SMSK=/cluster/bluearc/hg18/linSpecRep/notInOthers
SEQ1_LEN=/scratch/hg/hg18/chrom.sizes
SEQ1_IN_CONTIGS=0 
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY: Chicken GalGal2 - single chunk big enough to run entire genome
SEQ2_DIR=/scratch/hg/galGal2/nib
SEQ2_LEN=/cluster/bluearc/galGal2/chrom.sizes
SEQ2_SMSK=/scratch/hg/galGal2/linSpecRep
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=200000000
SEQ2_LAP=0

BASE=/cluster/data/hg18/bed/blastzGalGal2.2005-12-28
TMPDIR=/scratch/tmp
'_EOF_'
    # << happy emacs

    #	establish a screen to control this job
    screen
    bash
    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
	-stop=load \
	`pwd`/DEF > load.out 2>&1 &
    #	Started 2005-12-28 10:35

    # Two jobs stuck in the same node.  Did manual para stop and para push.  
    # Both finished within a few minutes.

    # Done! On Wed Dec 28 15:32:45 PST 2005.

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
	-swap -stop=load \
	`pwd`/DEF > swap-load.out 2>&1 &

    # Had an error at the net step

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
      -swap -continue=net -stop=load \
	`pwd`/DEF > swap-load.out 2>&1 &

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
    -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
    -continue=download \
    `pwd`/DEF > download.out 2>&1 &

    # the gzip job on kolossus seems not moving at all.
    # killed it manually.  Try again.

    # Seemed not moving, kill it again.  Now use pk instead of kolossus.

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
    -workhorse=pk \
    -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
    -continue=download \
    `pwd`/DEF > download.out 2>&1 &

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
    -workhorse=pk \
    -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
    -swap -continue=download \
    `pwd`/DEF > swap-download.out 2>&1 &

    # Done! Wed Dec 28 20:39:44 PST 2005

    #	Measurements:

    ssh hgwdev

    nice featureBits galGal2 chainHg18Link 
    # 91564024 bases of 1054197620 (8.686%) in intersection
    nice featureBits hg18 chainGalGal2Link 
    # 102417858 bases of 2881515245 (3.554%) in intersection

    nice featureBits galGal2 chainHg17Link 
    # 93277286 bases of 1054197620 (8.848%) in intersection
    nice featureBits hg17 chainGalGal2Link 
    # 103882699 bases of 2866216770 (3.624%) in intersection

#########################################################################
# BLASTZ DOG CanFam2 time (DONE - 2005-12-28 - 2005-12-29 Fan)

    ssh pk
    mkdir /cluster/data/hg18/bed/blastzCanFam2.2005-12-28
    cd /cluster/data/hg18/bed
    rm blastz.canFam2
    ln -s blastzCanFam2.2005-12-28 blastz.canFam2
    cd blastzCanFam2.2005-12-28

    cat << '_EOF_' > DEF
# human vs dog
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin:/parasol/bin

BLASTZ=blastz.v7.x86_64

# Specific settings for dog (per Webb email to Brian Raney)
BLASTZ_ABRIDGE_REPEATS=1

# TARGET: Human Hg18
SEQ1_DIR=/scratch/hg/hg18/nib
SEQ1_SMSK=/cluster/bluearc/hg18/linSpecRep/notInOthers
SEQ1_LEN=/scratch/hg/hg18/chrom.sizes
SEQ1_IN_CONTIGS=0 
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY: Dog CanFam2 - single chunk big enough to run entire genome
SEQ2_DIR=/scratch/hg/canFam2/nib
SEQ2_LEN=/cluster/bluearc/canFam2/chrom.sizes
SEQ2_SMSK=/san/sanvol1/scratch/canFam2/linSpecRep.notInHuman
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=200000000
SEQ2_LAP=0

BASE=/cluster/data/hg18/bed/blastzCanFam2.2005-12-28
TMPDIR=/scratch/tmp
'_EOF_'
    # << happy emacs

    #	establish a screen to control this job
    screen
    bash
    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
	-stop=load \
	`pwd`/DEF > load.out 2>&1 &
    #	Started 2005-12-28 21:33

    # Two jobs stuck in the same node.  Did manual para stop and para push.  
    # Both finished within a few minutes.

    # Done! On Thu Dec 29 05:27:31 PST 2005.

    # system seems hang on kolossus (3 processes of [tcsh -c nice chainMergeSort], not moving)
    # manually killed the jobs.
    # now use pk as the workhorse.

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
      -workhorse=pk \
      -continue=chainMerge \
	-stop=load \
	`pwd`/DEF > load2.out 2>&1 &

    # Done! Thu Dec 29 09:10:02 PST 2005.

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
	-workhorse=pk \
      -swap -stop=load \
	`pwd`/DEF > swap-load.out 2>&1 &

    # Had an error at the load step,
    # mySQL error 2013: Lost connection to MySQL server during query,
    # probably due to sys admin working on network connections,
    # continue at the load step

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
      -workhorse=pk \
      -swap -continue=load -stop=load \
	`pwd`/DEF > swap-load2.out 2>&1 &

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
    -bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
    -workhorse=pk \
    -continue=download \
    `pwd`/DEF > download.out 2>&1 &

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
    -workhorse=pk \
    -bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
    -swap -continue=download \
    `pwd`/DEF > swap-download.out 2>&1 &

    # Done! Dec 29 13:21

    #	Measurements:

    ssh hgwdev
nice featureBits canFam2 chainHg18Link 
# 1477551526 bases of 2384996543 (61.952%) in intersection
nice featureBits hg18 chainCanFam2Link 
# 1524764349 bases of 2881515245 (52.915%) in intersection
nice featureBits canFam2 chainHg17Link 
# 1487483112 bases of 2384996543 (62.368%) in intersection
nice featureBits hg17 chainCanFam2Link 
# 1530197469 bases of 2866216770 (53.387%) in intersection

# ENABLE GENBANK UPDATE (1/3/06 Fan)

# add hg18 to the following two files and check them in.

     src/hg/makeDb/genbank/etc/align.dbs
     src/hg/makeDb/genbank/etc/hgwdev.dbs

# then go to /cluster/data/genbank/etc and do cvs update on these two files.

#########################################################################
# BLASTZ RAT Rn3 (STARTED - 2005-12-22, DONE 2006-01-05 Fan)

    ssh pk
    mkdir /cluster/data/hg18/bed/blastzRn3.2005-12-22
    cd /cluster/data/hg18/bed
    rm blastz.rn3
    ln -s blastzRn3.2005-12-22 blastz.rn3
    cd blastzRn3.2005-12-22

    cat << '_EOF_' > DEF
# human vs rat
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin:/parasol/bin

BLASTZ=blastz.v7.x86_64

BLASTZ_ABRIDGE_REPEATS=1
    
# TARGET: Muman Hg18
SEQ1_DIR=/scratch/hg/hg18/nib
SEQ1_SMSK=/scratch/hg/hg18/linSpecRep/notInRat
SEQ1_LEN=/scratch/hg/hg18/chrom.sizes
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY: Rat Rn3 - chunk big enough to do all chroms in single whole
pieces
SEQ2_DIR=/scratch/rat/rn3/softNib
SEQ2_SMSK=/cluster/bluearc/rat/rn3/linSpecRep.notInHuman
SEQ2_LEN=/cluster/bluearc/rat/rn3/chrom.sizes
SEQ2_CHUNK=300000000
SEQ2_LAP=0

BASE=/cluster/data/hg18/bed/blastzRn3.2005-12-22
TMPDIR=/scratch/tmp
'_EOF_'
    # happy emacs

    #	establish a screen to control this job
    screen
    bash
    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
	-stop=load \
	`pwd`/DEF > to-load.out 2>&1 &

# start processing again on 12/31/05.

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
	-workhorse=pk \
      -swap \
      -stop=load \
	`pwd`/DEF > swap.out 2>&1 &

# Either UCSC RR and hgwdev systems or network went down around 11 AM 12/31/05.

# After holidays, start again on 1/3/06 and again on 1/5/06.

    ssh pk
    cd /cluster/data/hg18/bed
    cd blastzRn3.2005-12-22
    screen
    bash

      time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
	-workhorse=pk \
      -swap \
      -continue=net \
      -stop=load \
	`pwd`/DEF > swap6.out 2>&1 &

# DONE! Jan  5 13:39

# Measurements:
nice featureBits rn3 chainHg18Link 
# 962630574 bases of 2571104688 (37.440%) in intersection
nice featureBits hg18 chainRn3Link 
# 964251210 bases of 2881515245 (33.463%) in intersection

#########################################################################
# BLASTZ FUGU fr1 (STARTED - 2005-12-20, DONE 2006-01-05 Fan)
    ssh pk
    mkdir /cluster/data/hg18/bed/blastzFr1.2005-12-20
    cd /cluster/data/hg18/bed
    ln -s blastzFr1.2005-12-20 blastz.fr1
    cd blastzFr1.2005-12-20

    cat << '_EOF_' > DEF
# human vs. fugu
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin:/parasol/bin

BLASTZ=blastz.v7.x86_64

# Reuse parameters from human-chicken, except L=6000 (more relaxed)
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q

# TARGET: Human Hg18 - testing 100,000,000 sized chunk on pk kluster
SEQ1_DIR=/scratch/hg/hg18/nib
SEQ1_LEN=/scratch/hg/hg18/chrom.sizes
SEQ1_CHUNK=100000000
SEQ1_LAP=10000

# QUERY: Fugu Fr1 - chunk big enough to run the whole chrom at once
SEQ2_DIR=/san/sanvol1/scratch/fr1/nib
SEQ2_LEN=/san/sanvol1/scratch/fr1/chrom.sizes
SEQ2_CHUNK=400000000
SEQ2_LAP=0

BASE=/cluster/data/hg18/bed/blastzFr1.2005-12-20
'_EOF_'
    # << happy emacs

    #	establish a screen to control this job
    ssh pk
    cd /cluster/data/hg18/bed/blastzFr1.2005-12-20
    screen
    bash
    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 -stop=load \
	`pwd`/DEF > thruLoad.out 2>&1 &

    ssh pk
    cd /cluster/data/hg18/bed/blastzFr1.2005-12-20
    screen
    bash
    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 -continue=chainMerge -stop=load \
	`pwd`/DEF > thruLoad.out 2>&1 &

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 -continue=download \
	`pwd`/DEF > download.clean.out 2>&1 &

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 -swap \
	`pwd`/DEF > swap.out 2>&1 &

# Finish the remaining step, 1/4/05.

    ssh pk
    cd /cluster/data/hg18/bed/blastzFr1.2005-12-20
    screen
    bash

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 \
	-swap -continue=download \
	`pwd`/DEF > DownloadSwap.out 2>&1 &

# First try found the DEF was some how altered for rn3.
# Re-generated DEF and try again.

time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 \
	-swap -continue=download \
	`pwd`/DEF > DownloadSwap2.out 2>&1 &

# Done.  Jan  4 09:48.

# measurements

nice featureBits hg18 chainFr1Link
# 51795958 bases of 2881515245 (1.798%) in intersection
nice featureBits hg17 chainFr1Link
#50831650 bases of 2866216770 (1.773%) in intersection

nice featureBits hg18 netFr1
# 691148929 bases of 2881515245 (23.986%) in intersection
nice featureBits hg17 netFr1
# 714234935 bases of 2866216770 (24.919%) in intersection

nice featureBits fr1 chainHg18Link
# 43267869 bases of 315518167 (13.713%) in intersection
# nice featureBits fr1 chainHg17Link
0 bases of 315518167 (0.000%) in intersection
nice featureBits fr1 netHg18
# 140843080 bases of 315518167 (44.639%) in intersection
nice featureBits fr1 netHg17
# 0 bases of 315518167 (0.000%) in intersection

# BLASTZ TETRAODON TetNig1 second time (DONE - 2006-01-07 Fan)

ssh pk
mkdir /cluster/data/hg18/bed/blastzTetNig1.2006-01-07
cd /cluster/data/hg18/bed
rm blastz.tetNig1
ln -s blastzTetNig1.2006-01-07 blastz.tetNig1
cd blastzTetNig1.2006-01-07

    cat << '_EOF_' > DEF
# human vs tetraodon
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin:/parasol/bin

BLASTZ=blastz.v7.x86_64

BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q

# TARGET: Human Hg18
SEQ1_DIR=/scratch/hg/hg18/nib
SEQ1_LEN=/scratch/hg/hg18/chrom.sizes
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY: Tetraodon TetNig1 - single chunk big enough to run entire genome
SEQ2_DIR=/san/sanvol1/scratch/tetNig1/tetNig1.2bit
SEQ2_LEN=/san/sanvol1/scratch/tetNig1/chrom.sizes
SEQ2_CHUNK=410000000
SEQ2_LAP=0

BASE=/cluster/data/hg18/bed/blastzTetNig1.2006-01-07
TMPDIR=/scratch/tmp
'_EOF_'
    # << happy emacs

# establish a screen to control this job
screen
bash
time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
-stop=load \
`pwd`/DEF > load.out 2>&1 &
# Started Sat Jan  7 05:40:51 PST 2006

# Encountered an error:
startStep: 0, at step 5 net to stopStep 6
netChains: looks like previous stage was not successful (can't find [hg18.tetNig1.]all.chain[.gz]).

# Try it with pk as the workhorse.
time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
-workhorse=pk \
-continue=net \
-stop=load \
`pwd`/DEF > load2.out 2>&1 &

# Load done.  Sat Jan  7 07:34:56 PST 2006

time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
-workhorse=pk \
-swap -stop=load \
`pwd`/DEF > swap-load.out 2>&1 &

time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
-workhorse=pk \
-continue=download \
 `pwd`/DEF > download.out 2>&1 &

time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
-workhorse=pk \
-swap -continue=download \
`pwd`/DEF > swap-download.out 2>&1 &

# Done! Sat Jan  7 08:02:14 PST 2006
# The download and swap-download took less than 10 seconds each.  ???

# Measurements:

ssh hgwdev
nice featureBits tetNig1 chainHg18Link 
# 50026847 bases of 342403326 (14.611%) in intersection
nice featureBits hg18 chainTetNig1Link 
# 57654754 bases of 2881515245 (2.001%) in intersection

nice featureBits tetNig1 chainHg17Link 
# 34379509 bases of 342403326 (10.041%) in intersection
nice featureBits hg17 chainTetNig1Link 
# 35910128 bases of 2866216770 (1.253%) in intersection

# BLASTZ FROG XenTro1 second time (STARTED - 2006-01-06 Fan)

    ssh pk
    mkdir /cluster/data/hg18/bed/blastzXenTro1.2006-01-06
    cd /cluster/data/hg18/bed
    rm blastz.xenTro1
    ln -s blastzXenTro1.2006-01-06 blastz.xenTro1
    cd blastzXenTro1.2006-01-06

    cat << '_EOF_' > DEF
# human vs frog
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin:/parasol/bin

BLASTZ=blastz.v7.x86_64

BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=8000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q

# TARGET: Human Hg18
SEQ1_DIR=/scratch/hg/hg18/nib
SEQ1_LEN=/scratch/hg/hg18/chrom.sizes
SEQ1_CHUNK=20000000
SEQ1_LAP=10000

# QUERY: Frog XenTro1 - single chunk big enough to run entire genome
SEQ2_DIR=/scratch/hg/xenTro1/xenTro1.2bit
SEQ2_LEN=/scratch/hg/xenTro1/chrom.sizes
SEQ2_LIMIT=400
SEQ2_CHUNK=30000000
SEQ2_LAP=0

BASE=/cluster/data/hg18/bed/blastzXenTro1.2006-01-06
TMPDIR=/scratch/tmp
'_EOF_'
    # << happy emacs

    #	establish a screen to control this job
    screen
    bash
    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
      -stop=load \
	`pwd`/DEF > load.out 2>&1 &
# Started Fri Jan  6 20:19:30 PST 2006
# Blastz run done.  Jan  7 02:07 load.out

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
	-swap -stop=load \
	`pwd`/DEF > swap-load.out 2>&1 &

# got the following error:

startStep: 4, at step 5 net to stopStep 6
netChains: looks like previous stage was not successful (can't find [xenTro1.hg18.]all.chain[.gz]).

# Try it with pk instead of kolossus:

time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
-workhorse=pk \
-swap -stop=load \
`pwd`/DEF > swap-load2.out 2>&1 &

# It worked, swap-load done. Jan  7 06:05

time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
-workhorse=pk \
-continue=download \
`pwd`/DEF > download.out 2>&1 &

time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
-workhorse=pk \
-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
-swap -continue=download \
`pwd`/DEF > swap-download.out 2>&1 &

# Done! Jan  7 06:18

# Measurements:

ssh hgwdev
nice featureBits xenTro1 chainHg18Link 
# 61197900 bases of 1381238994 (4.431%) in intersection
nice featureBits hg18 chainXenTro1Link 
# 67810866 bases of 2881515245 (2.353%) in intersection

nice featureBits xenTro1 chainHg17Link 
# 81777842 bases of 1381238994 (5.921%) in intersection
nice featureBits hg17 chainXenTro1Link 
# 85701475 bases of 2866216770 (2.990%) in intersection

############################################################################
# BLASTZ COW BosTau2 second time (STARTED - 2006-01-07, DONE 2006-01-08 Fan)

    ssh pk
    mkdir /cluster/data/hg18/bed/blastzBosTau2.2006-01-07
    cd /cluster/data/hg18/bed
    rm blastz.bosTau2
    ln -s blastzBosTau2.2006-01-07 blastz.bosTau2
    cd blastzBosTau2.2006-01-07

    cat << '_EOF_' > DEF
# human vs cow
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin:/parasol/bin

BLASTZ=blastz.v7.x86_64
BLASTZ_M=50

# TARGET: Human Hg18
SEQ1_DIR=/scratch/hg/hg18/nib
SEQ1_LEN=/scratch/hg/hg18/chrom.sizes
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY: Cow BosTau2 - single chunk big enough to run entire genome
SEQ2_DIR=/san/sanvol1/scratch/bosTau2/bosTau2.2bit
SEQ2_LEN=/san/sanvol1/scratch/bosTau2/chrom.sizes
SEQ2_CHUNK=3200000000
SEQ2_LAP=0

BASE=/cluster/data/hg18/bed/blastzBosTau2.2006-01-07
TMPDIR=/scratch/tmp
'_EOF_'
    # << happy emacs

# establish a screen to control this job
screen
bash
time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
-bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
-stop=load \
-workhorse=pk \
`pwd`/DEF > load.out 2>&1 &

# Started Sat Jan  7 07:57:22 PST 2006
# blastz run (and load) done Jan  8 00:13

time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
-bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
-swap -stop=load \
`pwd`/DEF > swap-load.out 2>&1 &

# took a long time to finish.

time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
-bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
-continue=download \
`pwd`/DEF > download.out 2>&1 &

time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
-bigClusterHub=pk -chainMinScore=3000 -chainLinearGap=medium \
-swap -continue=download \
`pwd`/DEF > swap-download.out 2>&1 &

# Done! Jan  8 21:10

# Measurements:

ssh hgwdev
nice featureBits bosTau2 chainHg18Link 
# 1357027317 bases of 2812203870 (48.255%) in intersection
nice featureBits hg18 chainBosTau2Link 
# 1357291762 bases of 2881515245 (47.103%) in intersection
nice featureBits bosTau2 chainHg17Link 
# 0 bases of 2812203870 (0.000%) in intersection
nice featureBits hg17 chainBosTau2Link 
# 1350076765 bases of 2866216770 (47.103%) in intersection

#######################################################################
# MAKE 11.OOC FILE FOR BLAT (DONE - 2006-01-11 - Fan)
    ssh kkstore02
    cd /cluster/data/hg18

    blat hg18.2bit \
	 /dev/null /dev/null -tileSize=11 -makeOoc=11.ooc -repMatch=1024
# Wrote 30378 overused 11-mers to 11.ooc

# Copy over to the bluearc
   cp -p 11.ooc /cluster/bluearc/hg18
   
#######################################################################
# PLACE ASSEMBLY CLONES ON CONTIGS AND SEQUENCE (WORKING - 2006-01-12 - Hiram)
    ssh kkstore02
    mkdir /cluster/data/hg18/bed/coverage
    cd /cluster/data/hg18/bed/coverage
    #	find all the clones that were used in the assembly
    sed -e "/^#.*/d" ../../ncbi_build36.agp | \
        awk '{if (!match($5,"N")) {print $6}}' | \
        sort -u > placed_in_assembly.list
    wc -l placed_in_assembly.list
    #	27093 placed_in_assembly.list

    #	And all possible clones considered for assembly.
    #	The AADB clones are the Celera assembly, don't want them.
    sed -e "/^#.*/d" /cluster/store11/gs.19/ncbi/sequence.inf | \
	grep for_assembly | grep -v AADB | awk '{print $1}' | sort -u \
	    > allButOneClonesConsidered.list
    (grep AADB01066164.1 \
	/cluster/store11/gs.19/ncbi/sequence.inf | awk '{print $1}'; \
	cat allButOneClonesConsidered.list) | sort -u \
	    > allClonesConsidered.list
    #	The grep for AADB eliminates a single clone: AADB01066164.1
    #	Which actually should be in the list since it is in the
    #	ncbi_build36.agp file.  Back in Hg17, this was the only AADB
    #	clone in the sequence.inf file, now there are 400,673 of them in
    #	this Hg18 sequence.inf file marked "for_assembly"

    #	Later after a lot of this was done, it was discovered that some
    #	of the clones on this allConsidered list are actually obsolete
    #	and have newer versions in use.  They were identified by the
    #	following perl script:

    cat << '_EOF_' > ckMultipleVersions.pl
#!/usr/bin/env perl
use warnings;
use strict;
sub usage() {
    print "usage: ./ckMultipleVersions.pl allClonesConsidered.list\n";
    exit 255;
}
my $argc = scalar(@ARGV);
if ($argc != 1) { usage; }
my $fileName = shift;
open (FH,"<$fileName") or die "Can not open $fileName";
my %cloneAcc;   #       key is clone accession major number, value is version
while (my $clone = <FH>) {
    chomp $clone;
    my ($major, $version) = split('\.', $clone);
    if (exists($cloneAcc{$major})) {
        my $previousVersion = $cloneAcc{$major};
        if ($previousVersion >= $version) {
            printf STDERR "$major.$version - obsolete\n";
        } else {
            printf STDERR "$major.$previousVersion - obsolete\n";
            $cloneAcc{$major} = $version;
        }
    } else {
        $cloneAcc{$major} = $version;
    }
}
close (FH);
foreach my $major (sort keys %cloneAcc) {
    printf "$major.$cloneAcc{$major}\n";
}
'_EOF_'
    #	happy emacs
    chmod +x ckMultipleVersions.pl

    ./ckMultipleVersions.pl allClonesConsidered.list \
	2> obsoleteClone.list > allClones.notObsolete.list
    #	After this obsolete list was made, those clone results were
    #	removed from the kluster run hierarchies of results.

    comm -12 allClonesConsidered.list \
	/cluster/data/hg17/bed/contig_overlaps/sequence.list \
	    > allClones.InHg17AndHg18.list
    comm -23 allClonesConsidered.list \
	/cluster/data/hg17/bed/contig_overlaps/sequence.list \
	    > allClones.InHg18NotHg17.list
    comm -13 allClonesConsidered.list \
	/cluster/data/hg17/bed/contig_overlaps/sequence.list \
	    > allClones.InHg17NotHg18.list

    #	how many are the same as previous build:
    comm -12 /cluster/data/hg17/bed/contig_overlaps/placed_in_assembly.list \
	placed_in_assembly.list > sameAsHg17.list
    wc sameAsHg17.list
    #	26775  26775 300641 sameAsHg17.list
    #	There is one clone: AADB01066164.1
    #	Which is listed in allClones.InHg17NotHg18.list
    #	But it is on the Hg18 placed_in_assembly.list
    #	And it is on the Hg17 placed_in_assembly.list but it isn't
    #	actually found in Hg17 ?  Perhaps it didn't align good enough.
    comm -23 /cluster/data/hg17/bed/contig_overlaps/placed_in_assembly.list \
	placed_in_assembly.list > uniqueToHg17.list
    wc uniqueToHg17.list
    #	97   97 1080 uniqueToHg17.list
    #	and unique to hg18, not in hg17:
    comm -13 /cluster/data/hg17/bed/contig_overlaps/placed_in_assembly.list \
	placed_in_assembly.list > newToHg18.list
    wc newToHg18.list
    #	318  318 3547 newToHg18.list
    #	make a list of these new contigs:
    #	using the previous perl scripts:
    cp -p /cluster/data/hg17/bed/contig_overlaps/*.pl .

    #	Now, we need to distribute the clone sequence files in a
    #	directory hierarchy by chrom name.  Using the contigAcc.pl file
    #	from the previous release:
    cp /cluster/data/hg17/bed/contig_overlaps/contigAcc.pl .
    #	This newer version is generalized a bit better to take command
    #	line arguments for the two files it is to read instead of having
    #	them explicitly in the code, then:
    ./contigAcc.pl /cluster/data/hg18/ncbi_build36.agp \
	/cluster/data/hg18/seq_contig.md > cloneToChrom.list 2>&1
    #	And now, since most of the clone sequence already exists in the
    #	Hg17 work directory, we only need to make symlinks to the
    #	existing ones, and move only the new ones.  The following script
    #	will find an existing copy and symlink it correctly.

    cat << '_EOF_' > createPlacedHierarchy.sh
#!/bin/sh

mkdir -p placedClones

sed -e "/^#.*/d" cloneToChrom.list | while read L
do
    CHROM=`echo "${L}" | awk '{print $1}'`
    CLONE=`echo "${L}" | awk '{print $2}'`
    if [ ! -d "placedClones/${CHROM}" ]; then
	mkdir placedClones/${CHROM}
    fi
    HG17_version="/cluster/data/hg17/bed/contig_overlaps/${CHROM}/${CLONE}"
    HG18_version_0="/cluster/data/hg18/bed/coverage/newToHg18/${CLONE}"	    
  HG18_version_1="/cluster/data/hg18/bed/coverage/allClones.newToHg18/${CLONE}"	    
    if [ -f "${HG17_version}" ]; then
	if [ -f "${HG18_version_0}" -o -f "${HG18_version_1}" ]; then
	  echo "ERROR: Why is there both an Hg17 and Hg18 version for ${CLONE}"
	  exit 255
	fi
	ln -s "/cluster/data/hg17/bed/contig_overlaps/${CHROM}/${CLONE}" \
		"./placedClones/${CHROM}/${CLONE}"
    else
	if [ -f "${HG18_version_0}" -a -f "${HG18_version_1}" ]; then
	    echo "ERROR: Why are there two Hg18 copies for ${CLONE}"
	    exit 255
	fi
	if [ -f "${HG18_version_0}" ]; then
	    ln -s "${HG18_version_0}" "./placedClones/${CHROM}/${CLONE}"
	else
	    if [ -f "${HG18_version_1}" ]; then
		ln -s "${HG18_version_1}" "./placedClones/${CHROM}/${CLONE}"
	    else
		# must be on a different chrom in hg17
		HG17_chrom=`grep -v "^#" \
	/cluster/data/hg17/bed/contig_overlaps/disburseEm.list \
	| grep "^${L}$" | awk '{print $1}'`
    HG17_version="/cluster/data/hg17/bed/contig_overlaps/${HG17_chrom}/${CLONE}"
		if [ -f "${HG17_version}" ]; then
		    echo "ERROR: Why is there no version for ${CLONE}"
		    exit 255
		fi
		ln -s "${HG17_version}" "./placedClones/${CHROM}/${CLONE}"
	    fi
	fi
    fi
done
'_EOF_'
    #	happy emacs
    chmod +x createPlacedHierarchy.sh
    ./createPlacedHierarchy.sh
    #	There should be no errors

    #	We need masked contigs for the psLayout alignments
    ssh hgwdev
    mkdir /cluster/data/hg18/bed/coverage/maskedContigs
    cd /cluster/data/hg18/bed/coverage/maskedContigs
    hgsql -N \
	-e "select chrom,chromStart,chromEnd,contig,size from ctgPos;" hg18 \
	> ctgPos.txt

    ssh kkstore02
    cd /cluster/data/hg18/bed/coverage/maskedContigs
    #	verify each contig only listed once:
    awk '{print $4}' ctgPos.txt | sort | uniq -c | sort -n | less
    #	should all have a count of one
    #	verify all chrom sizes match the contig sizes:
    awk '{print $3-$2}' ctgPos.txt > chrSize.list
    awk '{print $5}' ctgPos.txt > ctgSize.list
    diff ctgSize.list chrSize.list
    #	should be no difference
    #	OK, now fetch the contigs from the twoBit file:
    
    cat << '_EOF_' > 2bitToFa.pl
#!/usr/bin/env perl
use warnings;
use strict;
while (my $line=<>) {
chomp $line;
my ($chrom, $start, $end, $contig, $size) = split('\s',$line);
$chrom =~ s/chr//;
printf "echo -n 'working $contig ...'; mkdir -p $chrom; twoBitToFa /cluster/data/hg18/hg18.2bit:chr$chrom:$start-$end stdout | sed -e 's/^>.*/>$contig/' > $chrom/$contig.fa; gzip $chrom/$contig.fa; echo 'done'\n";
}
'_EOF_'
    # happy emacs
    chmod +x 2bitToFa.pl
    cat ctgPos.txt | ./2bitToFa.pl > 2bitToFa.sh
    chmod +x 2bitToFa.sh
    time ./2bitToFa.sh

    #	and create a lift file for these contigs
    cat << '_EOF_' > mkCtgLift.pl
#!/usr/bin/env perl
use warnings;
use strict;
while (my $line=<>)
{
chomp $line;
my ($start, $chrCtg, $size, $chrom, $chrLen) = split('\s',$line);
$chrCtg =~ s#.*/##;
printf "%s\t%s\t%s\t%s\t%s\n", $start, $chrCtg, $size, $chrom, $chrLen;
}
'_EOF_'
    #	happy emacs
    chmod +x mkCtgLift.pl
    cat /cluster/data/hg18/jkStuff/liftAll.lft \
	| ./mkCtgLift.pl > liftContigs.lft

    #	Create individual ooc files for each contig
    mkdir ooc
    for C in `ls */*.fa.gz | sed -e "s/.fa.gz//"`
    do
	CONTIG=`basename ${C}`
	CHR=`dirname ${C}`
	mkdir -p ooc/${CHR}
	zcat ${C}.fa.gz | blat -repMatch=256 \
	    -makeOoc=ooc/${CHR}/${CONTIG}.10.ooc -tileSize=10 \
	    stdin /dev/null /dev/null
	echo "done: ${CONTIG}"
    done

    #	Copy everything to san filesystem for kluster run:
    ssh pk
    mkdir /san/sanvol1/scratch/hg18/coverage
    cd /san/sanvol1/scratch/hg18/coverage
    rsync -a --progress --copy-links \
	/cluster/data/hg18/bed/coverage/placedClones/ ./placedClones/
    rsync -a --progress --copy-links \
	/cluster/data/hg18/bed/coverage/maskedContigs/ ./maskedContigs/

    mkdir /san/sanvol1/scratch/hg18/coverage/runPlaced
    cd /san/sanvol1/scratch/hg18/coverage/runPlaced

    cat << '_EOF_' > runPsLayout.sh
#!/bin/sh
#   runPsLayout.sh <chrom> <clone> <contig>
#     where <chrom> is the chrom this contig is on
#      <clone> is one of the .fa.gz files in
#	  /san/sanvol1/scratch/hg18/coverage/placedClones/<chrom>/<clone>.fa.gz
#      <contig> is one of the contigs found in:
#	/san/sanvol1/scratch/hg18/coverage/maskedContigs/<chrom>/<contig>.fa.gz
#
HERE=`pwd`
CHROM=$1
CLONE=$2
CONTIG=$3
TARGET=/san/sanvol1/scratch/hg18/coverage/maskedContigs/$CHROM/$CONTIG.fa.gz
CLONESRC=/san/sanvol1/scratch/hg18/coverage/placedClones/$CHROM/$CLONE.fa.gz
OOC=/san/sanvol1/scratch/hg18/coverage/maskedContigs/ooc/$CHROM/$CONTIG.10.ooc
RESULT="${HERE}/psl/${CHROM}/${CONTIG}/${CLONE}.psl"
mkdir -p psl/${CHROM}/${CONTIG}
if [ ! -s ${CLONESRC} ]; then
        echo "Can not find: ${CLONESRC}" 1>/dev/stderr
        exit 255
fi
if [ ! -s ${TARGET} ]; then
        echo "Can not find: ${TARGET}" 1>/dev/stderr
        exit 255
fi
if [ ! -s ${OOC} ]; then
        echo "Can not find: ${OOC}" 1>/dev/stderr
        exit 255
fi
WRKDIR="/scratch/tmp/hg18_${CHROM}/${CONTIG}/${CLONE}"
mkdir -p "${WRKDIR}"
cd ${WRKDIR}
zcat ${CLONESRC} > ${CLONE}.fa
zcat ${TARGET} > ${CONTIG}.fa
cp -p ${OOC} ./10.ooc
/cluster/bin/x86_64/psLayout ${CONTIG}.fa ${CLONE}.fa genomic 10.ooc ${RESULT}
RET=$?
cd ${HERE}
rm -fr ${WRKDIR}
rmdir --ignore-fail-on-non-empty "/scratch/tmp/hg18_${CHROM}/${CONTIG}"
rmdir --ignore-fail-on-non-empty "/scratch/tmp/hg18_${CHROM}"
exit ${RET}
'_EOF_'
    #	happy emacs
    chmod +x runPsLayout.sh

    #	create jobList from cloneToChrom.list:
    grep -v "^#" /cluster/data/hg18/bed/coverage/cloneToChrom.list \
	| sed -e "s/.fa.gz//" \
	| awk '{
printf "./runPsLayout.sh %s %s %s {check out line+ psl/%s/%s/%s.psl}\n",
        $1, $2, $3, $1, $3, $2
}' > masterJobList

    #	To do a quick test, run just chrM:
    grep " M " masterJobList > jobList
s
    para create jobList
    para try ... check ... etc ...


    #	Then, the whole run:
    rm -fr psl err
    para create masterJobList
    para try ... check ... push ... etc ...
    #	running 2006-01-17  16:41

    #	We need the phase information from the sequence.inf file:
    ssh hgwdev
    cd /cluster/data/hg18/bed/coverage
    cp /cluster/data/hg17/phase.pl .
    #	this script was fixed up for hg18 to take an argument to the
    #	sequence.inf file:
    ./phase.pl /cluster/data/hg18/ncbi/sequence.inf > phase.txt
    #	what kind of phases do we have:
    awk '{print $2}' phase.txt | sort | uniq -c
    #	  1134 D
    #	562513 F
    #	 17270 P
    #	Compared to hg17 we had:
    awk '{print $2}' /cluster/data/hg17/phase.txt | sort | uniq -c
    #	  1088 D
    #	146900 F
    #	 17300 P

    #	Back in the kluster runPlaced directory, we put together the
    #	kluster run results with:
    ssh pk
    mkdir /san/sanvol1/scratch/hg18/coverage/runPlaced/filteredLifted
    cd /san/sanvol1/scratch/hg18/coverage/runPlaced/filteredLifted

    cat << '_EOF_' > filterLift.sh
#!/bin/sh

for C in 22
do
    echo -n "chr${C} working ... "
    mkdir -p ${C}
    OUT="${C}/filterLift.out"
    pslSort dirs ${C}/raw.psl tmp ../psl/${C}/N* > ${OUT} 2>&1
    pslReps -singleHit -nearTop=0.001 ${C}/raw.psl ${C}/repsSingle.psl \
        /dev/null >> ${OUT} 2>&1
    liftUp ${C}/chr${C}.psl ../../maskedContigs/liftContigs.lft warn \
        ${C}/repsSingle.psl >> ${OUT} 2>&1
    clusterClone -agp -minCover=80 -maxGap=60000 ${C}/repsSingle.psl \
        > ${C}/single.agp 2>> ${OUT} 2>&1
    liftUp ${C}/rawLifted.psl ../../maskedContigs/liftContigs.lft warn \
        ${C}/raw.psl >> ${OUT} 2>&1
    clusterClone -agp -minCover=80 -maxGap=60000 ${C}/chr${C}.psl \
        > ${C}/chr${C}.bed 2>> ${OUT}
    echo "done"
done
'_EOF_'
    #	happy emacs
    chmod +x filterLift.sh
    time ./filterLift.sh

    cp /cluster/data/hg17/fixPhase.pl .
    #	fixed up the script to take an argument pointing to the phase.txt file

    ssh kkstore02
    cd /cluster/data/hg18
    grep "for_assembly" ncbi/sequence.inf \
	| sed -e "s/\tW\t/\t3\t/;" > sequence.inf
    cd /cluster/store11/gs.19/ffa
    ln -s ../build36/sequence.inf .

    ssh hgwdev
    cd /cluster/data/hg18
    #	currently working only on chr22
    echo "22" > clonePos.list
    #	need to reload gold gap *and* gl at this time.  gl wasn't loaded
    #	before this.  It is required for the clonePos track.
    hgGoldGapGl -chrom=chr22 hg18 /cluster/store11/gs.19 build36
    hgClonePos  -maxErr=3 -maxWarn=2000 -chromLst=clonePos.list \
        hg18 /cluster/data/hg18 ./sequence.inf /cluster/store11/gs.19 \
        2> clone.pos.errors


    #	OK, now for the hard part.  The unplaced clones.
    #	First we will make an attempt to determine which clones they
    #	belong to by using information from the previous build, the
    #	sequence.inf file, the seq_contig.md file, and the
    #	ncbi_build36.agp file.

    ssh kkstore02
    cd /cluster/data/hg18/bed/coverage
    comm -13 placed_in_assembly.list allClonesConsidered.list \
	> unplaced.clone.list
    comm -12 unplaced.clone.list allClones.InHg17AndHg18.list \
	> common.to.hg17.unplaced.list
    comm -23 unplaced.clone.list allClones.InHg17AndHg18.list \
	> unique.to.hg18.unplaced.list
 
    awk '{print $1,$6}' /cluster/data/hg17/contig_overlaps.agp \
	| sed -e "s/_[0-9]*$//" | sort -u > hg17.contig.clone.list

    awk '{print $1,$6}' ../../sequence.inf | sed -e "s/(//; s/)//" \
	> cloneToChrom.from.seq.inf.txt

    #	using the contig to clone information from Hg17, attempt to
    #	locate the common.to.hg17.unplaced.list in terms of chrom and
    #	contig.  Along with the ncbi_build36.agp, seq_contig.md and
    #	cloneToChrom.from.seq.inf.txt infomation, we can attempt to
    #	place clones that have perhaps moved, or don't have entries in
    #	one file or another.  The relationships obtained from the
    #	various files:
    #   ncbi_build36.agp - gives clone to contig name and clone to chr name
    #			but for placed clones only, not useful here
    #			unless they moved from hg17 (try this with the
    #			placed list)
    #   seq_contig.md - gives contig to chrom relationship

    ./chrCloneContig.pl /cluster/data/hg18/ncbi_build36.agp \
	hg17.contig.clone.list /cluster/data/hg18/seq_contig.md \
	    common.to.hg17.unplaced.list cloneToChrom.from.seq.inf.txt \
		> chrCloneContigCommonToHg17.list \
		    2> common.to.hg17.unplaced.stderr

    #	With this chrCloneContigCommonToHg17.list list in hand, can now
    #	create a hierarchy of ./unPlacedClones/
    ./createUnplacedHierarchy.sh

    #	Then, copy them to the san for kluster run
    ssh pk
    cd /san/sanvol1/scratch/hg18/coverage
    rsync -a --progress --copy-links \
	/cluster/data/hg18/bed/coverage/unPlacedClones/ ./unPlacedClones/


    mkdir runUnPlaced
    cd runUnPlaced
    #	create jobList from the chrCloneContigCommonToHg17.list
    egrep -v "^#|XX_000" \
	/cluster/data/hg18/bed/coverage/chrCloneContigCommonToHg17.list \
	| sed -e "s/.fa.gz//" \
	| awk '{
printf "./runPsLayout.sh %s %s %s {check out line+ psl/%s/%s/%s.psl}\n",
        $1, $2, $3, $1, $3, $2
}' > masterJobList

    #	Test a subset:
    grep " Y " masterJobList > jobListY

    para create jobListY
    para try ... check ... etc ...

###########################################################################
# FISH CLONES (WORKING - 2006-01-13 - Hiram)
# The STS Marker, Coverage, and BAC End Pairs tracks must be completed prior to 
# creating this track  (and why is this ?)

    ssh kkstore01
    mkdir /cluster/data/ncbi/fishClones/fishClones.2006-01/
    cd /cluster/data/ncbi/fishClones/fishClones.2006-01/

# Download information from NCBI
        # point browser at:
#   http://www.ncbi.nlm.nih.gov/genome/cyto/cytobac.cgi?CHR=all&VERBOSE=ctg
        # change "Sequence tag:" to "placed on contig"
        # change "Show details on sequence-tag" to "yes"
        # change "Download or Display" to "Download table for UNIX"
        # press Submit - save as
# /cluster/data/ncbi/fishClones/fishClones.2006-01/hbrc.txt
    chmod 664 /cluster/data/ncbi/fishClones/fishClones.2006-01/hbrc.txt
#	Unfortunately the format of this hbrc file has changed since
#	last time.  The columns have been rearranged, and one important
#	column is missing, the contig information.  So, let's see if we
#	can recover the original format by putting this together with
#	some other things we have here.

# Get current clone/accession information
    wget --timestamping http://www.ncbi.nlm.nih.gov/genome/clone/DATA/clac.out

# Create initial Fish Clones bed file
    ssh kkstore02
    mkdir /cluster/data/hg18/bed/fishClones
    cd /cluster/data/hg18/bed/fishClones

# Copy previous sts info from fhcrc (take from previous build in future)
    cp -p /cluster/data/ncbi/fishClones/fishClones.2004-07/fhcrc.sts .
#	This fhcrc.sts listing doesn't change.  It is merely a listing
#	of aliases that remain in effect.

    #	Create cl_acc_gi_len file form cloneend information:
    grep -v "^#" /cluster/data/hg18/bed/cloneend/all.txt \
    | awk '{gsub("\.[0-9]*$", "", $2);
	printf "%s\t%s\t%s\t%s\t%s\t%s\n", $1,$2,$3,$4,$5,$8}' > cl_acc_gi_len

XXXX - looking for backends business - 2006-01-20 - Hiram

    fishClones -fhcrc=fhcrc.sts -noBin hg18 \
	/cluster/data/ncbi/fishClones/fishClones.2006-01/hbrc.fixed \
	/cluster/data/ncbi/fishClones/fishClones.2006-01/clac.out \
         ./cl_acc_gi_len \
         /cluster/data/hg18/bed/bacends/bacEnds.psl \
            fishClones

	     
###########################################################################
# CHROMOSOME BANDS TRACK (WORKING - 2006-01-20 - Hiram)
# This must wait until the Fish Clones tracks is done
    ssh kolossus
    mkdir /cluster/data/hg18/bed/cytoband
    cd /cluster/data/hg18/bed/cytoband

# Copy in some necessary files (usually from previous version)
    cp -p /cluster/data/hg17/bed/cytoband/pctSetBands.txt .
    cp -p /cluster/data/hg17/bed/cytoband/ISCN800.txt .

# Create some preliminary information files
    /cluster/bin/scripts/createSetBands pctSetBands.txt \
	/cluster/data/hg18/inserts /cluster/data/hg18  100 > setBands.txt
    /cluster/bin/scripts/makeBands ISCN800.txt \
        /cluster/data/hg18 > cytobands.pct.bed
    /cluster/bin/scripts/makeBandRanges cytobands.pct.bed \
        > cytobands.pct.ranges
XXXX - how do we get this fishClones file ? - 2006-01-20
# Reformat fishClones file
    /cluster/bin/scripts/createBanderMarkers \
	/cluster/data/hg18/bed/fishClones/fishClones.bed > fishClones.txt

# Create bed file
    ssh eieio
    cd /cluster/data/hg18/bed/cytoband.kate
    /cluster/bin/scripts/runBander fishClones.txt \
	ISCN800.txt setBands.txt /cluster/data/hg18
    # NOTE: fails on kolossus (C++ compiler different ??)

    # Should be 862 bands
    wc  -l cytobands.bed
    # 862    cytobands.bed

###########################################################################
#  BLASTZ SELF (DONE - 2006-01-17 - 2006-01-20 - Hiram)

    ssh pk
    mkdir /cluster/data/hg18/bed/blastzSelf.2006-01-17
    cd /cluster/data/hg18/bed/blastzSelf.2006-01-17

    cat << '_EOF_' > DEF
# human vs human
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin:/parasol/bin

BLASTZ=blastz.v7.x86_64
BLASTZ_M=400

# TARGET: Human Hg18
SEQ1_DIR=/san/sanvol1/scratch/hg18/selfNib
SEQ1_LEN=/san/sanvol1/scratch/hg18/self.sizes
SEQ1_CHUNK=10000000
SEQ1_LAP=10000
SEQ1_IN_CONTIGS=0

# QUERY: Human Hg18
SEQ2_DIR=/san/sanvol1/scratch/hg18/selfNib
SEQ2_LEN=/san/sanvol1/scratch/hg18/self.sizes
SEQ2_CHUNK=10000000
SEQ2_LAP=0
SEQ2_IN_CONTIGS=0

BASE=/cluster/data/hg18/bed/blastzSelf.2006-01-17
TMPDIR=/scratch/tmp
'_EOF_'
    #	happy emacs

    cd /cluster/data/hg18/bed/blastzSelf.2006-01-17
    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-chainMinScore=10000 -chainLinearGap=medium -bigClusterHub=pk \
	`pwd`/DEF > blastz.out 2>&1 &
    #	real    640m37.637s

    ssh kolossus
    cd /cluster/data/hg18/bed/blastzSelf.2006-01-17
    time HGDB_CONF=~/.hg.conf.read-only featureBits \
	-noRandom -noHap hg18 chainSelfLink > fb.chainSelfLink 2>&1 &
    #	real    21m52.697s
    #	324067552 bases of 2858034764 (11.339%) in intersection

    #	compared to Hg17:
    cd /cluster/data/hg17/bed/blastzSelf.2004-07-01
    time HGDB_CONF=~/.hg.conf.read-only featureBits \
	-noRandom -noHap hg17 chainSelfLink > fb.chainSelfLink 2>&1 &
    #	real    56m34.802s
    #	240976607 bases of 2851352871 (8.451%) in intersection

    #	reloaded these chains to add normalized score column
    ssh hgwdev
    cd /cluster/data/hg18/bed/blastzSelf.2006-01-17/axtChain
    chainSplit chain hg18.hg18.all.chain.gz
    cd /cluster/data/hg18/bed/blastzSelf.2006-01-17/axtChain/chain
    foreach f (*.chain)
	set c = $f:r
	hgLoadChain -normScore hg18 ${c}_chainSelf $f
    end
    cd ..
    rm -fr chain

##############################################################################
# CLONE ENDS - BACEND TRACK (DONE - 2006-01-11 - Fan)

    ssh kkstore02
    cd /cluster/data/hg18
    # check disk space: 73Gb free
    df -h .
# Filesystem            Size  Used Avail Use% Mounted on
# /export/cluster/store11
                      1.8T  1.4T  323G  82% /cluster/store11

    mkdir -p bed/cloneend/ncbi
    cd bed/cloneend/ncbi

    wget --timestamping ftp://ftp.ncbi.nih.gov/genomes/CLONEEND/homo_sapiens/*

# Somehow the wget did not work.  Did it by hand. 

    cd /cluster/data/hg18/bed/cloneend
    # seems like the *.mfa files were split just for convenience
    # concatenate
    
    bash
    for F in ncbi/*.mfa.gz
    do
	zcat ${F}
    done | gzip > all.mfa.gz

    exit

    # Convert the title line of the all.mfa file
    cat << '_EOF_' > convert.pl
#!/usr/bin/env perl

use strict;
use warnings;

while (my $line = <>) {
    if ($line !~ m/^>/) {
	print $line
    } else {
        my @fields = split('\|', $line);
	my $fieldCount = scalar(@fields);
        my $printed = 0;
        for (my $i = 0; $i < $fieldCount; $i++) {
                if ($fields[$i] eq "gb" || $fields[$i] eq "dbj" || $fields[$i] eq "emb") {
                        (my $name, my $vers) = split(/\./,$fields[$i+1]);
                        print ">$name\n";
                        $i= $fieldCount;
                        $printed = 1;
                }
        }
        if (!$printed) {
                die("Failed for $line\n");
        }
    }
}
'_EOF_'
    # < happy emacs
    chmod +x convert.pl
    zcat all.mfa | ./convert.pl | gzip > cloneEnds.fa.gz

    #	make sure nothing got broken:
    faSize all.mfa.gz
# 400704107 bases (5941742 N's 394762365 real 255711893 upper 139050472 lower) in 832860 sequences in 1 files
    faSize cloneEnds.fa.gz
# 400704107 bases (5941742 N's 394762365 real 255711893 upper 139050472 lower) in 832860 sequences in 1 files
    #	identical numbers

    # concatenate the text files, too
    bash
    for F in ncbi/*.txt.gz
    do
	zcat ${F}
    done | gzip > all.txt.gz

    # generate cloneEndPairs.txt and cloneEndSingles.txt
    cp -p /cluster/data/mm6/bed/cloneend/ncbi/convertTxt.pl .
    zcat all.txt.gz >all.txt
    ./convertTxt.pl all.txt


    # Reading in end info
    # Writing out pair info
    # Writing out singleton info
    # 249619 pairs and 318500 singles
    
    #	faSplit does not function correctly if given a .gz source file
    #	AND, we need the unzipped file for sequence loading below
    gunzip cloneEnds.fa.gz
    # split
    mkdir splitdir
    cd splitdir
    faSplit sequence ../cloneEnds.fa 100 cloneEnds
    #	Check to ensure no breakage:
    cat *.fa | faSize stdin
# 400704107 bases (5941742 N's 394762365 real 255711893 upper 139050472 lower) in 832860 sequences in 1 files
    #	same numbers as before

    #	Copy to san for cluster runs
    ssh pk
    cd /cluster/data/hg18/bed/cloneend/splitdir
    mkdir /san/sanvol1/scratch/hg18/cloneEnds
    cp -p *.fa /san/sanvol1/scratch/hg18/cloneEnds
    rm *
    cd ..
    rmdir splitdir

    # load sequences
    ssh hgwdev
    mkdir /gbdb/hg18/cloneend
    cd /gbdb/hg18/cloneend
      ln -s /cluster/data/hg18/bed/cloneend/cloneEnds.fa .
    cd /tmp
    hgLoadSeq hg18 /gbdb/hg18/cloneend/cloneEnds.fa
    #  Advisory lock created
    # Creating .tab file
    # Adding /gbdb/hg18/cloneend/cloneEnds.fa
    # 832860 sequences
    # Updating seq table
    # Advisory lock has been released
    # All done

############################################################################
# BACEND SEQUENCE ALIGNMENTS (STARTED - 2006-01-11, DONE 2006-01-18 - Fan)
    ssh kkstore02
    mkdir /cluster/data/hg18/noMask
    cd /cluster/data/hg18/
    #	Need an unmasked sequence for this work
    bash
    for CHR in ?/chr?.fa ??/chr??.fa ?/chr?_random.fa ??/chr??_random.fa *hap*/chr*.fa
    do
	C=`basename ${CHR}`
	echo -n "working ${C} ... "
	head -1 ${CHR} > noMask/${C}
	tail +2 ${CHR} | tr [:lower:] [:upper:] >> noMask/${C}
	echo "done"
    done
    mkdir ooc
    ls noMask/chr*.fa > fa.list

    #	The ooc file was created earlier into /cluster/bluearc/hg18/11.ooc
    cp -p /cluster/bluearc/hg18/11.ooc  /san/sanvol1/scratch/hg18/11.ooc

    ssh pk
    mkdir /san/sanvol1/scratch/hg18/noMask
    cd /san/sanvol1/scratch/hg18/noMask
    bash
    time cp --verbose -p /cluster/data/hg18/noMask/chr*.fa .
    
    # allow blat to run politely in /tmp while it writes output, then
    # copy results to results file:

    mkdir /cluster/data/hg18/bed/bacends
    cd    /cluster/data/hg18/bed/bacends

    cat << '_EOF_' > runBlat.sh
#!/bin/sh
path1=$1
path2=$2
root1=$3
root2=$4
result=$5
rm -fr /tmp/${root1}_${root2}
mkdir /tmp/${root1}_${root2}
pushd /tmp/${root1}_${root2}
/cluster/bin/x86_64/blat ${path1} ${path2} \
	-ooc=/san/sanvol1/scratch/hg18/11.ooc ${root1}.${root2}.psl
popd
rm -f ${result}
mv /tmp/${root1}_${root2}/${root1}.${root2}.psl ${result}
rm -fr /tmp/${root1}_${root2}
'_EOF_'
    # << emacs happy
    chmod +x runBlat.sh

    cat << '_EOF_' > template
#LOOP
./runBlat.sh {check in exists $(path1)} {check in exists $(path2)} $(root1) $(root2) {check out line+ bacEnds.out/$(root2)/$(root1).$(root2).psl}
#ENDLOOP
'_EOF_'
    # << emacs happy

    ls -1S /san/sanvol1/scratch/hg18/cloneEnds/cloneEnds???.fa > bacEnds.lst
    mkdir bacEnds.out
    #	create results directories for each to avoid the all result files in
    #	one directory problem
    foreach f (`cat bacEnds.lst`)
	set b = $f:t:r
	echo $b
	mkdir bacEnds.out/$b
    end

    ls -1S /san/sanvol1/scratch/hg18/noMask/chr*.fa > contig.lst
    gensub2 contig.lst bacEnds.lst template jobList
    para create jobList
# 4802 jobs in batch
    para try, check, push, etc ...

# There was some problem with many jobs that files not accessible.  Did chmod +x on 
# /san/sanvol1/scratch/hg18/noMask/chr*.fa and this seems to fix the problem.

# Per conversation with Kate and Hiram during code review, they think the 
# above, as suggested by Patrick (if Fan remembered correctly) is a myth.

# Completed: 4802 of 4802 jobs
# CPU time in finished jobs:     515910s    8598.50m   143.31h    5.97d  0.016 y
# IO & Wait Time:                 35141s     585.68m     9.76h    0.41d  0.001 y
# Average job time:                 115s       1.91m     0.03h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            1185s      19.75m     0.33h    0.01d
# Submission to last job:          2304s      38.40m     0.64h    0.03d

    ssh kkstore02
    cd /cluster/data/hg18/bed/bacends
    screen

    mv temp temp_old
    mv split split_old

    mkdir temp
    bash
    time pslSort dirs raw.psl temp bacEnds.out/* > pslSort.out 2>&1 &
# real    8m33.346s
# user    4m58.200s
# sys     0m33.400s

    time pslReps -nearTop=0.01 -minCover=0.7 -minAli=0.8 -noIntrons \
	raw.psl bacEnds.psl /dev/null > pslReps.out 2>&1 &

    cp -p /cluster/data/mm7/bed/bacends/split.pl .
    cp -p /cluster/data/mm7/bed/bacends/header .

# Commented out line 23 in split.pl to avoid generating extra header in split/...0.psl.
#print OUT $header;

    time ./split.pl header < bacEnds.psl

    cp -p bacEnds.psl bacEnds.psl.save
    time pslSort dirs bacEnds.psl temp split

    #	~ 1.5 minutes

    # Copy files to final destination and remove
    mkdir /cluster/data/hg18/bacends
    cp -p bacEnds.psl /cluster/data/hg18/bacends

############################################################################
# BACEND PAIRS TRACK (DONE - 2006-01-18 - Fan)

    ssh kolossus
    cd /cluster/data/hg18/bacends
    bash

time /cluster/bin/x86_64/pslPairs -tInsert=10000 -minId=0.91 -noBin -min=25000 \
-max=350000 -slopval=10000 -hardMax=500000 -slop -short -long -orphan \
-mismatch -verbose bacEnds.psl \
../bed/cloneend/cloneEndPairs.txt all_bacends bacEnds

    # create header required by "rdb" tools
echo -e \
"chr\tstart\tend\tclone\tscore\tstrand\tall\tfeatures\tstarts\tsizes" > header
echo -e "10\t10N\t10N\t10\t10N\t10\t10\t10N\t10\t10" >> header

cat header bacEnds.pairs | \
/cluster/bin/scripts/row score ge 300 | \
/cluster/bin/scripts/sorttbl chr start | \
/cluster/bin/scripts/headchg -del > bacEndPairs.bed

cat header bacEnds.slop bacEnds.short bacEnds.long bacEnds.mismatch \
bacEnds.orphan | /cluster/bin/scripts/row score ge 300 | \
/cluster/bin/scripts/sorttbl chr start | \
/cluster/bin/scripts/headchg -del > bacEndPairsBad.bed

/cluster/bin/scripts/extractPslLoad -noBin bacEnds.psl bacEndPairs.bed \
bacEndPairsBad.bed >j1.out
cat j1.out| /cluster/bin/scripts/sorttbl tname tstart >j2.out
cat j2.out | /cluster/bin/scripts/headchg -del > bacEnds.load.psl

rm j1.out j2.out

    #	CHECK bacEndPairs.bed ID's to make sure they have no blanks in them
    awk '{print $5}' bacEndPairs.bed | sort -u
    #	result should be the scores, no extraneous strings:
#	1000
#	300
#	375
#	500
#	750
    #	edit the file and fix it if it has a bad name.

    # load into database
    ssh hgwdev
    cd /cluster/data/hg18/bacends
    hgLoadBed -strict -notItemRgb hg18 bacEndPairs bacEndPairs.bed \
	-sqlTable=$HOME/kent/src/hg/lib/bacEndPairs.sql
    # Loaded 146284 elements of size 11

    # note - this track isn't pushed to RR, just used for assembly QA
    hgLoadBed -strict -notItemRgb hg18 bacEndPairsBad bacEndPairsBad.bed \
	-sqlTable=$HOME/kent/src/hg/lib/bacEndPairsBad.sql
    # Loaded 75995 elements of size 11

    # NOTE: truncates file to 0 if -nobin is used
    hgLoadPsl hg18 -table=all_bacends bacEnds.load.psl

    nice featureBits hg18 all_bacends
# 162081172 bases of 2881515245 (5.625%) in intersection
    nice featureBits hg17 all_bacends
# 225763317 bases of 2866216770 (7.877%) in intersection

    nice featureBits hg18 bacEndPairs
# 2835522069 bases of 2881515245 (98.404%) in intersection
    nice featureBits hg17 bacEndPairs
# 2846568377 bases of 2866216770 (99.314%) in intersection

    nice featureBits hg18 bacEndPairsBad
# 781697678 bases of 2881515245 (27.128%) in intersection
    nice featureBits hg17 bacEndPairsBad
# 797412909 bases of 2866216770 (27.821%) in intersection

##########################################################################
# BLASTZ OPOSSUM monDom2 (STARTED - 2006-01-23 - Hiram)

    ssh pk
    mkdir /cluster/data/hg18/bed/blastzMonDom2.2006-01-23
    cd /cluster/data/hg18/bed/blastzMonDom2.2006-01-23

    cat << '_EOF_' > DEF
# human vs. opossum
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/parasol/bin

BLASTZ=blastz.v7.x86_64

# Specific settings for chicken (per Webb email to Brian Raney)
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=10000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q
BLASTZ_ABRIDGE_REPEATS=0

# TARGET: Human (hg18)
SEQ1_DIR=/scratch/hg/hg18/nib
SEQ1_LEN=/scratch/hg/hg18/chrom.sizes
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY: Opossum monDom2
SEQ2_DIR=/san/sanvol1/scratch/monDom2/monDom2.2bit
SEQ2_LEN=/san/sanvol1/scratch/monDom2/chrom.sizes
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=30000000
SEQ2_LIMIT=100
SEQ2_LAP=0

BASE=/cluster/data/hg18/bed/blastzMonDom2.2006-01-23
TMPDIR=/scratch/tmp
'_EOF_'
    #	happy emacs

    cd /cluster/data/hg18/bed/blastzMonDom2.2006-01-23
    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
	`pwd`/DEF > blastz.out 2>&1 &
    #	real    912m22.818s

    #	This failed during the load of the chains due to the size of
    #	chr19.chain.  So, go to kolossus:
    ssh kolossus
    #	There isn't any hg18 db here yet, get it established with a
    #	chromInfo and a 2bit sequence:
    hgsql -e "create database hg18;" mysql
    cd /cluster/data/hg18
    twoBitInfo hg18.2bit stdout |
        awk '{printf "%s\t%s\t/gbdb/hg18/hg18.2bit\n", $1,$2}' \
		> chromInfo.kolossus.tab
    hgsql hg18 < $HOME/kent/src/hg/lib/chromInfo.sql
    hgsql hg18 \
-e 'load data local infile "chromInfo.kolossus.tab" into table chromInfo;'
    mkdir /gbdb/hg18
    ln -s /cluster/data/hg18/hg18.2bit /gbdb/hg18/hg18.2bit
    #	now, loading only chr19:
    cd /cluster/data/hg18/bed/blastzMonDom2.2006-01-23/axtChain
    hgLoadChain hg18 chr19_chainMonDom2 chain/chr19.chain
    #	while that is running, back on hgwdev, get the other chains loaded
    ssh hgwdev
    cd /cluster/data/hg18/bed/blastzMonDom2.2006-01-23/axtChain
    cp loadUp.csh loadUp.noChr19.csh
    #	change the foreach line to eliminate the chr19.chain:
    diff loadUp.csh loadUp.noChr19.csh
    < foreach f (*.chain)
    ---
    > foreach f (`ls *.chain | grep -v chr19.chain`)
    #	And then run that script
    time ./loadUp.noChr19.csh > load.noChr19.out 2>&1
    
    #	When the kolossus load finishes, email to push-request and ask
    #	for the two tables to be pushed from kolossus to hgwdev:
    #	chr19_chainMonDom2
    #	chr19_chainMonDom2Link

    #	then, continuing:
    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-continue=download -bigClusterHub=pk -chainMinScore=5000 \
	-chainLinearGap=loose `pwd`/DEF > download.out 2>&1 &
<<<<<<< makeHg18.doc
    #	real    2m42.505s

    ssh kolossus
    cd /cluster/data/hg18/bed/blastz.monDom2
    time HGDB_CONF=~/.hg.conf.read-only featureBits \
	hg18 chainMonDom2Link > fb.hg18.chainMonDom2Link 2>&1
    #	real    124m34.435s
    cat fb.hg18.chainMonDom2Link
    #	357258631 bases of 2881515245 (12.398%) in intersection

    #	then, to swap
    ssh pk
    cd /cluster/data/hg18/bed/blastzMonDom2.2006-01-23
    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-swap -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
	`pwd`/DEF > swap.out 2>&1 &
    #	running 2006-01-25 17:28
    #	real    51m27.447s
    #	this swap failed at:
    #	startStep: 4, at step 5 net to stopStep 9
    #	netChains: looks like previous stage was not successful
    #	(can't find [monDom2.hg18.]all.chain[.gz]).
    #	This failure does not make any sense.  The end of swapChains
    #	does an nfsNoodge on this file to verify it exists.
    #	I don't understand why it wouldn't be in existence
    #	as netChains starts up.
    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-swap -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
	-continue=net `pwd`/DEF > net-swap.out 2>&1 &
    #	running 2006-01-26 09:28
    #	real    27m57.077s
    #	This swap failed at the load chain:
    #	startStep: 5, at step 6 load to stopStep 9
    #	# chmod a+x
    #	# /cluster/data/monDom2/bed/blastz.hg18.swap/axtChain/loadUp.csh
    #	# ssh -x hgwdev nice
    #	# /cluster/data/monDom2/bed/blastz.hg18.swap/axtChain/loadUp.csh
    #	cd /cluster/data/monDom2/bed/blastz.hg18.swap/axtChain
    #	hgLoadChain -tIndex monDom2 chainHg18 monDom2.hg18.all.chain.gz
    #	Out of memory needMem - request size 56 bytes

    #	So, over to kolossus to give it a try:

    #	There isn't any monDom2 db here yet, get it established with a
    #	chromInfo and a 2bit sequence:
    hgsql -e "create database monDom2;" mysql
    cd /cluster/data/monDom2
    hgsql monDom2 < $HOME/kent/src/hg/lib/chromInfo.sql
    hgsql monDom2 \
-e 'load data local infile "chromInfo.tab" into table chromInfo;'
    mkdir /gbdb/monDom2
    ln -s /cluster/data/monDom2/monDom2.2bit /gbdb/monDom2/monDom2.2bit
    #	now, loading into monDom2
    cd /cluster/data/monDom2/bed/blastz.hg18.swap/axtChain
    time hgLoadChain -tIndex monDom2 chainHg18 monDom2.hg18.all.chain.gz \
	> kolossus.load
    #	running - 2006-01-26

##########################################################################
#  test BLASTZ Opossum MonDom1  (WORKING - 2006-01-30 - Hiram)
#	to see what happened with the blow up of data in monDom2
#

    ssh kk
    mkdir /cluster/data/hg18/bed/blastzMonDom1.2006-01-30
    cd /cluster/data/hg18/bed/blastzMonDom1.2006-01-30

    cat << '_EOF_' > DEF
# human vs. opossum
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/parasol/bin

BLASTZ=blastz.v7

# Specific settings for chicken (per Webb email to Brian Raney)
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=10000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q
BLASTZ_ABRIDGE_REPEATS=0

# TARGET: Human (hg18)
SEQ1_DIR=/scratch/hg/hg18/nib
SEQ1_LEN=/scratch/hg/hg18/chrom.sizes
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY: Opossum monDom1
SEQ2_DIR=/iscratch/i/monDom1/chunks
SEQ2_LEN=/iscratch/i/monDom1/chrom.sizes
SEQ2_IN_CONTIGS=1
SEQ2_CHUNK=10000000
SEQ2_LIMIT=100
SEQ2_LAP=0

BASE=/cluster/data/hg18/bed/blastzMonDom1.2006-01-30
TMPDIR=/scratch/tmp
'_EOF_'
    #	happy emacs

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=kk -chainMinScore=5000 -chainLinearGap=loose \
	`pwd`/DEF > blastz.out 2>&1 &
    #	started 2006-01-30 - 15:40

############################################################################
# STS MARKERS (STARTED 2006-01-27 Fan)

   # update from NCBI 
    ssh kkstore02
    # use store11 for space
    mkdir -p /cluster/store11/sts.2006-01
    ln -s /cluster/store11/sts.2006-01 /cluster/data/ncbi
    ln -s /cluster/data/ncbi/sts.2006-01 sts.10
    cd /cluster/data/ncbi/sts.2006-01
    wget --timestamping ftp://ftp.ncbi.nih.gov/repository/UniSTS/UniSTS.sts
    wget --timestamping ftp://ftp.ncbi.nih.gov/repository/UniSTS/UniSTS.aliases
# old
#    wget --timestamping ftp://ftp.ncbi.nih.gov/repository/dbSTS/dbSTS.sts
#    wget --timestamping ftp://ftp.ncbi.nih.gov/repository/dbSTS/dbSTS.aliases
    wget --timestamping ftp://ftp.ncbi.nih.gov/blast/db/FASTA/sts.gz
    gunzip sts.gz
    mv sts dbSTS.fa

    cp -p /cluster/data/ncbi/sts.9/all.STS.fa.prev .
    cp -p /cluster/data/ncbi/sts.9/stsInfo2.bed.prev .

    # Convert dbSTS.fa file to easier reading format, and get accessions
    /cluster/bin/scripts/convertGbFaFile dbSTS.fa > UniSTS.convert.fa
    grep ">" UniSTS.convert.fa | cut -f 2 -d ">" > UniSTS.acc

    # NOTE: updateStsInfo creates new stsInfo2.bed, all.primers, 
    #   all.STS.fa, stsAlias.bed files 
    updateStsInfo -verbose=1 -gb=UniSTS.acc stsInfo2.bed.prev all.STS.fa.prev \
    UniSTS.sts UniSTS.aliases UniSTS.convert.fa new

# Reading current stsInfo file
# Reading genbank accession file
# Reading current dbSTS.sts file
# Segmentation fault

###########################################################################
# CREATE HAPLOTYPEPOS TRACK (DONE 1/31/06, Fan)

  ssh kkstore02
  cd /cluster/data/hg18/bed

  mkdir haplotypePos
  cd haplotypePos

  cp /cluster/data/hg18/*hap*/*.fa . -p
  ls *.fa|sed -e 's/chr/split1 chr/' |sed -e 's/.fa//' >splitAll

cat << '_EOF_' > split1
echo processing $1
faSplit2 -lift=$1.lft -overlap=500 size $1.fa 3500 split/$1
'_EOF_'

chmod +x split*
mkdir split
mkdir result
splitAll

ls ./split/*.fa > split.lst

cat << '_EOF_' > gsub
#LOOP
/cluster/store11/gs.19/build36/bed/haplotypePos/hblat1 $(file1) {check out line+ /cluster/store11/gs.19/build36/bed/haplotypePos/result/$(root1).psl}
#ENDLOOP
'_EOF_'

gensub2 split.lst single gsub jobList

ssh pk
cd /cluster/data/hg18/bed/haplotypePos
mkdir result

para create jobList
para try, push, check ...

# Completed: 3091 of 3092 jobs
# Crashed: 1 jobs
# CPU time in finished jobs:      33164s     552.73m     9.21h    0.38d  0.001 y
# IO & Wait Time:                172783s    2879.72m    48.00h    2.00d  0.005 y
# Average job time:                  67s       1.11m     0.02h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:             300s       5.00m     0.08h    0.00d
# Submission to last job:           743s      12.38m     0.21h    0.01d

# The single job that crashed was due to chr5_h2_hap1368.fa, which
# does not have a decent alignment on chr5. 

# collect BLAT results
cat result/*.psl >all.psl

# keep the main alignments
pslReps -nohead -minCover=0.80 -minAli=0.80 -nearTop=0.002 all.psl all_filtered.psl all.psr

cat chr*.lft > hap.lft
liftUp lifted.psl hap.lft warn all_filtered.psl -pslQ

mkdir tNibs qNibs
cp -p /cluster/data/hg18/nib/*hap*.nib qNibs

cp -p /cluster/data/hg18/nib/chr5.nib tNibs
cp -p /cluster/data/hg18/nib/chr6.nib tNibs
cp -p /cluster/data/hg18/nib/chr22.nib tNibs

axtChain -psl -linearGap=medium lifted.psl tNibs qNibs out.chain

chainAntiRepeat tNibs qNibs out.chain final.chain

cat << '_EOF_' > hap.chrom.lis
/cluster/data/hg18/nib/chr5.nib
/cluster/data/hg18/nib/chr6.nib
/cluster/data/hg18/nib/chr22.nib
'_EOF_'

ls *.fa >q.lis

chainToPsl final.chain /cluster/data/hg18/chrom.sizes \
/cluster/data/hg18/chrom.sizes hap.chrom.lis q.lis haplotypePos.psl
# took about 20 minutes

hgLoadPsl hg18 haplotypePos.psl

# add haplotypePos entry in trackDb.ra

###########################################################################
# LOAD AFFYRATIO (DONE - 2006-02-01 - Fan)
#	Copied from Hg17 doc
    # Set up cluster job to align consenesus/exemplars to hg18
    ssh kkstore02
    mkdir /cluster/bluearc/hg18/affyGnf
    cp -p /projects/compbio/data/microarray/affyGnf/sequences/HG-U95/HG-U95Av2_all.fa \
    /cluster/bluearc/hg18/affyGnf

    ssh kkr1u00
    mkdir -p /iscratch/i/affyGnf
    cp -p /cluster/bluearc/hg18/affyGnf/* /iscratch/i/affyGnf
    /cluster/bin/iSync

    ssh kki
    mkdir /cluster/data/hg18/bed/affyGnf.2004-06-09
    cd /cluster/data/hg18/bed/affyGnf.2004-06-09
    ls -1 /iscratch/i/affyGnf/* > affy.lst
    ls -1 /iscratch/i/gs.19/build36/maskedContigs/* > allctg.lst
    cat << '_EOF_' > template.sub
#LOOP
/cluster/bin/i386/blat -fine -mask=lower -minIdentity=95 -ooc=/cluster/bluearc/hg18/11.ooc  $(path1) $(path2) {check out line+ psl/$(root1)_$(root2).psl}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    gensub2 allctg.lst affy.lst template.sub jobList
    mkdir psl
    para create jobList
    para try, push, check
# Completed: 378 of 378 jobs
# CPU time in finished jobs:       3055s      50.91m     0.85h    0.04d  0.000 y
# IO & Wait Time:                  1267s      21.12m     0.35h    0.01d  0.000 y
# Average job time:                  11s       0.19m     0.00h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:              78s       1.30m     0.02h    0.00d
# Submission to last job:           367s       6.12m     0.10h    0.00d

    # Do sort, best in genome filter, and convert to chromosome coordinates
    # to create affyU95.psl
    ssh kkstore02
    cd /cluster/data/hg18/bed/affyGnf.2004-06-09
    pslSort dirs raw.psl tmp psl

    # change filter parameters for these sequences. only use alignments that
    # cover 30% of sequence and have at least 95% identity in aligned
    # region. 
    # minAli = 0.97 too high. low minCover as a lot of n's in these
    # sequences
    pslReps -minCover=0.3 -minAli=0.95 -nearTop=0.005 raw.psl contig.psl /dev/null
    liftUp affyU95.psl ../../jkStuff/liftAll.lft warn contig.psl
    #   Eliminate the long names
    sed -e "s/U95Av2://" affyU95.psl | sed -e "s/;//" > affyU95shortQname.psl

    # Merge with spot data and load into database. added -chip flag to 
    # affyPslAndAtlasToBed to allow correct parsing
    ssh hgwdev
    cd /cluster/data/hg18/bed/affyGnf.2004-06-09

    bash
    /cluster/home/sugnet/bin/i386/affyPslAndAtlasToBed -chip=U95Av2 \
	affyU95shortQname.psl \
	/projects/compbio/data/microarray/affyGnf/human_atlas_U95_gnf.noquotes.txt \
	affyRatio.bed affyRatio.exr > affyPslAndAtlasToBed.log 2>&1

    hgLoadBed -sqlTable=$HOME/src/hg/lib/affyRatio.sql hg18 \
	affyRatio affyRatio.bed
    # Loaded 13043 elements of size 15

    mkdir affyU95
    hgLoadPsl hg18 -table=affyU95 affyU95shortQname.psl
    # sequences loaded 2006-02-1
    hgLoadSeq -abbr=U95Av2: hg18 /gbdb/hgFixed/affyProbes/HG-U95Av2_all.fa
    #	Advisory lock created
    #	Creating .tab file
    #	Adding /gbdb/hgFixed/affyProbes/HG-U95Av2_all.fa
    #	12386 sequences
    #	Updating seq table
    #	Advisory lock has been released
    #	All done

# Load AFFYUCLANORM, extended version of affyUcla track. Hopefully
# final freeze of data set.		(DONE - 2006-02-01 - Fan)
    ssh hgwdev
    mkdir /cluster/data/hg18/bed/affyUclaNorm
    cd /cluster/data/hg18/bed/affyUclaNorm

    cp -p /projects/compbio/data/microarray/affyUcla/sequences/HG-U133AB_all.fa .

    ssh pk
    cd /cluster/data/hg18/bed/affyUclaNorm
    ls -1 /scratch/hg/gs.19/build36/maskedContigs/* > contig.lst

    cat << '_EOF_' > gsub
#LOOP
/cluster/bin/x86_64/blat -fine -mask=lower -minIdentity=95 -ooc=/scratch/hg/h/11.ooc  $(path1) $(path2) {check out line+ psl/$(root1)_$(root2).psl}
#ENDLOOP
'_EOF_'
    # << keep emacs happy

    mkdir psl
    ls HG-U133AB_all.fa > affy.lst
    gensub2 contig.lst affy.lst gsub jobList
    para create jobList
    para try
    para check
    para push ... etc
# Completed: 378 of 378 jobs
# CPU time in finished jobs:       6766s     112.77m     1.88h    0.08d  0.000 y
# IO & Wait Time:                  1541s      25.68m     0.43h    0.02d  0.000 y
# Average job time:                  22s       0.37m     0.01h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:             202s       3.37m     0.06h    0.00d
# Submission to last job:           302s       5.03m     0.08h    0.00d

    ssh kkstore02
    cd /cluster/data/hg18/bed/affyUclaNorm
    pslSort dirs hg18.affyU133AB_all.psl tmp psl
    wc hg18.affyU133AB_all.psl
    # 62043  1302842 13163424 hg18.affyU133AB_all.psl

    liftUp hg18.affyU133AB_all.lifted.psl \
	/cluster/data/hg18/jkStuff/liftAll.lft warn hg18.affyU133AB_all.psl 
    pslReps -minCover=0.5 -sizeMatters -minAli=0.97 \
	-nearTop=0.005  hg18.affyU133AB_all.lifted.psl \
	hg18.affyU133AB_all.lifted.pslReps.psl out.psr
    # Processed 62038 alignments
    ~/kent/src/hg/affyGnf/affyUclaMergePslData \
    -pslFile=hg18.affyU133AB_all.lifted.pslReps.psl \
	-affyFile=/projects/compbio/data/microarray/affyUcla/data/feature_biomaterial_chip_logratios_formatForTrack.txt \
	-bedOut=hg18.affyUcla.bed \
	-expRecordOut=hg18.affyUcla.expRecords \
	-expFile=/projects/compbio/data/microarray/affyUcla/data/expNames.sorted.txt

    ~/kent/src/hg/affyGnf/addUclaAnnotations.pl hg18.affyUcla.expRecords \
	/projects/compbio/data/microarray/affyUcla/data/normal_tissue_database_annotations2.txt > hg18.affyUcla.annotations.expRecords

    # Load the databases
    ssh hgwdev
    cd /cluster/data/hg18/bed/affyUclaNorm
    sed -e 's/affyRatio/affyUclaNorm/' ~/kent/src/hg/lib/affyRatio.sql > affyUclaNorm.sql
    hgLoadBed hg18 affyUclaNorm hg18.affyUcla.bed -sqlTable=affyUclaNorm.sql

############################################################################
# MAKE AFFY U133 - made after above affyUclaNorm (DONE - 2006-02-01 - Fan)
    #	Someday the names can be fixed.
    ssh hgwdev
    mkdir /cluster/data/hg18/bed/affyU133
    cd /cluster/data/hg18/bed/affyU133
    ln -s ../affyUclaNorm/hg18.affyU133AB_all.lifted.pslReps.psl affyU133.psl
    
    hgLoadPsl hg18 affyU133.psl
    hgsql -e "select count(*) from affyU133;" hg18
    #	row count in hg17: 44620, in hg18: 45559
    hgLoadSeq hg18 /gbdb/hgFixed/affyProbes/HG-U133AB_all.fa
    #	44792 sequences

# GNF ATLAS 2 (DONE - 2006-02-01 - Fan)
    # Align probes from GNF1H chip.
    ssh pk
    cd /cluster/data/hg18/bed
    mkdir -p geneAtlas2/run/psl
    cd geneAtlas2/run
    #	This bluearc/geneAtlas2 directory already exists
    # mkdir -p /cluster/bluearc/geneAtlas2
    # cp /projects/compbio/data/microarray/geneAtlas2/human/gnf1h.fa /cluster/bluearc/geneAtlas2
    ls -1 /scratch/hg/gs.19/build36/maskedContigs > genome.lst
    ls -1 /cluster/bluearc/geneAtlas2/gnf1h.fa > mrna.lst
    cat << '_EOF_' > gsub
#LOOP
/cluster/bin/x86_64/blat -fine -ooc=/scratch/hg/h/11.ooc  /scratch/hg/gs.19/build36/maskedContigs/$(path1) $(path2) {check out line+ psl/$(root1)_$(root2).psl}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    gensub2 genome.lst mrna.lst gsub jobList
    para create jobList
    para try
    para check
    para push
    para time
# Completed: 378 of 378 jobs
# CPU time in finished jobs:       4038s      67.29m     1.12h    0.05d  0.000 y
# IO & Wait Time:                  2182s      36.37m     0.61h    0.03d  0.000 y
# Average job time:                  16s       0.27m     0.00h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:             250s       4.17m     0.07h    0.00d
# Submission to last job:           322s       5.37m     0.09h    0.00d
# Estimated complete:                 0s       0.00m     0.00h    0.00d

    # Do sort, best in genome filter, and convert to chromosome coordinates
    # to create gnf1h.psl.
    pslSort dirs raw.psl tmp psl
    pslReps -minCover=0.3 -minAli=0.95 -nearTop=0.005 raw.psl contig.psl /dev/null
    # Processed 79733 alignments
    liftUp ../affyGnf1h.psl ../../../jkStuff/liftAll.lft warn contig.psl
    rm -r contig.psl raw.psl psl

    # Load probes and alignments from GNF1H into database.
    ssh hgwdev
    cd /cluster/data/hg18/bed/geneAtlas2
    #	Already symlinked
    # ln -s /projects/compbio/data/microarray/geneAtlas2/human/gnf1h.fa \
    #	/gbdb/hgFixed/affyProbes
    hgLoadPsl hg18 affyGnf1h.psl
    hgLoadSeq hg18 /gbdb/hgFixed/affyProbes/gnf1h.fa

    grep -v U133B ../affyUclaNorm/hg18.affyU133AB_all.lifted.pslReps.psl \
	| sed -e "s/exemplar://; s/consensus://; s/U133A://" \
	| sed -e "s/;//" > affyU133A.psl

    hgMapMicroarray gnfAtlas2.bed hgFixed.gnfHumanAtlas2MedianRatio \
    	affyU133A.psl  /cluster/data/hg18/bed/geneAtlas2/affyGnf1h.psl

    # Loaded 44696 rows of expression data from hgFixed.gnfHumanAtlas2MedianRatio
    # Mapped 32926,  multiply-mapped 2000, missed 48, unmapped 11770

    hgLoadBed hg18 gnfAtlas2 gnfAtlas2.bed
    # Loaded 34926 elements of size 15

