#!/bin/csh
exit;

set dir = /cluster/data/encode/ortho3/mercatorInput
mkdir -p $dir
cd $dir

# start with Mercator data from Colin Dewey, and copy it to working directory: 
#    /cluster/data/encode/ortho3/mercatorInput/encodeMaps20050304.tar.gz
#
#-----Original Message-----
#From: Colin Dewey [mailto:cdewey@eecs.berkeley.edu] 
#Sent: Friday, March 04, 2005 1:09 PM
#To: Daryl Thomas
#Subject: Re: hg17 ENCODE region coordinates
#
#Hi Daryl,
#
#Thanks for the coordinates.  I should have realized that I could grab 
#these from the table browser, sorry!  The table browser is very nice 
#now... I was happily surprised that I could do table joins!
#
#Here are our new orthology mappings.  Everything is in the same form as 
#last time, so hopefully there will be no problems.  But do let me know 
#if you run into anything.
#
#Also, it seems that our group has yet to get alignments to you in MAF 
#format.  So I've decided to take on this task.  When I'm done, perhaps 
#I can send you our alignments from the last freeze just to make sure 
#that I have the formats right?
#
#Thanks,
#
#Colin

# ${HOME}/kent/src/hg/lib/encodeRegionMercator.sql
create table encodeRegionsMercator (
    bin        smallint(5)  unsigned not null,
    chrom      varchar(255)          not null,
    chromStart int(10)      unsigned not null,
    chromEnd   int(10)      unsigned not null,
    name       varchar(255)          not null,
    score      int(10)      unsigned not null,
    strand     char(1)               not null
)

#!/bin/csh -f
tar xfz encodeMaps20050304.tar.gz
chmod g+w $dir/*
foreach db (bosTau1 canFam1 danRer2 galGal2 mm5 monDom1 panTro1 rn3 tetNig1)
    echo ========= $db ==========
    rm -f $db.log $db.unmerged.bed5 $db.original.bed
    mv -f $db.bed $db.original.bed
    makeBed5.pl < $db.original.bed > $db.unmerged.bed5
    liftOverMerge -mergeGap=20000 -verbose=2 $db.unmerged.bed5 $db.bed5 >& $db.log
    hgLoadBed -tab -sqlTable=${HOME}/kent/src/hg/lib/encodeRegionsMercator.sql $db encodeRegionsMercator $db.original.bed >>& $db.log
    hgLoadBed -tab $db encodeRegionsMercatorMerged $db.bed5 >>& $db.log
end
rm -f bed.tab

########################################################################
#############  liftOver orthologous regions  ###########################
########################################################################
set dir = /cluster/data/encode/ortho3/liftOverRegions
mkdir -p $dir
cd $dir
hgsql hg17 -e "SELECT * FROM encodeRegions ORDER BY name" | tail +2 > hg17.bed
hgsql -h genome-testdb hgcentraltest -e "select toDb, path from liftOverChain " | grep /gbdb/hg17 | grep -v hg16 > hg17.over.chain.files

########################################################################
# CHIMP
    # THESE NOTES ARE FROM ORTHO2
    # used chains from reciprocal best net -- these were more
    # chopped up, but regions in these don't span unbridged gaps
    # minMatch .7 leaves ENr324 and ENm011 unmapped
    # Lowered match threshold until region coverage looked good (to .4)
    # (except m11, for some reason),  with minimum of fragmentation
    # Same at .3 and .2 -- at the point where regions look reasonable,
    # lowering the threshold in this way doesn't change them.
    # NOTE: reducing minSizeQ to 10000, due to fragmented panTro1 assembly
liftOver -minMatch=.01 -minSizeT=4000 \
    -minSizeQ=10000 -multiple -chainTable=hg17.chainPanTro1 \
    hg17.bed /cluster/data/hg17/bed/liftOver/hg17ToPanTro1.chain \
    panTro1.unmerged.bed5 panTro1.unmapped.bed -verbose=2 >& panTro1.log
liftOverMerge -mergeGap=20000 -verbose=2 panTro1.unmerged.bed5 panTro1.bed5 >>& panTro1.log
wc -l panTro1.*bed*
    #     81 panTro1.bed5
    #      0 panTro1.unmapped.bed
    #    107 panTro1.unmerged.bed5
awk '{printf "%s\t%s\t%s\t%s_%s\n", $1, $2, $3, $4, $5}' panTro1.bed5 > panTro1.bed
hgLoadBed -noBin panTro1 encodeRegionsLiftOver panTro1.bed >>& panTro1.log

########################################################################
# RAT
liftOver -minMatch=.01 -minSizeT=4000 \
    -minSizeQ=20000 -multiple -chainTable=hg17.chainRn3 \
    hg17.bed /cluster/data/hg17/bed/liftOver/hg17ToRn3.over.chain \
    rn3.unmerged.bed5 rn3.unmapped.bed -verbose=2 >& rn3.log
liftOverMerge -mergeGap=20000 -verbose=2 rn3.unmerged.bed5 rn3.bed5 >>& rn3.log
wc -l rn3.*bed*
    #     53 rn3.bed5
    #      0 rn3.unmapped.bed
    #     65 rn3.unmerged.bed5
awk '{printf "%s\t%s\t%s\t%s_%s\n", $1, $2, $3, $4, $5}' rn3.bed5 > rn3.bed
hgLoadBed -noBin rn3 encodeRegionsLiftOver rn3.bed >>& rn3.log

########################################################################
# CHICKEN
    # allow smaller chain sizes in query (only 1K, vs. 20K in close species)
liftOver -minMatch=.01 minSizeT=4000 \
    -multiple -minSizeQ=1000 -chainTable=hg17.chainGalGal2 \
    hg17.bed /cluster/data/hg17/bed/liftOver/hg17ToGalGal2.over.chain \
    galGal2.unmerged.bed5 galGal2.unmapped.bed -verbose=2 >& galGal2.log
liftOverMerge -mergeGap=20000 -verbose=2 galGal2.unmerged.bed5 galGal2.bed5 >>& galGal2.log
wc -l galGal2.*bed*
    #     94 galGal2.bed5
    #      4 galGal2.unmapped.bed
    #    131 galGal2.unmerged.bed5
    ##Partially deleted in new
    #chr13   29418015        29918015        ENr111
    #chr5    141880150       142380150       ENr212
awk '{printf "%s\t%s\t%s\t%s_%s\n", $1, $2, $3, $4, $5}' galGal2.bed5 > galGal2.bed
hgLoadBed -noBin galGal2 encodeRegionsLiftOver galGal2.bed >>& galGal2.log

########################################################################
# MOUSE MM5
liftOver -minMatch=.01 -minSizeT=4000 -minSizeQ=20000 -multiple \
    hg17.bed /cluster/data/hg17/bed/liftOver/hg17ToMm5.chain \
    mm5.unmerged.bed5 mm5.unmapped.bed -verbose=2 >& mm5.log
liftOverMerge -mergeGap=20000 -verbose=2 mm5.unmerged.bed5 mm5.bed5 >>& mm5.log
wc -l mm5.*bed*
    #     54 mm5.bed5
    #      0 mm5.unmapped.bed
    #     60 mm5.unmerged.bed5
awk '{printf "%s\t%s\t%s\t%s_%s\n", $1, $2, $3, $4, $5}' mm5.bed5 > mm5.bed
hgLoadBed -noBin mm5 encodeRegionsLiftOver mm5.bed >>& mm5.log

########################################################################
# DOG
liftOver -minMatch=.01 -minSizeT=4000 \
    -minSizeQ=20000 -multiple -chainTable=hg17.chainCanFam1 \
    hg17.bed /cluster/data/hg17/bed/liftOver/hg17ToCanFam1.chain \
    canFam1.unmerged.bed5 canFam1.unmapped.bed -verbose=2 >& canFam1.log
liftOverMerge -mergeGap=20000 -verbose=2 canFam1.unmerged.bed5 canFam1.bed5 >>& canFam1.log
wc -l canFam1.*bed*
    #     51 canFam1.bed5
    #      0 canFam1.unmapped.bed
    #     58 canFam1.unmerged.bed5
awk '{printf "%s\t%s\t%s\t%s_%s\n", $1, $2, $3, $4, $5}' canFam1.bed5 > canFam1.bed
hgLoadBed -noBin canFam1 encodeRegionsLiftOver canFam1.bed >>& canFam1.log

########################################################################
# OPOSSUM
liftOver -minMatch=.01 -minSizeT=4000 \
    -minSizeQ=5000 -multiple -chainTable=hg17.chainMonDom1 \
    hg17.bed /cluster/data/hg17/bed/liftOver/hg17ToMonDom1.chain \
    monDom1.unmerged.bed5 monDom1.unmapped.bed -verbose=2 >& monDom1.log
liftOverMerge -mergeGap=20000 -verbose=2 monDom1.unmerged.bed5 monDom1.bed5 >>& monDom1.log
wc -l monDom1.*bed*
    #    143 monDom1.bed5
    #      0 monDom1.unmapped.bed
    #    191 monDom1.unmerged.bed5
awk '{printf "%s\t%s\t%s\t%s_%s\n", $1, $2, $3, $4, $5}' monDom1.bed5 > monDom1.bed
hgLoadBed -noBin monDom1 encodeRegionsLiftOver monDom1.bed >>& monDom1.log

########################################################################
# TETRA
rm -f tetNig1.log tetNig1.*bed*
liftOver -minMatch=.01 -minSizeT=4000 \
    -minSizeQ=1000 -multiple -chainTable=hg17.chainTetNig1 \
    hg17.bed /cluster/data/hg17/bed/liftOver/hg17ToTetNig1.chain \
    tetNig1.unmerged.bed5 tetNig1.unmapped.bed -verbose=2 >& tetNig1.log
liftOverMerge -mergeGap=20000 -verbose=2 tetNig1.unmerged.bed5 tetNig1.bed5 >>& tetNig1.log
wc -l tetNig1.*bed*
    #    197 tetNig1.bed5
    #     12 tetNig1.unmapped.bed
    #    225 tetNig1.unmerged.bed5
awk '{printf "%s\t%s\t%s\t%s_%s\n", $1, $2, $3, $4, $5}' tetNig1.bed5 > tetNig1.bed
hgLoadBed -noBin tetNig1 encodeRegionsLiftOver tetNig1.bed >>& tetNig1.log

########################################################################
# ZEBRAFISH
rm -f danRer2.log danRer2.*bed*
liftOver -minMatch=.01 -minSizeT=4000 \
    -minSizeQ=1000 -multiple -chainTable=hg17.chainDanRer2 \
    hg17.bed /cluster/data/hg17/bed/liftOver/hg17ToDanRer2.chain \
    danRer2.unmerged.bed5 danRer2.unmapped.bed -verbose=2 >& danRer2.log
liftOverMerge -mergeGap=20000 -verbose=2 danRer2.unmerged.bed5 danRer2.bed5 >>& danRer2.log
wc -l danRer2.*bed*
    #    238 danRer2.bed5
    #      2 danRer2.unmapped.bed
    #    278 danRer2.unmerged.bed5
awk '{printf "%s\t%s\t%s\t%s_%s\n", $1, $2, $3, $4, $5}' danRer2.bed5 > danRer2.bed
hgLoadBed -noBin danRer2 encodeRegionsLiftOver danRer2.bed >>& danRer2.log

rm -f bed.tab


########################################################################
#############       Create consensus regions     #######################
#############  (Union of liftOver and Mercator)  #######################
########################################################################

hgsql -e "select chrom, chromStart+1, chromEnd, name from encodeRegions order by name" hg17 | tail +2 | awk '{printf "%s:%d-%d\t%s\n",$1,$2,$3,$4}' > positionFile

# create consensus regions and load them
set dir = /cluster/data/encode/ortho3/consensus
mkdir -p $dir
cd $dir

foreach db (panTro1 rn3 galGal2 mm5 canFam1 monDom1 tetNig1 danRer2)
    rm -f $db.log $db.*.bed*
    regionOrtho $db.encodeRegionsLiftOver $db.encodeRegionsMercatorMerged $db.encodeRegionsConsensusUnmerged.bed $db.regionOrtho.err >& $db.log
    makeBed5.pl < $db.encodeRegionsConsensusUnmerged.bed > $db.encodeRegionsConsensusUnmerged.bed5
    liftOverMerge -mergeGap=20000 -verbose=2 $db.encodeRegionsConsensusUnmerged.bed5 $db.encodeRegionsConsensus.bed5 >>& $db.log
    awk '{printf "%s\t%d\t%d\t%s\t%s", $1, $2, $3, $4, $5}' $db.encodeRegionsConsensus.bed5 > $db.encodeRegionsConsensus.bed
    hgLoadBed $db encodeRegionsConsensus -tab $db.encodeRegionsConsensus.bed >>& $db.log
end


########################################################################
#############  Make html frames pages for analysis  ####################
########################################################################

set dir = /cluster/data/encode/ortho3/html
mkdir -p $dir
cd $dir


#foreach db (mm5)
foreach db (panTro1 rn3 galGal2 mm5 canFam1 monDom1 tetNig1 danRer2)
    echo `date` $db
    rm -f $db.html
    mkOrthologAllFrame.pl descriptionFile positionFile headerFile hg17 $db ../consensus/$db.encodeRegionsConsensus.bed ../liftOverRegions/$db.bed ../mercatorInput/$db.original.bed > $db.html
end

set outDir = /usr/local/apache/htdocs/encode/ortho3
cp -f *.html $outDir
chmod o+r $outDir/*.html


foreach db (canFam1 danRer2 galGal2 mm5 monDom1 panTro1 rn3 tetNig1)
    hgsql $db -e "select * from encodeRegionsConsensus where chromEnd-chromStart<2000" > $db.smallRegions.bed
    hgsql $db -e "delete from encodeRegionsConsensus where chromEnd-chromStart<2000"
end


########################################################################
###############  Write AGP files from consensus regions  ###############
########################################################################

#!/bin/csh -f
# "Make doc" for creating AGP files for ENCODE ortholog regions based on
#  coordinates in bed files in the "consensus" directory
## based on makeAgp2.doc

# TODO: contigAcc files should go in $outDir (neater this way)

set orthoDir     = /cluster/data/encode/ortho3
set agpDir       = $orthoDir/agp
set consensusDir = $orthoDir/consensus
set downloadDir  = /usr/local/apache/htdocs/encode/ortho3/agps
set regionAgp    = regionAgp
mkdir -p $agpDir $downloadDir
chmod o+rx $downloadDir
rm -rf $agpDir/* $downloadDir/*
cd $agpDir

########################################################################
# create script to run AGP-maker on an assembly
rm -rf genomeAgps.csh >& /dev/null
cat > genomeAgps.csh << 'EOF'
# create AGP's for a genome assembly

if ($#argv != 3) then
    echo "usage: $0 <db> <org> <outdir>"
    exit 1
endif
echo
echo command line: $0 $1 $2 $3
echo working dir:  `pwd`

set consensusDir = /cluster/data/encode/ortho3/consensus
set db           = $1
set org          = $2
set outDir       = $3
set buildDir     = /cluster/data/$db
set bedFile      = $consensusDir/$db.encodeRegionsConsensus.bed5
set regionAgp    = regionAgp
mkdir -p $outDir

if (-f $org.contig.tab) then
    set contigArg = "-contigFile=$org.contig.tab"
else
    set contigArg = ""
endif

if (-f $buildDir/chrom.lst) then
    cat $buildDir/chrom.lst | \
    xargs -iX cat $buildDir/X/chr{X,X_random}.agp | \
    $regionAgp $contigArg -namePrefix=${org}_ $bedFile stdin -dir $outDir
else
    cat $buildDir/?{,?}/*.agp | \
    $regionAgp $contigArg -namePrefix=${org}_ $bedFile stdin -dir $outDir
endif
'EOF'
# << this line makes emacs coloring happy

rm -f *.contig.tab */*.packing.list >& /dev/null

########################################################################
# MOUSE
ssh hgwdev
cd /cluster/data/encode/ortho3/agp
set mouse = mm5

# Create AGP's
csh genomeAgps.csh $mouse mouse $mouse

# Create packing list
set consensusDir = $orthoDir/consensus
/cluster/data/encode/bin/scripts/encodeRegionPackingList \
    $consensusDir/$mouse.encodeRegionsConsensus.bed5 $mouse mouse \
    "Mus musculus" 10090  C57BL/6J MAY-2004 $mouse "NCBI Build 33" \
    > $mouse/$mouse.packing.list

# Copy  to downloads area
cp -f $mouse/$mouse.packing.list $downloadDir
tar cvfz $downloadDir/$mouse.agp.tar.gz $mouse/*.agp

hgsql -e "select count(*) from $mouse.encodeRegionConsensus"
tar tvfz $downloadDir/$mouse.agp.tar.gz | wc -l
grep Region $downloadDir/$mouse.packing.list | wc -l

n######################################################################a
# RAT
set rat = rn3

# Get contig to accession map
hgsql $rat -s -e "select * from contigAcc" > rat.contig.tab

# Create AGP's
csh genomeAgps.csh $rat rat $rat

# Create packing list
/cluster/data/encode/bin/scripts/encodeRegionPackingList \
    $consensusDir/$rat.encodeRegionsConsensus.bed5 $rat rat \
    "Rattus norvegicus" 10116 BN/SsNHsdMCW JUN-2003 $rat \
    "Baylor HGSC v3.1" > $rat/$rat.packing.list

# Copy  to downloads area
cp -f $rat/$rat.packing.list $downloadDir
tar cvfz $downloadDir/$rat.agp.tar.gz $rat/*.agp

hgsql -e "select count(*) from $rat.encodeRegionConsensus"
tar tvfz $downloadDir/$rat.agp.tar.gz | wc -l
grep Region $downloadDir/$rat.packing.list | wc -l

########################################################################
# CHICKEN
ssh hgwdev
cd /cluster/data/encode/ortho3/agp
set chicken = galGal2

#reversed order here to make $chicken.contig.tab before making agps
# Get contig to accession mapping (documented in makeGalGal2.doc) 
hgsql $chicken -s -e "select * from contigAcc" > chicken.contig.tab

# Create AGP's
# errors here: missing /cluster/data/galGal2/chr*_random.agp and chrM.agp files
csh genomeAgps.csh $chicken chicken $chicken

# Test a region
ssh kksilo
cd /cluster/data/encode/ortho3/agp
mkdir tests
cd tests
gunzip -c  /cluster/data/galGal2/chicken_contigs.tar.gz > \
                test.chicken.contigs.fa

ssh hgwdev
cd /cluster/data/encode/ortho3/agp/tests
set db = galGal2
set org = chicken
set region = ENm001_1
/cluster/data/encode/bin/scripts/agpAccToContig.pl $db \
        ../$db/${org}_${region}.agp > $db.$region.agp

ssh kolossus
cd /cluster/data/encode/ortho3/agp/tests
set org = chicken
set db = galGal2
set region = ENm001_1
set coords = `awk -v REGION=$region '$4 == REGION {printf "%s.%d.%d", $1, $2, $3}' /cluster/data/encode/ortho3/consensus/$db.bed`
echo $coords
set chr = $coords:r:r
nibFrag /cluster/data/$db/nib/${chr}.nib \
        $coords:r:e $coords:e + nibTest.$db.$region.fa

# takes 7 minutes on kksilo, 2-6 mins on kolossus
set fa = chicken.contigs.fa
time agpToFa $db.$region.agp ${org}_$region $db.$region.fa -simpleMulti $fa

faCmp nibTest.$db.$region.fa $db.$region.fa
    # test.fa and nibtest.fa are the same


# Create packing list
/cluster/data/encode/bin/scripts/encodeRegionPackingList \
    /cluster/data/encode/ortho3/consensus/$db.encodeRegionsConsensus.bed5 \
    $db $org "Gallus gallus" 9031 N/A FEB-2004 $db "CGSC Feb. 2004" \
    > ${db}/$db.packing.list

# Copy  to downloads area
cp -f $db/$db.packing.list $downloadDir
tar cvfz $downloadDir/$db.agp.tar.gz $db/*.agp

hgsql -e "select count(*) from $db.encodeRegionConsensus"
tar tvfz $downloadDir/$db.agp.tar.gz | wc -l
grep Region $downloadDir/$db.packing.list | wc -l


########################################################################
# DOG
ssh hgwdev
cd /cluster/data/encode/ortho3/agp
set dog = canFam1

#reversed order here to make $dog.contig.tab before making agps
# Get contig to accession mapping (documented in makeGalGal2.doc) 
hgsql $dog -s -e "select * from contigAcc" > dog.contig.tab

# Create AGP's
csh genomeAgps.csh $dog dog $dog

# Create packing list
/cluster/data/encode/bin/scripts/encodeRegionPackingList \
    /cluster/data/encode/ortho3/consensus/$dog.encodeRegionsConsensus.bed5 \
    $dog dog "Canis Familiaris" 9615 N/A JUL-2004 $dog \
    "Broad Institute v. 1.0" > $dog/$dog.packing.list

# Copy  to downloads area
cp -f $dog/$dog.packing.list $downloadDir
tar cvfz $downloadDir/$dog.agp.tar.gz $dog/*.agp

hgsql -e "select count(*) from $dog.encodeRegionConsensus"
tar tvfz $downloadDir/$dog.agp.tar.gz | wc -l
grep Region $downloadDir/$dog.packing.list | wc -l

########################################################################
# CHIMP
ssh hgwdev
cd /cluster/data/encode/ortho3/agp
set chimp = panTro1
mkdir -p $chimp
echo
echo CHIMP

# Get contig to accession map
hgsql $chimp -s -e "select * from contigAcc" > chimp.contig.tab

# Create AGP's
# NOTE: next time, put chimpChromContigs.agp into $outDir
cat /cluster/data/$chimp/?{,?}/*.agp | \
   chimpChromContigAgp stdin /cluster/data/$chimp/assembly.agp chimpChromContigs.agp
set consensusDir = /cluster/data/encode/ortho3/consensus
~kate/bin/i386/regionAgp -contigFile=chimp.contig.tab -namePrefix=chimp_ \
    $consensusDir/$chimp.encodeRegionsConsensus.bed5 chimpChromContigs.agp -dir $chimp

# Create packing list
/cluster/data/encode/bin/scripts/encodeRegionPackingList \
    $consensusDir/$chimp.encodeRegionsConsensus.bed5 \
    $chimp  chimp "Pan troglodytes" 9598 N/A NOV-2003 $chimp \
    "NCBI Build 1 v1" > $chimp/$chimp.packing.list

# Copy  to downloads area
cp -f $chimp/$chimp.packing.list $downloadDir
tar cvfz $downloadDir/$chimp.agp.tar.gz $chimp/*.agp

set downloadDir  = /usr/local/apache/htdocs/encode/ortho3/agp
hgsql -e "select count(*) from $chimp.encodeRegionConsensus"
tar tvfz $downloadDir/$chimp.agp.tar.gz | wc -l
grep Region $downloadDir/$chimp.packing.list | wc -l

########################################################################
# cleanup

chmod -R o+r $downloadDir/* >& /dev/null
exit

########################################################################
# TESTS 

rm -fr testMouse
mkdir -p testMouse
cat /cluster/data/$mouse/{6,11}/*.agp | $regionAgp tests/test.bed \
    stdin -namePrefix=mouse_ -dir testMouse
/cluster/data/encode/bin/scripts/encodeRegionPackingList tests/test.bed \
    testMouse mouse "Mus musculus" 10090 C57BL/6J MAY-2004 $mouse "NCBI Build 33"

rm -fr testChicken
mkdir -p testChicken
rm -f $chicken.contig.tab
hgsql $chicken -s -e "select * from contigAcc" > chicken.contig.tab
cat /cluster/data/$chicken/{1,13}/*.agp | \
    $regionAgp tests/test.chicken.bed stdin -namePrefix=chicken_ \
    -dir testChicken -contigFile=chicken.contig.tab

/cluster/data/encode/bin/scripts/encodeRegionPackingList \
    tests/test.chicken.bed testChicken chicken "Gallus gallus" \
    9031 N/A FEB-2004 $chicken "CGSC Feb. 2004"

########################################################################
# Test all chicken, dog, chimp AGP's
ssh hgwdev
cd /cluster/data/encode/ortho3/agp/tests
#foreach db (galGal2 canFam1 panTro1)
foreach db (panTro1)
    echo $db
    cat ../$db/*.agp > $db.agp
    /cluster/data/encode/bin/scripts/agpAccToContig.pl $db $db.agp > \
        $db.contig.agp
end

ssh kolossus
cd /cluster/data/encode/ortho3/agp/tests
cat > testFa.csh << 'EOF'
set db = $1
set org = $2
foreach f (../$db/*.agp)
    set seq = $f:t:r
    set region = `echo $seq | sed "s/${org}_//"`
    set coords = `awk -v REGION=$region '$4 == REGION {printf "%s.%d.%d", $1, $2, $3}' /cluster/data/encode/ortho3/consensus/$db.bed`
    set chr = $coords:r:r
    nibFrag /cluster/data/$db/nib/${chr}.nib \
        $coords:r:e $coords:e + $db.nibTest/$db.$region.fa
    faCmp $db.nibTest/$db.$region.fa $db.test/${org}_$region.fa
end
'EOF'
# << for emacs

# chicken
set db = galGal2
set org = chicken
set fa = ../$org.contigs.fa
mkdir $db.test $db.nibTest
time agpAllToFaFile $db.contig.agp $fa $db.fa
faSplit byname $db.fa $db.test/
csh  testFa.csh $db $org >&! testFa.$org.log &

# dog
set db = canFam1
set org = dog
set fa = ../$org.contigs.fa
mkdir $db.test $db.nibTest
time agpAllToFaFile $db.contig.agp $fa $db.fa
faSplit byname $db.fa $db.test/
csh  testFa.csh $db $org >&! testFa.$org.log &

# chimp
set db = panTro1
set org = chimp
set fa = /cluster/data/panTro1/contigs.bases
mkdir $db.test $db.nibTest
time agpAllToFaFile $db.contig.agp $fa $db.fa
faSplit byname $db.fa $db.test/
csh  testFa.csh panTro1 chimp >&! testFa.chimp.log &


cd /cluster/data/encode/ortho3/agp/tests
cat > getSeqFromAcc.pl <<EOF
#!/usr/bin/perl -W
 
$URL = "http://www.ncbi.nih.gov/entrez/eutils/efetch.fcgi?db=nucleotide&rettype=fasta&id=";
$db=shift;
$dir = "/cluster/data/encode/ortho3/agp/tests/$db.contig.fa";
system("mkdir -p $dir");
 
while (<>)
{
    chomp();
    system("wget -O $dir/$_ \"$URL$_\"");
}
EOF
#
chmod ug+x getSeqFromAcc.pl

cat > accHeader.pl <<EOF
#!/usr/bin/perl -W
$dir=shift;
$db=shift;
opendir(DIR,$dir) or die "Can't open $dir";
open (OUT, ">$db.contigAccs.fa");
@files = readdir(DIR);
foreach $file (@files)
{
    open (IN,  "<$dir/$file");
    while (<IN>)
    {
        if (/^>/)
        {
            @f=split /\|/;
            print OUT ">$f[3]\n";
        }
        else
        {
            print OUT;
        }
    }
    close(IN);
}
close(OUT);
EOF
#
chmod ug+x accHeader.pl

# rat
set db  = rn3
set org = rat
set fa  = $db.contig.fa
mkdir -p $db.test $db.nibTest

cat ../$db/${org}_EN*.agp | grep W | cut -f6 > $db.accessions
./getSeqFromAcc.pl $db < $db.accessions >& $db.log
./accHeader.pl $db.contig.fa $db >>& $db.log

agpAllToFaFile $db.contig.agp $fa $db.fa
faSplit byname $db.fa $db.test/
csh  testFa.csh $db $org >&! testFa.$org.log &

# mouse
set db  = mm5
set org = mouse
set fa  = $db.contig.fa
mkdir -p $db.test $db.nibTest

cat ../$db/${org}_EN*.agp | grep W | cut -f6 > $db.accessions
./getSeqFromAcc.pl $db < $db.accessions >& $db.log
./accHeader.pl $db.contig.fa $db >>& $db.log

agpAllToFaFile $db.contig.agp $fa $db.fa
faSplit byname $db.fa $db.test/
csh  testFa.csh $db $org >&! testFa.$org.log &

