#!/bin/csh -f
exit

# This is the make doc for the hg17 ENCODE data.
# NOTE: many of these tracks were lifted from hg16 with
# semi-automated processing. The liftOver leftovers were moved
# to the subdirectories "mapped" and "unmapped" of the main\
# work area, /cluster/data/encode/convertHg17

    # create work area
    mkdir /cluster/data/encode/convertHg17
    cd  /cluster/data/encode/convertHg17
    ln -s /cluster/data/hg16/bed/liftOver/hg16ToHg17.over.chain hg16ToHg17.chain

    # Inventory ENCODE tables on hg16 (hgwbeta)

    ssh hgwbeta "echo select tableName from trackDb where tableName like \'encode%\' and settings not like \'%composite%\' order by tableName | hgsql hg16" > tables.txt
    wc -l tables.txt
        #     350 tables.txt

    set encodeBin = /cluster/data/encode/bin/scripts
    csh $encodeBin/listEncodeTables.csh hg16 > tableTypes.txt
    grep bed tableTypes.txt > tables.bed.txt

##########################################################################
# DOWNLOADS

    ssh hgwdev
    cd /usr/local/apache/htdocs/hg17
    mkdir -p encode
    cd encode
    # release terms
    cp ../../hg16/encode/README.txt .
    # annotation database
    # request admin set up automated database dump
    mkdir database
    # auxiliary data files
    mkdir datafiles 
    # sequences
    mkdir regions
    cp ../../hg16/encode/regions/README.txt regions
    # edit README
    cd /cluster/data/encode/convertHg17
    hgsql hg17 -N -e \
      "SELECT name, chrom, chromStart, chromEnd FROM encodeRegions ORDER BY name">regions.txt 

    ssh kolossus
    cd /cluster/data/encode/convertHg17
    mkdir regions
    cd regions
    /cluster/data/encode/bin/scripts/encodeSequences.pl -upper \
        ../regions.txt /iscratch/i/hg17/nib  > hg17.fa
    /cluster/data/encode/bin/scripts/encodeSequences.pl -masked \
        ../regions.txt /iscratch/i/hg17/nib  > hg17.msk.fa
    faSize detailed=on hg17.fa > hg17_count.txt
    gzip *.fa
    md5sum *.fa.gz > md5sum.txt
    # copy regions/README.txt from hg16 and edit

    ssh hgwdev
    cd /usr/local/apache/htdocs/goldenPath/hg17/encode
    ln -s /cluster/data/encode/convertHg17/regions .

    # October MSA freeze
    ssh hgwdev
    cd /usr/local/apache/htdocs/goldenPath/hg17/encode
    mkdir alignments
    ln -s /cluster/data/encode/downloads/msa/SEP-2005 .
    # terms of use
    cp /usr/local/apache/htdocs/goldenPath/hg16/encode/alignments/README.txt .


!##########################################################################
###########################################################################
# Tracks lifted from hg16

##########################################################################
# GIS PET  (2005-08-23 kate)
# New genome-wide data (cMyc) submitted by Chialin (2006-10-25)

    cd  /cluster/data/encode/convertHg17

    # use mysqldump to generate .sql w/ schema, and .txt with data
    set t = encodeGisRnaPetHCT116
    $encodeBin/dumpTable.csh hg16 $t
    wc -l $t.txt
        # 112782 encodeGisRnaPetHCT116.txt

    # create table
    hgsql hg17 < $t.sql

    # convert data coordinates
    ~/bin/i386/liftOver $t.txt -hasBin -bedPlus=12 \
            hg16ToHg17.chain $t.tab $t.unmapped
    wc -l $t.tab $t.unmapped
         # 112701 encodeGisRnaPetHCT116.tab
         #    162 encodeGisRnaPetHCT116.unmapped

    # load into database
    echo "LOAD DATA local INFILE '$t.tab' INTO TABLE $t" | hgsql hg17
    hgsql hg17 -N -s -e "SELECT COUNT(*) FROM $t"
        # 112701
    checkTableCoords hg17 $t

    # Now try scripted version
    csh $encodeBin/convertBedTable.csh hg16 hg17 encodeGisRnaPetMCF7 12
        # encodeGisRnaPetMCF7     hg16 104304   hg17 104187
    csh $encodeBin/convertBedTable.csh hg16 hg17 encodeGisChipPet 12
        # encodeGisChipPet        hg16 65513   hg17 65510

    # 2006-10-25 cMyc data
    cd /cluster/data/encode/GIS
    mkdir -p cMyc/2006-10-25/lab
    # copy files from ftp dir
    cd cMyc/2006-10-25
    # use Angie's methods from hg16 to generate score from cluster count
    grep '^chr' lab/GIS_c-Myc_P493.bed | \
        perl -wpe 'chomp; @w = split; \
                 if ($w[3] =~ /^\d+-(\d+)$/) { \
                   $w[4] = ($1 >= 4 ? 1000 : ($1 >= 3 ? 800 : 333)); \
                 } else { die "parse"; } \
                 $_ = join("\t", @w) . "\n";' > myc.bed
    hgLoadBed -strict hg17 encodeGisChipPetMycP493 myc.bed
        # Loaded 276788 elements of size 12
    checkTableCoords encodeGisChipPetMycP493

    # Create a composite track and merge in P53 and STAT1 data
    # as subtracks

##########################################################################
# KNOWN+PRED RNA (2005-08-29 kate)

    cd  /cluster/data/encode/convertHg17
    grep encodeRna tables.bed.txt
        # encodeRna       encodeGenes     bed 6 +

    $encodeBin/convertBedTable.csh hg16 hg17 encodeRna 6

##########################################################################
# TBA23 Evofold (2005-08-23 kate)

    cd  /cluster/data/encode/convertHg17
    csh $encodeBin/convertBedTable.csh hg16 hg17 encode_tba23EvoFold 6
            # 739 encode_tba23EvoFold.txt
        # Reading liftover chains
        # Mapping coordinates
            # 739 encode_tba23EvoFold.tab
              # 0 encode_tba23EvoFold.unmapped
            # 739 total
        # encode_tba23EvoFold     hg16 739   hg17 739



##########################################################################
# Transcription Levels Group
# BU FIRST EXON
    grep encodeBu tables.bed.txt
        # encodeBuFirstExonCerebrum       encodeTxLevels  bed 12 +
        # encodeBuFirstExonColon  encodeTxLevels  bed 12 +
        # encodeBuFirstExonHeart  encodeTxLevels  bed 12 +
        # encodeBuFirstExonKidney encodeTxLevels  bed 12 +
        # encodeBuFirstExonLiver  encodeTxLevels  bed 12 +
        # encodeBuFirstExonLung   encodeTxLevels  bed 12 +
        # encodeBuFirstExonSkMuscle       encodeTxLevels  bed 12 +
        # encodeBuFirstExonSpleen encodeTxLevels  bed 12 +
        # encodeBuFirstExonStomach        encodeTxLevels  bed 12 +
        # encodeBuFirstExonTestis encodeTxLevels  bed 12 +
    
    set buTables = `echo "SHOW TABLES LIKE 'encodeBuFirstExon%'" | hgsql -N -s hg16`
    foreach t ($buTables)
        csh $encodeBin/convertBedTable.csh hg16 hg17 $t 12
        checkTableCoords hg17 $t
    end


# RIKEN CAGE
    grep encodeRikenCage tables.bed.txt
        # encodeRikenCageMinus    encodeTxLevels  bedGraph 4
        # encodeRikenCagePlus     encodeTxLevels  bedGraph 4

    csh $encodeBin/convertBedTable.csh hg16 hg17 encodeRikenCageMinus 4
        # Creating hg16 encodeRikenCageMinus.sql and encodeRikenCageMinus.txt
           # 6156 encodeRikenCageMinus.txt
        # Reading liftover chains
        # Mapping coordinates
           # 6153 encodeRikenCageMinus.tab
              # 6 encodeRikenCageMinus.unmapped
           # 6159 total
        # encodeRikenCageMinus    hg16 6156   hg17 6153

    csh $encodeBin/convertBedTable.csh hg16 hg17 encodeRikenCagePlus 4
            # csh $encodeBin/convertBedTable.csh hg16 hg17 encodeRikenCagePlus 4
        # Creating hg16 encodeRikenCagePlus.sql and encodeRikenCagePlus.txt
           # 5688 encodeRikenCagePlus.txt
        # Reading liftover chains
        # Mapping coordinates
           # 5639 encodeRikenCagePlus.tab
             # 98 encodeRikenCagePlus.unmapped
           # 5737 total
        # encodeRikenCagePlus     hg16 5688   hg17 5639


##########################################################################
# CHIP/CHIP GROUP
# 
# STANFORD CHIP
# encodeStanfordChip* bedGraph 4 tracks

cat > doStan.csh << 'EOF'
    set stanTables = \
        `echo "SHOW TABLES LIKE 'encodeStanfordChip%'" | hgsql -N -s hg16`
    foreach t ($stanTables)
        csh /cluster/data/encode/bin/scripts/convertBedTable.csh \
                hg16 hg17 $t 4
    end
'EOF'
    csh doStan.csh >&! doStan.log
    grep hg17 doStan.log | wc -l
        # 12 tracks (6 smoothed)
        # encodeStanfordChipHCT116Sp1     hg16 369633   hg17 369465
        # encodeStanfordChipSmoothedHCT116Sp1     hg16 137439   hg17 137361

# UCD Ng
        csh $encodeBin/convertBedTable.csh hg16 hg17 encodeUCDavisE2F1Median 4
        # encodeUCDavisE2F1Median hg16 382884   hg17 382713

# UCSD/LI CHIP
# encodeUcsdChip* bedGraph 4 tracks (total 36)

cat > doUcsd.csh << 'EOF'
    set ucsdTables = \
        `echo "SHOW TABLES LIKE 'encodeUcsdChip%'" | hgsql -N -s hg16`
    foreach t ($ucsdTables)
        csh /cluster/data/encode/bin/scripts/convertBedTable.csh \
                hg16 hg17 $t 4
    end
'EOF'
    csh doUcsd.csh >&! doUcsd.log
    grep hg17 doUcsd.log | wc -l
    # 36 tracks
    # encodeUcsdChipAch3Imr90 hg16 24348   hg17 24339
    # encodeUcsdChipHeLaH3H4tmH3K4_p30        hg16 24537   hg17 24528


##########################################################################
# TRANSCRIPTION LEVELS TRACKS (2005-08-24 kate)

    # grep encodeTxLevels in tables.bed.txt and edit out already
    # completed tracks.  Prefix each table with a call to convertBedTable
    # and suffix with bed field count
    # Tracks are: Stanford RTPCR, Yale TARS

    csh doTx.csh >&! doTx.log
    grep hg17 doTx.log | wc -l
        # 9 tracks

##########################################################################
# CHROMATIN & CHROMOSOMES TRACKS (2005-08-24 kate)

    # Regulome, NHGRI DNase, Stanford Meth, UVA
    csh doChrom.csh >&! doChrom.log
    # 37 tables
    # do Stanford Meth Smoothed tables that weren't converted because
        # hg16 tables had incorrect capitalization wrt trackDb
        # and so weren't being displayed
    csh doChrom2.csh >&! doChrom2.log

##########################################################################
# CHIP/CHIP TRACKS (2005-08-24 kate)
# Sanger, UCSD Nimblegen

    doChip.csh >&! doChip.log

##########################################################################
# VARIATION TRACKS (2005-08-24 kate)
#  HapMap, Reseq, Sanger Gene Expr

    csh doVar.csh >&! doVar.log
    grep hg17 doVar.log
        # encodeReseqRegions      hg16 10   hg17 10
        # encodeSangerGenoExprAssociation hg16 13674   hg17 13674
    csh doHap.csh >&! doHap.log
    grep hg17 doHap.log
        # encodeHapMapAlleleFreqCEU       hg16 20772   hg17 20772
        # encodeHapMapAlleleFreqCHB       hg16 19629   hg17 19629
        # encodeHapMapAlleleFreqJPT       hg16 19629   hg17 19629
        # encodeHapMapAlleleFreqYRI       hg16 19520   hg17 19520
    csh /cluster/data/encode/bin/scripts/convertBedTable.csh \
                hg16 hg17 encodeRecomb         4


##########################################################################
# AFFY CHIP/CHIP TRACKS (2005-08-24 kate)
    csh doAffy.csh >&! doAffy.log
        # 41 doAffy.csh
    grep hg17 doAffy.log | wc -l
        # 41 

    # do tracks missing from RR!
    csh doAffy2.csh >&! doAffy2.log
    wc -l doAffy2.csh
        # 6 doAffy2.csh
    grep hg17 doAffy2.log | wc -l
        # 6

##########################################################################
# WIG TRACKS (2005-08-24 kate)
        doWig.csh > doWig.log
        # 75 tables

##########################################################################
# YALE TRACKS (2005-08-31 kate)
        doYale.csh > doYale.log
        wc -l doYale.csh
            # 54 doYale.csh
        grep hg17 doYale.log | wc -l
            # 50 
            # redo the 4 that failed
        doYale2.csh > doYale2.log
        grep hg17 doYale2.log | wc -l
            # 4 tracks

!##########################################################################
##########################################################################
# Tracks submitted in hg17 coords

##########################################################################
# GENCODE Sanger Havana annotations  (2005-08-18 kate)
    # Used latest (6/7/05) data submission, which was submitted
    #   in hg17 coords and lifted to hg16.  This was described in makeEncodeHg16.doc
    ssh hgwdev
    cd /cluster/data/encode/Gencode
    cd 2005-06-07

    ldHgGene -gtf -genePredExt hg17 encodeGencodeGene gencode.vega.gtf
        # 2888 gene predictions
    checkTableCoords hg17 encodeGencodeGene

    grep intron gencode.gtf | wc -l
        # 15814
    grep -v not_tested gencode.gtf | sed -e 's/-intron/-/g' | \
        ldGencodeIntron hg17 encodeGencodeIntron stdin
            # 469 introns

    # load gene class table 
    hgsql hg17 < ~/kent/src/hg/lib/gencodeGeneClass.sql
    echo "LOAD DATA LOCAL INFILE 'gencodeGeneClass.tab' into table gencodeGeneClass" | hgsql hg17
    wc -l gencodeGeneClass.tab
        #    2888 gencodeGeneClass.tab


##########################################################################
# EGASP Partial (2005-08-18 kate)
# Gene tracks submitted for the EGASP competition were hg17-based
#       by the Gencode group (Roderic Guigo, Julien Legarde, IMIM) 
# These were lifted to hg17, as described in makeEncodeHg16.doc
# NOTE: Problem with encodeEgaspPartAugustusAny table detected
# and fixed on 2006-01-09.  It was somehow loaded with Genemark full data...
    cd /cluster/data/encode
    cd EGASP/Partial
    wc -l lab/*.gtf
       # 1778 lab/ASPic.gtf
       # 4215 lab/AceSCAN.gtf
       # 2692 lab/Augustus_EST-Protein.gtf
       # 2347 lab/Augustus_abinitio.gtf
       # 2736 lab/Augustus_any.gtf
       # 2567 lab/Augustus_dualgenome.gtf
       # 3458 lab/GeneZilla.gtf
       # 2194 lab/SAGA.gtf
    # NOTE: exclude ASPic, which contains only intron records
    # Filenames above, with _CHR_COORDS_hg17.gff appended, are chrom coordinate versions

    # GeneZilla
    ldHgGene hg17 encodeEgaspPartGenezilla lab/GeneZilla.*.gff
        # 656 gene predictions
    genePredCheck -db=hg17 encodeEgaspPartGenezilla

    # SAGA
    # Strip out trailing ## on lines where manual changes were made
    #   (see notes in .gtf file)
    sed -e 's/ ##.*//' lab/SAGA.*.gff | \
        ldHgGene hg17 encodeEgaspPartSaga stdin
        # 378 gene predictions
    genePredCheck -db=hg17 encodeEgaspPartSaga

    # Augustus
   ln -s lab/Augustus_EST-Protein.gtf_CHR_COORDS_hg17.gff augustus.est.gff
   ln -s lab/Augustus_abinitio.gtf_CHR_COORDS_hg17.gff augustus.abinitio.gff
   ln -s lab/Augustus_any.gtf_CHR_COORDS_hg17.gff augustus.any.gff
   ln -s lab/Augustus_dualgenome.gtf_CHR_COORDS_hg17.gff augustus.dual.gff

    foreach f (augustus.*.gff)
        set t = `echo $f | sed -e 's/augustus.\(.*\).gff/encodeEgaspPartAugustus\u\1/'`
        ldHgGene -genePredExt hg17 $t $f
        checkTableCoords hg17 $t
    end
        # augustus.abinitio.gff 418 gene predictions
        # augustus.any.gff      399 gene predictions
        # augustus.dual.gff     413 gene predictions
        # augustus.est.gff      381 gene predictions

    # Reload .est predictions (2006-01-09 kate)
    ldHgGene -genePredExt hg17 encodeEgaspPartAugustusEst augustus.est.gff
        # augustus.est.gff      381 gene predictions
    checkTableCoords hg17 encodeEgaspPartAugustusEst

    # AceSCAN
    # Split into two tracks -- conserved, and other, based on feature
    ldHgGene -predTab hg17 encodeEgaspPartAceCons aceCons.gp
        # 117 gene predictions
    ldHgGene -predTab hg17 encodeEgaspPartAceOther aceOther.gp
        # 727 gene predictions
    genePredCheck -db=hg17 encodeEgaspPartAceCons encodeEgaspPartAceOther

##########################################################################
# EGASP Full (2005-06-27 kate)
# Gene tracks submitted for the EGASP competition were hg17-based
#       by the Gencode group (Roderic Guigo, Julien Legarde, IMIM) 
    cd /cluster/data/encode
    cd EGASP/Full

    # Process "standard" gff files
    # NOTE: must dummy out scores -- float values
cat > doGene.hg17.csh << 'EOF'
ls *.gp | grep -v hg16 > gpList
foreach f (`cat gpList`)
    wc -l $f 
    set b = $f:r
    set t = encodeEgaspFull$b
    ldHgGene -predTab hg17 $t $f
    genePredCheck -db=hg17 $t
end
'EOF'
csh doGene.hg17.csh >&! doGene.hg17.log

    # process special files
    cd custom
cat > doGene.hg17.csh << 'EOF'
foreach f (Jigsaw.gp Ensembl.gp EnsemblPseudo.gp Exonhunter.gp GeneId.gp Sgp2.gp Twinscan.gp)
    set b = $f:r
    set t = encodeEgaspFull$b
    ldHgGene -genePredExt -predTab hg17 $t $b.gp
    genePredCheck -db=hg17 $t
end
'EOF'
# << for emacs
csh doGene.hg17.csh >&! doGene.hg17.log
    # NOTE: OK to have missing exonFrames
# Reading Ensembl.gp
# 735 gene predictions
# Reading EnsemblPseudo.gp
# 34 gene predictions
# Reading Exonhunter.gp
# 1435 gene predictions
# Reading GeneId.gp
# 476 gene predictions
# Reading Sgp2.gp
# 930 gene predictions
# Reading Twinscan.gp
# 954 gene predictions

end
'EOF'
# << for emacs
csh doGene.hg17.csh >&! doGene.hg17.log

    # process others
    set t = "encodeEgaspFullGenemark"
    ldHgGene -predTab hg17 $t Genemark.gp
        # 890 gene predictions
    genePredCheck -db=hg17 $t

    # create genepreds containing just exons flanking U12 introns
    set t = encodeEgaspFullGeneIdU12
    ldHgGene -predTab -genePredExt hg17 $t geneId.introns.gp
        # 24 gene predictions
    genePredCheck -db=hg17 $t
    set t = encodeEgaspFullSgp2U12
    ldHgGene -predTab -genePredExt hg17 $t sgp2.introns.gp
        # 20 gene predictions
    genePredCheck -db=hg17 $t


##########################################################################
# EGASP Update
# Submitted in hg17 coords

    # Jigsaw
    cd /cluster/data/encode
    cd EGASP/Jigsaw/2005-06-01
    ldHgGene -predTab -genePredExt hg17 encodeEgaspUpdJigsaw jigsaw.gp
        # 454 gene predictions
    genePredCheck -db=hg17 encodeEgaspUpdJigsaw

    # Augustus
    cd /cluster/data/encode
    cd EGASP/Augustus/2005-06-22
    foreach f (abinitio.gp any.gp dual.gp est.gp)
        genePredCheck $f
        set t = `echo $f | sed -e 's/\(.*\).gp/encodeEgaspUpdAugustus\u\1/'`
        ldHgGene -predTab -genePredExt hg17 $t $f
        checkTableCoords hg17 $t
    end
        # Reading abinitio.gp
        # 622 gene predictions
        # Reading any.gp
        # 571 gene predictions
        # Reading dual.gp
        # 617 gene predictions
        # Reading est.gp
        # 543 gene predictions

    # Exogean
    cd /cluster/data/encode
    cd EGASP/Exogean/2005-06-23
    ldHgGene -predTab hg17 encodeEgaspUpdExogean exogean.gp
        # 850 gene predictions
    genePredCheck -db=hg17 encodeEgaspUpdExogean

    # GeneIDU12 and SgpU12
    cd /cluster/data/encode
    cd EGASP/GeneIdU12/2005-06-10/
    # create GTF files from submitted GFF's
    awk -F\\t '/^chr/ {printf "%s\t%s\tCDS\t%s\t%s\t.\t%s\t%s\tgene_id \"%s\"; transcript_id \"%s\"; exon_type \"%s\";\n", $1, $2, $4, $5, $7, $8, $9, $9, $3}' < lab/UCSC-hg17-GeneID-U12-track.gff | grep -v intron > geneId.hg17.gtf
    ldHgGene -genePredExt hg17 encodeEgaspUpdGeneId geneId.hg17.gtf
        # 476 gene predictions
    genePredCheck -db=hg17 encodeEgaspUpdGeneId

    awk -F\\t '/^chr/ {printf "%s\t%s\tCDS\t%s\t%s\t.\t%s\t%s\tgene_id \"%s\"; transcript_id \"%s\"; exon_type \"%s\";\n", $1, $2, $4, $5, $7, $8, $9, $9, $3}' < lab/UCSC-hg17-SGP2-U12-track.gff | grep -v intron > sgp2.hg17.gtf
    ldHgGene -genePredExt hg17 encodeEgaspUpdSgp2 sgp2.hg17.gtf
        # 930 gene predictions
    genePredCheck -db=hg17 encodeEgaspUpdSgp2

    # create genepreds containing just exons flanking U12 introns
    # use U12 annotation as gene name, so it appears on details page
    grep U12 geneId.hg17.gtf | perl -wpe \
 's/(^.*gene_id) (\S+) (.*exon_type) (.*)(U12[^-]+)(.*)/$1 "$5"; $3 $4$5$6/' \
        > geneId.introns.hg17.gtf
    ldHgGene -genePredExt hg17 encodeEgaspUpdGeneIdU12 geneId.introns.hg17.gtf
        # 24 gene predictions

    grep U12 sgp2.hg17.gtf | perl -wpe \
 's/(^.*gene_id) (\S+) (.*exon_type) (.*)(U12[^-]+)(.*)/$1 "$5"; $3 $4$5$6/' \
        > sgp2.introns.hg17.gtf
    ldHgGene -genePredExt hg17 encodeEgaspUpdSgp2U12 sgp2.introns.hg17.gtf
        # 20 gene predictions


    # EGASP Yale Pseudogenes
    # Update submitted by Deyou Zheng 8/18/05
    cd /cluster/data/encode
    cd EGASP/yale/latest
    wc -l lab/*.submitted
        #  184 lab/YalePgene-NCBI35.gtf.submitted
        # NOTE: this is fewer than the previous submission -- I confirmed
        # with Deyou that this is correct.

    # munge to create CDS entries to display, and assign pseudogene
    # name as transcript_id, and pseudogene type as gene_id so
    # it displays on details page
    sed -e 's/pseudogene\t/CDS\t/' -e 's/pgene_type/gene_id/'  \
        -e 's/alt_name ENCODE_Yale/transcript_id /' \
                lab/YalePgene-NCBI35.gtf.submitted > yale.hg17.gtf
    ldHgGene -genePredExt hg17 encodeEgaspUpdYalePseudo yale.hg17.gtf
        # 184 gene predictions 
    genePredCheck -db=hg17 encodeEgaspUpdYalePseudo

    # Fgenesh++
    # Update submitted 9/30/05 by Victor Solovyev to Julien Legarde at
    # IMIM, to fix 4 regions (predictions originally on hg16, redone
    # for hg17)
    cd /cluster/data/encode/EGASP
    mkdir -p Fgenesh/2005-09-30/lab
    cd Fgenesh/2005-09-30/lab
    wget ftp://genome.imim.es/pub/projects/gencode/data/egasp05/submitted_predictions/EGASP_Update/FGenesh++_corrected_update.gtf_CHR_COORDS_hg17.gff
    wget ftp://genome.imim.es/pub/projects/gencode/data/egasp05/submitted_predictions/EGASP_partial/FGenesh++_corrected_partial.gtf_CHR_COORDS_hg17.gff
    cd ..
    cat *.gff | ldHgGene hg17 encodeEgaspUpdFgenesh stdin
    genePredCheck -db=hg17 encodeEgaspUpdFgenesh
        # 820 gene predictions
    
##########################################################################
# UCSD/LI Nimblegen Hela
# Data submitted on hg17 for June freeze

    cd /cluster/data/encode/UCSD/nimblegen/2005-06-01
    foreach f (lab/Nim*/*.wig)
        set t =  `echo $f:t:r | sed -e \
         's/rnap/encodeUcsdNgHeLaRnap/; s/tmh3k4/encodeUcsdNgHeLaH3K4me3/;'`
        echo $t
        grep "^chr" $f | hgLoadBed -onServer -bedGraph=4 hg17 $t stdin
        checkTableCoords hg17 $t
    end
        # Produces 4 tables, encodeUcsdNgHeLa{Rnap,H3K4me3}_p{0,30}
        # Loaded 385149 elements of size 4 

##########################################################################
# STANFORD PROMOTERS
    cd /cluster/data/encode/StanfordPromoters
    rm previous
    mv latest previous
    mkdir 2005-08-23
    ln -s 2005-08-23 latest
    mkdir latest/lab
    # copy updated files from Sara Hartman's email.
    # Both hg16 and hg17 versions were included:
    # hg16: StanfordPromoters_<cell>_08.23.txt
    # hg17: StanfordPromoters_hg17_<cell>_08.24.txt
    # Use Angie's processing from hg16, slightly modified
    cd latest
cat > doProm.csh << 'EOF'
    foreach f (lab/StanfordPromoters_hg17*.txt)
      set cellType = `echo $f | perl -wpe 's^lab/StanfordPromoters_hg17_(.*)_.*^$1^'`
      echo $cellType
      if ($cellType == "Average") then
        tail +2 $f \
        | perl -wpe 'chomp; @w = split("\t"); $w[7] =~ s/^\"(.*)\"$/$1/; \
                     $w[3] =~ tr/01/-+/; \
                     $_ = join("\t", \
  $w[2], $w[4], $w[5], $w[0], $w[9], $w[3], $w[4], $w[5], 0, $w[1], $w[7], \
  $w[8]) . "\n";' \
        | makeColoredBed > encodeStanfordPromoters$cellType.hg17.bed
      else
        tail +2 $f \
        | grep -v "Bad Txfn" \
        | perl -wpe 'chomp; @w = split("\t"); $w[7] =~ s/^\"(.*)\"$/$1/; \
                     $w[3] =~ tr/01/-+/; \
                     $_ = join("\t", \
  $w[2], $w[4], $w[5], $w[0], $w[15], $w[3], $w[4], $w[5], 0, $w[1], $w[7], \
  $w[8], $w[9], $w[10], $w[11], $w[12], $w[13], $w[14]) . "\n";' \
        | makeColoredBed > encodeStanfordPromoters$cellType.hg17.bed
      endif
    end
'EOF'
    csh doProm.csh >&! doProm.log

cat > doLoad.csh << 'EOF'
    foreach f (encode*.bed)
      set track = $f:r:r
      if ($track == "encodeStanfordPromotersAverage") then
        hgLoadBed -tab -noBin -sqlTable=$HOME/kent/src/hg/lib/$track.sql \
          hg17 $track $f
      else
        sed -e "s/encodeStanfordPromoters/$track/" \
          $HOME/kent/src/hg/lib/encodeStanfordPromoters.sql > /tmp/esp.sql
        hgLoadBed -tab -noBin -sqlTable=/tmp/esp.sql hg17 $track $f
      endif
    end
'EOF'
    csh doLoad.csh >&! doLoad.log 

    # Put the negative control data spreadsheet out for download.
    ssh kkstore03
    cd /cluster/data/encode/StanfordPromoters/latest/lab
    nice gzip hg17_NegControlDataStanfordPromoters.txt
    ssh hgwdev
    cd /usr/local/apache/htdocs/goldenPath/hg17/encode/datafiles
    mkdir -p stanfordPromoters
    cd stanfordPromoters
    cp -p \
        /cluster/data/encode/StanfordPromoters/latest/lab/hg17_NegControlDataStanfordPromoters.txt.gz \
                NegativeControlDataStanfordPromoters.txt.gz
    # Added a README.txt (edited form Angie's hg16 version)

##########################################################################
# UV Replication -- Segregation, Origins, and Origin Confidence tracks
#       New data for Oct. freeze (but submitted in hg16 coords)
#       All data are bed3
# Contact: Chris Taylor (cmt5n@cs.virginia.edu)

    cd /cluster/data/encode/UVa
    mkdir -p 2005-08-30
    cd 2005-08-30
    mkdir lab

    # Segregation data - 4 subtracks (Early, Mid, Late, Pan-S)
    # 4 custom tracks in a single file -- use Hiram's script to split
    /cluster/data/encode/BU/2005-06-09/splitTracks.pl \
                lab/segchunks.hg16.qced.bed
    # creates t0, t1, t2, t3
    awk < lab/segchunks.hg16.qced.bed '/track/ {print $2}'
#name=early
#name=mid
#name=late
#name=pans
    grep -v "^track" t0 > encodeUvaDnaRepEarly.hg16.bed
    grep -v "^track" t1 > encodeUvaDnaRepMid.hg16.bed
    grep -v "^track" t2 > encodeUvaDnaRepLate.hg16.bed
    grep -v "^track" t3 > encodeUvaDnaRepPanS.hg16.bed
    rm t0 t1 t2 t3
    foreach f (encodeUvaDnaRep*.hg16.bed)
        set d = $f:r:r
        echo $d
        liftOver $f /cluster/data/encode/convertHg17/hg16ToHg17.chain \
                $d.hg17.bed $d.unmapped
        hgLoadBed -noBin -strict hg17 $d $d.hg17.bed
    end

    # Redo with hg17 resubmitted data
    cd /cluster/data/encode/UVa
    cd 2005-10-15
    /cluster/data/encode/bin/scripts/splitTracks.pl lab/segregation.hg17.bed
    grep -v "^track" t0 > encodeUvaDnaRepEarly.bed
    grep -v "^track" t1 > encodeUvaDnaRepMid.bed
    grep -v "^track" t2 > encodeUvaDnaRepLate.bed
    grep -v "^track" t3 > encodeUvaDnaRepPanS.bed
    rm t0 t1 t2 t3
    foreach f (encodeUvaDnaRep*.bed)
        set d = $f:r
        echo $d
        hgLoadBed -noBin -strict hg17 $d $d.bed
    end

    # Origin predictions -- fixed at 200bp
    set t = encodeUvaDnaRepOriginsPred
    ln -s lab/originspred.hg16.qced.bed $t.hg16.bed
    liftOver $t.hg16.bed \
        /cluster/data/encode/convertHg17/hg16ToHg17.chain \
                $t.hg17.bed $t.unmapped
    hgLoadBed -noBin -strict hg17 $t $t.hg17.bed
        # Loaded 289 elements of size 3

    # Origin confidence intervals -- varying length for averaged origins
    set t = encodeUvaDnaRepOriginsConf
    ln -s  lab/originsconf.hg16.qced.bed $t.hg16.bed
    liftOver $t.hg16.bed \
        /cluster/data/encode/convertHg17/hg16ToHg17.chain \
                $t.hg17.bed $t.unmapped
    hgLoadBed -noBin -strict hg17 $t $t.hg17.bed
        # Loaded 270 elements of size 3

    # Smoothed TR50 data
    #  500K 1bp float scores
    # wiggle with span=1
    set table = encodeUvaDnaRepTr50
    grep -v '^track' lab/smoothedtr50.hg17.wig | \
        wigEncode stdin $table.wig $table.wib
            #  upper limit 6.36, lower limit 2.05
    set dir = /gbdb/hg17/encode/UVa/2005-10-15
    mkdir -p $dir
    hgLoadWiggle -pathPrefix=$dir hg17 $table $table.wig
    ln -s `pwd`/$table.wib $dir


##########################################################################
# Indels from Jim Mullikin
# Heather, Sept. 2005

    ssh hgwdev
    cd /cluster/data/encode/NHGRI/mullikin/hg17
    hgsql hg17 < encodeIndels.sql
    split4.pl < hg17.ENCODE.DIPtrack.Q23.bed4+ > split4.out
    # use a modified makeColoredBed
    ./makeColoredBed < split4.out > encodeIndels.bed  
    # don't use -strict because we have lots of simple insertions (where chromStart = chromEnd)
    hgLoadBed hg17 encodeIndels -tab -sqlTable=encodeIndels.sql encodeIndels.bed
    # check reference length
    mysql> select chrom, chromStart, chromEnd, (chromEnd-chromStart) as size, traceName, reference, length(reference) as refsize from encodeIndels where (chromEnd-chromStart) != length(reference) and length(reference) > 1;
    # Empty set (0.07 sec)

##########################################################################
# Boston University ORChID track - (2005-09-18 kate)
#	data developer contact:  Jay Greenbaum jj@bu.edu
    ssh hgwdev
    cd /cluster/data/encode/BU
    mkdir -p orchid/2005-09-08/lab
    cd -p orchid/2005-09-08/lab
    wget --timestamping "http://dna.bu.edu/%7Ejj/cleavage_data_hg17/oh_cleavage_hg17.wig.gz"
    cd ..
    mkdir wib
    # NOTE: continue reluctantly with non-standard table name
    # as in hg16
    wigEncode lab/oh_cleavage_hg17.wig.gz \
        encodeBu_ORChID1.wig wib/encodeBu_ORChID1.wib
                # upper limit 1.58, lower limit -0.56
    # load
    set dir = /gbdb/hg17/encode/Bu/2005-09-08
    mkdir -p $dir
    hgLoadWiggle -pathPrefix=$dir hg17 encodeBu_ORChID1 encodeBu_ORChID1.wig
    mkdir -p $dir/wib
    ln -s `pwd`/wib/encodeBu_ORChID1.wib $dir/wib

##########################################################################
# Genome Institute of Singapore -ChIP/PET of STAT1 TFBS (2005-09-29 kate)
# Submitted 9/19 by Atif Shahab
    cd /cluster/data/encode/GIS
    mkdir chip
    mkdir -p 2005-09-19/lab
    ln -s 2005-09-19 latest
    cd latest
    # copy files from FTP dir to lab subdi4
    # files: 2 bed files (stim and nonstim) and 1 doc file
    # use antiword to convert doc file to txt
    ln -s lab/STAT1+stimulation.bed Gif.bed
    ln -s lab/STAT1+w:o_stimulation.bed NoGif.bed

    # Use cluster-count info, now embedded into the name, to make scored BED:
    # (Angie's methods from hg16)
    foreach f (Gif.bed NoGif.bed)
        set d = $f:r
        echo $d
        set table = encodeGisChipPetStat1$d
        perl -wpe 'chomp; @w = split; \
                 if ($w[3] =~ /^\d+-(\d+)$/) { \
                   $w[4] = ($1 >= 4 ? 1000 : ($1 >= 3 ? 800 : 333)); \
                 } else { die "parse"; } \
                 $_ = join("\t", @w) . "\n";' \
               $f > ${table}.tab
       hgLoadBed hg17 $table ${table}.tab
       checkTableCoords hg17 $table
    end
    # Reading encodeGisChipPetStat1Gif.tab
    # Loaded 4007 elements of size 12
    # Reading encodeGisChipPetStat1NoGif.tab
    # Loaded 3180 elements of size 12
    # NOTE: These counts correspond with the doc file they provided
    # Unlike the previous GIS Chip/chip dataset, these are only
    # in the ENCODE regions.  I requested the genome-wide
    # data -- they will provide  this later.


##########################################################################
# Genome Institute of Singapore - PET RNA (2005-10-19 kate)
# Submitted 10/11 by Atif Shahab
#       3 datasets - 5FU treated HCT116 cells, 
#                    MCF7 untreated 
#                    Estrogen-treated MCF7 (new)
# Replace data in existing subtracks, and add new one
    cd /cluster/data/encode/GIS
    mkdir -p rna/2005-10-11/lab
    # copy files from FTP dir
    cd rna/2005-10-11/lab
    ln -s MCF7_estrogen_treated.bed lab/MCF7Estr-hg17.bed

    # use Angie's loading process from hg16
cat > load.csh << 'EOF'
    foreach f (lab/HCT116-hg17.bed lab/MCF7-hg17.bed lab/MCF7Estr-hg17.bed)
      set cellType = `echo $f:t:r | sed -e 's/-hg17//'`
      echo $cellType
      set table = encodeGisRnaPet$cellType
      grep '^chr' $f | \
      perl -wpe \
     'chomp; @w = split; \
      if ($w[3] =~ /\d+-(\d+)-(\d+)/) { \
        ($mc, $ac) = ($1, $2, $3); \
        if ($mc == 1)   { $w[8] = ($ac > 1) ? "35,35,175" : "160,160,188"; } \
        elsif ($mc > 1) { $w[8] = ($ac > 1) ? "180,120,0" : "225,150,0"; } \
        else { die "mc $mc" } \
      } else { die "parse"; } \
      $_ = join(" ", @w) . "\n";'  > $table.bed
      hgLoadBed hg17 $table $table.bed
    end
'EOF'
csh load.csh >&! load.log
    rm *.bed

##########################################################################
# UCSD/Ludwig Institute Nimblegen chip/chip (2005-10-07 KATE)
#   New data submission

    ssh hgwdev
    cd /cluster/data/encode/UCSD/nimblegen
    mkdir 2005-09-29
    cd 2005-09-29
    mkdir lab
    # copy file from FTP dir, unzip and untar, into lab dir
    # 12 data files and README

cat > load.csh << 'EOF'
    foreach f (`ls lab/*.wig`)
        set table = `echo $f:t:r | sed -e 's/\(.*\)/encodeUcsdNgHeLa\u\1/'`
        echo $table
        grep '^chr' $f | hgLoadBed -onServer -bedGraph=4 hg17 $table stdin
        checkTableCoords hg17 $table
    end
'EOF'
    csh load.csh >&! load.log
    # Created hg17 composite track with all 16 datasets
    # The hg16 composite only has the first 4 submitted

##########################################################################
# UT-Austin (Vishy Iyer lab) Chip/chip  (2005-10-10 kate)
    cd /cluster/data/encode
    mkdir UTexas/2005-10-01/lab
    cd UTexas/2005-10-01/lab
    # copy file from FTP dir
    # 8 .wig data files (4 experiments, with raw data, and "peaks"), plus description file

cat > load.csh << 'EOF'
    foreach f (`ls lab/*.wig`)
        set table = `echo $f:t:r | sed -e 's/HeLa/HeLa_NoSerum/;s/NoSerum//;s/Serum4hr/Stim/;s/2091/2091fib/;s/\(.*\)_\(.*\)_\(.*\)_\(.*\)/encodeUtexChip\1\3\2\u\4/'`
        echo $table
        grep '^chr' $f | hgLoadBed -onServer -bedGraph=4 hg17 $table stdin
    checkTableCoords hg17 $table
    end
'EOF'
    csh load.csh >&! load.log
    # Created composite track with 8 subtracks

    
##########################################################################
# Affy Chip/chip and RNA (kate)
# submitted by Hari_Tammana@affymetrix.com (Oct. 3)
#  with clarifications as to display from Phil Kapranov at Affy
# HeLa data update submitted 12/15 by Hari Tamani

    cd /cluster/data/encode/Affy
    mkdir 2005-10-03/lab
    cd 2005-10-03/lab
    # copy file from FTP dir (500M) affy_oct1.tar.gz
    # two data dirs: CHIP, RNA
    # 10 descriptions for CHIP dir, 3 for RNA dir
    # RNA has 2 dirs (bed, wig) with each
    #   having 3 cell lines (GM06990, HeLa, HL60; the HL60
    #   data has 4 timepoints (0, 2, 8, 32)
    #  README's (and discussions with Phil) indicate the 
    #     wig's are replacements for previous RNA Signal
    #     data, and bed's are replacement Transfrags
    # The CHIP .wig files are similar to the previous
    #   Affy Pval data, but analyzed with stricter analysis
    #   criteria.  The .bed files are comparable to the Sites
    #  track.  2 factors are repeats from previous track
    #   (HisH4 TetraAc, Pol2), and 3 are new (H3K9K14DiAc, 
    # p63_ActD (with Actinomycin D treatment), 
    # p63_mActD (without Actinomycin D treatment)
    #  The Pol2, HisH4, and H3* data are at 4 timepoints.
    # These should be loaded in addition to previous tracks
    #  (not replacements).  Later the earlier ("lenient")
    #  analysis will be submitted for the 3 new factors,
    #  and these will be added to the previous Affy Chip/chip tracks
    #  on hg17.

    # Transfrags (6 subtracks)
    cd /cluster/data/encode/Affy/2005-10-03
    tail +2 lab/RNA/bed/GM06990/EC_AS_GM06990_RCyP+_C01vsNULL.sig.gr.bed \
    | hgLoadBed -noBin hg17 encodeAffyRnaGm06990Sites stdin
        # 4377 elements
    tail +2 lab/RNA/bed/HeLa/EC_AS_HeLaS3_RCyP+_C01vsNULL.sig.gr.bed \
    | hgLoadBed -noBin hg17 encodeAffyRnaHeLaSites stdin
        # 2037 elements
cat > loadSites.csh << 'EOF'
    foreach f (lab/RNA/bed/HL60/??/*HL60*.bed)
      set track = `echo $f:t:r:r:r | perl -wpe \
        's/EC_AS_HL60_RWP\+_RA_(\d+)hr_C01vsNULL/encodeAffyRnaHl60SitesHr$1/;'`
      echo $track
      tail +2 $f \
        | hgLoadBed -noBin hg17 $track stdin
    end    
'EOF'
    csh loadSites.csh >&! loadSites.log

    # Update HeLa sites (12/15)
    cd /cluster/data/encode/Affy/2005-11-22
    tail +2 lab//Affy_HeLa/bed/EC_AS_HeLa_RCyP+_C01vsNULL.sig.gr.bed | \
        hgLoadBed -strict -noBin hg17 encodeAffyRnaHeLaSites stdin
            # 7254 elements

    # RNA Signal (6 subtracks)
    set gbdbDir = /gbdb/hg17/encode/Affy/2005-10-03
    mkdir -p $gbdbDir/wib
    mkdir wib wig

    set track = encodeAffyRnaGm06990Signal
    cat lab/RNA/wig/GM06990/EC_AS_GM06990_RCyP+_C01vsNULL.sig.wig \
    | wigEncode stdin wig/$track.wig wib/$track.wib
    ln -s `pwd`/wib/$track.wib $gbdbDir/wib/
    nice hgLoadWiggle hg17 $track wig/$track.wig -pathPrefix=$gbdbDir

    set track = encodeAffyRnaHeLaSignal
    cat lab/RNA/wig/HeLa/EC_AS_HeLa_RCyP+_C01vsNULL.sig.wig \
    | wigEncode stdin wig/$track.wig wib/$track.wib
    ln -s `pwd`/wib/$track.wib $gbdbDir/wib/
    nice hgLoadWiggle hg17 $track wig/$track.wig -pathPrefix=$gbdbDir

cat > loadSig.csh << 'EOF'
    set gbdbDir = /gbdb/hg17/encode/Affy/2005-10-03
    foreach f (lab/RNA/wig/HL60/??/*HL60*C01vsNULL.sig.wig)
      set track = `echo $f:t:r:r:r | perl -wpe \
       's/EC_AS_HL60_RWP\+_RA_(\d+)hr_C01vsNULL/encodeAffyRnaHl60SignalHr$1/;'`
      echo $track
      cat $f \
      | wigEncode stdin wig/$track.wig wib/$track.wib
      ln -s `pwd`/wib/$track.wib $gbdbDir/wib/
      nice hgLoadWiggle hg17 $track wig/$track.wig -pathPrefix=$gbdbDir
    end
'EOF'
    csh loadSig.csh >&! loadSig.log
    # Create a single composite track for RNA and Transfrags

    # Update HeLa signal (2005-12-15 kate)
    cd /cluster/data/encode/Affy/2005-11-22
    mkdir wib wig
    set gbdbDir = /gbdb/hg17/encode/Affy/2005-10-03
    rm $gbdbDir/wib/$track.wib
    set track = encodeAffyRnaHeLaSignal
    cat lab/Affy_HeLa/wig/EC_AS_HeLa_RCyP+_C01vsNULL.sig.wig | \
        wigEncode stdin wig/$track.wig wib/$track.wib
            # Converted stdin, upper limit 1591.50, lower limit -779.75
    ln -s `pwd`/wib/$track.wib $gbdbDir/wib/
    nice hgLoadWiggle hg17 $track wig/$track.wig -pathPrefix=$gbdbDir

    # CHIP/Chip sites (2005-10-24 kate)
    cd /cluster/data/encode/Affy/2005-10-03
    # Load up 12 tables of ChIP/chip sites at (3 factors, 4 timepoints)
    # plus 2 more for ActD at 1 timepoint
cat > loadChipBed.csh << 'EOF'
    foreach f (lab/CHIP/bed/*/??/*.bed)
      set factor = `echo $f:h:h:t | sed 's/Pol2/Rnap/; s/Hish4/H4Kac4/'`
      set hr = $f:h:t
      set table = encodeAffyChIpHl60SitesStrict${factor}Hr$hr
      echo $table
      grep "^chr" $f | hgLoadBed -noBin hg17 $table stdin
    end
    grep "^chr" lab/CHIP/bed/p63_ActD/*.bed | hgLoadBed -noBin hg17 \
                encodeAffyChIpHl60SitesStrictP63_ActD stdin
    grep "^chr" lab/CHIP/bed/p63_mActD/*.bed | hgLoadBed -noBin hg17 \
                encodeAffyChIpHl60SitesStrictP63_mActD stdin
'EOF'
    csh loadChipBed.csh >&! loadChipBed.log

    # Chip/chip signal and pvalue 
cat > loadChipWig.csh << 'EOF'
    set gbdbDir = /gbdb/hg17/encode/Affy/2005-10-03
    foreach d (lab/CHIP/wig/p63_ActD lab/CHIP/wig/p63_mActD)
        set factor = $d:t
        set prefix = encodeAffyChIpHl60;
        set track = ${prefix}SignalStrict$factor
        echo $track
        cat $d/*.sig.median.wig \
            | wigEncode stdin wig/$track.wig wib/$track.wib
        ln -s `pwd`/wib/$track.wib $gbdbDir/wib/
        nice hgLoadWiggle hg17 $track wig/$track.wig -pathPrefix=$gbdbDir
        set track = ${prefix}PvalStrict$factor
        echo $track
        cat $d/*.pval.median.wig \
            | wigEncode stdin wig/$track.wig wib/$track.wib
        ln -s `pwd`/wib/$track.wib $gbdbDir/wib/
        nice hgLoadWiggle hg17 $track wig/$track.wig -pathPrefix=$gbdbDir
    end
    foreach d (lab/CHIP/wig/*/??)
        set hr = $d:t
        set factor = $d:h:t
        set track = ${prefix}SignalStrict${factor}Hr$hr
        echo $track
        cat $d/*.sig.median.wig \
            | wigEncode stdin wig/$track.wig wib/$track.wib
        ln -s `pwd`/wib/$track.wib $gbdbDir/wib/
        nice hgLoadWiggle hg17 $track wig/$track.wig -pathPrefix=$gbdbDir
        set track = ${prefix}PvalStrict${factor}Hr$hr
        echo $track
        cat $d/*.pval.median.wig \
            | wigEncode stdin wig/$track.wig wib/$track.wib
        ln -s `pwd`/wib/$track.wib $gbdbDir/wib/
        nice hgLoadWiggle hg17 $track wig/$track.wig -pathPrefix=$gbdbDir
    end
'EOF'
    csh loadChipWig.csh >&! loadChipWig.log 
    # create 2 composite tracks:
    #  Affy Strict ChIP  (contains Oct freeze Sites and Pval subtracks)
    #  Affy Strict Sig  (contains Oct freeze Signal) 
    # and reformat June freeze tracks (2 composites w/ 10 factors each) as:
    #  Affy Loose ChIP  (contains Jun freeze Sites and Pval subtracks)

##########################################################################
# U North Carolina FAIRE  (2005-10-24 kate)
#       Peaks data updated (2006-04-13 kate and 2006-05-01, hartera)
# Added description for updated Peaks data - provided by 
# Paul Giresi (paulg@email.unc.edu) (2006-06-13, hartera)
# Finished data update for ChIPOTle peaks track and updatd the downloads
# and changed the original Signal and Peaks subtracks to a BED graph
# so that data in tables looks more like the raw data (on request of 
# Paul Giresi) (DONE, 2006-08-16, hartera)
    # submitted by Paul Giresi, from Jason Lieb's lab
    # later, Paul submitted an "averages" file for the
    # raw data (but doesn't include the "peaks")
    # On 10/24, submitted peaks averages.
    # The averages files are:
    # FAIREavg_data.gff (for Signal, averages of all four replicates)
    # FAIREavg_peaks.gff (for Peaks, data after running peak-finding software
    # on the Signal averages data above).
    # Both of these files are in wiggle format. 
    cd /cluster/data/encode
    mkdir UNC/2005-10-10/lab
    cd UNC/2005-10-10/lab
    # copy files from FTP dir
    # 8 .gff data files plus description file
    # NOTE: these are actually .bed and .wig files
    # the .bed files are "peaks", and the .wig are "raw"
    # NOTE: these files are basically replicates,
    # we really want to show just the averages -- 
    # Submitter says OK to just post for download

    mkdir -p download
    # convert to UNIX format
    foreach f (lab/*norm*.gff)
        set t = $f:t:r
        echo $t
        dos2unix -n $f download/$t.bed
    end
    # slightly different format for "peaks" files
    foreach f (lab/*fpr01*.gff)
        set t = $f:t:r
        echo $t
        dos2unix -l -n $f download/$t.bed
    end
    cd download
    gzip *.bed
    md5sum *.bed > md5sum.txt
    # add README file with data terms

    ssh hgwdev
    set dir = /usr/local/apache/htdocs/goldenPath/hg17/encode/datafiles
    mkdir -p $dir
    ln -s /cluster/data/encode/UNC/2005-10-10/download $dir/UncFaire

    # averages 
    # Probes are 50 bp with 12 bp overlap and the start of each spot on 
    # the chromosomes were listed. Changing the span to 38 bp removed the
    # overlap. This should only have been done for the Signal and not Peaks
    # data (from e-mail from Paul Giresi, 2006-08-08)  
    sed 's/span=50/span=38/' lab/FAIREavg_data.gff > Signal.wig
    sed 's/span=50/span=38/' lab/FAIREavg_peaks.gff > Peaks.wig
    #_data.gff:  -2.61 to 3.63
    #_peaks.gff:   .47 to 3.63
    # using viewLimits: .2 to 2.6
    # wiggle0 with span=50
    # around 380K records, so load it wiggle, not bedGraph
cat > load.csh << 'EOF'
    foreach f (Signal.wig Peaks.wig)
        set type = $f:r
        set table = encodeUncFaire$type
        wigEncode $f $table.wig $table.wib
        set dir = /gbdb/hg17/encode/UNC/2005-10-10
        mkdir -p $dir
        hgLoadWiggle -pathPrefix=$dir hg17 $table $table.wig
        mkdir -p $dir
        ln -s `pwd`/$table.wib $dir
    end
'EOF'
    csh load.csh >&! load.log 

    # update peaks data (2006-04-13 kate)
    cd /cluster/data/encode
    mkdir UNC/2006-04-13/lab
    cd UNC/2006-04-13/lab
    # lab/OfficialChIPOTle_PEAKS.gff is a bedGraph format and this contains
    # the new peaks data after a data reanalysis.
    # lab/FAIREavg_OfficialPeaks.gff is a file in wiggle format with
    # the Signal track data first (the same as for the original track)
    # and then the new Peaks data.

    # trim precision for the peaks data:
    awk 'NR !=1 {printf("%s\t%d\t%d\t%.3f\n", $1, $2, $3, $4)}' \
        lab/OfficialChIPOTle_PEAKS.gff > peaks.bedGraph
    # data range: 0 - 3.627
    # load data as bedGraph (2006-05-01, hartera)
    # edit file and remove line: track    0       0       0.000
    # and then load
    hgLoadBed -strict -bedGraph=4 hg17 \
              encodeUncFairePeaksApr2006 peaks.bedGraph
    # added this as a new subtrack to human/trackDb.encode.ra to see what
    # it looks like. 
    # In ~/kent/src/hg/makeDb/trackDb/human/trackDb.encode.ra
    # add the following lines to the subtrack entry since it will inherit
    # from the parent track otherwise which is a wiggle type.
    # track encodeUncFairePeaksApr2006
    subTrack encodeUncFaire
    shortLabel UNC FAIRE Peaks Apr. 06
    longLabel UNC FAIRE Peaks (Formaldehyde Assisted Isolation of Regulatory Elements) Apr. 2006 Update
    noInherit on
    type bedGraph 4
    maxHeightPixels 128:16:16
    autoScale off
    windowingFunction mean
    viewLimits .2:2.6
    color 20,150,20
    altColor 50,100,50
    priority 3
    # Description update added (2006-06-13, hartera).
    # Add new description for new Peaks data subtrack from 
    # FAIRE_peaks_DESC.htm to trackDb/human/encodeUncFaire.html 
    # Data is not correct so reload the Peaks data sent by Paul Giresi
    # FAIRE_peaks1e-025_feat_track.gff in lab directory
    ssh hgwdev
    mkdir -p /cluster/data/encode/UNC/2006-05/lab
    cd /cluster/data/encode/UNC/2006-05
    # remove first line
    tail +2 lab/FAIRE_peaks1e-025_feat_track.gff > peaks.bedGraph
    # and then load as bedGraph
    hgLoadBed -strict -bedGraph=4 hg17 \
              encodeUncFairePeaksChipotle peaks.bedGraph
    # edit human/trackDb.encode.ra entry above to use May2006 table.
    # edit these lines too:
    # track encodeUncFairePeaksChipotle
    # longLabel UNC FAIRE Peaks (Formaldehyde Assisted Isolation of Regulatory
    # Elements) (ChIPOTle)
    # viewLimits 0.0:2.7
    # color 0,0,255

    # Reload the original Peaks and Signal data as BED graph so that the 
    # table downloads look more like the original data so that the number
    # of lines in the table is the same as the number of peaks for the Peaks
    # track. Used original data with span=50. This is the size of the probes.
    cd /cluster/data/encode/UNC/2005-10-10
    mkdir -p bedGraphFormat
    cd bedGraphFormat
    /cluster/bin/scripts/varStepToBedGraph.pl ../lab/FAIREavg_data.gff \
           > signal.bedGraph
    # Processed 385194 lines input, 385149 data lines, 44 variable step
    # declarations 
    /cluster/bin/scripts/varStepToBedGraph.pl ../lab/FAIREavg_peaks.gff \
           > peaksOriginal.bedGraph
    # Processed 845 lines input, 800 data lines, 44 variable step declarations
    # Reload the Signals and Peaks tables with this data.
    hgsql -e "drop table encodeUncFaireSignal;" hg17
    hgsql -e "drop table encodeUncFairePeaks;" hg17
    
    hgLoadBed -strict -bedGraph=4 hg17 \
              encodeUncFaireSignal signal.bedGraph
    hgLoadBed -strict -bedGraph=4 hg17 \
              encodeUncFairePeaks peaksOriginal.bedGraph
    # update human/trackDb.encode.ra so that type is
    # type bedGraph 4
    # for the parent track.
    cd /cluster/data/encode/UNC/2005-10-10/lab
    cp FAIREavg_data.gff ../download/FAIREavg_data.wig
    cp FAIREavg_peaks.gff ../download/FAIREavg_peaks.wig
    cd ../download
    gzip *.wig
    # change the Signal files to wig extension as these are wiggle format
    foreach f (*CHR.bed.gz)
       set g=$f:r:r
       mv $f ${g}.wig.gz
    end
    # Add ChIPOTle data to downloads. 
    cp /cluster/data/encode/UNC/2006-05/lab/FAIRE_peaks1e-025_feat_track.gff \
       FAIRE_peaks1e-025_feat_track.bed
    gzip FAIRE_peaks1e-025_feat_track.bed 
    # Add description of these files to README.txt and update the md5sum.txt
    rm md5sum.txt
    md5sum *.gz > md5sum.txt
   
    # Look at data using histogram to help decide viewLimit:
    cd /cluster/data/encode/UNC/2005-10-10/bedGraphFormat
    textHistogram -binSize=0.1 -maxBinCount=65 -col=4 -minVal=-2.7 -real \
                  signal.bedGraph > signal.hist
    textHistogram -binSize=0.1 -maxBinCount=40 -col=4 -real \
                  peaksOriginal.bedGraph > peaksOriginal.hist
    textHistogram -binSize=0.1 -maxBinCount=40 -col=4 -real \
                  ../../2006-05/peaks.bedGraph > peaksChipotle.hist
    # set minLimit, maxLimit, viewLimit and increased default pixel size for 
    # subtrack so that y axis scale is shown, in human/trackDb.encode.ra:
    # maxHeightPixels 128:24:16
    # minLimit -2.61
    # maxLimit 3.63
    # viewLimits -0.6:0.7
    # for the Peaks subtracks, the viewLimits were set as:
    # viewLimits 0.4:3.7

##########################################################################
# Gencode Genes (2005-10-10 kate)
#    Files are on Gencode/IMIM web site, our contact for this round is France Denoed
#    France requested 3 subtracks: genes, putatives, and pseudogenes        
# NTOE: reloaded encodeGencodeKnown from updated _genes_ file 10/14 (kate)
    cd /cluster/data/encode
    mkdir -p Gencode/2005-10-07/lab
    cd Gencode/2005-10-07/lab

    wget ftp://genome.imim.es/pub/other/gencode/data/havana-encode/current/44regions/README
    wget ftp://genome.imim.es/pub/other/gencode/data/havana-encode/current/44regions/44regions_genes_CHR_coord.gtf
    wget ftp://genome.imim.es/pub/other/gencode/data/havana-encode/current/44regions/44regions_putative_CHR_coord.gtf
    wget ftp://genome.imim.es/pub/other/gencode/data/havana-encode/current/44regions/44regions_pseudogenes_CHR_coord.gtf

    cd ..
    ldHgGene -gtf -genePredExt hg17 encodeGencodeKnown \
        lab/44regions_genes_CHR_coord.gtf

            # Read 2637 transcripts in 45565 lines in 1 files
            # 2637 groups 21 seqs 13 sources 5 feature types
            # 2608 gene predictions
    genePredCheck -db=hg17 encodeGencodeKnown

    ldHgGene -gtf -genePredExt hg17 encodeGencodePutative lab/44regions_putative_CHR_coord.gtf
            # 156 gene predictions
    genePredCheck -db=hg17 encodeGencodePutative

    ldHgGene -gtf -genePredExt hg17 encodeGencodePseudo lab/44regions_pseudogenes_CHR_coord.gtf
            # 197 gene predictions
    genePredCheck -db=hg17 encodeGencodePutative
    # create composite track: "Gencode Oct Gene" with 3 subtracks

    # Introns track
    grep intron lab/*.gtf | wc -l
        # 25421
    # ignore "not tested" introns
    grep intron lab/*.gtf | grep -v not_tested | wc -l
        # 483
    # NOTE: need verision of loader with new status value added
    cat lab/*.gtf | grep -v not_tested | sed -e 's/-intron/-/g' | \
        ~/bin/i386/ldGencodeIntron hg17 encodeGencodeIntronOct stdin
            # 483 introns in 1 files

    # create gene class table
    sed 's/gencodeGeneClass/gencodeGeneClassOct/' \
        ~/kent/src/hg/lib/gencodeGeneClass.sql | hgsql hg17
    cat lab/*.gtf | grep VEGA | \
        awk '{printf "%s\t%s\n", $10, $2}' | \
        sed -e 's/"//g' -e 's/;//' -e 's/VEGA_//' \
            -e 's/_val/_gencode_conf/' -e 's/Antisense/Novel_transcript/' | \
        sort | uniq > gencodeGeneClassOct.tab
    wc -l gencodeGeneClassOct.tab
        #  2961
    echo "LOAD DATA LOCAL INFILE 'gencodeGeneClassOct.tab' into table gencodeGeneClassOct" | hgsql hg17


##########################################################################
# NHGRI DNaseI HS (2005-10-24 kate)
#       Submitter: Greg Crawford
#       2 datasets:  CD4, GM06690 with different methodology from previous
#       Additional (raw) data for both cell types submitted 12/6/05

# Additional data submitted 8/10/06: Raw & Pval for HelaS3 and GM cells
# Submitted new PVAL (3 cell lines) data on 8/11
# Additional data submission 9/22/06  -- HepG2 cell line

    cd /cluster/data/encode/NHGRI/crawford
    mkdir -p 2005-10-11/lab
    cd 2005-10-11/lab
    # copy 2 data files from FTP site

    # lift to hg17
    ln -s lab/Crawford_DNase_chip_CD4_hg16.txt Cd4.hg16.bed
    awk '{printf "%s\t%s\t%s\t%s\t%s\n", $1,$2,$3,$4,$6}' \
        lab/Crawford_DNase_chip_GM06990_hg16.txt > Gm06990.hg16.bed
    # oops - mistakenly deleted lab/Crawford*txt files
cat > load.csh << 'EOF'
    foreach f (Cd4.hg16.bed Gm06990.hg16.bed)
        set cell = $f:r:r
        liftOver $cell.hg16.bed \
                 /cluster/data/hg16/bed/liftOver/hg16ToHg17.over.chain \
                        $cell.hg17.bed $cell.unmapped
        hgLoadBed hg17 encodeNhgriDnaseHsChip$cell $cell.hg17.bed
    end
'EOF'
    csh load.csh >&! load.log

    # Add these two tracks to the hg17 NHGRI DNase track
    # Rename 2 data tables lifted from hg16
    hgsql hg17 -e "ALTER TABLE encodeNhgriDnaseHsAct RENAME TO encodeNhgriDnaseHsMpssCd4Act"
    hgsql hg17 -e "ALTER TABLE encodeNhgriDnaseHsNonAct RENAME TO encodeNhgriDnaseHsMpssCd4"

    # Raw data
    ln -s lab/NHGRI_DNase_chip_CD4_na_RAW.bed Cd4.raw.hg16.bed
    ln -s lab/NHGRI_DNase_chip_GM_RAW.bed  Gm06990.raw.hg16.bed
cat > loadRaw.csh << 'EOF'
    foreach f (Cd4.raw.hg16.bed Gm06990.raw.hg16.bed)
        set cell = $f:r:r:r
        liftOver $f \
                 /cluster/data/hg16/bed/liftOver/hg16ToHg17.over.chain \
                        $cell.raw.hg17.bed $cell.raw.unmapped
        hgLoadBed -strict -bedGraph=4 hg17 \
                encodeNhgriDnaseHsChipRaw$cell $cell.raw.hg17.bed
    end
'EOF'
    csh loadRaw.csh >&! loadRaw.log
    # Loaded 382713 elements of size 4

    # 8/10/06 Data submission
    cd /cluster/data/encode/NHGRI/crawford
    mkdir -p 2006-08-10/lab
    cd 2006-08-10/lab
    cp -p /var/ftp/encode/Crawford* .
    ls
# Crawford_DNase-chip_GM06990_PVAL_hg17.bed
# Crawford_DNase-chip_GM06990_RAW_HG17.bed
# Crawford_DNase-chip_HeLaS3_PVAL_HG17.bed
# Crawford_DNase-chip_HeLaS3_RAW_hg17.bed

    # NOTE: the GM Raw data is identical to that submitted on 10/11/05
    set cell = Gm06990
    awk '{print $1, $2, $3, $5}' lab/Crawford_DNase-chip_GM06990_RAW_HG17.bed \
        > $cell.raw.hg17.bed
    hgLoadBed -strict -bedGraph=4 hg17 \
                encodeNhgriDnaseHsChipRaw$cell $cell.raw.hg17.bed
    # Loaded 382713 elements of size 4
    set cell = Hela
    awk '{print $1, $2, $3, $5}' lab/Crawford_DNase-chip_HeLaS3_RAW_hg17.bed \
        > $cell.raw.hg17.bed
    hgLoadBed -strict -bedGraph=4 hg17 \
                encodeNhgriDnaseHsChipRaw$cell $cell.raw.hg17.bed
    # Loaded 385149 elements of size 4
    # Note: different item count from data for CD4 and GM

# Submitted new PVAL data on 8/11
#Crawford_DNase-chip_CD4_PVAL_hg17.bed10
#Crawford_DNase-chip_GM06990_PVAL_hg17.bed10
#Crawford_DNase-chip_HeLaS3_PVAL_hg17.bed12

  ln -s lab/Crawford_DNase-chip_GM06990_PVAL_hg17.bed10 Gm06990.pval.hg17.bed
  ln -s lab/Crawford_DNase-chip_HeLaS3_PVAL_hg17.bed12 Hela.pval.hg17.bed
  ln -s lab/Crawford_DNase-chip_CD4_PVAL_hg17.bed10 Cd4.pval.hg17.bed

# load Pval data as bed5floatscore, with pval mapped to integer score (0-1000)
# for display purposes
# format: chr start end name score pVal
cat > loadPval.csh << 'EOF'
    foreach cell (Cd4 Gm06990 Hela)
        set lcell = `echo $cell | sed 's/\(.*\)/\L\1/'`
        awk -v CELL=$lcell '/^chr/ {printf("%s\t%d\t%d\t%s_%d\t%d\t%.3f\n", $1, $2, $3, CELL, NR-1, $5 * 35 + 100, $5)}' $cell.pval.hg17.bed > $cell.pval.bed5+
        set table = encodeNhgriDnaseHsChipPval$cell
        sed "s/bed5FloatScore/$table/" ~/kent/src/hg/lib/bed5FloatScore.sql > \
                $table.sql
        hgsql hg17 -e "DROP TABLE IF EXISTS $table"
        hgsql hg17 < $table.sql
        hgLoadBed -strict -sqlTable=$table.sql hg17 \
                $table $cell.pval.bed5+
        checkTableCoords hg17 $table
        end
'EOF'
    csh loadPval.csh >&! loadPval.log 
# min = 3.13 max  = 24.513
    # Reading Cd4.pval.bed5+
    #Loaded 1262 elements of size 6
    #Reading Gm06990.pval.bed5+
    #Loaded 1098 elements of size 6
    #Reading Hela.pval.bed5+
    #Loaded 1042 elements of size 6

# data submission 9/22/06  -- HepG2 cell line
    cd /cluster/data/encode/NHGRI/crawford
    mkdir -p 2006-09-22/lab
    cd 2006-09-22
    ln -s lab/Crawford_DNase-chip_HepG2_PVAL_HG17.bed HepG2.pval.bed
    ln -s lab/Crawford_DNase-chip_HepG2_RAW_hg17.bed HepG2.raw.bed

    set cell = HepG2

    # load RAW
    awk '{print $1, $2, $3, $5}' $cell.raw.bed | \
                hgLoadBed -strict -bedGraph=4 hg17 \
                        encodeNhgriDnaseHsChipRaw$cell stdin
        # Loaded 385149 elements

    # load PVAL
    set lcell = hepg2
    awk -v CELL=$lcell '/^chr/ {printf("%s\t%d\t%d\t%s_%d\t%d\t%.3f\n", $1, $2, $3, CELL, NR, $5 * 35 + 100, $5)}' $cell.pval.bed > $cell.pval.bed5+
    set table = encodeNhgriDnaseHsChipPval$cell
    sed "s/bed5FloatScore/$table/" ~/kent/src/hg/lib/bed5FloatScore.sql > \
            $table.sql
    hgsql hg17 -e "DROP TABLE IF EXISTS $table"
    hgsql hg17 < $table.sql
    hgLoadBed -strict -sqlTable=$table.sql hg17 \
            $table $cell.pval.bed5+
    checkTableCoords hg17 $table

##########################################################################
# Sanger Chip/chip Hits and Centers (2005-10-24 kate)
# From Paul Flicek, at EBI 
# 14 files (3 cells, most with 5 factors), each file having
    # 3 tracks:  chip/chip, HMM regions, HMM centers
    # Christoph says to just display the HMM regions & centers tracks
    # from Paul's files. These were generated from June freeze
    # chip/chip, plus newly submitted HeLa data from Christoph Koch (10/7).
    cd /cluster/data/encode/sanger/chipchip
    mkdir -p 2005-10-18/lab
    cd 2005-10-18/lab

    # HeLa chip/chip
cat > loadHela.csh << 'EOF'
    foreach f (lab/*_HeLa-S3_1.wig.txt)
        set b = `echo $f:t:r:r | sed 's/-S3_1//; s/_//'`
        echo $b
        grep "^chr" $f | sort -k1,1 -k2,2n > chip.$b.wig
        hgLoadBed -bedGraph=4 hg17 encodeSangerChip$b chip.$b.wig 
    end
'EOF'
    csh loadHela.csh >&! loadHela.log &

    # split HMM tracks out of files
cat > load.csh << 'EOF'
    foreach f (lab/*.split.wig.txt)
        set b = `echo $f:t:r:r:r | sed 's/-2//; s/-//g'`
        echo $b
        /cluster/data/encode/bin/scripts/splitTracks.pl $f
        rm t0
        grep '^chr' t1 | sort -k1,1 -k2,2n > $b.wig; rm t1
        hgLoadBed -bedGraph=4 hg17 encodeSangerChipHit$b $b.wig
        checkTableCoords hg17 encodeSangerChipHit$b
        grep '^chr' t2 | sed 's/	1$//' > $b.bed; rm t2
        hgLoadBed -noBin hg17 encodeSangerChipCenter$b $b.bed
        checkTableCoords hg17 encodeSangerChipCenter$b
    end
'EOF'
    csh load.csh >&! load.log 
    
#############################################################################
#  Measuring TARs and TransFrags distances to SINEs and LINEs
#
#
#	Using the table browser on genome.ucsc.edu on Hg17, select the
#	Alu SINEs and L1,L2 LINEs by setting filter at:
#	repClass=LINE or SINE
#	repFamily=L1, L2 or Alu
#	request fields: swScore, genoName, genoStart, genoEnd, repNames
#	save to file L1_LINE_Hg17.txt.gz, L2_LINE_Hg17.txt.gz
#	Alu_SINE_Hg17.txt.gz
##########################################################################
# UW/Regulome DnaseI HS (2005-10-28, 11-17 kate)
# NOTE: trimmed overlaps in baseline files, as per Scott Kuehn

    cd /cluster/data/encode/Regulome
    mkdir -p 2005-11-16
    cd 2005-11-16
cat > load.csh << 'EOF'
    foreach cell (CACO2 CD34 GM HeLa HepG2 Huh7 K562 SKNSH)
        echo $cell
        hgLoadBed -noBin -strict hg17 \
            encodeRegulomeQuality$cell lab/$cell.qc.bed
        hgLoadBed -noBin -strict hg17 \
            encodeRegulomeAmplOdd$cell lab/$cell.oddAmps.bed
        hgLoadBed -noBin -strict hg17 \
            encodeRegulomeAmplEven$cell lab/$cell.evenAmps.bed
        hgLoadBed -noBin -strict -bedGraph=5 hg17 \
            encodeRegulomeProb$cell lab/$cell.hs.bed
        sort -k1,1 -k2,2n lab/$cell.baseline.bed | \
            /cluster/data/encode/bin/scripts/trimOverlap.pl | \
            hgLoadBed -noSort -noBin -strict -bedGraph=5 hg17 \
                encodeRegulomeBase$cell stdin
    end
'EOF'
    csh load.csh >&! load.log &

##########################################################################
# UC Davis Chip/chip (new C-Myc data) (2005-10-29 kate)
# Add as subtrack to existing track
# New datafiles for hits (c-Myc and E2F1) submitted 2006-10-24
# by Mark Bieda <mcbieda@ucdavis.edu>

    # convert to bedGraph
    cd /cluster/data/encode/UcDavis/2005-10-12
    set table = encodeUCDavisChipMyc
    awk '{printf "%s\t%s\t%s\t%s\n", $1,$4,$5,$6}' lab/myc_median.gff | \
	sort -k1,1 -k2,2n > $table.bed
    hgLoadBed -strict -bedGraph=4 hg17 $table $table.bed
        # Loaded 385149 elements

    # hits data
    # 2 files: E2F1_HelaFIGS_T02P0001S50G2CHR.gff  myc_helafix_hg17_T02P0001S50G2CHR.gff
    # NOTE: E2F1 data submitted in hg16 coords
    # Load as bed 5 -- generating item names from <chr>_<start>
    #  at recommendation of Mark Bieda
    cd /cluster/data/encode/UcDavis
    mkdir -p 2006-10-24/lab
    # copy files from FTP dir
    cd 2006-10-24
    awk '{printf "%s\t%d\t%d\t%s_%s\t%d\n", $1,$4,$5,$1,$4,$6}' \
        lab/E2F1_HelaFIGS_T02P0001S50G2CHR.gff > e2f1.hg16.bed
    liftOver e2f1.hg16.bed \
        /cluster/data/encode/convertHg17/hg16ToHg17.over.chain.gz \
                e2f1.hg17.bed e2f1.unmapped
    # 1 unmapped (in ENm006)
    #  chrX    152137876       152138192       chrX_152137876  2
    hgLoadBed -strict hg17 encodeUcDavisChipHitsE2F1 e2f1.hg17.bed
        # Loaded 204 elements of size 5 
    checkTableCoords hg17 encodeUcDavisChipHitsE2F1

    awk '{printf "%s\t%d\t%d\t%s_%s\t%d\n", $1,$4,$5,$1,$4,$6}' \
        lab/myc_helafix_hg17_T02P0001S50G2CHR.gff > myc.hg17.bed
    hgLoadBed -strict hg17 encodeUcDavisChipHitsMyc myc.hg17.bed
        # Loaded 172 elements of size 5
    checkTableCoords hg17 encodeUcDavisChipHitsMyc

    # NOTE: drop old tables after review

##########################################################################
# Yale TAR and TransMap (2005-10-31 kate)
# Submitted: 10/14 by Joel Rozowsky
# 5 bed files (TARs) and 5 wig files (Signal)
# Replacements for June tracks (and drop individual 10 Neu samples)
# Methods changed somewhat -- use new description from Joel's email
# NOTE: adjusted start coord -1 to coorespond to their DART entries --
# verifying with Joel

    cd /cluster/data/encode/yale/rna/2005-10-14
cat > loadBed.csh << 'EOF'
    foreach f (lab/*.bed)
        set table = `echo $f:t:r | sed 's/_//g; s/CTRL/Untr/; s/ncbi35//; s/Placenta/Plac/; s/Neutrophil/Neut/'`
        echo $table
        sed 's/http.*acc=//' $f | \
            awk '{printf "%s\t%d\t%d\t%s\n", $1, $2-1, $3, $4}' | \
                hgLoadBed -strict hg17 $table stdin
    end
'EOF'
    csh loadBed.csh >&! loadBed.log 

# NOTE: trim overlaps in regions resulting from array design
# Joel should have done this for us -- he will verify the files.
# Also, need to adjust coords +1 as per J. Rozowsky
cat > loadSig.csh << 'EOF'
    mkdir -p wig wib
    set gdir = /gbdb/hg17/encode/YaleRna/2005-10-14
    mkdir -p $gdir/wib
    foreach f (lab/*.wig)
        set table = `echo $f:t:r | sed 's/_//g; s/Transcript/Trans/; s/CTRL/Untr/; s/ncbi35//; s/Placenta/Plac/; s/Neutrophil/Neut/'`
        echo $table
        grep "^chr" $f | \
            awk '{printf "%s\t%d\t%d\t%s\n", $1, $2+1, $3+1, $4}' | \
            sort -k1,1 -k2,2n | \
            /cluster/data/encode/bin/scripts/trimOverlap.pl > $table.trim
        wigEncode $table.trim wig/$table.wig wib/$table.wib
        hgLoadWiggle -pathPrefix=$gdir hg17 $table wig/$table.wig
        ln -s `pwd`/wib/$table.wib $gdir/wib
    end
'EOF'
    csh loadSig.csh >&! loadSig.log 
    rm -f *.trim
    
##########################################################################
# Yale Chip/chip (2005-10-31 kate)
# Final submission: 10/26 by Zhengdong Zhang
# signal, pval, and sites for 5 factors (50x38 array)
# Sites file has URL to Gerstein lab as 5th field.
# I'm extracting the accession from it and saving
# as the name field in a BED5.  Score range: .602-3.23
#  Scale *330 produces integer range 200-1000.
# NOTE: >50% of sites are < 1 data value, so use 200 as low score

    cd /cluster/data/encode/yale/chip/2005-10-26
    mkdir -p wig wib
cat > loadSites.csh << 'EOF'
    set pfx = encodeYaleChip
    foreach d (lab/{jun,fos,taf,baf155,baf170})
        set factor = $d:t
        set Factor = `echo $factor | sed 's/\(.*\)/\u\1/'`
        echo $Factor
        set p = $d/Encode_Yale_ChIpChip_${factor}_Hela_Maskless50merevery38bp

        # load sites
        set table = ${pfx}Sites$Factor
        echo $table
        set f = ${p}_Sites.bed
        dos2unix $f
        sed -e "s/bed5FloatScore/$table/" \
            $HOME/kent/src/hg/lib/bed5FloatScore.sql > $table.sql
        sed 's/=/ /' $f | \
          awk '{printf "%s\t%d\t%d\t%s\t%d\t%.3f\n",$1,$2-1,$3,$6,($4 * 330),$4}' |\
            hgLoadBed -strict -sqlTable=$table.sql hg17 $table stdin
        end
'EOF'
    csh loadSites.csh >&! loadSites.log 

cat > loadSig.csh << 'EOF'
    set pfx = encodeYaleChip
    foreach d (lab/{jun,fos,taf,baf155,baf170})
        set factor = $d:t
        set Factor = `echo $factor | sed 's/\(.*\)/\u\1/'`
        echo $Factor
        set p = $d/Encode_Yale_ChIpChip_${factor}_Hela_Maskless50merevery38bp

        # load pval 
        set table = ${pfx}Pval$Factor
        echo $table
        set f = ${p}_Pvalue.wig
        sort -k1,1 -k2,2n $f | \
            /cluster/data/encode/bin/scripts/trimOverlap.pl > $table.trim
        hgLoadBed -strict -bedGraph=4 hg17 $table $table.trim

        # load signal 
        set table = ${pfx}Signal$Factor
        echo $table
        set f = ${p}_Signal.wig
        sort -k1,1 -k2,2n $f | \
            /cluster/data/encode/bin/scripts/trimOverlap.pl > $table.trim
        hgLoadBed -strict -bedGraph=4 hg17 $table $table.trim 
    end
'EOF'
    csh loadSig.csh >&! loadSig.log &
    rm -f *.trim *.sql

    # description files
    # use server with "antiword" available
    foreach d (lab/{jun,fos,taf,baf155,baf170})
        antiword $d/*.doc > ${d:t}.txt
    end

##########################################################################
# UTexas STAGE (2005-10-31, 11-17 kate)
# Submitted 10/15 by Akshay Bhinge <akshayb@mail.utexas.edu>
# Resubmitted 11/17
#  2 files - raw and peaks, for c-Myc in HeLa
# range .001 to 1.0.  Peaks restricted to >.8
# Adjusted data in Tags file:  set score=1 items to 300 so
# they'll be visible with gray-scale tags requested by Akshay.
# (This is why it's loaded as blocked bed).  Huh ??
#
# New data (raw tags for STAT1 in HeLa) submitted 2006-10-16
    #cd /cluster/data/encode/UTexas/stage/2005-10-15
    cd /cluster/data/encode/UTexas/stage/2005-11-17
    grep '^chr' lab/myc.tag.prob.bed | \
        awk '{if ($5 == 1) $5 = 300; \
                printf("%s\t%d\t%d\t%s\t%d\t%s\t%d\t%d\t0\t1\t%d,\t0\n", \
                $1, $2, $3, $4, $5, $6, $2, $3, $3 - $2)}' | \
        hgLoadBed -noBin -strict hg17 encodeUtexStageMycHelaTags stdin
            # 813 elements
    grep '^chr' lab/myc.stage.peaks.bed | \
        hgLoadBed -noBin -strict hg17 \
                encodeUtexStageMycHelaPeaks  stdin
            # 26 elements
    # Created composite track with 2 subtracks

    # 2006-10-30
    # Reload cMyc data in simple bed w/o score adjustment
    # and load new data

    cd /cluster/data/encode/UTexas/stage
    mkdir -p 2006-10-16/lab
    rm latest; ln -s 2006-10-16 latest
    cd latest
    grep '^chr' lab/stat1.tags.ucsc.bed | \
       awk '{printf("%s\t%d\t%d\t%s\t%d\n",$1,$2,$3,$4,$5)}' | \
    hgLoadBed -noBin -strict hg17 encodeUtexStageStat1HelaTags stdin
        # Loaded 937 elements of size 6
    checkTableCoords hg17 encodeUtexStageStat1HelaTags

    cd ../2005-11-17
    grep '^chr' lab/myc.tag.prob.bed | \
       awk '{printf("%s\t%d\t%d\t%s\t%d\n",$1,$2,$3,$4,$5)}' | \
    hgLoadBed -noBin -strict hg17 encodeUtexStageCMycHelaTags stdin

    
##########################################################################
# Univ. Uppsala, Sweden Chip/chip
# Submitted by Claes & Ola
# 4 files with chrom, start, end, integer score
# Sites file in GFF format
# NOTE: this was submitted in hg16, without notifying us.
# I'm reloading after lifting, 2005-12-05

    cd /cluster/data/encode/Uppsala/2005-11-07
    /cluster/data/encode/bin/scripts/splitTracks.pl lab/chip.wig
    mv t0 Usf1.wig
    mv t1 Hnf3b.wig
    mv t2 Hnf4a.wig
    mv t3 Ach3.wig
    mv t4 Sites.gff

    # load data for individual factors
    # NOTE: rounded overly long float scores
cat > load.csh << 'EOF'
    foreach factor (Usf1 Hnf3b Hnf4a Ach3)
      awk '/^chr/ {printf("%s\t%s\t%s\t%.3f\n", $1, $2, $3, $4)}' $factor.wig |\
        liftOver stdin /cluster/data/encode/convertHg17/hg16ToHg17.chain \
                $factor.hg17.bed $factor.unmapped
        hgLoadBed -strict -bedGraph=4 hg17 encodeUppsalaChip$factor $factor.hg17.bed
    end
'EOF'
    csh load.csh >&! load.log &
    
    # sites (they refer to as Tentative Binding Sites)
    # NOTE: I added an item name, of the form "uutbs.#"
    grep -v track Sites.gff | sort -k1,1 -k2,2n | \
        awk '{printf ("%s\t%d\t%d\tuutbs.%d\t%d\n", $1, $4, $5, NR, $6)}' | \
            liftOver stdin /cluster/data/encode/convertHg17/hg16ToHg17.chain \
                sites.hg17.bed sites.unmapped
            hgLoadBed -noSort -noBin -strict hg17 encodeUppsalaChipSites sites.hg17.bed
                # Loaded 327 elements of size 5

##########################################################################
# MSA tracks from Sept. 2005 freeze
# Use links from Wiki for data submission (as per Elliott Margulies)
# NOTE: mapping of sequence name to assembly is in column 7 of
# metadata.txt file in Elliott's MSA release
# Assemblies in this freeze are: canFam1 danRer2 fr1 galGal2 mm6
#       monDom1 panTro1 rheMac1 rn3 tetNig1
# NOTE: reloaded phastCons scores (previously only manual regions
#       were loaded (2006-05-03 kate)
# Reloaded elements with updated files from Elliott (2006-06-22 kate)
    # TBA alignments
    cd /cluster/data/encode/TBA
    mkdir -p SEP-05/lab
    cd SEP-05/lab
    wget ftp://kronos.nhgri.nih.gov/pub/outgoing/elliott/msa/SEP-2005/tba.v2.maf.tar

    cd ..
    foreach f (lab/*/*.maf.gz)
        echo $f
        gunzip -c $f | \
            sed 's/^s human\./s hg17./;          s/^s dog\./s canFam1./; \
                 s/^s zebrafish\./s danRer2./;   s/^s fugu\./s fr1./; \
                 s/^s chicken\./s galGal2./;     s/^s mouse\./s mm6./; \
                 s/^s monodelphis\./s monDom1./; s/^s chimp\./s panTro1./; \
                 s/^s macaque\./s rheMac1./;     s/^s rat\./s rn3./; \
                 s/^s tetraodon\./s tetNig1./;' \
                        > $f:t:r:r:e.maf
    end
    set gdir = /gbdb/hg17/encode/TBA/maf
    mkdir -p $gdir
    rm -f $gdir/*.maf
    ln -s /cluster/data/encode/TBA/SEP-05/*.maf $gdir
    hgLoadMaf -pathPrefix=$gdir -WARN hg17 encodeTbaAlign >&! load.log
    # lots of "score too small" messages -- these are OK.
    cat *.maf | hgLoadMafSummary hg17 encodeTbaSummary stdin

    # create tree image:
    # edit tree.nh to create species.nh with common names
    cd /cluster/data/encode/MSA/SEP-2005
    mkdir phylo
    cd phylo
    wget ftp://kronos.nhgri.nih.gov/pub/outgoing/elliott/msa/SEP-2005/phylo/tree_4d.tba.v2.nh
    /cluster/bin/phast/draw_tree -b -s tree_4d.tba.v2.nh > species28.ps
        # photoshop to enhance, then save as gif/jpg
        cp /cluster/data/encode/MSA/SEP-2005/phylo/species28.jpg \
            /usr/local/apache/htdocs/images/phylo/species28.jpg

    #  MLAGAN alignments
    cd /cluster/data/encode/MLAGAN
    mkdir -p SEP-05/lab
    cd SEP-05/lab
    wget http://ai.stanford.edu/~asimenos/ENCODE_Oct-2005_maf.tgz
    cd ..
cat > project.csh << 'EOF'
    mkdir -p tmp
    set tmpDir = tmp
    foreach d (lab/EN[mr]*)
        set r = $d:t
        echo $r
        set c = `echo "SELECT chrom from encodeRegions WHERE name='$r'" | \
                        hgsql -N hg17`
        set start =  \
                `echo "SELECT chromStart from encodeRegions WHERE name='$r'" | \
                        hgsql -N hg17`
        set size = \
                `echo "SELECT size from chromInfo WHERE chrom='$c'" | \
                        hgsql -N hg17`
        /cluster/data/encode/MLAGAN/mafCoord.pl < $d/$r.maf \
                human.1 hg17.$c $start $size | \
            sed 's/^a$/a score=0.0/' > $tmpDir/$r.db.maf
        echo "projecting $r"
        /cluster/bin/penn/maf_project $tmpDir/$r.db.maf hg17.$c > $r.maf
        echo "finished $r"
    end
'EOF'
    set gdir = /gbdb/hg17/encode/MLAGAN/SEP-05/maf
    mkdir -p $gdir
    rm -f $gdir/*.maf
    ln -s /cluster/data/encode/MLAGAN/SEP-05/*.maf $gdir
    hgLoadMaf -pathPrefix=$gdir -WARN hg17 encodeMlaganAlign >&! load.log
    # lots of "score too small" messages -- these are OK.
    cat *.maf | hgLoadMafSummary hg17 encodeMlaganSummary stdin

    # MAVID alignments
    cd /cluster/data/encode/MAVID
    mkdir -p SEP-05/lab
    cd SEP-05/lab
    wget http://hanuman.math.berkeley.edu/~cdewey/encode/alignments/ENCODE_SEP-2005_MAVID_MAF_ABS.tar.gz
    cd ..
cat > project.csh << 'EOF'
    set tmpDir = tmp
    mkdir $tmpDir
    foreach f (lab/ABS/*.maf)
        set r = $f:t:r
        echo $r
        set c = `echo "SELECT chrom from encodeRegions WHERE name='$r'" | \
                        hgsql -N hg17`
        sed 's/^a$/a score=0.0/; s/^s  *human/s hg17/' $f > $tmpDir/$r.maf
        echo "projecting $r"
        /cluster/bin/penn/maf_project $tmpDir/$r.maf hg17.$c > $r.maf
        echo "finished $r"
    end
'EOF'
    set gdir = /gbdb/hg17/encode/MAVID/SEP-05/maf
    mkdir -p $gdir
    rm -f $gdir/*.maf
    ln -s /cluster/data/encode/MAVID/SEP-05/*.maf $gdir
    hgLoadMaf -pathPrefix=$gdir -WARN hg17 encodeMavidAlign >&! load.log
    cat *.maf | hgLoadMafSummary hg17 encodeMavidSummary stdin

    # conserved elements
    # Scores:  binCons are all 1000, gerp range is 6.75 - 4813.26
    #          phastCons is 10-18088
    # Force gerp to integer for consistent table format, but don't
    # bother scaling at this point (and don't use to score on display)
    # For some reason, phastCons has + strand -- strip this out
    # NOTE: coords are ENCODE-region based, so need to adjust
    # by start of region (Elliott used custom tracks offset= to do this).

    # NOTE: Updated GERP elements 2/1/06, with new data from Greg Cooper
    # overwriting Elliott's elements.  This is doc'ed in the GERP section.
    cd /cluster/data/encode/MSA
    mkdir -p SEP-05/elements.2005-12-12/lab
    cd SEP-05/elements.2005-12-12/lab
    wget ftp://kronos.nhgri.nih.gov/pub/outgoing/elliott/msa/SEP-2005/cons/target.align.conservation.v1.tar.gz

    # data update from Elliott , to fix off-by-one start coords
    mkdir -p SEP-05/elements.2006-06-22/lab
    cd SEP-05/elements.2006-06-22/lab
    # copy in align_elements_tracks.tar.gz
    # contains 9 tracks of elements (3 aligners * binCons, gerp, phastCons)
    cd ..
cat > load.csh << 'EOF'
    foreach f (lab/*.bed)
        set root = $f:t:r
        set align = `echo $root:e | perl -wpe  's/(.*)/\u$1/'`
        set cons = `echo $root:r | perl -wpe 's/(.*)/\u$1/'`
        set table = encode${align}${cons}El
        hgLoadBed -strict hg17 $table $f
    end
'EOF'
    csh load.csh >&! load.log &

    # CONSENSUS ELEMENTS
    cd /cluster/data/encode/MSA
    mkdir -p SEP-05/consensus/lab
    cd SEP-05/consensus/lab
    wget ftp://kronos.nhgri.nih.gov/pub/outgoing/elliott/msa/SEP-2005/cons/consensus.conservation.v1.tar.gz
    cd ..
    ln -s lab/or.or.bed MsaElUnion.bed
    ln -s lab/and.and.bed MsaElIntersect.bed
    ln -s lab/two.two.bed MsaElModerate.bed
cat > load.csh << 'EOF'
    foreach f (MsaEl*.bed)
        echo $f
        set b = $f:r
        set t = encode$b
        hgLoadBed -strict -noBin hg17 $t $f
    end
'EOF'
    csh load.csh >&! load.log
        #Reading MsaElIntersect.bed
        #Loaded 30645 elements of size 4
        #Reading MsaElModerate.bed
        #Loaded 36793 elements of size 4

    # conservation
    cd /cluster/data/encode/MSA
    mkdir -p SEP-05/conservation/lab
    cd SEP-05/conservation/lab
    wget ftp://kronos.nhgri.nih.gov/pub/outgoing/elliott/msa/SEP-2005/cons/phastCons.wig.tar.gz
    cd ..

cat > load.csh << 'EOF'
    # TBA
    gunzip -c lab/tba/*/phast/human.EN*.gz | \
        wigEncode stdin tbaPhastCons.wig tbaPhastCons.wib
    set d = /gbdb/hg17/encode/TBA/SEP-05
    ln -s `pwd`/tbaPhastCons.wib $d
    hgLoadWiggle -pathPrefix=$d hg17 encodeTbaPhastCons tbaPhastCons.wig

    # MLAGAN
    gunzip -c lab/mlagan/*/phast/human.EN*.gz | \
        wigEncode stdin mlaganPhastCons.wig mlaganPhastCons.wib
    set d = /gbdb/hg17/encode/MLAGAN/SEP-05
    ln -s `pwd`/mlaganPhastCons.wib $d
    hgLoadWiggle -pathPrefix=$d hg17 encodeMlaganPhastCons mlaganPhastCons.wig

    # MAVID
    gunzip -c lab/mavid/*/phast/human.EN*.gz | \
        wigEncode stdin mavidPhastCons.wig mavidPhastCons.wib
    set d = /gbdb/hg17/encode/MAVID/SEP-05
    ln -s `pwd`/mavidPhastCons.wib $d
    hgLoadWiggle -pathPrefix=$d hg17 encodeMavidPhastCons mavidPhastCons.wig

'EOF'
    csh load.csh >&! load.log &

##########################################################################
# MSA GERP Conservation (2005-02-06 kate)
#  Submitted 2/1/06 by Greg Coooper

    cd /cluster/data/encode/MSA/Gerp
    mkdir -p 2006-02-01/lab
    cd 2006-02-01/lab
    wget http://baumbox.stanford.edu/~coopergm/ENCODE/GERP_Cons_SepFreeze_Jan.zip 
    unzip GERP_Cons_SepFreeze_Jan.zip 
    cd ..

    # TBA
    cat lab/chr*_GERP_TBA_scores.wig | \
        wigEncode stdin tbaGerpCons.wig tbaGerpCons.wib
        #  upper limit 4.48, lower limit -29.86
    set d = /gbdb/hg17/encode/TBA/SEP-05
    ln -s /cluster/data/encode/MSA/Gerp/2006-02-01/tbaGerpCons.wib $d
    hgLoadWiggle -pathPrefix=$d hg17 encodeTbaGerpCons tbaGerpCons.wig

    # MLAGAN 
    cat lab/chr*_GERP_MLAGAN_scores.wig | \
        wigEncode stdin mlaganGerpCons.wig mlaganGerpCons.wib
        #  upper limit 4.48, lower limit -25.74
    set d = /gbdb/hg17/encode/MLAGAN/SEP-05
    ln -s /cluster/data/encode/MSA/Gerp/2006-02-01/mlaganGerpCons.wib $d
    hgLoadWiggle -pathPrefix=$d hg17 encodeMlaganGerpCons mlaganGerpCons.wig

    # MAVID
    cat lab/chr*_GERP_MAVID_scores.wig | \
        wigEncode stdin mavidGerpCons.wig mavidGerpCons.wib
        # upper limit 4.48, lower limit -22.58
    set d = /gbdb/hg17/encode/MAVID/SEP-05
    ln -s /cluster/data/encode/MSA/Gerp/2006-02-01/mavidGerpCons.wib $d
    hgLoadWiggle -pathPrefix=$d hg17 encodeMavidGerpCons mavidGerpCons.wig

    # Elements.  Note: scores from 307-1000.  This data also
        # upper limit 4.48, lower limit -22.58
    # includes a 6th field with an unscaled float score, which
    # will be included in the table, but not used for display
    # with unscaled scores.

    # Adding item names (<region>.#) for consistency with other MSA elements
    # subtracks

    lab/GERP_TBA_Cons.bed


##########################################################################
# MSA SCONE Conservation (2005-12-12 kate)
# From Harvard Med School, Saurabh Asthana <faplap@gmail.com>
# Reusbmitted 12/21/05

    cd /cluster/data/encode/MSA
    mkdir -p SconeCons/2005-12-21/lab
    ln -s SconeCons/2005-12-21 latest
    cd latest/lab
    mkdir bed; cd bed
    wget http://genetics.bwh.harvard.edu/graft/bed/sconeRegions.NOV-2005.bed.tar.bz2
    bunzip2 sconeRegions.NOV-2005.bed.tar.bz2
    tar xvf sconeRegions.NOV-2005.bed.tar
    cd ..

    cd ..; mkdir wig; cd wig
    wget http://ika.bwh.harvard.edu/graft/wig/scone.NOV-2005.wig.tar.bz2
    bunzip2 scone.NOV-2005.wig.tar.bz2
    tar xvf scone.NOV-2005.wig.tar
    cd ../..

    # elements
cat > load.csh << 'EOF'
    set out = sconeRegions.bed
    rm -f $out
    foreach f (lab/bed/*.bed)
        set r = $f:t:r
        echo $r
        grep '^chr' $f | \
            awk '{printf("%s\t%d\t%d\t%s\t1000\n", \
                        $1,$2,$3,$4)}' >> $out
    end
    hgLoadBed -strict hg17 encodeTbaSconeEl $out
'EOF'
    csh load.csh >&! load.log &
        # Loaded 18817 elements 
    featureBits -enrichment hg17 encodeRegions encodeTbaSconeEl
        # encodeRegions 1.047%, encodeTbaSconeEl 0.083%, both 0.083%, cover 7.92%, enrich 95.55x
    featureBits -enrichment hg17 encodeRegions encodeTbaPhastConsEl
       # encodeRegions 1.047%, encodeTbaPhastConsEl 0.063%, both 0.063%, cover 6.04%, enrich 95.55x
    featureBits -enrichment hg17 encodeRegions encodeTbaGerpEl
       # encodeRegions 1.047%, encodeTbaGerpEl 0.057%, both 0.057%, cover 5.47%, enrich 95.55x
    featureBits -enrichment hg17 encodeRegions encodeTbaBinConsEl
        # encodeRegions 1.047%, encodeTbaBinConsEl 0.060%, both 0.060%, cover 5.71%, enrich 95.55x

    # conservation
    cat lab/wig/*.wig | \
        wigEncode stdin tbaScone.wig tbaScone.wib
    set d = /gbdb/hg17/encode/TBA/SEP-05
    ln -s `pwd`/tbaScone.wib $d
    hgLoadWiggle -pathPrefix=$d hg17 encodeTbaSconeCons tbaScone.wig 

##########################################################################
# MSA Conservation  (2005-12-07 kate)
# Just phastCons and GERP for this freeze  (x3 aligners)

    cd /cluster/data/encode/MSA/SEP-05
    mkdir -p conservation/lab
    cd conservation/lab
    wget ftp://kronos.nhgri.nih.gov/pub/outgoing/elliott/msa/SEP-2005/cons/phastCons.wig.tar.gz
    tar xvfz phastCons.wig.tar.gz
    cd ..

cat > load.csh << 'EOF'
    # TBA
    gunzip -c lab/tba/*/phast/human.ENm*.gz | \
        wigEncode stdin tbaPhastCons.wig tbaPhastCons.wib
    set d = /gbdb/hg17/encode/TBA/SEP-05
    ln -s `pwd`/tbaPhastCons.wib $d
    hgLoadWiggle -pathPrefix=$d hg17 encodeTbaPhastCons tbaPhastCons.wig 

    # MLAGAN
    gunzip -c lab/mlagan/*/phast/human.ENm*.gz | \
        wigEncode stdin mlaganPhastCons.wig mlaganPhastCons.wib
    set d = /gbdb/hg17/encode/MLAGAN/SEP-05
    ln -s `pwd`/mlaganPhastCons.wib $d
    hgLoadWiggle -pathPrefix=$d hg17 encodeMlaganPhastCons mlaganPhastCons.wig 

    # MAVID
    gunzip -c lab/mavid/*/phast/human.ENm*.gz | \
        wigEncode stdin mavidPhastCons.wig mavidPhastCons.wib
    set d = /gbdb/hg17/encode/MAVID/SEP-05
    ln -s `pwd`/mavidPhastCons.wib $d
    hgLoadWiggle -pathPrefix=$d hg17 encodeMavidPhastCons mavidPhastCons.wig 
'EOF'
    csh load.csh >&! load.log

          
##########################################################################
# MSA alignment agreement
#  From Ariel Schwartz, UC Berkeley

    cd /cluster/data/encode/MSA
    mkdir alignAgreement/2005-11-16/lab
    cd alignAgreement/2005-11-16/lab
    touch Mean.wig MavidMlagan.wig MavidTba.wig MlaganTba.wig 
    touch MavidUngapped.wig MlaganUngapped.wig TbaUngapped.wig
cat > split.csh << 'EOF'
    foreach f (lab/*.wig.gz)
        echo $f
        gunzip $f
        /cluster/data/encode/bin/scripts/splitTracks.pl $f:r
        cat t0 >> Mean.wig
        cat t1 >> MavidMlagan.wig
        cat t2 >> MavidTba.wig
        cat t3 >> MlaganTba.wig
        cat t4 >> MavidUngapped.wig
        cat t5 >> MlaganUngapped.wig
        cat t6 >> TbaUngapped.wig
        rm t?
        gzip $f:r
    end
'EOF'
    csh split.csh >&! split.log &
    mkdir wig wib
cat > load.csh << 'EOF'
    set dir = /gbdb/hg17/encode/MSA/alignAgree/2005-11-16
    mkdir -p $dir
    foreach f (*.wig)
        set table = encodeMsaAlign$f:r
        echo $table
        egrep -v "browser|track" $f | \
            wigEncode stdin wig/$table.wig wib/$table.wib
        hgLoadWiggle -pathPrefix=$dir hg17 $table wig/$table.wig
        ln -s `pwd`/wib/$table.wib $dir
    end
'EOF'
    csh load.csh >&! load.log &

##########################################################################
# Harvard TBA Conservation (2005-12-12 kate)
#  From <faplap@gmail.com> Saurabh Asthana
# Dept. of Medicine, Brigham & Women's Hospital, Harvard Medical School

    cd /cluster/data/encode/MSA/SconeCons
    mkdir -p 2005-12-01/lab
    cd 2005-12-01/lab
    wget http://ika.bwh.harvard.edu/graft/wig/scone.NOV-2005.wig.tar.bz2
    wget http://genetics.bwh.harvard.edu/graft/bed/sconeRegions.NOV-2005.bed.tar.bz2
    mkdir -p bed wig
    # NOTE: files are actually gzipped
    mv scone.NOV-2005.wig.tar.bz2 wig/scone.wig.tar.gz
    mv sconeRegions.NOV-2005.bed.tar.bz2 bed/sconeRegions.bed.tar.gz
    cd ..

    # Conservation scores
    cat lab/wig/*.wig | grep -v track | \
        wigEncode 
    # Conserved Elements
    # Add these to the TBA Elements track as a subtrack
    # For table consistency, assign item names of the form <region>.#,
    # and a score=1000
    set bed = sconeRegions.bed
    rm $out
    foreach f (lab/bed/*.bed)
        set r = $f:t:r
        echo $r
        grep '^chr' $f | \
            awk -v REGION=$r '{printf("%s\t%d\t%d\t%s.%d\t%d\n", \
                $1,$2,$3,REGION, NR,1000)}' >> $bed
    end
    hgLoadBed -strict hg17 encodeTbaSconeEl $bed
        # Loaded 18784 elements of size 5

    
##########################################################################
# UW/Regulome Chromatin Accessibility Profiling (CAP) - RENAMED, see below
#  Submitted 2006-1-17 by Scott Kuehn
# update of data received on 2006-05-04 (sent to Kate) by Scott Kuehn
# Update done 2006-05-19 - 2006-05-23 (hartera)
# Not called CAP anymore, now called DNase array for DNase I
# Track short label is now: UW DNase GM 
# long label: ENCODE UW DNase/Array GM06690 - DNase I 
# sensitivity/hypersensitivity in GM06990 Cells 
# Data is for lymphoblastoid cells (GM06990).
    cd /cluster/data/encode/Regulome
    mkdir -p 2006-05-04/lab
    cd 2006-05-04
    awk '{printf("%s\t%s\t%s\t%.3f\n", $1, $2, $3, $5)}' \
                lab/Encode.DNase-Array-GM06990.Probes.hg17.bed | \
        sort -k1,1 -k2,2n  | \
        /cluster/data/encode/bin/scripts/trimOverlap.pl > sens.bed
    hgLoadBed -strict -bedGraph=4 hg17 encodeRegulomeDnaseGM06990Sens sens.bed

    # the Encode.DNase-Array-GM06990.DHSs.hg17.bed file has a float score
    # use the encodeRegulomeDnaseSitesSKNSH.sql renamed as sites.sql which 
    # has an int and a float score field
    perl -pi.bak -e 's/SitesSKNSH/GM06990Sites/' sites.sql
    rm *.bak
    # scale scores to 0-1000. use linear transform.
    awk '{printf "%s\t%d\t%d\t%s\t%d\t%s\n", $1,$2,$3,$4,($5 * 105),$5}' \
        lab/Encode.DNase-Array-GM06990.DHSs.hg17.bed \
        | sort -k1,1 -k2,2n > linearScaledSites.bed
    hgLoadBed -sqlTable=sites.sql hg17 encodeRegulomeDnaseGM06990Sites \
        linearScaledSites.bed
    # authors provided track-description.html
    # Add this to trackDb/human/hg17 as encodeRegulomeDnaseArray.html
    # trackDb entry - track is renamed as encodeRegulomeDnaseArray
    # Previously track was called encodeRegulomeCap.

##########################################################################
# SANGER CHIP/CHIP  (2006-03-16 kate)
# Updated (2006-08-08 kate)
#  5 histone mods in HFL cells, to be added to existing track
#  Submitted by Rob Andrews
#  Data in two additional cell lines (MOLT4 and PTR8) submitted
#  8/8/08 by Rob -- 8 additional subtracks.

    ssh hgwdev
    cd /cluster/data/encode/sanger/chipchip
    mkdir -p 2006-03-16/lab
    cd 2006-03-16
    cp /var/ftp/encode/*.wig.txt lab

    grep "^chr" lab/H3K4me1_HFL-1_1.wig.txt | sort -k1,1 -k2,2n > \
	H3K4me1HFL1.bed
    grep "^chr" lab/H3K4me2_HFL-1_1.wig.txt | sort -k1,1 -k2,2n > \
	H3K4me2HFL1.bed
    grep "^chr" lab/H3K4me3_HFL-1_1.wig.txt | sort -k1,1 -k2,2n > \
	H3K4me3HFL1.bed
    grep "^chr" lab/H3ac_HFL-1_1.wig.txt | sort -k1,1 -k2,2n > \
	H3acHFL1.bed
    grep "^chr" lab/H4ac_HFL-1_1.wig.txt | sort -k1,1 -k2,2n > \
	H4acHFL1.bed
cat > load.csh << 'EOF'
    foreach f (*.bed)
        set t = $f:r
        echo $t
	hgLoadBed -bedGraph=4 hg17 encodeSangerChip$t $t.bed
    end
'EOF'
    csh load.csh >&! load.log &
    # loaded 23996 elements for 5 tables

    ssh hgwdev
    cd /cluster/data/encode/sanger/chipchip
    mkdir -p 2008-08-8/lab
    cd 2006-08-08
    cp /var/ftp/encode/*.wig.txt lab

    grep "^chr" lab/H3K4me1_PTR8_1.wig.txt | sort -k1,1 -k2,2n > \
	H3K4me1Ptr8.bed
    grep "^chr" lab/H3K4me2_PTR8_1.wig.txt | sort -k1,1 -k2,2n > \
	H3K4me2Ptr8.bed
    grep "^chr" lab/H3K4me3_PTR8_1.wig.txt | sort -k1,1 -k2,2n > \
	H3K4me3Ptr8.bed
    grep "^chr" lab/H3K4me1_MOLT4_1.wig.txt | sort -k1,1 -k2,2n > \
	H3K4me1Molt4.bed
    grep "^chr" lab/H3K4me2_MOLT4_1.wig.txt | sort -k1,1 -k2,2n > \
	H3K4me2Molt4.bed
    grep "^chr" lab/H3K4me3_MOLT4_1.wig.txt | sort -k1,1 -k2,2n > \
	H3K4me3Molt4.bed
    grep "^chr" lab/H3ac_MOLT4_1.wig.txt | sort -k1,1 -k2,2n > \
	H3acMolt4.bed
    grep "^chr" lab/H4ac_MOLT4_1.wig.txt | sort -k1,1 -k2,2n > \
	H4acMolt4.bed
cat > load.csh << 'EOF'
    foreach f (*.bed)
        set t = $f:r
        echo $t
	hgLoadBed -bedGraph=4 hg17 encodeSangerChip$t $t.bed
    end
'EOF'
    csh load.csh >&! load.log &
    # loaded 23983 elements for 8 tables


# ENCODE PSEUDOGENE TRACK (DONE, 2006-03-30, hartera)
    # Yontao reloaded the encodePseudogeneUcsc2 table with shorter 
    # names for the pseudogenes as they were cut off in the browser so now
    # NM_001017421|chr2|+|1 would be NM_001017421|1
    # The class table needs to be reloaded. Yontao provided a file:
    # encodePseudogeneUcsc2-forload.class
    # get a dump of the current table without the ucsc2 entries.
    ssh hgwdev
    cd /cluster/data/encode/pseudogene/class
    hgsql -N -e 'select * from encodePseudogeneClass where owner != "ucsc2";' \
          hg17 > encodePseudogeneClassNoUcsc2.txt
    cat encodePseudogeneClassNoUcsc2.txt encodePseudogeneUcsc2-forload.class \
        > allPseudogenesClass.txt
    sort -k3,3 allPseudogenesClass.txt > encodePseudogeneClass2.txt 
    # the consensus sequences have different names in the Class table, the
    # names had been changed to Vega gene names. Get the Class from the gtf in
    # /cluster/data/encode/pseudogene/consensus
    awk 'BEGIN {OFS="\t"} {print $10, $2}' \
         ../consensus/consensus.jan6.hg17.gtf | sort | uniq \
         > pgConsensus.class
    sed -e 's/VEGA_//' pgConsensus.class | sed -e 's/"//g' \
           | sed -e 's/;//' > pgConsensusClass.txt
    wc -l pgConsensusClass.txt
    # 201 pgConsensusClass.txt
    awk 'BEGIN {OFS="\t"}{print $0,"consensus"}' pgConsensusClass.txt | sort \
        > pgConsensusClassSorted.txt
    # reload the encodePseudogeneClass table
    hgsql -N -e 'select * from encodePseudogeneClass where owner != "ucsc2" and owner != "consensus";' \
          hg17 > pseudoClassNoUcsc2OrConsensus.txt
    cat pseudoClassNoUcsc2OrConsensus.txt encodePseudogeneUcsc2-forload.class \
        pgConsensusClassSorted.txt > allPseudogenesClass.txt
    sort -k3,3 allPseudogenesClass.txt > encodePseudogeneClass2.txt 
    wc -l encodePseudogeneClass2.txt
    # 995 encodePseudogeneClass2.txt
    # only 830 load as there are dupicate names - 165 names are shared
    # between the consensus and havana subtracks. These names
    # need to be unique as they are the primary key. Checked that the 
    # class is the same for havana and consensus subtracks where the 
    # name is the same so reload table with one entry for these genes.
    # remove havana and consensus pseudogenes
    grep -v havana allPseudogenesClass.txt | grep -v consensus \
         > pseudoNoHavananNoCons.txt
    wc -l pseudoNoHavananNoCons.txt
    # 616 pseudoNoHavananNoCons.txt
    # prepare consensus set not in havana
    awk 'BEGIN {OFS="\t"}{print $0,"consensus"}' consOnly > consOnlyWithOwner
    # prepare havana set not in consensus
    awk 'BEGIN {OFS="\t"}{print $0,"havana"}' havanaOnly2 > havanaOnlyWithOwner
    # prepare set common to consensus and havana
    awk 'BEGIN {OFS="\t"}{print $0,"havana or consensus"}' \
        nameAndClass.ConsAndHavana > havanaAndConsWithOwner
    wc -l *Owner
    # 36 consOnlyWithOwner
    # 165 havanaAndConsWithOwner
    # 2 havanaOnlyWithOwner
    cat pseudoNoHavananNoCons.txt consOnlyWithOwner havanaAndConsWithOwner \
        havanaOnlyWithOwner > allPseudogenesClass2.txt
    sort -k3,3 allPseudogenesClass2.txt > encodePseudogeneClass2.txt 
    wc -l encodePseudogeneClass2.txt
    # 819 encodePseudogeneClass2.txt
    # reload table
    hgsql -e 'drop table encodePseudogeneClass;' hg17
    hgsql hg17 < encodePseudogeneClass.sql
    echo "load data local infile 'encodePseudogeneClass2.txt' into \
         table encodePseudogeneClass" | hgsql hg17


##########################################################################
# Univ. Uppsala, Sweden Chip/chip (butyrate-treated H3Ac, H4Ac)
# Submitted 5/29/06 by Adam Ameur <mada@lcb.uu.se>
# 6 subtracks
#	DONE - 2006-06-13 - Hiram

    cd /cluster/data/encode/Uppsala
    mkdir -p 2006-05-09/lab
    cd 2006-05-09/lab
    unzip H3H4ac_butyrate.zip
    cat << '_EOF_' > splitTrack.pl
#!/usr/bin/env perl

use warnings;
use strict;

open (FH,"<H3H4ac_butyrate.tracks") or die "can not open H3H4ac_butyrate.tracks: $!";

my $trackCount = 1;
my $outFile="track1";

while(my $line=<FH>)
{
    if ($line =~ m/^track/) {
        $outFile = sprintf("track_%d", $trackCount++);
        open (OUT,">$outFile") or die "can not open $outFile: $!";
    } else {
            print OUT $line;
    }
}

close(OUT);

close (FH)
'_EOF_'
    # << emacs happy
    chmod +x splitTrack.pl
    ./splitTrack.pl
    #	looking at the track definitions to get some reasonable names:
    grep "^track" H3H4ac_butyrate.tracks
    mv track_1 encodeUppsalaChipH3acBut0h.wig.txt
    mv track_2 encodeUppsalaChipH3acBut12h.wig.txt
    mv track_3 encodeUppsalaChipH4acBut0h.wig.txt
    mv track_4 encodeUppsalaChipH4acBut12h.wig.txt
    mv track_5 encodeUppsalaChipH3acBut0vs12.itemRgb.txt
    mv track_6 encodeUppsalaChipH4acBut0vs12.itemRgb.txt

    #	encoding
    wigEncode encodeUppsalaChipH3acBut0h.wig.txt \
    	encodeUppsalaChipH3acBut0h.wig encodeUppsalaChipH3acBut0h.wib
    #	upper limit 15.68, lower limit 0.17
    wigEncode encodeUppsalaChipH3acBut12h.wig.txt \
	encodeUppsalaChipH3acBut12h.wig encodeUppsalaChipH3acBut12h.wib
    #	upper limit 6.55, lower limit 0.22
    wigEncode encodeUppsalaChipH4acBut0h.wig.txt \
	encodeUppsalaChipH4acBut0h.wig encodeUppsalaChipH4acBut0h.wib
    #	upper limit 14.47, lower limit 0.19
    wigEncode encodeUppsalaChipH4acBut12h.wig.txt \
	encodeUppsalaChipH4acBut12h.wig encodeUppsalaChipH4acBut12h.wib
    #	upper limit 6.58, lower limit 0.05

    mkdir /gbdb/hg17/encode/Uppsala
    ln -s `pwd`/*.wib /gbdb/hg17/encode/Uppsala/

    #	using the -tmpDir will cause the temp file to be removed
    hgLoadWiggle -tmpDir=/scratch/tmp hg17 encodeUppsalaChipH3acBut0h \
	-pathPrefix=/gbdb/hg17/encode/Uppsala encodeUppsalaChipH3acBut0h.wig
    hgLoadWiggle -tmpDir=/scratch/tmp hg17 encodeUppsalaChipH3acBut12h \
	-pathPrefix=/gbdb/hg17/encode/Uppsala encodeUppsalaChipH3acBut12h.wig
    hgLoadWiggle -tmpDir=/scratch/tmp hg17 encodeUppsalaChipH4acBut0h \
	-pathPrefix=/gbdb/hg17/encode/Uppsala encodeUppsalaChipH4acBut0h.wig
    hgLoadWiggle -tmpDir=/scratch/tmp hg17 encodeUppsalaChipH4acBut12h \
	-pathPrefix=/gbdb/hg17/encode/Uppsala encodeUppsalaChipH4acBut12h.wig


    #	they don't have their score data normalized, find min, max, etc...
    ave -col=5 encodeUppsalaChipH3acBut0vs12.itemRgb.txt
    #	min 0.404995
    #	max 7.091458
    # -> max - min = 6.686463
    echo "7.091458 - 0.404995" | bc
    #	6.686463
   
    #	plugging in those numbers, normalize the score column
    #	0.000001 from the min value to avoid -0 in the output
    awk '
    {
    score = 1000.0*($5 - 0.404994)/6.686463
    for (i=1; i < 5; ++i) { printf "%s\t", $i }
    printf "%d\t", score
    for (i=6; i < 9; ++i) { printf "%s\t", $i }
    printf "%s\n", $9
    } ' encodeUppsalaChipH3acBut0vs12.itemRgb.txt | \
	hgLoadBed -tmpDir=/scratch/tmp -strict hg17 \
	    encodeUppsalaChipH3acBut0vs12 stdin

    #	using the -tmpDir will cause the temp file to be removed

    #	same deal for the other one
    ave -col=5 encodeUppsalaChipH4acBut0vs12.itemRgb.txt
    #	min 0.347273
    #	max 2.833333
    echo "2.833333 - 0.347273" | bc
    #	2.486060
    #	plugging in those numbers, normalize the score column
    #	0.000001 from the min value to avoid -0 in the output
    awk '
    {
    score = 1000.0*($5 - 0.347272)/2.486060
    for (i=1; i < 5; ++i) { printf "%s\t", $i }
    printf "%d\t", score
    for (i=6; i < 9; ++i) { printf "%s\t", $i }
    printf "%s\n", $9
    } ' encodeUppsalaChipH4acBut0vs12.itemRgb.txt | \
	hgLoadBed -tmpDir=/scratch/tmp -strict hg17 \
	    encodeUppsalaChipH4acBut0vs12 stdin

    #	To see what would be reasonable view limits, look at these
    #	histograms and see where the majority of the data is
    hgWiggle -doHistogram -hBinSize=0.16 -hBinCount=100 -hMinVal=0.0 \
	-db=hg17 encodeUppsalaChipH3acBut0h
    #	running each of the wiggle tracks, it looks like 95% of the data
    #	is in the region 0 to 2.0


##########################################################################
# UW/Regulome QCP data
# To replace existing tracks
# Submitted 5/19/06 by John Stam <jstam@U.WASHINGTON.EDU>
# 1 zip file data: UW_may06_ENCODE_data, plus Description.doc

    cd /cluster/data/encode/Regulome
    mkdir -p 2006-05-19/lab
    cd 2006-05-19/lab
    # deposit data
    mkdir data
    cd data
    unzip ../*.zip
    cd ..
    ls data
# CD4.baseline.hg17.bed       HMEC.baseline.hg17.bed   NHBE.baseline.hg17.bed
# CaCo2.baseline.hg17.bed     HRE.baseline.hg17.bed    PANC.baseline.hg17.bed
# CaLU3.baseline.hg17.bed     HeLa.baseline.hg17.bed   SAEC.baseline.hg17.bed
# EryAdult.baseline.hg17.bed  HepG2.baseline.hg17.bed  SKnSH.baseline.hg17.bed
# EryFetal.baseline.hg17.bed  Huh7.baseline.hg17.bed
# GM.baseline.hg17.bed        K562.baseline.hg17.bed

    #	loading bedGraph 5 data type:
    for CELL in CD4 CaCo2 CaLU3 EryAdult EryFetal GM HMEC HRE HeLa HepG2 \
        Huh7 K562 NHBE PANC SAEC SKnSH
    do
	sort -k1,1 -k2,2n data/$CELL.baseline.hg17.bed \
	    | /cluster/data/encode/bin/scripts/trimOverlap.pl \
		| hgLoadBed -noSort -noBin -strict -bedGraph=5 hg17 \
		    encodeUWRegulomeBase$CELL stdin
    done

    #	gross statistics for the data
    awk '{print $5}' data/*.baseline.hg17.bed | ave stdin
    #	Q1 -0.201029
    #	median 0.000000
    #	Q3 0.207335
    #	average 0.018950
    #	min -5.454980
    #	max 6.327273
    #	count 291642
    #	total 5526.507662
    #	standard deviation 0.489442

    #	a histogram of the data:
    awk '{print $5}' data/* | textHistogram -verbose=2 -binSize=0.12 \
	-maxBinCount=100 -minVal=-5.5 -real -pValues stdin \
	    > histogram.data
    #	looking at that, it appers that 95% of the data is within the
    #	range of -1.0 to 1.0
    #	The note that came with this data said to set view limits
    #	at 0.5 : 3.0

    #	Making the trackDb entries, taking colors from:
    #	http://genome-test.cse.ucsc.edu/~hiram/rgbItemExamples.html

    rm -f trackDb.entries.txt
    I=1
    export I
    for CELL in CD4 CaCo2 CaLU3 EryAdult EryFetal GM HMEC HRE HeLa HepG2 \
        Huh7 K562 NHBE PANC SAEC SKnSH
    do
	echo "    track encodeUWRegulomeBase${CELL}"
	echo "    subTrack encodeUWRegulomeBase"
	echo "    shortLabel ${CELL}"
	echo "    longLabel ${CELL} DNaseI Sensitivity"
	case $I in
	    1) echo "    color 0,0,255";;
	    2) echo "    color 0,48,224";;
	    3) echo "    color 0,96,176";;
	    4) echo "    color 0,119,153";;
	    5) echo "    color 0,153,119";;
	    6) echo "    color 0,187,85";;
	    7) echo "    color 56,238,0";;
	    8) echo "    color 0,255,0";;
	    9) echo "    color 68,238,0";;
	    10) echo "    color 96,192,326";;
	    11) echo "    color 136,170,0";;
	    12) echo "    color 170,136,0";;
	    13) echo "    color 204,102,0";;
	    14) echo "    color 238,68,0";;
	    15) echo "    color 255,0,0";;
	    16) echo "    color 255,0,255";;
	esac
	echo "    priority ${I}"
	echo
	I=`expr $I + 1`
    done > trackDb.entries.txt


##########################################################################
# EBI PECAN Alignments (IN PROGRESS 2006-06-22 kate)
#  From Ben Paten

    cd /cluster/data/encode
    mkdir -p PECAN/SEP-05/lab
    cd PECAN/SEP-05/lab
    wget http://www.ebi.ac.uk/~bjp/pecan/encode_sept_pecan_mafs.tar.bz2
    bunzip2 encode_sept_pecan_mafs.tar.bz2
    tar xvf encode_sept_pecan_mafs.tar
    cd ..


cat > project.csh << 'EOF'
    mkdir -p tmp
    set tmpDir = tmp
    foreach f (lab/*MAF/EN[mr]*)
        set r = $f:t:r
        echo $r
        set c = `echo "SELECT chrom from encodeRegions WHERE name='$r'" | \
                        hgsql -N hg17`
        set start =  \
                `echo "SELECT chromStart from encodeRegions WHERE name='$r'" | \
                        hgsql -N hg17`
        set size = \
                `echo "SELECT size from chromInfo WHERE chrom='$c'" | \
                        hgsql -N hg17`
        /cluster/data/encode/bin/scripts/mafCoord.pl < $f \
                human.0 hg17.$c $start $size | \
            sed 's/^a$/a score=0.0/' > $tmpDir/$r.db.maf
        echo "projecting $r"
        /cluster/bin/penn/maf_project $tmpDir/$r.db.maf hg17.$c > $r.maf
        echo "finished $r"
    end
'EOF'
    csh project.csh >&! project.log &
    rm -fr tmp
    set gdir = /gbdb/hg17/encode/PECAN/SEP-05/maf
    mkdir -p $gdir
    rm -f $gdir/*.maf
    ln -s /cluster/data/encode/PECAN/SEP-05/*.maf $gdir
    hgLoadMaf -pathPrefix=$gdir -WARN hg17 encodePecanAlign >&! load.log
    # lots of "score too small" messages -- these are OK.
    cat *.maf | hgLoadMafSummary hg17 encodePecanSummary stdin



##########################################################################
# UW/Regulome QCP data again 2006-07-05 - Hiram

    ssh hgwdev
    cd /cluster/data/encode/Regulome/2006-06-13/lab

    for CELL in CD4 CaCo2 CaLU3 EryAdult EryFetal GM HMEC HRE HeLa HepG2 \
        Huh7 K562 NHBE PANC SAEC SKnSH
    do
	ls -og ${CELL}.normalized_060206.hg17.bed
	sort -k1,1 -k2,2n ${CELL}.normalized_060206.hg17.bed \
	    | /cluster/data/encode/bin/scripts/trimOverlap.pl \
		| hgLoadBed -noSort -noBin -strict -bedGraph=5 hg17 \
		    encodeUWRegulomeBase${CELL} stdin
    done

    #	gross statistics for the data
    awk '{print $5}' *.normalized_060206.hg17.bed | ave stdin
    #	Q1 -0.494604
    #	median -0.000000
    #	Q3 0.510167
    #	average 0.046097
    #	min -13.409856
    #	max 15.554195
    #	count 297650
    #	total 13720.736560
    #	standard deviation 1.203400

    #	calculate histogram 100 bin size:
    echo -13.5 15.6 | awk '{print ($2-$1)/100}'
    #	0.291

    #	a histogram of the data:
    awk '{print $5}' *.normalized_060206.hg17.bed | \
	textHistogram -verbose=2 -binSize=0.292 \
	-maxBinCount=100 -minVal=-13.5 -real -pValues stdin \
	    > histogram.data
    #	looks like the majority of the data is within -1.0 to 1.0

    #	The trackDb entries made previously should be OK

############################################################
# DLESS acs 05/02/06
# sorry this is a bit sketchy.  See me with questions

    cd /cluster/home/acs/encode-dless/hg17

    # make tree model
    tree_doctor /cluster/home/acs/DLESS-CSHL/encode17.mod --rename "human->hg17" > tree.mod
    tree_doctor --tree-only tree.mod > tree.nh

    # make SS files, annotated with indels by Brian
    cat > prepAlignmentsIndels.sh <<EOF
#!/bin/sh

TARGET=$1
CHR=$2
START=$3
END=$4

/cluster/bin/phast/msa_view /cluster/bluearc/encode/TBA/SEP-05/maf-indels/human.${TARGET}.maf -i MAF --refseq /cluster/bluearc/hg17/chrom/${CHR}.fa -o SS --start $START --end $END --refidx 1 > /scratch/${TARGET}.sso

/cluster/bin/phast/msa_view /scratch/$TARGET.sso -i SS -o SS --seqs hg17,chimp,baboon,macaque,marmoset,galago,rat,mouse,rabbit,cow,dog,rfbat,armadillo,elephant,tenrec,monodelphis,platypus --gap-strip ALL | /cluster/home/acs/phast-opteron/bin/msa_view - -i SS -o SS --order  hg17,chimp,baboon,macaque,marmoset,galago,rat,mouse,rabbit,cow,dog,rfbat,armadillo,elephant,tenrec,monodelphis,platypus > /cluster/bluearc/encode/TBA/SEP-05/ss-indels/${TARGET}.sso
# second call adds rows of missing data for missing species

rm /scratch/$TARGET.sso
EOF
    chmod +x prepAlignmentsIndels.sh

    mkdir -p /cluster/bluearc/encode/TBA/SEP-05/maf-indels /cluster/bluearc/encode/TBA/SEP-05/ss-indels
    rsync -avz /cluster/store11/encodeMafAnno/TBA/APR-26/human.*.maf /cluster/bluearc/encode/TBA/SEP-05/maf-indels   # location of Brian's files
    hgsql hg17 -e "select * from encodeRegions" --skip-column-names > regions.txt
    awk '{printf "prepAlignmentsIndels.sh %s %s %s %s\n", $1, $2, $3, $4}' regions.txt > jobList7  # never mind numbering; there were some other experimental runs that I've omitted
    # para create, para push, etc.

    # get indel histories and estimate indel params
    mkdir -p consElements
    awk '{printf "select chrom, chromStart - %d + 1, chromEnd - %d + 1 from encodeTbaPhastConsEl where chrom = \"%s\" and chromStart >= %d and chromEnd <= %d\n", $3, $3, $2, $3, $4 > $1 ".sql"}' regions.txt 
    for file in *.sql ; do hgsql hg17 --skip-column-names < $file > consElements/`basename $file .sql`.bed ; done
    rm *.sql

    cat > indelHistoryParsBrian.sh <<EOF
#!/bin/sh

TARGET=$1
/cluster/bin/phast/indelHistory /cluster/bluearc/encode/TBA/SEP-05/ss-indels/$TARGET.sso tree.nh -i SS > /cluster/bluearc/encode/DLESS/IH-indels/$TARGET.pars.ih
EOF
    chmod +x indelHistoryParsBrian.sh

    rm -f jobList8
    mkdir -p /cluster/bluearc/encode/DLESS/IH-indels
    awk '{print $1}' regions.txt > targets
    for t in `cat targets` ; do echo "indelHistoryParsBrian.sh $t" >> jobList8 ; done
    # para create, para push, etc.

    cat > indelModelsBrian.sh <<EOF
#!/bin/sh

TARGET=$1
/cluster/bin/phast/indelFit /cluster/bluearc/encode/DLESS/IH-indels/$TARGET.pars.ih tree.nh --features consElements/$TARGET.bed --reference hg17 > IM-indels/$TARGET.pars.im
EOF
    chmod +x indelModelsBrian.sh

    rm -f jobList9
    mkdir -p IM-indels
    for t in `cat targets` ; do echo "indelModelsBrian.sh $t" >> jobList9 ; done
    # para create, para push, etc.

    # average estimates across targets
    sed 's/,//g' IM-indels/*.pars.im | awk '{if ($2 == 0) {nbg++; a_bg += $6; b_bg += $9; t_bg += $12} else if ($2 == 1) {nco++; a_co += $6; b_co += $9; t_co += $12}} END {printf "bg: alpha = %f, beta = %f, tau = %f\nco: alpha = %f, beta = %f, tau = %f\n", a_bg/nbg, b_bg/nbg, t_bg/nbg, a_co/nco, b_co/nco, t_co/nco}' > ave.pars.brian.im
    #bg: alpha = 0.033417, beta = 0.053284, tau = 0.052852
    #co: alpha = 0.011655, beta = 0.020610, tau = 0.065395

    # now estimate DLESS params by ML
    cat > doDlessEstimateParsBrian.sh <<EOF
#!/bin/sh

TARGET=$1
CHR=$2
/cluster/bin/phast/dless /cluster/bluearc/encode/TBA/SEP-05/ss-indels/$TARGET.sso tree.mod --expected-length 20 --target-coverage ~0.2 -i SS --seqname $CHR --idpref $TARGET --indel-model 0.033417,0.053284,0.052852,0.011655,0.020610,0.065395 --phi ~0.5 --indel-history /cluster/bluearc/encode/DLESS/IH-indels/$TARGET.pars.ih 1> /cluster/bluearc/encode/DLESS/ESTIMATE-indels/$TARGET.pars.gff 2> /cluster/bluearc/encode/DLESS/ESTIMATE-indels/$TARGET.pars.stderr
EOF
    chmod +x doDlessEstimateParsBrian.sh

    mkdir -p /cluster/bluearc/encode/DLESS/ESTIMATE-indels
    awk '{printf "doDlessEstimateParsBrian.sh %s %s\n", $1, $2}' regions.txt > jobList10
    # para create, para push, etc.

    # average estimates across targets
    rm -f estimates.pars.brian.txt 
    grep '^Done' -l /cluster/bluearc/encode/DLESS/ESTIMATE-indels/*.pars.stderr > tmp1
    for file in `cat tmp1` ; do tail -9 $file | head -1 | awk '{print $2, $3}'>> estimates.pars.brian.txt ; done
    awk '{x += $1; y += $2} END {print "Pars:", x/NR, y/NR}' estimates.pars.brian.txt
    rm tmp1
    # Pars: 0.0551889 0.261488

    # predict elements
    cat > doDlessPars.sh <<EOF
#!/usr/local/bin/bash -e

TARGET=$1
CHR=$2
/cluster/bin/phast/dless /cluster/bluearc/encode/TBA/SEP-05/ss-indels/$TARGET.sso tree.mod --expected-length 20 --target-coverage 0.055 --phi 0.261 -i SS --seqname $CHR --idpref $TARGET --indel-model 0.033417,0.053284,0.052852,0.011655,0.020610,0.065395 --indel-history /cluster/bluearc/encode/DLESS/IH-indels/$TARGET.pars.ih 1> /cluster/bluearc/encode/DLESS/GFF/$TARGET.pars.gff 2> /cluster/bluearc/encode/DLESS/STDERR/$TARGET.pars.stderr
EOF
    chmod +x doDlessPars.sh

    mkdir -p /cluster/bluearc/encode/DLESS/GFF /cluster/bluearc/encode/DLESS/STDERR
    awk '{printf "doDlessPars.sh %s %s\n", $1, $2}' regions.txt > jobList4
    # para create, para push, etc.

    # compute P-values with phyloP
    cat > doGeneric.sh <<EOF
#!/usr/local/bin/bash

echo $1 ${*:3} '>' $2
$1 ${*:3} > $2
EOF
    chmod +x doGeneric.sh

    mkdir -p /cluster/bluearc/encode/DLESS/DLESSP
    rm -f jobList5
    for t in `cat targets` ; do \
            echo "./doGeneric.sh /cluster/bin/phast/dlessP /cluster/bluearc/encode/DLESS/DLESSP/$t.pars.dlessP /cluster/bluearc/encode/TBA/SEP-05/ss-indels/$t.sso -i SS /cluster/home/acs/encode-dless/hg17/tree.mod /cluster/bluearc/encode/DLESS/GFF/$t.pars.gff" >> jobList5 ;\
    done
    # para create, para push, etc.

    # load track
    echo "drop table if exists encodeDless" | hgsql hg17
    cat /cluster/bluearc/encode/DLESS/DLESSP/*.pars.dlessP | grep -v '^#' | sort -k1,1 -k2,2n | awk 'NF == 24' |sed 's/hg17/human/'  > dless.dat
    awk '{ if (($6 == "conserved" && $8 < 0.05) || ($6 == "gain" && $8 < 0.05 && $9 > 0.05 && $10 < 0.05) || ($6 == "loss" && $8 > 0.05 && $9 < 0.05 && $11 < 0.05)) print $0}' dless.dat > dless.filtered.dat
    sed 's/dless/encodeDless/g' ~/kent/src/hg/lib/dless.sql | hgsql hg17
    echo "load data local infile 'dless.filtered.dat' into table encodeDless" | hgsql hg17
