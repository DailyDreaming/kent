# for emacs: -*- mode: sh; -*-


# Danio Rerio (zebrafish) from Sanger, version Zv5 (released 5/20/05)
#  Project website:
#    http://www.sanger.ac.uk/Projects/D_rerio/
#  Assembly notes:
#    http://www.sanger.ac.uk/Projects/D_rerio/
#    ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6_assembl_information.shmtl

#  NOTE:  this doc may have genePred loads that fail to include
#  the bin column.  Please correct that for the next build by adding
#  a bin column when you make any of these tables:
#
#  mysql> SELECT tableName, type FROM trackDb WHERE type LIKE "%Pred%";
#  +-----------+-------------------------+
#  | tableName | type                    |
#  +-----------+-------------------------+
#  | refGene   | genePred refPep refMrna |
#  | mgcGenes  | genePred                |
#  | genscan   | genePred genscanPep     |
#  +-----------+-------------------------+


###########################################################################
# DOWNLOAD SEQUENCE (DONE, 2006-03-29, hartera)
# CHANGED NAME OF SCAFFOLDS AGP FILE (DONE, 2006-04-13, hartera)
     ssh kkstore01
     mkdir /cluster/store8/danRer4 
     ln -s /cluster/store8/danRer4 /cluster/data
     cd /cluster/data/danRer4
     wget --timestamp \
      ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/README
     wget --timestamp \
      ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6.chunks.agp
     wget --timestamp \
      ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6.scaffold.agp
     wget --timestamp \
      ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6_scaffolds.fa
     wget --timestamp \
    ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6_scaffolds.stats
     # keep agp file name consistent with Zv5 (hartera, 2006-04-13)
     mv Zv6.scaffold.agp Zv6.scaffolds.agp

###########################################################################
# DOWNLOAD MITOCHONDRION GENOME SEQUENCE (DONE, 2006-03-29, hartera)
# ADDED CHUNKS AGP FILE (DONE, 2006-04-13, hartera)
     ssh kkstore01
     mkdir -p /cluster/data/danRer4/M
     cd /cluster/data/danRer4/M
     # go to http://www.ncbi.nih.gov/ and search the Nucleotide database for
     # "Danio mitochondrion genome".  That shows the gi number:
     # 8576324 for the accession, AC024175
 # Use that number in the entrez linking interface to get fasta:
     wget -O chrM.fa \
      'http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Text&db=Nucleotide&uid=8576324&dopt=FASTA'
     # Edit chrM.fa: make sure the header line says it is the
     # Danio Rerio mitochondrion complete genome, and then replace the
     # header line with just ">chrM".
     perl -pi.bak -e 's/>.+/>chrM/' chrM.fa
     rm *.bak
     # Make a "pseudo-contig" for processing chrM too:
     mkdir ./chrM_1
     sed -e 's/chrM/chrM_1/' ./chrM.fa > ./chrM_1/chrM_1.fa
     mkdir ./lift
     echo "chrM_1/chrM_1.fa.out" > ./lift/oOut.lst
     echo "chrM_1" > ./lift/ordered.lst
     # make sure this is tab delimited:
     echo "0\tM/chrM_1\t16596\tchrM\t16596" > ./lift/ordered.lft
# create a .agp file for chrM as hgGoldGapGl and other
# programs require a .agp file so create chrM.agp
     echo "chrM\t1\t16596\t1\tF\tAC024175.3\t1\t16596\t+" \
          > chrM.agp
     # Create a chrM.chunks.agp (hartera, 2006-04-13)
     mkdir -p /cluster/data/danRer4/M/agps
     cd /cluster/data/danRer4/M/agps
     awk 'BEGIN {OFS="\t"} \
        {print $1, $2, $3, $4, $5, $6, $7, $8, $1, $7, $8}' \
        ../chrM.agp > chrM.chunks.agp
     # make sure that all above *.agp files are tab delimited

###########################################################################
# CREATE LIST OF CHROMOSOMES (DONE, 2006-04-12, hartera)
# Change names of random chroms to chrNA_random and chrUn_random
# (DONE, hartera, 2006-04-21)
     ssh kkstore01
     cd /cluster/data/danRer4
     awk '{if ($1 !~ /Zv6/) print $1;}' Zv6.scaffolds.agp \
         | sort -n | uniq > chrom.lst
     cp chrom.lst chrom1to25.lst
     # add chrM, chrUn and chrNA
     echo "M" >> chrom.lst
     echo "NA" >> chrom.lst
     echo "Un" >> chrom.lst
     # Change names of random chroms to reflect that
     perl -pi.bak -e 's/NA/NA_random/' chrom.lst
     perl -pi.bak -e 's/Un/Un_random/' chrom.lst
     rm *.bak

###########################################################################
# MAKE JKSTUFF AND BED DIRECTORIES (DONE, 2006-04-12, hartera)
    ssh kkstore01
    cd /cluster/data/danRer4
    # This used to hold scripts -- better to keep them inline here 
    # Now it should just hold lift file(s) and
    # temporary scripts made by copy-paste from this file.
    mkdir /cluster/data/danRer4/jkStuff
    # This is where most tracks will be built:
    mkdir /cluster/data/danRer4/bed

###########################################################################
# CHECK AGP FILES AND FASTA SIZE CONSISTENCY (DONE, 2006-04-13, hartera)
# 
    ssh kkstore01
    cd /cluster/data/danRer4
    mkdir -p /cluster/data/danRer4/scaffolds
    cd /cluster/data/danRer4/scaffolds
    faSize detailed=on ../Zv6_scaffolds.fa > Zv6.scaffolds.sizes
    # Check that these sizes correspond to the sizes in the scaffolds agp file
    # use script compareSizes2.pl
    cat << '_EOF_' > ../jkStuff/compareSizes2.pl
#!/usr/bin/perl -w
use strict;

my ($file, $agp);

$file = $ARGV[0];
$agp = $ARGV[1];

open(FILE, $file) || die "Can not open $file: $!\n";
open(AGP, $agp) || die "Can not open $agp: $!\n";
open(OUT, ">log.txt") || die "Can not create log.txt: $!\n";

my ($l, @f, $name, $size, %scafsHash);
while (<FILE>)
{
$l = $_;
@f = split(/\t/, $l);

$name = $f[0]; 
$size = $f[1];
$scafsHash{$name} = $size;
}
close FILE;

while (<AGP>)
{
my ($line, @fi, $scaf, $end);
$line = $_;

if ($line =~ /Zv/)
   {
   @fi = split(/\t/, $line);
   $scaf = $fi[5];
   $end = $fi[7];

   if (exists($scafsHash{$scaf}))
      {
      if ($scafsHash{$scaf} == $end)
         {
         print OUT "$scaf - ok\n";
         }
      else
         {
         print OUT "$scaf - different size to sequence\n";
         }
      }
   else
      {
      print OUT "$scaf - does not exist in list of sizes\n";
      }
   }
}
close AGP;
close OUT;
'_EOF_'
   # << happy emacs
   chmod +x ../jkStuff/compareSizes2.pl
   perl /cluster/data/danRer4/jkStuff/compareSizes2.pl \
        Zv6.scaffolds.sizes ../Zv6.scaffolds.agp 
   grep different log.txt
   grep not log.txt
   # these are all consistent with the sequence sizes
   # check that the co-ordinates in the agp files are consistent:
   # field 2 is the start position, field 3 is the end and field 8 is the size
   # so check that this is consistent.
   cd /cluster/data/danRer4
   awk '{if ($6 ~ /^Zv6/ && (($3-$2+1) != $8)) print $6;}' Zv6.scaffolds.agp \
       > Zv6.scaffolds.coordCheck 
   # this file is empty so they are ok. do the same for the chunks.agp file
   awk '{if ($6 ~ /^Zv6/ && (($3-$2+1) != $8)) print $6;}' Zv6.chunks.agp \
       > Zv6.chunks.coordCheck
   # this file is empty so ok
   # check that the difference between 7th and 8th fields is the same as the 
   # difference between 11th and 12th fields. 
   awk '{if ($5 != "N" && (($8 - $7) != ($12 - $11))) print $6;}' \
       Zv6.chunks.agp > Zv6.chunks.coordCheck2
   # these are all ok
   rm Zv6.*.coord*

   cat << '_EOF_' > ./jkStuff/checkSizesInAgps.pl
#!/usr/bin/perl -w
use strict;

my ($ch, $sc, %scafsHash);
$sc = $ARGV[0]; # scaffolds agp
$ch = $ARGV[1]; # chunks or contigs agp

open(SCAFS, $sc) || die "Can not open $sc: $!\n";
open(CHUNKS, $ch) || die "Can not open $ch: $!\n";

while (<SCAFS>)
{
my ($l, @f, $name, $e);
$l = $_;
@f = split(/\t/, $l);
if ($f[5] =~ /^Zv/)
   {
   $name = $f[5];
   $e = $f[2];
   $scafsHash{$name} = $e;
   }
}
close SCAFS;

my $scaf = "";
my $prev = "";
my $prevEnd = 0;

while (<CHUNKS>)
{
my ($line, @fi);
$line = $_;
@fi = split(/\t/, $line);

# if it is not a gap line
if ($fi[4] ne "N")
   {
   $scaf = $fi[9];
   if (($scaf ne $prev) && ($prev ne ""))
      {
      checkCoords($prev, $prevEnd);
      }
$prev = $scaf;
$prevEnd = $fi[2];
   }
}
# check last entry in file
checkCoords($prev, $prevEnd);
close CHUNKS;

sub checkCoords {
my ($name, $end) = @_;
if (exists($scafsHash{$prev}))
   {
   if ($scafsHash{$prev} != $prevEnd)
      {
      my $ed = $scafsHash{$prev};
      print "Scaffold $prev is not consistent between agps\n";
      }
   else
      {
      my $ed = $scafsHash{$prev};
      print "Scaffold $prev - ok\n";
      }
   }
}
'_EOF_'
   # << happy emacs
   chmod +x ./jkStuff/checkSizesInAgps.pl
   cd scaffolds
   perl /cluster/data/danRer4/jkStuff/checkSizesInAgps.pl \
        Zv6.scaffolds.agp Zv6.chunks.agp > Zv6.scafsvschunks
   grep "not consistent" Zv6.scafsvschunks
   # no lines were inconsistency was reported
   wc -l Zv6.scafsvschunks
   # 6653 Zv6.scafsvschunks
   grep "Zv6" Zv6.scaffolds.agp | wc -l
   # 6653
   # so all the scaffolds were checked and were ok.
   cd ..
   rm -r scaffolds
 
###########################################################################
# SPLIT AGP FILES BY CHROMOSOME (DONE, 2006-04-13, hartera)
# GENOME FASTA FROM SANGER WAS CREATED USING SCAFFOLDS AGP
   ssh kkstore01
   cd /cluster/data/danRer4
   # There are 2 .agp files: one for scaffolds (supercontigs on danRer1) and
   # then one for chunks (contigs on danRer1) showing how they map on to
   # scaffolds.
  
   # get list of scaffolds from FASTA file and check these are in agp
   grep '>' Zv6_scaffolds.fa | sed -e 's/>//' | sort | uniq > Zv6FaScafs.lst
   # get list of scaffolds from agp - do not print from gap lines
   awk '{if ($7 !~ /contig/) print $6;}' Zv6.scaffolds.agp \
        | sort | uniq > Zv6AgpScafs.lst
   diff Zv6FaScafs.lst Zv6AgpScafs.lst
   # no difference so all scaffolds are in the FASTA file
   # add "chr" prefix for the agp files
   perl -pi -e 's/^([0-9]+)/chr$1/' ./*.agp
   # for chromosomes 1 to 25, create 2 agps for each chrom, one for scaffolds 
   # and one for chunks:
   foreach c (`cat chrom1to25.lst`)
     echo "Processing $c ..."
     mkdir $c
     perl -we "while(<>){if (/^chr$c\t/) {print;}}" \
          ./Zv6.chunks.agp \
          > $c/chr$c.chunks.agp
     perl -we "while(<>){if (/^chr$c\t/) {print;}}" \
          ./Zv6.scaffolds.agp \
          > $c/chr$c.scaffolds.agp
   end
   
###########################################################################
# CREATE AGP FILES FOR chrNA AND chrUn (DONE, 2006-04-13, hartera)
# RECREATE AGP FILES WITH chrNA and chrUn RENAMED AS chrNA_random 
# AND chrUn_random (DONE, 2006-04-21, hartera)
# NOTE: IN THIS ASSEMBLY AND IN FUTURE, NAME chrNA AND chrUn AS 
# chrNA_random AND chrUn_random TO REFLECT THAT THEY ARE UNORDERED 
# COLLECTIONS OF SCAFFOLDS.  
   ssh kkstore01
   # chrNA_random consists of WGS contigs that could not be related to any
   # FPC contig and the scaffolds and contigs are named Zv5_NAN in the
   # first field of the agp files where the second N is an number.
   cd /cluster/data/danRer4
   mkdir ./NA_random
   awk '{if ($1 ~ /Zv6_NA/) print;}' Zv6.chunks.agp \
       > ./NA_random/NA_random.chunks.agp
   awk '{if ($1 ~ /Zv6_NA/) print;}' Zv6.scaffolds.agp \
       > ./NA_random/NA_random.scaffolds.agp
   # change the first field to "chrNA_random" then can use agpToFa to process
   perl -pi.bak -e 's/Zv6_NA[0-9]+/chrNA_random/' ./NA_random/*.agp
   wc -l ./NA_random/NA_random.scaffolds.agp
   # 2898 ./NA_random/NA_random.scaffolds.agp
   # check files and remove backup files
   # these are not sorted numerically by scaffold number
   rm ./NA_random/*.bak
   # then process chrUn_random - this is made from scaffolds and
   # contigs where the name is Zv6_scaffoldN in the first field of the
   # agp files. These scaffolds and contigs are unmapped to chromosomes
   # in the agp file. chrUn_random is made up of WGS scaffolds that mapped to
   # FPC contigs, but the chromosome is unknown.
   mkdir ./Un_random
   awk '{if ($1 ~ /Zv6_scaffold/) print;}' Zv6.chunks.agp \
       > ./Un_random/Un_random.chunks.agp
   awk '{if ($1 ~ /Zv6_scaffold/) print;}' Zv6.scaffolds.agp \
       > ./Un_random/Un_random.scaffolds.agp
   # change the first field to "chrUn_random" then can use agpToFa to process
   perl -pi.bak -e 's/Zv6_scaffold[0-9]+/chrUn_random/' ./Un_random/*.agp
   wc -l ./Un_random/Un_random.scaffolds.agp
   # 68 ./Un_random/Un_random.scaffolds.agp
   # check files and remove backup files
   rm ./Un_random/*.bak
   # get FASTA file of sequences for NA_random and Un_random and create agp with
   # Ns between scaffolds
   # from scaffolds agp, get name of scaffolds to be retrieved from the 
   # FASTA file to make the NA_random and Un_random chromosomes.
   cd /cluster/data/danRer4
   foreach c (NA_random Un_random)
     awk '{print $6;}' $c/$c.scaffolds.agp > $c/chr$c.scaffolds.lst
         $HOME/bin/i386/faSomeRecords /cluster/data/danRer4/Zv6_scaffolds.fa \
         $c/chr$c.scaffolds.lst $c/chr$c.fa
   end
   # check that all scaffolds in the list are in the FASTA file for 
   # NA_random and Un_random.
   # made a change to scaffoldFaToAgp.c so that the the number of Ns to be
   # inserted between scaffolds can be specified as an option.
   # There are less and smaller random scaffolds than before so use 50,000 Ns
   # between scaffolds as for the human random chromosomes.
   foreach c (NA_random Un_random)              
     $HOME/bin/i386/scaffoldFaToAgp -scaffoldGapSize=50000 $c/chr$c.fa
     mv $c/chr$c.fa $c/chr$c.scaffolds.fa
   end
   # change chrUn to chrNA_random for NA_random, change chrUn to chrUn_random
   # forUn_random. Change D to W for NA_random and Un_random..
   sed -e 's/chrUn/chrNA_random/' ./NA_random/chrNA_random.agp \
       | sed -e 's/D/W/' > ./NA_random/chrNA_random.scaffolds.agp
   # the scaffolds agp for chrNA_random is now sorted numerically by 
   # scaffold number
   sed -e 's/chrUn/chrUn_random/' ./Un_random/chrUn_random.agp \
       | sed -e 's/D/W/' > ./Un_random/chrUn_random.scaffolds.agp
   # edit ./NA_random/chrNA_random.scaffolds.agp and 
   # ./Un_random/chrUn_random.scaffolds.agp and remove last line as this 
   # just adds an extra 50000 Ns at the 
   # end of the sequence.
   rm ./NA_random/chrNA_random.agp ./Un_random/chrUn_random.agp
cat << '_EOF_' > ./jkStuff/createAgpWithGaps.pl
#!/usr/bin/perl
use strict;

# This script takes a chunks agp and inserts Ns between scaffolds for 
# the chunks (contigs) agp file. Could also insert Ns between scaffolds
# for scaffolds agp.

my ($chrom, $numN, $name, $prev, $st, $end, $prevEnd, $id);
my $chrom = $ARGV[0]; # chromosome name
my $numN = $ARGV[1];  # number of Ns to be inserted 
my $type = $ARGV[2]; # contigs or scaffolds

$prev = "";
$st = 1;
$prevEnd = 0;
$id = 0;

while (<STDIN>)
{
my $l = $_;
my @f = split(/\t/, $l);

if ($type eq "contigs")
   {
   $name = $f[9];
   }
else
   {
   $name = $f[5]
   }

my $currSt = $f[1];
my $currEnd = $f[2];
my $size = $currEnd - $currSt;

$id++;
$st = $prevEnd + 1;
$end = $st + $size;

if (($prev ne "") && ($prev ne $name))
   {
   $st = $prevEnd + 1;
   $end = ($st + $numN) - 1;
   print "$chrom\t$st\t$end\t$id\tN\t$numN\tcontig\tno\n";
   $prevEnd = $end;
   $id++;
   }

$st = $prevEnd + 1;
$end = $st + $size;
print "$chrom\t$st\t$end\t$id\t$f[4]\t$f[5]\t$f[6]\t$f[7]\t$f[8]";
if ($type eq "contigs")
   {
   print "\t$f[9]\t$f[10]\t$f[11]";
   }

$prevEnd = $end;
$prev = $name;
}
'_EOF_'
   chmod +x ./jkStuff/createAgpWithGaps.pl
   cd /cluster/data/danRer4/NA_random
   # for NA_random, sort the chunks.agp by contig number
   perl -pi.bak -e 's/Zv6_NA//' NA_random.chunks.agp
   sort -k6,6n NA_random.chunks.agp > NA_random.chunks2.agp
   # then put back Zv6_NA
   perl -pi.bak -e 's/([0-9]+\.[0-9]+)/Zv6_NA$1/' NA_random.chunks2.agp
   mv NA_random.chunks2.agp NA_random.chunks.agp
   # Un_random.chunks.agp is already sorted by scaffold number
   cd /cluster/data/danRer4
   foreach c (NA_random Un_random)
      cd $c
      perl /cluster/data/danRer4/jkStuff/createAgpWithGaps.pl \
           chr${c} 50000 contigs < ${c}.chunks.agp > chr${c}.chunks.agp
      cd ..
   end
   # check co-ordinates
   # field 2 is the start position, field 3 is the end and field 8 is the size
   # so check that this is consistent in scaffolds and chunks agp. 
   # check that the difference between 7th and 8th fields is the same as the 
   # difference between 11th and 12th fields for chunks agp. 
   cd /cluster/data/danRer4
   foreach c (NA_random Un_random)
        awk '{if ($6 ~ /^Zv6/ && (($3-$2+1) != $8)) print $6;}' \
            $c/chr${c}.scaffolds.agp > $c/chr${c}.scaffolds.coordCheck 
        awk '{if ($6 ~ /^Zv6/ && (($3-$2+1) != $8)) print $6;}' \
            $c/chr${c}.chunks.agp > $c/chr${c}.chunks.coordCheck 
        awk '{if ($5 != "N" && (($8 - $7) != ($12 - $11))) print $6;}' \
            $c/chr${c}.chunks.agp > $c/chr${c}.chunks.coordCheck2 
   end
   # check the outputs are empty
   wc -l NA_random/*.coord*
   wc -l Un_random/*.coord*
   rm NA_random/*.coord* Un_random/*.coord*
   # check that the scaffolds and chunks agp files are consistent with
   # each other. 
cat << '_EOF_' > ./jkStuff/checkSizesInAgps.pl
#!/usr/bin/perl -w
use strict;

my ($ch, $sc, %scafsHash);
$sc = $ARGV[0]; # scaffolds agp
$ch = $ARGV[1]; # chunks or contigs agp

open(SCAFS, $sc) || die "Can not open $sc: $!\n";
open(CHUNKS, $ch) || die "Can not open $ch: $!\n";

while (<SCAFS>)
{
my ($l, @f, $name, $e);
$l = $_;
@f = split(/\t/, $l);
if ($f[5] =~ /^Zv/)
   {
   $name = $f[5];
   $e = $f[2];
   $scafsHash{$name} = $e;
   }
}
close SCAFS;

my $scaf = "";
my $prev = "";
my $prevEnd = 0;

while (<CHUNKS>)
{
my ($line, @fi);
$line = $_;
@fi = split(/\t/, $line);

# if it is not a gap line
if ($fi[4] ne "N")
   {
   $scaf = $fi[9];
   if (($scaf ne $prev) && ($prev ne ""))
      {
      checkCoords($prev, $prevEnd);
      }
$prev = $scaf;
$prevEnd = $fi[2];
   }
}
# check last entry in file
checkCoords($prev, $prevEnd);
close CHUNKS;

sub checkCoords {
my ($name, $end) = @_;
if (exists($scafsHash{$prev}))
   {
   if ($scafsHash{$prev} != $prevEnd)
      {
      my $ed = $scafsHash{$prev};
      print "Scaffold $prev is not consistent between agps\n";
      }
   else
      {
      my $ed = $scafsHash{$prev};
      print "Scaffold $prev - ok\n";
      }
   }
}
'_EOF_'
   # <<< happy emacs   
   chmod +x jkStuff/checkSizesInAgps.pl
   foreach c (NA_random Un_random)
      perl /cluster/data/danRer4/jkStuff/checkSizesInAgps.pl \
           $c/chr${c}.scaffolds.agp $c/chr${c}.chunks.agp \
           > $c/${c}.scafsvschunks
   end
   foreach c (NA_random Un_random)
     grep "not consistent" $c/${c}.scafsvschunks
   end 
   wc -l NA_random/NA_random.scafsvschunks 
   wc -l Un_random/Un_random.scafsvschunks 
   # no lines were inconsistency was reported
   rm NA_random/NA_random.scafsvschunks Un_random/Un_random.scafsvschunks
   # clean up
   foreach c (NA_random Un_random)
      rm $c/${c}.scaffolds.agp $c/${c}.chunks.agp $c/chr${c}.scaffolds.fa \
         $c/chr${c}.scaffolds.lst $c/*.bak
   end
'_EOF_'

###########################################################################
# BUILD CHROM-LEVEL SEQUENCE (DONE, 2006-04-13, hartera)
# REPEAT THIS FOR chrNA_random AND chrUn_random (DONE, 2006-04-21, hartera)
   ssh kkstore01
   cd /cluster/data/danRer4
   # Ignore warnings about chrM files not existing - this chrom has 
   # already been processed - see mitochondrion section above.
   # Sequence is already in upper case so no need to change
   foreach c (`cat chrom.lst`)
     echo "Processing ${c}"
     $HOME/bin/i386/agpToFa -simpleMultiMixed $c/chr$c.scaffolds.agp chr$c \
         $c/chr$c.fa ./Zv6_scaffolds.fa
     echo "${c} - DONE"
   end
   # move scaffolds agp to be chrom agp and clean up
   foreach c (`cat chrom.lst`)
      cd $c
      cp chr${c}.scaffolds.agp chr${c}.agp
      mkdir -p agps
      mv chr${c}.*.agp ./agps/
      cd ..
   end
   # Repeat just for chrNA_random and chrUn_random (2006-04-21, hartera)
   foreach c (NA_random Un_random)
     echo "Processing ${c}"
     $HOME/bin/i386/agpToFa -simpleMultiMixed $c/chr$c.scaffolds.agp chr$c \
         $c/chr$c.fa ./Zv6_scaffolds.fa
     echo "${c} - DONE"
   end
   # move scaffolds agp to be chrom agp and clean up
   foreach c (NA_random Un_random)
      cd $c
      cp chr${c}.scaffolds.agp chr${c}.agp
      mkdir -p agps
      mv chr${c}.*.agp ./agps/
      cd ..
   end

##########################################################################
# CHECK CHROM AND VIRTUAL CHROM SEQUENCES (DONE, 2006-04-14, hartera)
# RE-CHECK THESE AFTER CREATING chrNA_random AND chrUn_random SEQUENCE FILES 
# (DONE, 2006-04-20, hartera)
   # Check that the size of each chromosome .fa file is equal to the last
   # co-ordinate of the corresponding agp file.
   ssh hgwdev
   cd /cluster/data/danRer4
   foreach c (`cat chrom.lst`)
     foreach f ( $c/chr$c.agp )
       set agpLen = `tail -1 $f | awk '{print $3;}'`
       set h = $f:r
       set g = $h:r
       echo "Getting size of $g.fa"
       set faLen = `faSize $g.fa | awk '{print $1;}'`
       if ($agpLen == $faLen) then
         echo "   OK: $f length = $g length = $faLen"
       else
         echo "ERROR:  $f length = $agpLen, but $g length = $faLen"
       endif
     end
   end
   # all are the OK so FASTA files are the expected size

###########################################################################
# CREATING DATABASE (DONE, 2006-04-14, hartera)
    # Create the database.
    # next machine
    ssh hgwdev
    echo 'create database danRer4' | hgsql ''
    # if you need to delete that database:  !!! WILL DELETE EVERYTHING !!!
    echo 'drop database danRer4' | hgsql danRer4
    # Use df to make sure there is at least 10 gig free on
    df -h /var/lib/mysql
# Before loading data:
# Filesystem            Size  Used Avail Use% Mounted on
# /dev/sdc1             1.8T  1.5T  173G  90% /var/lib/mysql

###########################################################################
# CREATING GRP TABLE FOR TRACK GROUPING (DONE, 2006-04-14, hartera)
    # next machine
    ssh hgwdev
    #  the following command copies all the data from the table
    #  grp in the database mm8 to the new database danRer4. Use one of the
    #  newest databases to copy from to make sure that the groupings are
    #  up to date.
    echo "create table grp (PRIMARY KEY(NAME)) select * from mm8.grp" \
      | hgsql danRer4
    # if you need to delete that table:   !!! WILL DELETE ALL grp data !!!
    echo 'drop table grp;' | hgsql danRer4

###########################################################################
# MAKE HGCENTRALTEST ENTRY FOR DANRER4 (DONE, 2006-04-14, hartera)
# CHANGE DATE FORMAT ON HGCENTRALTEST ENTRY (DONE, 2006-04-21, hartera)
    # Make entry into dbDb and defaultDb so test browser knows about it.
    ssh hgwdev
    # Add dbDb and defaultDb entries:
    echo 'insert into dbDb (name, description, nibPath, organism,  \
          defaultPos, active, orderKey, genome, scientificName,  \
          htmlPath, hgNearOk, hgPbOk, sourceName)  \
          values("danRer4", "March 2006", \
          "/gbdb/danRer4", "Zebrafish", "chr2:15,906,734-15,926,406", 1, \
          37, "Zebrafish", "Danio rerio", \
          "/gbdb/danRer4/html/description.html", 0,  0, \
          "Sanger Centre, Danio rerio Sequencing Project Zv6");' \
    | hgsql -h genome-testdb hgcentraltest
    # reformat the date (2006-04-21, hartera)
    echo 'update dbDb set description = "Mar. 2006" where name = "danRer4";' \
         | hgsql -h genome-testdb hgcentraltest

    # Create /gbdb directory for danRer4
    mkdir /gbdb/danRer4
    # SET AS DEFAULT LATER WHEN READY FOR RELEASE
    # set danRer4 to be the default assembly for Zebrafish
    echo 'update defaultDb set name = "danRer4" \
          where genome = "Zebrafish";' \
          | hgsql -h genome-testdb hgcentraltest

###########################################################################
# BREAK UP SEQUENCE INTO 5MB CHUNKS AT CONTIGS/GAPS FOR CLUSTER RUNS
# (DONE, 2006-04-14, hartera)
# RE-DONE JUST FOR chrNA_random AND chrUn_random (DONE, 2006-04-20, hartera)
     ssh kkstore01
     cd /cluster/data/danRer4
     foreach c (`cat chrom.lst`)
       foreach agp ($c/chr$c.agp)
         if (-e $agp) then
           set fa = $c/chr$c.fa
           echo splitting $agp and $fa
           cp -p $agp $agp.bak
           cp -p $fa $fa.bak
           splitFaIntoContigs $agp $fa . -nSize=5000000
         endif
       end
     end
     
     # Repeat just for chrNA_random and chrUn_random (2006-04-21, hartera)
     ssh kkstore01
     cd /cluster/data/danRer4
     foreach c (NA_random Un_random)
       foreach agp ($c/chr$c.agp)
         if (-e $agp) then
           set fa = $c/chr$c.fa
           echo splitting $agp and $fa
           cp -p $agp $agp.bak
           cp -p $fa $fa.bak
           splitFaIntoContigs $agp $fa . -nSize=5000000
         endif
       end
     end

###########################################################################
# MAKE LIFTALL.LFT (DONE, 2006-04-14, hartera)
# REMAKE LIFTALL.LFT WITH chrNA_random AND chrUn_random 
# (DONE, 2006-04-21, hartera)
     ssh kkstore01
     cd /cluster/data/danRer4
     rm jkStuff/liftAll.lft
     foreach c (`cat chrom.lst`)
       cat $c/lift/ordered.lft >> jkStuff/liftAll.lft
     end

###########################################################################
# MAKE TRACKDB ENTRY FOR DANRER4 (DONE, 2006-04-14, hartera)
# Should add this later when adding gold/gap tracks. Angie created a 
# temporary chromInfo table otherwise make update/alpha causes an error
# (2006-04-17)
    # Make trackDb table so browser knows what tracks to expect.
    ssh hgwdev
    mkdir -p ~/kent/src/hg/makeDb/trackDb/zebrafish/danRer4
    cd ~/kent/src/hg/makeDb/trackDb/zebrafish
    cvs add danRer4
    cvs commit danRer4
    cd ~/kent/src/hg/makeDb/trackDb
    cvs up -d -P
    # Edit that makefile to add danRer4 in all the right places and do
    make update DBS=danRer4
    make alpha DBS=danRer4
    cvs commit -m "Added danRer4." makefile

###########################################################################
# MAKE DESCRIPTION/SAMPLE POSITION HTML PAGE (DONE, 2006-04-14, hartera)
    ssh hgwdev
    mkdir /cluster/data/danRer4/html
    # make a symbolic link from /gbdb/danRer4/html to /cluster/data/danRer4/html
    ln -s /cluster/data/danRer4/html /gbdb/danRer4/html
    # Add a description page for zebrafish
    cd /cluster/data/danRer4/html
    cp $HOME/kent/src/hg/makeDb/trackDb/zebrafish/danRer3/description.html .
    # Edit this for zebrafish danRer4

    # create a description.html page here
    cd ~/kent/src/hg/makeDb/trackDb/zebrafish/danRer4
    # Add description page here too
    cp /cluster/data/danRer4/html/description.html .
    cvs add description.html
    cvs commit -m "First draft of description page for danRer4." \
        description.html
    cd ~/kent/src/hg/makeDb/trackDb
    make update DBS=danRer4
    make alpha  DBS=danRer4

###########################################################################
# SIMPLE REPEAT [TRF] TRACK  (DONE, 2006-04-14, hartera)
# RE-RUN FOR chrNA AND chrUn RENAMED AS chrNA_random AND chrUn_random
# AND RELOAD THE TABLE (DONE, 2006-04-21, hartera)
# MADE A NOTE IN THE HISTORY TABLE TO EXPLAIN WHY THE simpleRepeats TABLE
# WAS RELOADED (DONE, 2006-04-22, hartera)
    # TRF can be run in parallel with RepeatMasker on the file server
    # since it doesn't require masked input sequence.
    # Run this on the kilokluster. Need to mask contig and chromosome 
    # sequences so run trf using contig sequences.
    # First copy over contig sequences to iscratch and then rsync to cluster.
    ssh kkr1u00
    rm -r /iscratch/i/danRer4/contigsNoMask
    mkdir -p /iscratch/i/danRer4/contigsNoMask
    cd /cluster/data/danRer4
    foreach d (/cluster/data/danRer4/*/chr*_?{,?})
       set ctg = $d:t
       foreach f ($d/${ctg}.fa)
          echo "Copyig $f ..."
          cp $f /iscratch/i/danRer4/contigsNoMask/
       end
    end
    ls /iscratch/i/danRer4/contigsNoMask/*.fa | wc -l
    # 317 sequence files
    # rsync to cluster machines
    foreach R (2 3 4 5 6 7 8)
       rsync -a --progress /iscratch/i/danRer4/ kkr${R}u00:/iscratch/i/danRer4/
    end
     
    ssh kki
    mkdir -p /cluster/data/danRer4/bed/simpleRepeat
    cd /cluster/data/danRer4/bed/simpleRepeat
    mkdir trf
cat << '_EOF_' > runTrf
#!/bin/csh -fe
#
set path1 = $1
set inputFN = $1:t
set outpath = $2
set outputFN = $2:t
mkdir -p /tmp/$outputFN
cp $path1 /tmp/$outputFN
pushd .
cd /tmp/$outputFN
/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $inputFN /dev/null -bedAt=$outputFN -tempDir=/tmp
popd
rm -f $outpath
cp -p /tmp/$outputFN/$outputFN $outpath
rm -fr /tmp/$outputFN/*
rmdir --ignore-fail-on-non-empty /tmp/$outputFN
'_EOF_'
 # << keep emacs coloring happy
    chmod +x runTrf

cat << '_EOF_' > gsub
#LOOP
./runTrf {check in line+ $(path1)}  {check out line trf/$(root1).bed}
#ENDLOOP
'_EOF_'
    # << keep emacs coloring happy

    ls -1S /iscratch/i/danRer4/contigsNoMask/chr*.fa > genome.lst
    gensub2 genome.lst single gsub jobList
    # 317 jobs
    para create jobList
    para try, check, push, check etc...
    para time
# Completed: 317 of 317 jobs
# CPU time in finished jobs:      25083s     418.05m     6.97h    0.29d  0.001 y
# IO & Wait Time:                   933s      15.55m     0.26h    0.01d  0.000 y
# Average job time:                  82s       1.37m     0.02h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            2732s      45.53m     0.76h    0.03d
# Submission to last job:          4604s      76.73m     1.28h    0.05d

    # Re-do only for chrNA_random and chrUn_random (2006-04-21, hartera)
    ssh kki
    cd /cluster/data/danRer4/bed/simpleRepeat
    rm trf/chrNA*.bed
    rm trf/chrUn*.bed
    rm simpleRepeat.bed
    mkdir -p randomsRun/trf
    cd randomsRun
    cp ../runTrf .
    cp ../gsub .
    ls -1S /iscratch/i/danRer4/contigsNoMask/chr*_random*.fa > genome.lst
    gensub2 genome.lst single gsub jobList
    para create jobList
    # 46 jobs
    para try, check, push, check etc...
    para time
# Completed: 46 of 46 jobs
# CPU time in finished jobs:       1904s      31.73m     0.53h    0.02d  0.000 y
# IO & Wait Time:                   103s       1.72m     0.03h    0.00d  0.000 y
# Average job time:                  44s       0.73m     0.01h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:             241s       4.02m     0.07h    0.00d
# Submission to last job:           269s       4.48m     0.07h    0.00d 
    cp ./trf/*.bed /cluster/data/danRer4/bed/simpleRepeat/trf/
    # lift up to chrom level
    cd /cluster/data/danRer4/bed/simpleRepeat
    rm simpleRepeat.bed
    liftUp simpleRepeat.bed /cluster/data/danRer4/jkStuff/liftAll.lft warn \
           trf/*.bed
    # Reload into the database
    ssh hgwdev
    cd /cluster/data/danRer4/bed/simpleRepeat
    hgsql -e 'drop table simpleRepeat;' danRer4
    hgLoadBed danRer4 simpleRepeat simpleRepeat.bed \
      -sqlTable=$HOME/kent/src/hg/lib/simpleRepeat.sql
    # Loaded 759659 elements of size 16
    # Make a note in the history table to explain why the simpleRepeats
    # table was reloaded (2006-04-22, hartera)
    hgsql -e 'update history set errata = \
      "Dropped table for reloading after changing names of random chroms." \
      where ix = 2;' danRer4

###########################################################################
# CREATE MICROSAT TRACK (done 2006-7-5 JK)
     ssh hgwdev
     cd /cluster/data/danRer4/bed
     mkdir microsat
     cd microsat
     awk '($5==2 || $5==3) && $6 >= 15 && $8 == 100 && $9 == 0 {printf("%s\t%s\t%s\t%dx%s\n", $1, $2, $3, $6, $16);}' ../simpleRepeat/simpleRepeat.bed > microsat.bed 
    /cluster/bin/i386/hgLoadBed canFam2 microsat microsat.bed

###########################################################################
# PROCESS SIMPLE REPEATS INTO MASK (DONE, 2005-06-14, hartera)
# RE-DO AFTER RENAMING RANDOM CHROMS AS chrNA_random AND chrUn_random
# (DONE, 2006-04-21, hartera)
   # After the simpleRepeats track has been built, make a filtered version
   # of the trf output: keep trf's with period <= 12:
   ssh kkstore01
   cd /cluster/data/danRer4/bed/simpleRepeat
   rm -r trfMask
   mkdir -p trfMask
   foreach f (trf/chr*.bed)
     awk '{if ($5 <= 12) print;}' $f > trfMask/$f:t
   end

   # Lift up filtered trf output to chrom coords as well:
   cd /cluster/data/danRer4
   rm -r ./bed/simpleRepeat/trfMaskChrom
   mkdir bed/simpleRepeat/trfMaskChrom
   
   foreach c (`cat chrom.lst`)
     if (-e $c/lift/ordered.lst) then
       perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
         $c/lift/ordered.lst > $c/lift/oTrf.lst
       liftUp bed/simpleRepeat/trfMaskChrom/chr$c.bed \
         jkStuff/liftAll.lft warn `cat $c/lift/oTrf.lst`
     endif
     if (-e $c/lift/random.lst) then
       perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
          $c/lift/random.lst > $c/lift/rTrf.lst
       liftUp bed/simpleRepeat/trfMaskChrom/chr${c}_random.bed \
         jkStuff/liftAll.lft warn `cat $c/lift/rTrf.lst`
     endif
   end

###########################################################################
# GET ADDITIONAL ZEBRAFISH REPBASE LIBRARY FOR REPEATMASKER AND ADD TO
# DANIO LIBRARY FOR REPEATMASKER (DONE, 2006-04-14, hartera)
# Go to http://www.girinst.org/server/RepBase/RepBase11.02.fasta
# (03-15-2006) and download zebunc.ref.txt containing unclassified zebrafish 
# repeats.
# Need username and password. Copy to /cluster/bluearc/RepeatMasker/Libraries/
   ssh hgwdev
   cd /cluster/bluearc/RepeatMasker/Libraries
   # This is /cluster/bluearc/RepeatMasker060320/Libraries
   # Do a dummy run of RepeatMasker with the -species option. This creates
   # a zebrafish-specific library from the EMBL format RepBase library.
   # Then the zebunc.ref unclassified repeats can be added to this library.
   /cluster/bluearc/RepeatMasker/RepeatMasker -spec danio /dev/null
   # RepeatMasker version development-$Id: RepeatMasker,v 1.13 2006/03/21 
   # This creates a specieslib in Libraries/20060315/danio
   # Format the zebunc.ref library:
   # Sequence is upper case, change to lower case like the specieslib
   cat zebunc.ref.txt | tr '[A-Z]' '[a-z]' > zebunc.ref.format 
   perl -pi.bak -e 's/>dr([0-9]+)/>Dr$1#Unknown/' zebunc.ref.format
   grep '>' zebunc.ref.format | wc -l
   # 958
   cd /cluster/bluearc/RepeatMasker/Libraries/20060315/danio
   grep '>' specieslib | wc -l
   # 219
   mv specieslib danio.lib
   cat danio.lib ../../zebunc.ref.format > specieslib  
   grep '>' specieslib | wc -l
   # 1177
   rm danio.lib
   # make a copy in Libraries directory in case this directory of libraries
   # is removed.
   cp specieslib /cluster/bluearc/RepeatMasker/Libraries/danio.lib
 
###########################################################################
# SPLIT SEQUENCE FOR REPEATMASKER RUN (DONE, 2006-04-14, hartera)
# SPLIT SEQUENCE AGAIN JUST FOR chrNA_random AND chrUn_random AFTER RENAMING
# THESE RANDOM CHROMS (DONE, 2006-04-21, hartera)
   ssh kkstore01
   cd /cluster/data/danRer4
   
   # break up into 500 kb sized chunks at gaps if possible 
   # for RepeatMasker runs
   foreach c (`cat chrom.lst`)
      foreach d ($c/chr${c}*_?{,?})
        cd $d
        echo "splitting $d"
        set contig = $d:t
        faSplit gap $contig.fa 500000 ${contig}_ -lift=$contig.lft \
            -minGapSize=100
        cd ../..
      end
   end
   # took about 3 minutes. 
   # split just for chrNA_random and chrUn_random (2006-04-21, hartera)
   cd /cluster/data/danRer4
   foreach c (NA_random Un_random)
      foreach d ($c/chr${c}*_?{,?})
        cd $d
        echo "splitting $d"
        set contig = $d:t
        faSplit gap $contig.fa 500000 ${contig}_ -lift=$contig.lft \
            -minGapSize=100
        cd ../..
      end
    end

###########################################################################
# REPEATMASKER RUN (DONE, 2006-04-21, hartera)
   # Originally run 2006-04-14. There was one sequence chr16_4_10.fa that 
   # failed with a division by zero error. Sent this as a test case with the 
   # danio library to Robert Hubley who fixed the bug and sent a new
   # version of ProcessRepeats. Checked this into CVS for 
   # /cluster/bluearc/RepeatMasker on 2006-04-19.
   # When a new library is added for this version of RepeatMasker, need to 
   # check in /cluster/bluearc/RepeatMasker/Libraries for a directory made 
   # up of a date e.g. 20060315 here and inside this are species directories
   # for which RepeatMasker has already been run. In this directory it creates
   # a specieslib of the danio repeats. If this exists, this is used for the
   # RepeatMasker run for that species. Check that this contains the 
   # unclassified Zebrafish repeats with IDs beginning with Dr. This library
   # with these repeats should have been created in the section above:
   # Use sequence split into 500 kb chunks.
   ssh kkstore01
   cd /cluster/data/danRer4
   mkdir RMRun
   # Record RM version used:
   ls -l /cluster/bluearc/RepeatMasker
   # lrwxrwxrwx  1 angie protein 18 Mar 20 16:50 /cluster/bluearc/RepeatMasker -> RepeatMasker060320
   # March 20 2006 (open-3-1-5) version of RepeatMasker
   # get RM database version
   grep RELEASE /cluster/bluearc/RepeatMasker/Libraries/RepeatMaskerLib.embl \
        > RMdatabase.version
   # RELEASE 20060315

   cd /cluster/data/danRer4
   cat << '_EOF_' > jkStuff/RMZebrafish
#!/bin/csh -fe

cd $1
pushd .
/bin/mkdir -p /tmp/danRer4/$2
/bin/cp $2 /tmp/danRer4/$2/
cd /tmp/danRer4/$2
/cluster/bluearc/RepeatMasker060320/RepeatMasker -ali -s -species danio $2
popd
/bin/cp /tmp/danRer4/$2/$2.out ./
if (-e /tmp/danRer4/$2/$2.align) /bin/cp /tmp/danRer4/$2/$2.align ./
if (-e /tmp/danRer4/$2/$2.tbl) /bin/cp /tmp/danRer4/$2/$2.tbl ./
if (-e /tmp/danRer4/$2/$2.cat) /bin/cp /tmp/danRer4/$2/$2.cat ./
/bin/rm -fr /tmp/danRer4/$2/*
/bin/rmdir --ignore-fail-on-non-empty /tmp/danRer4/$2
/bin/rmdir --ignore-fail-on-non-empty /tmp/danRer4
'_EOF_'
   # << emacs
   chmod +x jkStuff/RMZebrafish

   # move old files out the way and re-run on 2006-04-19
   cd /cluster/data/danRer4
   mkdir RMOutOld
   foreach d (*/chr*_?{,?})
      set contig = $d:t
      echo $contig
      foreach c ($d/$contig*.fa.*)
         set t=$c:t
         mv $c /cluster/data/danRer4/RMOutOld/$t.bak
      end
   end  
 
   cp /dev/null RMRun/RMJobs
   foreach c (`cat chrom.lst`)
      foreach d ($c/chr${c}_?{,?})
          set ctg = $d:t
          foreach f ( $d/${ctg}_?{,?}.fa )
            set f = $f:t
            echo /cluster/data/danRer4/jkStuff/RMZebrafish \
                 /cluster/data/danRer4/$d $f \
               '{'check out line+ /cluster/data/danRer4/$d/$f.out'}' \
              >> RMRun/RMJobs
          end
      end
   end

   # Do the run again with new version of ProcessRepeats used 
   # for RepeatMasker.
   ssh pk
   cd /cluster/data/danRer4/RMRun
   para create RMJobs
   # 4382 jobs written to batch
   para try, check, push, check ... etc.
   para time
# Completed: 4382 of 4382 jobs
# CPU time in finished jobs:   11745656s  195760.94m  3262.68h  135.95d  0.372 y
# IO & Wait Time:                 18953s     315.88m     5.26h    0.22d  0.001 y
# Average job time:                2685s      44.75m     0.75h    0.03d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            3878s      64.63m     1.08h    0.04d
# Submission to last job:         41887s     698.12m    11.64h    0.48d 
   
   #- Lift up the 500KB chunk .out's to 5MB ("pseudo-contig") level
   ssh kkstore01
   cd /cluster/data/danRer4
   foreach d (*/chr*_?{,?})
     set contig = $d:t
     echo $contig
     liftUp $d/$contig.fa.out $d/$contig.lft warn $d/${contig}_*.fa.out \
        > /dev/null
   end

   #- Lift pseudo-contigs to chromosome level
   foreach c (`cat chrom.lst`)
      echo lifting $c
      cd $c
      if (-e lift/ordered.lft && ! -z lift/ordered.lft) then
        liftUp chr$c.fa.out lift/ordered.lft warn `cat lift/oOut.lst` \
        > /dev/null
      endif
      cd ..
   end
   # Re-run for just chrNA_random and chrUn_random (start on 2006-04-21)
   ssh kkstore01
   mkdir /cluster/data/danRer4/RMRun/randomsRun
   cd /cluster/data/danRer4
   cp /dev/null RMRun/randomsRun/RMJobs
   foreach c (NA_random Un_random)
      foreach d ($c/chr${c}_?{,?})
          set ctg = $d:t
          foreach f ( $d/${ctg}_?{,?}.fa )
            set f = $f:t
            echo /cluster/data/danRer4/jkStuff/RMZebrafish \
                 /cluster/data/danRer4/$d $f \
               '{'check out line+ /cluster/data/danRer4/$d/$f.out'}' \
              >> RMRun/randomsRun/RMJobs
          end
      end
   end

   # Do the run again for chrNA_random and chrUn_random.
   ssh pk
   cd /cluster/data/danRer4/RMRun/randomsRun
   para create RMJobs
   # 468 jobs written to batch
   para try, check, push, check ... etc.
   para time
# Completed: 468 of 468 jobs
# CPU time in finished jobs:     551863s    9197.71m   153.30h    6.39d  0.017 y
# IO & Wait Time:                  2217s      36.96m     0.62h    0.03d  0.000 y
# Average job time:                1184s      19.73m     0.33h    0.01d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            3836s      63.93m     1.07h    0.04d
# Submission to last job:          9086s     151.43m     2.52h    0.11d

   #- Lift up the 500KB chunk .out's to 5MB ("pseudo-contig") level
   ssh kkstore01
   cd /cluster/data/danRer4
   foreach c (NA_random Un_random)
     foreach d (${c}/chr*_?{,?})
       set contig = $d:t
       echo $contig
       liftUp $d/$contig.fa.out $d/$contig.lft warn $d/${contig}_*.fa.out \
              > /dev/null
     end
   end

   #- Lift pseudo-contigs to chromosome level
   foreach c (NA_random Un_random)
      echo lifting $c
      cd $c
      if (-e lift/ordered.lft && ! -z lift/ordered.lft) then
        liftUp chr$c.fa.out lift/ordered.lft warn `cat lift/oOut.lst` \
        > /dev/null
      endif
      cd ..
   end

   # Load tables
   #- Load the .out files into the database with:
   ssh hgwdev
   cd /cluster/data/danRer4
   hgLoadOut danRer4 */chr*.fa.out -verbose=2 >& load.log
# bad rep range [5031, 4990] line 51895 of 14/chr14.fa.out 
# bad rep range [4559, 4558] line 59431 of 16/chr16.fa.out 
# bad rep range [1202, 1201] line 131633 of 16/chr16.fa.out 
# bad rep range [280, 252] line 93608 of 17/chr17.fa.out 
# bad rep range [429, 272] line 43230 of 22/chr22.fa.out 
# bad rep range [262, 261] line 167346 of 3/chr3.fa.out 
# bad rep range [889, 888] line 28495 of 5/chr5.fa.out 
# bad rep range [349, 348] line 113404 of 5/chr5.fa.out 
# bad rep range [1133, 1132] line 200654 of 5/chr5.fa.out 
# bad rep range [965, 920] line 3567 of 8/chr8.fa.out 
# bad rep range [292, 291] line 6354 of NA_random/chrNA_random.fa.out
# note: 11 records dropped due to repStart > repEnd
   # Not too many errors so just ignore, but send examples to Arian Smit
   # and Robert Hubley. 
   # check coverage of repeats masked
   featureBits -chrom=chr1 danRer3 rmsk
   # 25822888 bases of 55500710 (46.527%) in intersection 
   featureBits -chrom=chr1 danRer4 rmsk 
   # 32880041 bases of 70589895 (46.579%) in intersection

###########################################################################
# MASK SEQUENCE WITH REPEATMASKER AND SIMPLE REPEAT/TRF AND BUILD NIB FILES
# (DONE, 2006-04-22, hartera)
# MASK PSEUDO-CONTIGS AS NOT DONE BEFORE (DONE, 2006-05-27, hartera)
    ssh kkstore01
    cd /cluster/data/danRer4
    # Soft-mask (lower-case) the contig and chr .fa's,
    # then make hard-masked versions from the soft-masked.
    set trfCtg=bed/simpleRepeat/trfMask
    set trfChr=bed/simpleRepeat/trfMaskChrom
    # for the chromosomes:
    foreach f (*/chr*.fa)
      echo "repeat- and trf-masking $f"
      maskOutFa -soft $f $f.out $f
      set chr = $f:t:r
      maskOutFa -softAdd $f $trfChr/$chr.bed $f
      echo "hard-masking $f"
      maskOutFa $f hard $f.masked
    end

    # check percent sequence masked
    faSize /cluster/data/danRer4/1/chr1.fa
    # 70589895 bases (904883 N's 69685012 real 36751306 upper 
    # 32933706 lower) in 1 sequences in 1 files

    faSize /cluster/data/danRer3/1/chr1.fa
    # 55805710 bases (1047706 N's 54758004 real 28887275 upper 
    # 25870729 lower) in 1 sequences in 1 files
    # 47% of danRer4 chr1.fa is in lower case so masked
    # Build nib files, using the soft masking in the fa
    mkdir nib
    foreach f (*/chr*.fa)
      faToNib -softMask $f nib/$f:t:r.nib
    end
    ls ./nib/* | wc
    # 28
    
    # for the contigs (2006-05-27, hartera)
    ssh kkstore04
    cd /cluster/data/danRer4
    set trfCtg=bed/simpleRepeat/trfMask
    set trfChr=bed/simpleRepeat/trfMaskChrom
    foreach c (`cat chrom.lst`)
      echo "repeat- and trf-masking contigs of chr$c"
      foreach d ($c/chr*_?{,?})
        set ctg=$d:t
        set f=$d/$ctg.fa
        maskOutFa -soft $f $f.out $f
        maskOutFa -softAdd $f $trfCtg/$ctg.bed $f
        maskOutFa $f hard $f.masked
      end
    end
    
###########################################################################
# STORING O+O SEQUENCE AND ASSEMBLY INFORMATION AND CREATE 2BIT FILE
# (DONE, 2006-04-23, hartera)
# CHANGE FILENAME TO 2BIT FILE IN CHROMINFO AND REMOVE NIB DIR IN /gbdb
# (DONE, 2006-05-24, hartera)
    # Make symbolic links from /gbdb/danRer4/nib to the real nibs
    ssh hgwdev
    cd /cluster/data/danRer4
    mkdir -p /gbdb/danRer4/nib
    foreach f (/cluster/data/danRer4/nib/chr*.nib)
      ln -s $f /gbdb/danRer4/nib
    end
    # Load /gbdb/danRer4/nib paths into database and save size info
    # hgNibSeq creates chromInfo table
    hgNibSeq -preMadeNib danRer4 /gbdb/danRer4/nib */chr*.fa
    echo "select chrom,size from chromInfo" | hgsql -N danRer4 > chrom.sizes
    # take a look at chrom.sizes, should be 28 lines
    wc chrom.sizes
    # 28      56     422 chrom.sizes

    # Make one big 2bit file as well, and make a link to it in
    # /gbdb/danRer4 because hgBlat looks there:
    faToTwoBit */chr*.fa danRer4.2bit
    # check the 2bit file
    twoBitInfo danRer4.2bit 2bit.tab
    diff 2bit.tab chrom.sizes
    # should be the same and they are so ok.
    rm 2bit.tab
    # add link to this 2bit file from gbdb danRer4 directory 
    ln -s /cluster/data/danRer4/danRer4.2bit /gbdb/danRer4/
    # (hartera, 2006-05-24)
    # change chromInfo table to have 2bit file for filename
    hgsql -e 'update chromInfo set fileName = "/gbdb/danRer4/danRer4.2bit";' \
          danRer4
    # then remove nib directory in /gbdb/danRer4 as do not need both nibs
    # and 2 bit file which is in /gbdb/danRer4.
    rm -r /gbdb/danRer4/nib

###########################################################################
# MAKE GOLD AND GAP TRACKS (DONE, 2006-04-23, hartera)
    ssh hgwdev
    cd /cluster/data/danRer4
    # the gold and gap tracks are created from the chrN.agp file and this is
    # the scaffolds or supercontigs agp 
    hgGoldGapGl -noGl -chromLst=chrom.lst danRer4 /cluster/data/danRer4 .

    # featureBits danRer4 gold
    # 1626093931 bases of 1626093931 (100.000%) in intersection
    # featureBits danRer3 gold
    # 1630323462 bases of 1630323462 (100.000%) in intersection

    # featureBits danRer4 gap
    # 148566200 bases of 1626093931 (9.136%) in intersection
    # featureBits danRer3 gap
    # 13709500 bases of 1630323462 (0.841%) in intersection
    # there are larger gaps now in chrNA and chrUn so compare just chr1
    # featureBits -chrom=chr1 danRer4 gap
    # 16000 bases of 70573895 (0.023%) in intersection
    # featureBits -chrom=chr1 danRer3 gap
    # 305000 bases of 55500710 (0.550%) in intersection
    # without random or chrUn chroms:
    # featureBits -noRandom danRer4 gap
    # 366200 bases of 1546950119 (0.024%) in intersection
    # featureBits -noRandom danRer3 gap
    # 6240000 bases of 1200146216 (0.520%) in intersection
# Add trackDb.ra entries for gold and gap tracks and also create
# gap.html and gold.html pages.

###########################################################################
# PUT MASKED SEQUENCE OUT ON iSERVERS AND THE SAN FOR CLUSTER RUNS
# (DONE, 2006-04-23, hartera)
# TRFFA SEQUENCED WAS NOT MASKED SO ADD MASKED SEQUENCE TO iSERVERS AND 
# THE SAN FOR CLUSTER RUNS (DONE, 2006-05-30, hartera)
    ssh kkr1u00
    # Chrom-level mixed nibs that have been repeat- and trf-masked:
    rm -rf /iscratch/i/danRer4/nib
    mkdir -p /iscratch/i/danRer4/nib
    cp -p /cluster/data/danRer4/nib/chr*.nib /iscratch/i/danRer4/nib
    # Pseudo-contig fa that have been repeat- and trf-masked:
    # Add these pseudo-contigs that have been repeat- and trf-masked
    # and rsync again. (2006-05-30, hartera)
    rm -rf /iscratch/i/danRer4/trfFa
    mkdir /iscratch/i/danRer4/trfFa
    foreach d (/cluster/data/danRer4/*/chr*_?{,?})
      cp -p $d/$d:t.fa /iscratch/i/danRer4/trfFa
    end
    rm -rf /iscratch/i/danRer4/rmsk
    mkdir -p /iscratch/i/danRer4/rmsk
    cp -p /cluster/data/danRer4/*/chr*.fa.out /iscratch/i/danRer4/rmsk
    cp -p /cluster/data/danRer4/danRer4.2bit /iscratch/i/danRer4/
    # rsync files - faster than using iSync
    # rsync again - still can not rsync to kkr2u00 (hartera, 2006-05-30)
    foreach R (2 3 4 5 6 7 8)
      echo "rsync for kkr${R}u00 ..."
      rsync -a --progress /iscratch/i/danRer4/ kkr${R}u00:/iscratch/i/danRer4/
    end
    # error rsyncing to kkr2u00: 
    # connect to host kkr2u00 port 22: No route to host

    # then add the same sequence files to the san
    ssh kkstore01
    # Chrom-level mixed nibs that have been repeat- and trf-masked:

    mkdir -p /san/sanvol1/scratch/danRer4/nib
    rm -rf /san/sanvol1/scratch/danRer4/nib
    cp -p /cluster/data/danRer4/nib/chr*.nib /san/sanvol1/scratch/danRer4/nib
    cp /cluster/data/danRer4/danRer4.2bit /san/sanvol1/scratch/danRer4
    # Pseudo-contig fa that have been repeat- and trf-masked:
    # Add these pseudo-contigs again (2006-05-30, hartera)
    ssh kkstore04
    rm -rf /san/sanvol1/scratch/danRer4/trfFa
    mkdir /san/sanvol1/scratch/danRer4/trfFa
    foreach d (/cluster/data/danRer4/*/chr*_?{,?})
      cp -p $d/$d:t.fa /san/sanvol1/scratch/danRer4/trfFa
    end
    
###########################################################################
# ADD CONTIGS TRACK (DONE, 2006-04-23, hartera)
    # make ctgPos2 (contig name, size, chrom, chromStart, chromEnd) from 
    # chunks (contigs) agp files.
    ssh kkstore01
    mkdir -p /cluster/data/danRer4/bed/ctgPos2
    cd /cluster/data/danRer4/bed/ctgPos2
    # ctgPos2 .sql .as .c and .h files exist - see makeDanRer1.doc
    foreach c (`cat /cluster/data/danRer4/chrom.lst`)
         awk 'BEGIN {OFS="\t"} \
         {if ($5 != "N") print $6, $3-$2+1, $1, $2-1, $3, $5}' \
         /cluster/data/danRer4/$c/agps/chr${c}.chunks.agp >> ctgPos2.tab
    end
    # load the ctgPos2 table
    ssh hgwdev
    cd /cluster/data/danRer4/bed/ctgPos2
    # use hgLoadSqlTab as it gives more error messages than using 
    # "load data local infile ...".
    /cluster/bin/i386/hgLoadSqlTab danRer4 ctgPos2 \
            ~/kent/src/hg/lib/ctgPos2.sql ctgPos2.tab
# create trackDb.ra entry and html page for ctgPos2 track.
# add search for the track and make sure the termRegex will handle
# contigs named "Zv6_scaffoldN.N" where N is an integer and all the 
# contig accessions in the *.chunks.agp files.

###########################################################################
# CREATE gc5Base WIGGLE TRACK (DONE, 2006-04-23, hartera)
    ssh kkstore01
    mkdir -p /cluster/data/danRer4/bed/gc5Base
    cd /cluster/data/danRer4/bed/gc5Base
    nice hgGcPercent -wigOut -doGaps -file=stdout -win=5 danRer4 \
        /cluster/data/danRer4 | wigEncode stdin gc5Base.wig gc5Base.wib
    #       Calculating gcPercent with window size 5
    #       Using twoBit: /cluster/data/danRer4/danRer4.2bit
    #       File stdout created
    #   Converted stdin, upper limit 100.00, lower limit 0.00
    # runs for about 7 minutes 

    # load database with the .wig file and add .wib file to /gbdb/danRer4
    ssh hgwdev
    cd /cluster/data/danRer4/bed/gc5Base
    mkdir /gbdb/danRer4/wib
    ln -s `pwd`/gc5Base.wib /gbdb/danRer4/wib
    time hgLoadWiggle -pathPrefix=/gbdb/danRer4/wib danRer4 gc5Base gc5Base.wig
    # 17 second load time

    #   verify index is correct:
    hgsql danRer4 -e "show index from gc5Base;"
    #   should see good numbers in Cardinality column

###########################################################################
# MAKE 10.OOC, 11.OOC FILES FOR BLAT (DONE, 2005-04-24, hartera)
    # Use -repMatch=512 (based on size -- for human we use 1024, and
    # the zebrafish genome is ~50% of the size of the human genome
    ssh kkr1u00
    mkdir /cluster/data/danRer4/bed/ooc
    cd /cluster/data/danRer4/bed/ooc
    mkdir -p /san/sanvol1/scratch/danRer4
    ls -1 /cluster/data/danRer4/nib/chr*.nib > nib.lst
    blat nib.lst /dev/null /dev/null -tileSize=11 \
      -makeOoc=/san/sanvol1/scratch/danRer4/danRer4_11.ooc -repMatch=512
    # Wrote 50424 overused 11-mers to /cluster/bluearc/danRer4/11.ooc
    # For 10.ooc, repMatch = 4096 for human, so use 2048
    blat nib.lst /dev/null /dev/null -tileSize=10 \
      -makeOoc=/san/sanvol1/scratch/danRer4/danRer4_10.ooc -repMatch=2048
    # Wrote 12231 overused 10-mers to /cluster/bluearc/danRer4/10.ooc 
    # keep copies of ooc files in this directory and copy to iscratch
    cp /san/sanvol1/scratch/danRer4/*.ooc .
    cp -p /san/sanvol1/scratch/danRer4/*.ooc /iscratch/i/danRer4/
    # rsync to iServers
    foreach R (2 3 4 5 6 7 8)
       rsync -a --progress /iscratch/i/danRer4/*.ooc \
             kkr${R}u00:/iscratch/i/danRer4/
    end
     
###########################################################################
# MAKE HGCENTRALTEST BLATSERVERS ENTRY FOR danRer4 (DONE, 2006-04-27, hartera)
   ssh hgwdev
   # DNA port is "0", trans prot port is "1"  
   echo 'insert into blatServers values("danRer4", "blat17", "17788", "1", "0");  insert into blatServers values("danRer4", "blat17", "17789", "0", "1");' \
    | hgsql hgcentraltest
   # this enables blat and isPcr, isPcr is enabled by loading blat server
   # with tilesize=5 (ask for this when request blat servers from
   # cluster admin).
   # if you need to delete those entries
   echo 'delete from blatServers where db="danRer4";' | hgsql hgcentraltest
 
###########################################################################
# AFFYMETRIX ZEBRAFISH GENOME ARRAY CHIP (DONE, 2006-04-24, hartera)
# NOTE: Jim recommends that, in the future, all AFFY blat alignments should drop
# -mask=lower for blat and drop -minIdentity=95 to -minIdentity=90 as the
# higher minIdentity is causing alignments to be dropped that should not be. 
# e.g.  /cluster/bin/i386/blat -fine -minIdentity=90 -ooc=11.ooc  
# $(path1) $(path2) {check out line+ psl/$(root1)_$(root2).psl}
# pslReps can be used to handle filtering at a later step. Blat's minIdentity 
# seems to be more severe than that for pslReps as it takes insertions and 
# deletions into account. 
# CHECKED ALIGNMENTS USING MASKED TRFFA AND RESULTS ARE THE SAME
# (DONE, 2006-05-30, hartera)
# array chip sequences already downloaded for danRer1
    ssh hgwdev
    # need to copy sequences to the bluearc first to transfer to the iServers
    cd /projects/compbio/data/microarray/affyZebrafish
    mkdir -p /cluster/bluearc/affy
    cp -p \
      /projects/compbio/data/microarray/affyZebrafish/Zebrafish_consensus.fa \
      /cluster/bluearc/affy/
    # Set up cluster job to align Zebrafish consensus sequences to danRer3
    ssh kkr1u00
    mkdir -p /cluster/data/danRer4/bed/affyZebrafish.2006-04-24
    ln -s /cluster/data/danRer4/bed/affyZebrafish.2006-04-24 \
          /cluster/data/danRer4/bed/affyZebrafish
    cd /cluster/data/danRer4/bed/affyZebrafish
    mkdir -p /iscratch/i/affy
    cp /cluster/bluearc/affy/Zebrafish_consensus.fa /iscratch/i/affy
    foreach R (2 3 4 5 6 7 8)
       rsync -a --progress /iscratch/i/affy/*.fa \
             kkr${R}u00:/iscratch/i/affy/
    end
    # small cluster run to align sequences
    ssh kki
    cd /cluster/data/danRer4/bed/affyZebrafish
    ls -1 /iscratch/i/affy/Zebrafish_consensus.fa > affy.lst
    ls -1 /iscratch/i/danRer4/trfFa/chr[0-9M]*.fa > genome.lst
    # for output:
    mkdir -p psl
    echo '#LOOP\n/cluster/bin/i386/blat -fine -minIdentity=90 -ooc=/iscratch/i/danRer4/danRer4_11.ooc $(path1) $(path2) {check out line+ psl/$(root1)_$(root2).psl}\n#ENDLOOP' > template.sub

    gensub2 genome.lst affy.lst template.sub para.spec
    para create para.spec
    para try, check, push, check .... etc.
# para time
# Completed: 271 of 271 jobs
# CPU time in finished jobs:      15331s     255.51m     4.26h    0.18d  0.000 y
# IO & Wait Time:                   737s      12.29m     0.20h    0.01d  0.000 y
# Average job time:                  59s       0.99m     0.02h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:             101s       1.68m     0.03h    0.00d
# Submission to last job:          1557s      25.95m     0.43h    0.02d

    # do pslSort and liftUp
    ssh kkstore04
    cd /cluster/data/danRer4/bed/affyZebrafish
    # Do sort, best in genome filter, and convert to chromosome coordinates
    # to create affyZebrafish.psl
    pslSort dirs raw.psl tmp psl
    # only use alignments that have at least 95% identity in aligned region.
    # try minCover as now there is less sequence in chrUn and chrNA
    # so less likely that genes are split up.
    grep '>' /cluster/bluearc/affy/Zebrafish_consensus.fa | wc -l
    # 15502
    pslReps -minAli=0.95 -nearTop=0.005 raw.psl contig.psl /dev/null
    # see how many sequences are aligned:
    awk '{print $10;}' contig.psl > contigAligned
    tail +6 contigAligned | sort | uniq -c | sort -nr > contigAligned.count
    wc -l contigAligned.count 
    # 14819 contigAligned.count 
    tail +6 contig.psl | wc -l
    # 21486
    # 96% of sequences are aligned. The sequence with the most alignments 
    # aligns 177 times, then the next is 105, then 86, 85, 69, 69, 54, 54 etc.
    # for danRer3, 14335 were aligned (92% aligned). The sequence with 
    # the most alignments aligned 96 times, then 31, 27, 22, 20, 19 times. 
    # also 854 sequences aligned for danRer4 that did not align for danRer3.
    # 370 were aligned in danRer3 but not for danRer4.
    # USED THESE pslReps PARAMETERS:
    pslReps -minCover=0.30 -minAli=0.95 -nearTop=0.005 \
            raw.psl contig2.psl /dev/null
    # see how many sequences are aligned:
    awk '{print $10;}' contig2.psl > contig2Aligned
    tail +6 contig2Aligned | sort | uniq -c | sort -nr > contig2Aligned.count
    wc -l contig2Aligned.count 
    # 14528 contig2Aligned.count
    tail +6 contig2.psl | wc -l
    # 18744
    # danRer3 has 21196 total alignments and 14335 sequences aligned. 
    # 94% of sequences are aligned.
    # 785 sequences were aligned for danRer4 using minCover but not for 
    # danRer3 after using pslReps. 592 sequences were aligned for danRer3 
    # but not for danRer4 using minCover after using pslReps.
    # the sequence with the most alignments aligns 105 times, then 85, 69,
    # 54, 50, 47, 44, 37, 26, 31, 29:
# No. of alignments Sequence Name
# 105 Zebrafish:Dr.15955.1.A1_at
# 85 Zebrafish:Dr.20178.1.A1_at
# 69 Zebrafish:Dr.885.1.S1_at
# 54 Zebrafish:Dr.15958.1.S1_at
# 50 Zebrafish:Dr.25427.1.A1_at
# 47 Zebrafish:Dr.16470.1.A1_at
# 44 Zebrafish:Dr.490.1.S1_at
# 37 Zebrafish:Dr.7806.1.A1_at
# 36 Zebrafish:Dr.19.1.A1_at
# 31 Zebrafish:Dr.2825.1.A1_at
# 29 Zebrafish:Dr.19556.1.A1_at
    # aligning with the -mask=lower option doesn't make a difference to the
    # number of alignments and sequences aligned.  
    # there are 291 extra sequences that align when minCover option is
    # not used. Only 7 of these have 22 or more alignments. 
# 86 Zebrafish:Dr.24316.1.S1_at
# 69 Zebrafish:Dr.14452.1.A1_at
# 39 Zebrafish:Dr.12372.1.S1_at
# 26 Zebrafish:Dr.18296.2.S1_a_at
# 23 Zebrafish:Dr.7519.1.A1_at
# 22 Zebrafish:Dr.8680.1.S1_at
# 22 Zebrafish:Dr.22175.1.S1_at
    # clean up 
    rm contig* 
    # use pslReps without the minCover option as it does allow quite a lot
    # more alignments and the number of total alignments/number of sequences
    # aligned is still close to that for danRer3. Using nearTop=0.001 does
    # decrease the number of alignments but also means that some good 
    # alignments are lost.  
    pslReps -minAli=0.95 -nearTop=0.005 raw.psl contig.psl /dev/null
    liftUp affyZebrafish.psl ../../jkStuff/liftAll.lft warn contig.psl
    # shorten names in psl file:
    sed -e 's/Zebrafish://' affyZebrafish.psl > affyZebrafish.psl.tmp
    mv affyZebrafish.psl.tmp affyZebrafish.psl
    pslCheck affyZebrafish.psl
    # co-ordinates are ok. psl is good.
    # load track into database
    ssh hgwdev
    cd /cluster/data/danRer4/bed/affyZebrafish
    hgLoadPsl danRer4 affyZebrafish.psl
    # Add consensus sequences for Zebrafish chip
    # Copy sequences to gbdb if they are not there already
    mkdir -p /gbdb/hgFixed/affyProbes
    ln -s \
       /projects/compbio/data/microarray/affyZebrafish/Zebrafish_consensus.fa \
      /gbdb/hgFixed/affyProbes

    hgLoadSeq -abbr=Zebrafish: danRer4 \
              /gbdb/hgFixed/affyProbes/Zebrafish_consensus.fa
    # Clean up
    rm batch.bak contig.psl raw.psl
# trackDb.ra entry and html are already there in trackDb/zebrafish/
    
###########################################################################
# CREATE ZEBRAFISH AND OTHER SPECIES LINEAGE-SPECIFIC REPEATS DIRECTORY AND 
# ADD CHROM SIZES FOR BLASTZ CLUSTER RUNS (DONE, 2006-04-24, hartera)
    # There are no lineage-specific repeats for zebrafish and other species
    # so use all repeats.
    ssh pk
    mkdir -p /san/sanvol1/scratch/danRer4/linSpecRep.notInOthers
    foreach f (/cluster/data/danRer4/*/chr*.fa.out)
     cp -p $f \
        /san/sanvol1/scratch/danRer4/linSpecRep.notInOthers/$f:t:r:r.out.spec
    end
    cp -p /cluster/data/danRer4/chrom.sizes \
          /san/sanvol1/scratch/danRer4/

###########################################################################
# BLASTZ, CHAIN, NET, MAFNET, AXTNET AND ALIGNMENT DOWNLOADS FOR
# HUMAN (hg18) (DONE, 2006-04-24 - 2006-04-25, hartera)
# LOAD BLASTZ PSLS INTO DATABASE AND CHECK FOR HUMAN CONTAMINATION
# (DONE, 2006-05-11, hartera)
    ssh pk
    # Blastz uses lineage-specific repeats. There are none for human
    # and zebrafish so use all repeats.
    # There is a lineage-specific repeats directory for zebrafish (see
    # section on CREATE ZEBRAFISH AND OTHER SPECIES LINEAGE-SPECIFIC REPEATS
    # DIRECTORY. lineage-specific repeats for hg18 already made - see
    # makeHg18.doc (BLASTZ ZEBRAFISH section).
    mkdir -p /cluster/data/danRer4/bed/blastz.hg18.2006-04-24
    cd /cluster/data/danRer4/bed
    ln -s blastz.hg18.2006-04-24 blastz.hg18
    cd /cluster/data/danRer4/bed/blastz.hg18.2006-04-24
    # only 5% of the danRer4 genome is now in the random unordered chroms
    # so not running only scaffolds for these chroms - run as virtual chroms
    # and use same parameters as for danRer2.
    cat << 'EOF' > DEF
# danRer4 zebrafish target, human hg18 query
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin:/parasol/bin

BLASTZ=blastz.v7.x86_64
BLASTZ_ABRIDGE_REPEATS=1

# use parameters suggested for human-fish evolutionary distance
# recommended in doBlastzChainNet.pl help
# (previously used for  hg16-fr1, danRer1-mm5)
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_Q=/san/sanvol1/scratch/blastz/HoxD55.q

# TARGET: zebrafish (danRer4)
# Use all chroms, including both randoms (chrNA_random and chrUn_random)
SEQ1_DIR=/san/sanvol1/scratch/danRer4/nib
SEQ1_SMSK=/san/sanvol1/scratch/danRer4/linSpecRep.notInOthers
SEQ1_LEN=/san/sanvol1/scratch/danRer4/chrom.sizes
SEQ1_CHUNK=50000000
SEQ1_LAP=10000

# QUERY: human (hg18) - single chunk big enough to run each chrom by itself
# Use all chroms, including all randoms 
SEQ2_DIR=/san/sanvol1/scratch/hg18/nib
SEQ2_LEN=/san/sanvol1/scratch/hg18/hg18Chroms.len
SEQ2_SMSK=/san/sanvol1/scratch/hg18/linSpecRep.notInOthers
SEQ2_CHUNK=300000000
SEQ2_LAP=0

BASE=/cluster/data/danRer4/bed/blastz.hg18.2006-04-24
TMPDIR=/scratch/tmp
'EOF'
   # << happy emacs
   chmod +x DEF 
   nohup nice /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
        -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
        `pwd`/DEF >& doBlastz.log &
   # Start: Mon Apr 24 19:20 Stop: Tues Apr 25 05:42
   # Did not finish:
   # netChains: looks like previous stage was not successful     
   # (can't find [danRer4.hg18.]all.chain[.gz]).
   # This file is there so run again. Continue chainMerge step so remove
   # all.chain file and chain directory.
   # NOTE: can leave these files and continue from the net step and it 
   # will work.
   cd /cluster/data/danRer4/bed/blastz.hg18.2006-04-24
   rm ./axtChain/*.all.chain.gz
   rm -r ./axtChain/chain
   nohup nice /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
        -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
        -continue chainMerge `pwd`/DEF >& chainMerge.log &
   # Took about 10 minutes.
   # Check results with featureBits and compare to those 
   # for danRer3 and danRer2:
   ssh hgwdev
   featureBits danRer4 chainHg18Link
   # 64196991 bases of 1626093931 (3.948%) in intersection
   featureBits danRer3 chainHg18Link
   # 69559338 bases of 1630323462 (4.267%) in intersection
   featureBits danRer2 chainHg17Link
   # 70046373 bases of 1560497282 (4.489%) in intersection

   # After Genbank tracks are loaded, (hartera, 2006-04-27)
   featureBits -chrom=chr1 danRer4 refGene:cds chainHg18Link -enrichment
   # refGene:cds 0.732%, chainHg18Link 4.140%, both 0.558%, cover 76.19%, 
   # enrich 18.40x
   featureBits -chrom=chr1 danRer3 refGene:cds chainHg18Link -enrichment
   # refGene:cds 0.769%, chainHg18Link 4.124%, both 0.604%, cover 78.49%, 
   # enrich 19.03x
   featureBits -chrom=chr1 danRer4 refGene:cds netHg18 -enrichment
   # refGene:cds 0.732%, netHg18 31.154%, both 0.624%, cover 85.21%, 
   # enrich 2.73x
   featureBits -chrom=chr1 danRer3 refGene:cds netHg18 -enrichment
   # refGene:cds 0.774%, netHg18 35.434%, both 0.679%, cover 87.73%, 
   # enrich 2.48x
   # Similar coverage and enrichment as for hg18 chains and net on danRer3.
   # do the swap for Blastz chains over to human (hg18) and create net,
   # axtNet, mafNet, liftOver and Downloads. see also makeHg18.doc for
   # featureBits on these alignments.
   ssh pk
   cd /cluster/data/danRer4/bed/blastz.hg18.2006-04-24
   nice /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
        -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
        -swap `pwd`/DEF >& doSwap.log &
   # Took about 15 minutes.
   # Load Blastz results into database (DONE, 2006-05-11, hartera)
   ssh kkstore04
   cd /cluster/data/danRer4/bed/blastz.hg18/pslParts
   # cat together Blastz for each chrom
   mkdir pslChrom
   foreach c (`cat /cluster/data/danRer4/chrom.lst`)
      echo "Processing $c ..."
      foreach p (chr${c}.nib*)
         zcat $p >> ./pslChrom/chr${c}_blastzHg18.psl
      end
   end
   # load Blastz psls into the database
   ssh hgwdev
   cd /cluster/data/danRer4/bed/blastz.hg18/pslParts/pslChrom
   foreach f (*.psl)
      /cluster/bin/i386/hgLoadPsl danRer4 $f
      echo "$f Done"
   end
   # Then determine how much sequence has 100% identity to human with a 
   # stretch of at least 300 bp. Human contamination was also found in
   # danRer1 and a user reported it more recently.

   foreach c (`cat /cluster/data/danRer4/chrom.lst`)
      echo "chr$c" >> humanContamination.txt
      hgsql -e "select count(*) from chr${c}_blastzHg18 where matches >= 300 and        misMatches = 0;" danRer4 >> humanContamination.txt
   end
   # There are 4 on chr11 that fit this criteria (same if decrease to regions
   # of >= 200 bp with 100% ID).
   hgsql -e \
   'select * from chr11_blastzHg18 where matches >= 300 and mismatches = 0;' \
   danRer4 > chr11HumanSeq
   # only 2 of these also have no query inserts and 1 of the others only has
   # a 1 base insert: regions are of size 303, 310 and 367 bp. The region of 
   # 330 bp has a 45 bp insert on the query side - see below
#bin	matches	misMatches	repMatches	nCount	qNumInsert	qBaseInsert	tNumInsert	tBaseInsert	strand	qName	qSize	qStart	qEnd	tName	tSize	tStart	tEnd	blockCount	blockSizes	qStarts	tStarts
#588	303	0	0	0	0	0	0	0	-	chr4	191273063	69879746	69880049	chr11	52342180	502145	502448	1	303,	121393014,	502145,
#588	330	0	0	0	1	45	0	0	-	chr4	191273063	69879319	69879694	chr11	52342180	502545	502875	2	1,329,	121393369,121393415,	502545,502546,
#588	310	0	0	0	0	0	0	0	-	chr4	191273063	69878956	69879266	chr11	52342180	502928	503238	1	310,	121393797,	502928,
#588	667	0	0	0	1	1	0	0	-	chr4	191273063	69878268	69878936	chr11	52342180	503258	503925	2	453,214,	121394127,121394581,	503258,503711,

###########################################################################
# BLASTZ/CHAIN/NET PREP (DONE 4/25/06 angie)
    ssh kkstore04
    cd /cluster/data/danRer4
    cp -p danRer4.2bit /san/sanvol1/scratch/danRer4/

    # Create a 2bit file for danRer4 with all chroms (1-25 and M) and the
    # scaffolds for NA and Un:
    awk '$1 == $6 {print $1;}' Zv6.scaffolds.agp \
    | faSomeRecords Zv6_scaffolds.fa stdin stdout \
    | faToTwoBit [1-9]/chr*.fa [12][0-9]/chr*.fa M/chrM.fa stdin \
       /san/sanvol1/scratch/danRer4/danRer4ChrUnNAScafs.2bit
    twoBitInfo /san/sanvol1/scratch/danRer4/danRer4ChrUnNAScafs.2bit \
      /san/sanvol1/scratch/danRer4/chromsUnNAScafs.sizes

    # Make a lift file for scaffolds --> {chrUn, chrNA}:
    mkdir /cluster/data/danRer4/liftSupertoChrom
    cd /cluster/data/danRer4/liftSupertoChrom
    /cluster/bin/scripts/agpToLift \
      < ../NA_random/agps/chrNA_random.scaffolds.agp \
      > chrNA_random.lft
    /cluster/bin/scripts/agpToLift \
      < ../Un_random/agps/chrUn_random.scaffolds.agp \
      > chrUn_random.lft
    cat chr*.lft > liftNAandUnScaffoldsToChrom.lft
    cp -p liftNAandUnScaffoldsToChrom.lft /san/sanvol1/scratch/danRer4/

    # Distribute on /iscratch/i too (danRer4.2bit is already there):
    ssh kkr1u00
    cd /iscratch/i/danRer4
    cp -p /san/sanvol1/scratch/danRer4/danRer4ChrUnNAScafs.2bit .
    twoBitInfo danRer4ChrUnNAScafs.2bit chromsUnNAScafs.sizes
    cp -p \
      /cluster/data/danRer4/liftSupertoChrom/liftNAandUnScaffoldsToChrom.lft .
    iSync


###########################################################################
# BLASTZ/CHAIN/NET XENTRO2 (DONE 4/26/06 angie)
    ssh kkstore04
    mkdir /cluster/data/danRer4/bed/blastz.xenTro2.2006-04-25
    cd /cluster/data/danRer4/bed/blastz.xenTro2.2006-04-25
    cat << '_EOF_' > DEF
# zebrafish vs. frog
BLASTZ=/cluster/bin/penn/i386/blastz

# Use same params as used for danRer1-xenTro1 (see makeXenTro1.doc)
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q

# TARGET: Zebrafish danRer4
SEQ1_DIR=/iscratch/i/danRer4/danRer4.2bit
SEQ1_CTGDIR=/iscratch/i/danRer4/danRer4ChrUnNAScafs.2bit
SEQ1_LIFT=/iscratch/i/danRer4/liftNAandUnScaffoldsToChrom.lft
SEQ1_LEN=/cluster/data/danRer4/chrom.sizes
SEQ1_CTGLEN=/iscratch/i/danRer4/chromsUnNAScafs.sizes
SEQ1_CHUNK=50000000
SEQ1_LAP=10000
SEQ1_LIMIT=100

# QUERY: Frog xenTro2 - single chunk big enough to run two of the
#               largest scaffolds in one job
SEQ2_DIR=/scratch/hg/xenTro2/xenTro2.2bit
SEQ2_LEN=/cluster/bluearc/xenTro2/chrom.sizes
SEQ2_CHUNK=20000000
SEQ2_LAP=0
SEQ2_LIMIT=100

BASE=/cluster/data/danRer4/bed/blastz.xenTro2.2006-04-25
'_EOF_'
    # << emacs
    # kkstore04 can't see /iscratch so use an iServer as fileServer:
    doBlastzChainNet.pl -blastzOutRoot=/cluster/bluearc/danRer4XenTro2 \
      -bigClusterHub=kk -fileServer=kkr8u00 -workhorse=kkr8u00 \
      -chainMinScore=5000 -chainLinearGap=loose DEF \
      >& do.log & tail -f do.log
    ln -s blastz.xenTro2.2006-04-25 /cluster/data/danRer4/bed/blastz.xenTro2

###########################################################################
# CREATE LIFT FILES FOR RANDOM CHROMOSOMES' SCAFFOLDS
# (DONE, 2006-04-25, hartera)
   # scaffolds lift files created by scaffoldFaToAgp when agp files created 
   # for chrNA_random and chrUn_random. remove last line as this is an extra
   # gap line that was removed from the agp.

   ssh kkstore01
   cd /cluster/data/danRer4
   foreach c (NA_random Un_random)
     mkdir -p /cluster/data/danRer4/$c/tmp
   end
   # NA_random doesn't have .lft and .gap files from scaffoldFaToAgp so
   # recreate. It had no tmp dir with the NA_random.scaffolds.agp.
   awk '{if ($1 ~ /Zv6_NA/) print;}' Zv6.scaffolds.agp \
       > ./NA_random/tmp/NA_random.scaffolds.agp
   # change the first field to "chrNA_random" then can use agpToFa to process
   perl -pi.bak -e 's/Zv6_NA[0-9]+/chrNA_random/' ./NA_random/tmp/*.agp
   wc -l ./NA_random/tmp/NA_random.scaffolds.agp
   # 2898 ./NA_random/tmp/NA_random.scaffolds.agp

   cd /cluster/data/danRer4
   foreach c (NA_random)
     awk '{print $6;}' $c/tmp/$c.scaffolds.agp > $c/tmp/chr$c.scaffolds.lst
         $HOME/bin/i386/faSomeRecords /cluster/data/danRer4/Zv6_scaffolds.fa \
         $c/tmp/chr$c.scaffolds.lst $c/tmp/chr$c.fa
   end
   cd /cluster/data/danRer4/NA_random/tmp
   scaffoldFaToAgp -scaffoldGapSize=50000 chrNA_random.fa
   # change chrUn to chrNA_random for NA_random, change chrUn to chrUn_random
   # forUn_random. Change D to W for NA_random and Un_random..
   sed -e 's/chrUn/chrNA_random/' chrNA_random.agp \
       | sed -e 's/D/W/' > chrNA_random.scaffolds.agp
   mv chrNA_random.fa chrNA_random.scaffolds.fa
   # also move the Un_random .lft and .gap files to Un_random/tmp
   mv ./Un_random/chrUn_random.lft ./Un_random/tmp/chrUn_random.lft
   mv ./Un_random/chrUn_random.gap ./Un_random/tmp/chrUn_random.gap
   # for chrNA_random and chrUn_random: remove last line as this is an extra
   # gap line that was removed from the chrN_random.agp. Add these 
   # scaffold lift files to liftAll.lft. Also need to change the last 
   # field so that the correct total number of bases is being shown in the
   # last column.
   cd /cluster/data/danRer4
   foreach c (NA_random Un_random)
     head -n -1 $c/tmp/chr${c}.lft > $c/tmp/chr${c}.scaffolds.lft
     perl -pi.bak -e "s/chrUn/chr${c}/" $c/tmp/chr${c}.scaffolds.lft
     if ($c == "NA_random") then
        perl -pi.bak -e 's/208064280/208014280/' \
             $c/tmp/chrNA_random.scaffolds.lft
     else
        perl -pi.bak -e 's/19379532/19329532/' \
             $c/tmp/chrUn_random.scaffolds.lft
     endif
     cat $c/tmp/chr${c}.scaffolds.lft >> ./jkStuff/liftAll.lft
     rm $c/tmp/chr${c}.lft $c/tmp/chr${c}.gap *.bak
   end

###########################################################################
# AUTO UPDATE GENBANK MRNA AND EST AND MGC GENES RUN 
# (DONE, 2006-04-25 - 2006-04-26, hartera)
   ssh hgwdev
   cd ~kent/src/hg/makeDb/genbank
   cvs update -d -P etc
   # edit etc/genbank.conf to add danRer4 and commit this to CVS.
# danRer4 (zebrafish)
# Lift file partitions unplaced sequence pseudo-chroms
danRer4.serverGenome = /cluster/data/danRer4/danRer4.2bit
danRer4.clusterGenome = /iscratch/i/danRer4/danRer4.2bit
danRer4.ooc = /iscratch/i/danRer4/danRer4_11.ooc
danRer4.align.unplacedChroms = chrNA_random chrUn_random
danRer4.lift = /cluster/data/danRer4/jkStuff/liftAll.lft
danRer4.refseq.mrna.native.pslCDnaFilter  = ${ordered.refseq.mrna.native.pslCDnaFilter}
danRer4.refseq.mrna.xeno.pslCDnaFilter    = ${ordered.refseq.mrna.xeno.pslCDnaFilter}
danRer4.genbank.mrna.native.pslCDnaFilter = ${ordered.genbank.mrna.native.pslCDnaFilter}
danRer4.genbank.mrna.xeno.pslCDnaFilter   = ${ordered.genbank.mrna.xeno.pslCDnaFilter}
danRer4.genbank.est.native.pslCDnaFilter  = ${ordered.genbank.est.native.pslCDnaFilter}
danRer4.downloadDir = danRer4
danRer4.mgcTables.default = full
danRer4.mgcTables.mgc = all
   # end of section added to etc/genbank.conf
   cvs commit -m "Added danRer4." etc/genbank.conf
   # update /cluster/data/genbank/
   make etc-update

   # ~/kent/src/hg/makeDb/genbank/src/lib/gbGenome.c already contains
   # danRer genome information

   ssh kkstore02
   cd /cluster/data/genbank
   nice bin/gbAlignStep -initial danRer4 &
   # Start: Tues Apr 25 12:53 Finish: Wed Apr 26 08:38
   # logFile: var/build/logs/2006.04.25-12:53:39.danRer4.initalign.log
   # check log file
   tail -f var/build/logs/2006.04.25-12:53:39.danRer4.initalign.log
   # check it has finished (last line in log file):
   # kkstore02 2006.04.26-08:38:36 danRer4.initalign: finish
   # load database when finished
   ssh hgwdev
   cd /cluster/data/genbank
   nice ./bin/gbDbLoadStep -drop -initialLoad danRer4 &
   # logFile: var/dbload/hgwdev/logs/2006.04.26-15:45:19.dbload.log
   # check it is finished: hgwdev 2006.04.26-17:48:07 dbload: finish
   # Took about 2 hours.

###########################################################################
# SPLIT UP ZEBRAFISH MASKED SEQUENCE FROM chrUn AND chrNA INTO SCAFFOLDS
# ADD SOFT-MASKED SCAFFOLDS TO ISERVERS AND THE SAN FOR CLUSTER RUNS
# (DONE, 2006-04-27, hartera)

    ssh kkstore01
    cd /cluster/data/danRer4
    # for chrNA_random and chrUn_random, get soft-masked sequence.
    foreach c (NA_random Un_random)
      cd $c
      mkdir scaffoldsSoftMask
      awk 'BEGIN {FS="\t"}{if ($5 != "N") \
       print "faFrag -mixed chr'${c}'.fa",$2-1, $3, $6".fa";}' chr${c}.agp \
       >> ./scaffoldsSoftMask/faFragSoftMask.csh
      cd ..
    end
    # change permissions run scripts to get sequences
    foreach d (NA_random Un_random)
       chmod +x $d/scaffoldsSoftMask/faFragSoftMask.csh
    end
    # wrapper shell script to run script to get the soft-masked scaffolds
    cat << '_EOF_' > jkStuff/getMaskedScaffolds.csh
#!/bin/csh
foreach c (NA_random Un_random)
   set dir=/cluster/data/danRer4
   echo "Processing $c"
   cd $dir/$c/scaffoldsSoftMask
   cp ../chr${c}.fa .
   echo "Getting soft-masked sequences ..."
   nice faFragSoftMask.csh >& faFrag.log
end
'_EOF_'
   chmod +x jkStuff/getMaskedScaffolds.csh
   nice ./jkStuff/getMaskedScaffolds.csh &
   # Took about 2.5 hours.
   # check a few sequences that they are correct
   # add name of scaffold to sequence fasta and cat together
   foreach c (NA_random Un_random)
      set dir = /cluster/data/danRer4
      cd $dir/$c/scaffoldsSoftMask
      foreach f (Zv*)
        set g=$f:r
        set sc=scaffold${c}.fa
        perl -pi.bak -e "s/>chr[0-9A-Za-z\-\:_]+/>$g/" $f
        cat $f >> $sc
        rm *.bak
      end
      cp scaffold* $dir/$c/
   end
   grep '>' NA_random/scaffoldNA_random.fa | wc -l
   # 2898
   grep '>' Un_random/scaffoldUn_random.fa | wc -l
   # 68
   # check sizes of final FASTA file with all sequences. check a few
   # sequence files to see that they are correct - ok 
   cd /cluster/data/danRer4
cat << '_EOF_' > ./jkStuff/checkFastaSizes.csh
#!/bin/csh -fe

set scafName=$1
set agpLen=$2

set pref=`echo $scafName | cut -c1-2`
if ($pref == "Zv") then
  set g=/cluster/data/danRer4/*/scaffoldsSoftMask/${scafName}.fa
  set h=$g:t
  echo "Getting size of $h"
  set faLen = `faSize $g | awk '{print $1;}'`

  if ($agpLen == $faLen) then
     echo "   OK: apg length = $h length = $faLen"
  else
     echo "ERROR:  length = $agpLen, but $h length = $faLen"
  endif
endif
'_EOF_'
   # << happy emacs
   chmod +x ./jkStuff/checkFastaSizes.csh
   # use bash as doing a cat in C shell seems to split the line up by space
   bash
   for c in NA_random Un_random
   do
     echo "Processing $c scaffolds ..."; 
     cat $c/chr${c}.agp  | while read line;
     do
     scaf=`echo $line | cut -d " " -f6`;
     size=`echo $line | cut -d " " -f8`; 
     nice ./jkStuff/checkFastaSizes.csh $scaf $size >> checkFastaSizes.log;
     done
   done 
   exit # back to C shell
   grep "ERROR:" checkFastaSizes.log | wc -l

   # No errors so all are the OK so FASTA files are the expected size
   # Add soft-masked scaffolds to the Iservers and the san for cluster runs 
   
   ssh kkr1u00
   cd /cluster/data/danRer4
   mkdir /iscratch/i/danRer4/scaffoldsSoftMask
   foreach c (NA_random Un_random)
      foreach f (/cluster/data/danRer4/$c/scaffoldsSoftMask/Zv*.fa)
         cp -p $f /iscratch/i/danRer4/scaffoldsSoftMask
      end
      cp -p /cluster/data/danRer4/$c/scaffold${c}.fa /iscratch/i/danRer4
   end
   ls /iscratch/i/danRer4/scaffoldsSoftMask/ | wc
   # 2966
   # all files are there   
   # rsync to cluster machines
   foreach R (2 3 4 5 6 7 8)
      rsync -a --progress /iscratch/i/danRer4/ kkr${R}u00:/iscratch/i/danRer4/
   end

   ssh pk
   mkdir -p /san/sanvol1/scratch/danRer4/scaffoldsSoftMask
   foreach c (NA_random Un_random)
      foreach f (/cluster/data/danRer4/$c/scaffoldsSoftMask/Zv*.fa)
        rsync -a --progress $f /san/sanvol1/scratch/danRer4/scaffoldsSoftMask/
      end
      rsync -a --progress /cluster/data/danRer4/${c}/scaffold${c}.fa \
            /san/sanvol1/scratch/danRer4/
   end
   foreach f (/san/sanvol1/scratch/danRer4/scaffoldsSoftMask/*.fa)
     echo $f >> files.log
   end
   wc -l files.log 
   # 2966 files.log
   rm files.log
   # All files have transferred.

###########################################################################
## SWAP MM8 blastz result (DONE - 2006-04-28 - Hiram)
# ADD SYMBOLIC LINK TO SWAP DIR (DONE, 2006-05-04, hartera)
# RE-MAKE MM8 CHAINS AND NET SWAP WITH DANRER4 RANDOM CHROMS 
# (DONE, 2006-05-24, hartera) ADDED LINK TO SWAP DIR (2006-05-27, hartera)
    ssh pk
    cd /cluster/data/mm8/bed/blastzDanRer4.2006-05-22
    # blastz parameters used in blastz alignment of danRer4 on mm8:
    # BLASTZ_ABRIDGE_REPEATS=1
    # BLASTZ_H=2000
    # BLASTZ_Y=3400
    # BLASTZ_L=6000
    # BLASTZ_K=2200
    # BLASTZ_M=50
    # BLASTZ_Q=/cluster/data/blastz/HoxD55.q
    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
	-swap `pwd`/DEF > swap.out 2>&1 &
    
    ssh hgwdev
    cd /cluster/data/mm8/bed/blastzDanRer4.2006-05-22
    time nice -n +19 featureBits danRer4 chainMm8Link \
	> fb.danRer4.chainDanRer4Link 2>&1 &
    cat fb.danRer4.chainDanRer4Link
    # 60721886 bases of 1626093931 (3.734%) in intersection
    # Add symbolic link to new swap directory (2006-05-27, hartera)
    ssh kkstore04
    cd /cluster/data/danRer4/bed
    ln -s blastz.mm8.swap blastz.mm8
   
###########################################################################
# MONDOM4 BLASTZ TESTS USING LINEAGE-SPECIFIC REPEATS OR DYNAMIC MASKING
# AND SWAP (DONE, 2006-04-28, hartera)
   # used no lineage specific-repeats and M=50 for dynamic masking
   featureBits danRer4 chainMonDom4
   # 541863023 bases of 1626093931 (33.323%) in intersection
   featureBits danRer4 chainMonDom4NoDyMsk
   # 534445657 bases of 1626093931 (32.867%) in intersection
   featureBits monDom4 chainDanRer4
   # 856404995 bases of 3501643220 (24.457%) in intersection
   featureBits monDom4 chainDanRer4NoDyMsk
   # 812142533 bases of 3501643220 (23.193%) in intersection
   featureBits -chrom=chr1 danRer4 refGene:cds chainMonDom4Link -enrichment
   # refGene:cds 0.732%, chainMonDom4Link 5.573%, both 0.550%, cover 75.20%, 
   # enrich 13.49x
   featureBits -chrom=chr1 danRer4 refGene:cds chainMonDom4NoDyMskLink 
   -enrichment
   # refGene:cds 0.732%, chainMonDom4NoDyMskLink 4.083%, both 0.550%, 
   # cover 75.15%, enrich 18.40x
   featureBits -chrom=chr1 monDom4 refGene:cds chainDanRer4Link -enrichment
   # refGene:cds 0.001%, chainDanRer4Link 2.448%, both 0.000%, 
   # cover 55.63%, enrich 22.73x 
   featureBits -chrom=chr1 monDom4 refGene:cds chainDanRer4NoDyMskLink 
   -enrichment
   # refGene:cds 0.001%, chainDanRer4NoDyMskLink 1.807%, both 0.000%, 
   # cover 43.85%, enrich 24.27x
   # There are only 36 RefSeq genes for monDom4 so results are misleading.
   # Try mrna and xenoRefGene table.
   # for mrna tables, not much difference:
   featureBits -chrom=chr1 monDom4 mrna chainDanRer4Link -enrichment
   # mrna 0.004%, chainDanRer4Link 2.448%, both 0.002%, cover 54.59%, 
   # enrich 22.30x
   featureBits -chrom=chr1 monDom4 mrna chainDanRer4NoDyMskLink -enrichment
   # mrna 0.004%, chainDanRer4NoDyMskLink 1.807%, both 0.002%, 
   # cover 52.67%, enrich 29.15x
   featureBits -chrom=chr1 monDom4 xenoRefGene:cds chainDanRer4Link -enrichment
   # xenoRefGene:cds 0.820%, chainDanRer4Link 2.448%, both 0.655%, 
   # cover 79.88%, enrich 32.63x
   featureBits -chrom=chr1 monDom4 xenoRefGene:cds chainDanRer4NoDyMskLink 
   -enrichment
   # xenoRefGene:cds 0.820%, chainDanRer4NoDyMskLink 1.807%, both 0.661%, 
   # cover 80.63%, enrich 44.63x

   # For the nets:
   featureBits -chrom=chr1 danRer4 refGene:cds netMonDom4 -enrichment
   # refGene:cds 0.732%, netMonDom4 31.056%, both 0.612%, 
   # cover 83.58%, enrich 2.69x
   featureBits -chrom=chr1 danRer4 refGene:cds netMonDom4NoDyMsk -enrichment
   # refGene:cds 0.732%, netMonDom4NoDyMsk 31.002%, both 0.617%, 
   # cover 84.31%, enrich 2.72x
   featureBits -chrom=chr1 monDom4 refGene:cds netDanRer4 -enrichment
   # refGene:cds 0.001%, netDanRer4 25.224%, both 0.000%, 
   # cover 66.95%, enrich 2.65x
   featureBits -chrom=chr1 monDom4 refGene:cds netDanRer4NoDyMsk -enrichment
   # refGene:cds 0.001%, netDanRer4NoDyMsk 24.539%, both 0.000%, 
   # cover 49.19%, enrich 2.00x
   # rows in tables for chr1
   # Assembly  Table 		Number of rows
   # danRer4   chainMonDom4          	36931
   # danRer4   chainMonDom4Link        	426659 
   # danRer4   chainMonDom4NoDyMsk    	34363
   # danRer4   chainMonDom4NoDyMskLink	361572
   # monDom4   chainDanRer4            	170759
   # monDom4   chainDanRer4Link	        2552995
   # monDom4   chainDanRer4NoDyMsk	139797	
   # monDom4   chainDanRer4NoDyMskLink	1806858
   # all chroms:
   # danRer4   netMonDom4	        399531
   # danRer4   netMonDom4NoDyMsk	346482
   # monDom4   netDanRer4		395881
   # monDom4   netDanRer4NoDyMsk	321288

   # Use lineage-specific repeats and no dynamic masking, seem to get 
   # better enrichment and coverage compared to gene CDS regions and also 
   # there are less chains being produced.

###########################################################################
# BLASTZ, CHAIN, NET, MAFNET, AXTNET AND ALIGNMENT DOWNLOADS FOR
# OPOSSUM (monDom4) (DONE, 2006-04-28 - 2006-04-29, hartera)
    ssh hgwdev
    # Remove all test chain and net tables and start again
    foreach c (`cat chrom.lst`)
       hgsql -e "drop table chr${c}_chainMonDom4;" danRer4
       hgsql -e "drop table chr${c}_chainMonDom4Link;" danRer4
       hgsql -e "drop table chr${c}_chainMonDom4NoDyMsk;" danRer4
       hgsql -e "drop table chr${c}_chainMonDom4NoDyMskLink;" danRer4
    end
    hgsql -e "drop table netMonDom4;" danRer4
    hgsql -e "drop table netMonDom4NoDyMsk;" danRer4
    # remove downloads
    rm -r /usr/local/apache/htdocs/goldenPath/danRer4/vsMonDom4
    rm \
    /usr/local/apache/htdocs/goldenPath/danRer4/liftOver/danRer4ToMonDom4.over.chain.gz
    rm /cluster/data/danRer4/bed/liftOver/danRer4ToMonDom4.over.chain.gz
    # remove old Blastz swap
    rm -r /cluster/data/danRer4/bed/blastz.monDom4.swap
    # remove link to old blastz directory
    rm -r /cluster/data/danRer4/bed/blastz.monDom4

    # see makeMonDom4.doc for removal of test tables and download files
    # and swap directory on monDom4.
    
    ssh pk
    # Blastz uses lineage-specific repeats. There are none for human
    # and zebrafish so use all repeats.
    # There is a lineage-specific repeats directory for zebrafish (see
    # section on CREATE ZEBRAFISH AND OTHER SPECIES LINEAGE-SPECIFIC REPEATS
    # DIRECTORY. lineage-specific repeats for monDom4 made and also nibs - see
    # makeMonDom4.doc. Need nib files when running Blastz with
    # lineage-specific repeats.
    
    mkdir -p /cluster/data/danRer4/bed/blastz.monDom4.2006-04-28
    cd /cluster/data/danRer4/bed
    ln -s blastz.monDom4.2006-04-28 blastz.monDom4
    cd /cluster/data/danRer4/bed/blastz.monDom4.2006-04-28
    # only 5% of the danRer4 genome is now in the random unordered chroms
    # so not running only scaffolds for these chroms - run as virtual chroms
    # and use same parameters as for danRer2 but use all repeats as 
    # lineage-specific as monDom4 is now mapped to chroms.
    cat << 'EOF' > DEF
# danRer4 zebrafish target, opossum monDom4 query
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin:/parasol/bin

BLASTZ=blastz.v7.x86_64
BLASTZ_ABRIDGE_REPEATS=1

# use parameters suggested for human-fish evolutionary distance
# recommended in doBlastzChainNet.pl help.
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_Q=/san/sanvol1/scratch/blastz/HoxD55.q

# TARGET: zebrafish (danRer4)
# Use all chroms, including both randoms (chrNA_random and chrUn_random)
SEQ1_DIR=/san/sanvol1/scratch/danRer4/nib
SEQ1_SMSK=/san/sanvol1/scratch/danRer4/linSpecRep.notInOthers
SEQ1_LEN=/san/sanvol1/scratch/danRer4/chrom.sizes
SEQ1_CHUNK=100000000
SEQ1_LAP=10000

# QUERY: opossum (monDom4) 
SEQ2_DIR=/san/sanvol1/scratch/monDom4/nib
SEQ2_LEN=/san/sanvol1/scratch/monDom4/chrom.sizes
SEQ2_SMSK=/san/sanvol1/scratch/monDom4/linSpecRep.notInOthers
SEQ2_CHUNK=50000000
SEQ2_LIMIT=100
SEQ2_LAP=0

BASE=/cluster/data/danRer4/bed/blastz.monDom4.2006-04-28
TMPDIR=/scratch/tmp
'EOF'
   # << happy emacs
   chmod +x DEF 
   nohup nice /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
        -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
        `pwd`/DEF >& doBlastz.log &
   # Start: Fri Apr 28 13:27 Finish: Apr 29 01:28
   # Stopped after making and merging chains:
   # netChains: looks like previous stage was not successful 
   # (can't find [danRer4.monDom4.]all.chain[.gz]). 
   # Start again with net step and continue:

   cd /cluster/data/danRer4/bed/blastz.monDom4.2006-04-28
   nohup nice /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
        -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
        -continue net `pwd`/DEF >& net.log &
   # Took about 15 minutes to finish.
   # Do swap to get danRer4 alignments on monDom4:
   # see also makeMonDom4.doc  
   cd /cluster/data/danRer4/bed/blastz.monDom4.2006-04-28
   nice /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
        -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
        -swap `pwd`/DEF >& doSwap.log &
   # Took about 15 minutes.

###########################################################################
# BLASTZ FOR FUGU (fr1) (DONE, 2006-04-28 - 2006-04-29, hartera)
# CREATE CHAIN AND NET TRACKS, AXTNET, MAFNET AND ALIGNMENT DOWNLOADS
    # No lineage-specific repeats for this species pair. fr1 is in scaffolds
    # so not so easy to use repeats with this run anyway. There is a 2bit 
    # file of scaffolds on the Iservers. 
    # Run this with dynamic masking instead. 
    # copy masked fr1 scaffolds 2 bit file to the san - see makeFr1.doc    
    # size of scaffolds FASTA file:
    ssh kkr1u00
    faSize /panasas/store/fr1/scaffolds/scaffoldMaskedUnFr1.fa
    # 329140338 bases
    ssh pk
    mkdir /cluster/data/danRer4/bed/blastz.fr1.2006-04-28
    cd /cluster/data/danRer4/bed
    ln -s blastz.fr1.2006-04-28 blastz.fr1
    cd /cluster/data/danRer4/bed/blastz.fr1.2006-04-28
# use parameters for fr1 in makeDanRer2.doc. Using scaffolds makes this run
# slower so it is best to have the scaffolds in the query. Use HoxD55.q 
# matrix as Fugu is quite distant from zebrafish. Blastz uses 
# lineage-specfic repeats but there are none for these two species.
# Use soft-masked scaffolds and dynamic masking.
cat << '_EOF_' > DEF
# zebrafish (danRer4) vs. Fugu (fr1)
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin:/parasol/bin

BLASTZ=blastz.v7.x86_64
BLASTZ_ABRIDGE_REPEATS=0

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
BLASTZ_M=50
BLASTZ_Q=/cluster/data/blastz/HoxD55.q

# TARGET - zebrafish (danRer4)
SEQ1_DIR=/san/sanvol1/scratch/danRer4/danRer4.2bit
SEQ1_LEN=/san/sanvol1/scratch/danRer4/chrom.sizes
# 0.5 Mb chunk for target with 5 kb overlap
SEQ1_LIMIT=30
SEQ1_CHUNK=500000
SEQ1_LAP=5000

# QUERY - Fugu (fr1)
SEQ2_DIR=/san/sanvol1/scratch/fr1/fr1.2bit
# soft-masked scaffolds in 2bit format
SEQ2_CTGDIR=/san/sanvol1/scratch/fr1/UnScaffolds/fr1UnScaffolds.2bit
SEQ2_LIFT=/san/sanvol1/scratch/fr1/UnScaffolds/ordered.lft
SEQ2_LEN=/san/sanvol1/scratch/fr1/chrom.sizes
SEQ2_CTGLEN=/san/sanvol1/scratch/fr1/UnScaffolds/scaffolds.sizes
# large enough chunk to do whole genome at once
SEQ2_CHUNK=500000000
SEQ2_LAP=0

BASE=/cluster/data/danRer4/bed/blastz.fr1.2006-04-28
TMPDIR=/scratch/tmp
'_EOF_'
   # << this line keeps emacs coloring happy
   chmod +x DEF
   nohup nice /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
        -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
        `pwd`/DEF >& doBlastz.log &

   # Start: Fri Apr 28 18:54 Finish: Apr 29 06:35
   # Stopped after making and merging chains:
   # netChains: looks like previous stage was not successful 
   # (can't find [danRer4.fr1.]all.chain[.gz]). 
   # Start again with net step and continue:

   cd /cluster/data/danRer4/bed/blastz.fr1.2006-04-28
   nohup nice /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
        -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
        -continue net `pwd`/DEF >& net.log &
   # Took about an hour to finish.
   # check coverage:
   featureBits danRer4 chainFr1Link
   # 139280554 bases of 1626093931 (8.565%) in intersection
   featureBits danRer3 chainFr1Link
   # 137698495 bases of 1630323462 (8.446%) in intersection

   featureBits -chrom=chr1 danRer4 refGene:cds chainFr1Link -enrichment
   # refGene:cds 0.732%, chainFr1Link 8.464%, both 0.660%, 
   # cover 90.18%, enrich 10.66x
   featureBits -chrom=chr1 danRer3 refGene:cds chainFr1Link -enrichment
   # refGene:cds 0.774%, chainFr1Link 8.364%, both 0.713%, 
   # cover 92.09%, enrich 11.01x
   featureBits -chrom=chr1 danRer4 refGene:cds netFr1 -enrichment
   # refGene:cds 0.732%, netFr1 52.712%, both 0.710%, 
   # cover 96.97%, enrich 1.84x
   featureBits -chrom=chr1 danRer3 refGene:cds netFr1 -enrichment
   # refGene:cds 0.774%, netFr1 58.353%, both 0.759%, 
   # cover 97.95%, enrich 1.68x
   # Do the Blastz swap to get danRer4 alignments on fr1
   # see also makeFr1.doc for featureBits on these alignments.
   ssh pk
   cd /cluster/data/danRer4/bed/blastz.fr1.2006-04-28
   nice /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
        -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
        -swap `pwd`/DEF >& doSwap.log &
   # Took about 30 minutes.

###########################################################################
# BLASTZ FOR TETRAODON (tetNig1) (DONE, 2006-04-29 - 2006-04-30, hartera)
# CREATE CHAIN AND NET TRACKS, AXTNET, MAFNET AND ALIGNMENT DOWNLOADS
    # No lineage-specific repeats for this species pair. 
    # Tetraodon also has no species-specific repeats in the RepeatMasker
    # library so run this using dynamic masking instead as for danRer2 and
    # danRer3.
    # The tetraodon 2bit file of chroms and scaffolds 
    # (tetNig1ChromsRandomScafs.2bit) - this contains sequences for chroms
    # and for scaffolds of random chroms.
    ssh pk
    mkdir /cluster/data/danRer4/bed/blastz.tetNig1.2006-04-29
    cd /cluster/data/danRer4/bed
    ln -s blastz.tetNig1.2006-04-29 blastz.tetNig1
    cd /cluster/data/danRer4/bed/blastz.tetNig1.2006-04-29
# use parameters for tetNig1 in makeDanRer3.doc. Using scaffolds makes this run
# slower so it is best to have the scaffolds in the query. Use HoxD55.q 
# matrix as tetraodon is quite distant from zebrafish. Blastz uses 
# lineage-specfic repeats but there are none for these two species.
# Use soft-masked scaffolds and dynamic masking.
cat << '_EOF_' > DEF
# zebrafish (danRer4) vs. tetraodon (tetNig1)
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/x86_64:/cluster/home/angie/schwartzbin:/parasol/bin

BLASTZ=blastz.v7.x86_64
BLASTZ_ABRIDGE_REPEATS=0

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2500
BLASTZ_M=50
BLASTZ_Q=/cluster/data/blastz/HoxD55.q

# TARGET - zebrafish (danRer4)
SEQ1_DIR=/san/sanvol1/scratch/danRer4/danRer4.2bit
SEQ1_LEN=/san/sanvol1/scratch/danRer4/chrom.sizes
# 0.5 Mb chunk for target with 5 kb overlap
SEQ1_LIMIT=30
SEQ1_CHUNK=500000
SEQ1_LAP=5000

# QUERY - Tetraodon (tetNig1)
SEQ2_DIR=/san/sanvol1/scratch/tetNig1/tetNig1.2bit
# soft-masked chroms and random scaffolds in 2bit format
SEQ2_CTGDIR=/san/sanvol1/scratch/tetNig1/chromsAndScafs/tetNig1ChromsRandomScafs.2bit
SEQ2_LIFT=/san/sanvol1/scratch/tetNig1/chromsAndScafs/chromsAndScafs.lft
SEQ2_LEN=/san/sanvol1/scratch/tetNig1/chrom.sizes
SEQ2_CTGLEN=/san/sanvol1/scratch/tetNig1/chromsAndScafs/chromsAndScafs.sizes
# large enough chunk to do whole genome at once
SEQ2_CHUNK=1000000000
SEQ2_LAP=0

BASE=/cluster/data/danRer4/bed/blastz.tetNig1.2006-04-29
TMPDIR=/scratch/tmp
'_EOF_'
   # << this line keeps emacs coloring happy
   chmod +x DEF
   nohup nice /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
        -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
        `pwd`/DEF >& doBlastz.log &
   # Start: Sat Apr 29 18:10 Finish: Apr 29 22:41 
   # Stopped after making and merging chains:
   # netChains: looks like previous stage was not successful 
   # (can't find [danRer4.tetNig1.]all.chain[.gz]). However, this file
   # is there so start again with net step and continue:

   cd /cluster/data/danRer4/bed/blastz.tetNig1.2006-04-29
   nohup nice /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
        -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
        -continue net `pwd`/DEF >& net.log &
   # Took about 20 minutes to finish.
   # check coverage compared to danRer3:
   featureBits danRer4 chainTetNig1Link
   # 119439512 bases of 1626093931 (7.345%) in intersection
   featureBits danRer3 chainTetNig1Link
   # 109205244 bases of 1630323462 (6.698%) in intersection
   featureBits -chrom=chr1 danRer4 refGene:cds chainTetNig1Link -enrichment
   # refGene:cds 0.732%, chainTetNig1Link 7.536%, both 0.645%, 
   # cover 88.08%, enrich 11.69x
   featureBits -chrom=chr1 danRer3 refGene:cds chainTetNig1Link -enrichment
   # refGene:cds 0.774%, chainTetNig1Link 6.821%, both 0.692%, 
   # cover 89.34%, enrich 13.10x
   featureBits -chrom=chr1 danRer4 refGene:cds netTetNig1 -enrichment
   # refGene:cds 0.732%, netTetNig1 55.116%, both 0.705%, 
   # cover 96.33%, enrich 1.75x
   featureBits -chrom=chr1 danRer3 refGene:cds netTetNig1 -enrichment
   # refGene:cds 0.774%, netTetNig1 61.540%, both 0.753%, 
   # cover 97.24%, enrich 1.58x
   # Similar coverage as for tetNig1 chains and nets on zebrafish danRer3.
   # Do the Blastz swap to get danRer4 alignments on tetNig1
   # see also makeTetNig1.doc for featureBits for these alignments on tetNig1.
   ssh pk
   cd /cluster/data/danRer4/bed/blastz.tetNig1.2006-04-29
   nice /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
        -bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
        -swap `pwd`/DEF >& doSwap.log &
   # Took about 22 minutes to run.

###########################################################################
# MAKE DOWNLOADABLE SEQUENCE FILES (DONE, 2006-05-01, hartera)
   ssh kkstore01
   cd /cluster/data/danRer4
   #- Build the .tar.gz and *.gz files for bigZips
   cat << '_EOF_' > jkStuff/zipAll.csh
rm -rf bigZips
mkdir bigZips
tar cvzf bigZips/chromAgp.tar.gz ?{,?}/chr*.agp
tar cvzf bigZips/chromOut.tar.gz ?{,?}/chr*.fa.out
tar cvzf bigZips/chromFa.tar.gz ?{,?}/chr*.fa
tar cvzf bigZips/chromFaMasked.tar.gz ?{,?}/chr*.fa.masked
# soft masked chrNA and chrUn scaffolds
tar cvzf bigZips/scaffoldRandomsFa.tar.gz NA_random/scaffoldNA_random.fa \
    Un_random/scaffoldUn_random.fa
cd bed/simpleRepeat
tar cvzf ../../bigZips/chromTrf.tar.gz trfMaskChrom/chr*.bed
cd ../..
# get GenBank native mRNAs
cd /cluster/data/genbank
./bin/i386/gbGetSeqs -db=danRer4 -native GenBank mrna \
        /cluster/data/danRer4/bigZips/mrna.fa
# get GenBank xeno mRNAs
./bin/i386/gbGetSeqs -db=danRer4 -xeno GenBank mrna \
        /cluster/data/danRer4/bigZips/xenoMrna.fa
# get native RefSeq mRNAs
./bin/i386/gbGetSeqs -db=danRer4 -native refseq mrna \
/cluster/data/danRer4/bigZips/refMrna.fa
# get native GenBank ESTs
./bin/i386/gbGetSeqs -db=danRer4 -native GenBank est \
/cluster/data/danRer4/bigZips/est.fa

# gzip the Genbank sequences and create upstream sequence files for RefSeq.
cd /cluster/data/danRer4/bigZips
gzip *.fa
'_EOF_'
   # << this line makes emacs coloring happy
   chmod +x jkStuff/zipAll.csh
   csh -ef ./jkStuff/zipAll.csh >& zipAll.log &
   # Took about 35 minutes.    
   #- Look at zipAll.log to make sure all file lists look reasonable.
   # Make upstream files for zebrafish RefSeq and Copy the .gz files to
   # hgwdev:/usr/local/apache/...
   ssh hgwdev
   cd /cluster/data/danRer4/bigZips
   foreach I (1000 2000 5000)
     featureBits danRer4 refGene:upstream:${I} -fa=stdout \
              | gzip -c > upstream${I}.fa.gz
     echo "upstream${I} done"
   end
   set gp = /usr/local/apache/htdocs/goldenPath/danRer4
   mkdir -p $gp/bigZips
   cp -p *.gz $gp/bigZips
   mkdir -p $gp/chromosomes

   # Add individual chromosomes and file of scaffolds for each random chrom
   # to chromosomes downloads directory.
   foreach f (../*/chr*.fa)
     cp $f $gp/chromosomes
   end
   foreach c (NA_random Un_random)
     cd /cluster/data/danRer4/$c
     cp scaffold${c}.fa $gp/chromosomes
   end
   # create md5sum for bigZips
   cd $gp/bigZips
   md5sum *.gz > md5sum.txt
   # gzip each chrom or scaffolds for chrom separately in chromosomes dir
   cd $gp/chromosomes
   foreach f (*.fa)
      gzip $f
   end
   # create md5sum for chromosomes
   md5sum *.gz > md5sum.txt
   # Take a look at bigZips/* and chromosomes/*
   # copy README.txt's from danRer3 and update

###########################################################################
# HUMAN (hg18) PROTEINS TRACK FOR hg18 (DONE, 2006-04-28 - 2006-05-03, hartera)
   ssh kkstore01
   bash # if not using bash shell already
   # make Blast database for non-random chrom sequences
   mkdir -p /cluster/data/danRer4/blastDb
   cd /cluster/data/danRer4/blastDb
   cut -f 1 ../chrom.sizes | sed "s/chr//" | sed "/NA_random/d" \ 
       | sed "/Un_random/d" > chrom.list
   for i in `cat chrom.list`; 
       do ls -1 ../$i/*/*.fa . ; done | sed -n "/.*_.*_.*_.*/p" > list
   ln -s `cat list` .
   for i in *.fa
    do
        /projects/compbio/bin/i686/formatdb -i $i -p F
    done
   rm *.log *.fa list
   cd /cluster/data/danRer4
   for i in `cat blastDb/chrom.list`; 
       do cat  $i/chr*/*.lft  ; done > jkStuff/subChr.lft
   rm blastDb/chrom.list
   # Now make Blast database for random scaffolds sequences.
   mkdir /cluster/data/danRer4/scaffoldBlastDb
   cd /cluster/data/danRer4/scaffoldBlastDb

   # Take file of all scaffolds for NA_random and Un_random and cat together
   cat ../NA_random/scaffoldNA_random.fa ../Un_random/scaffoldUn_random.fa \
       > allRandomScafs.fasta
   grep '>' allRandomScafs.fasta | wc -l
   # 2966
   faSplit sequence allRandomScafs.fasta 500 scaf
   rm allRandomScafs.fasta
   for i in *.fa
     do
       /projects/compbio/bin/i686/formatdb -i $i -p F
     done
   rm *.log *.fa
   # combine databases for chroms and random chroms 
   mkdir -p /san/sanvol1/scratch/danRer4/comboBlastDb
   cd /cluster/data/danRer4/blastDb
   for i in nhr nin nsq; 
    do cp *.$i /san/sanvol1/scratch/danRer4/comboBlastDb; 
    done
   cd /cluster/data/danRer4/scaffoldBlastDb
   for i in nhr nin nsq; 
     do cp *.$i /san/sanvol1/scratch/danRer4/comboBlastDb; 
     done
   mkdir -p /cluster/data/danRer4/bed/tblastn.hg18KG
   cd /cluster/data/danRer4/bed/tblastn.hg18KG
   echo  /san/sanvol1/scratch/danRer4/comboBlastDb/*.nsq  \
         | xargs ls -S | sed "s/\.nsq//"  > query.lst
   wc -l query.lst
   # 4377 query.lst
   # we want around 250000 jobs
   calc `wc /cluster/data/hg18/bed/blat.hg18KG/hg18KG.psl | awk "{print \\\$1}"`/\(250000/`wc query.lst | awk "{print \\\$1}"`\)
   # 36727/(250000/4377) = 643.016316
   mkdir -p /cluster/bluearc/danRer4/bed/tblastn.hg18KG/kgfa
   split -l 643 /cluster/data/hg18/bed/blat.hg18KG/hg18KG.psl \
        /cluster/bluearc/danRer4/bed/tblastn.hg18KG/kgfa/kg
   ln -s /cluster/bluearc/danRer4/bed/tblastn.hg18KG/kgfa kgfa
   cd kgfa
   for i in *; do 
     nice /cluster/home/braney/bin/x86_64/pslxToFa $i $i.fa; 
     rm $i; 
     done
   cd ..
   ls -1S kgfa/*.fa > kg.lst
   mkdir -p /cluster/bluearc/danRer4/bed/tblastn.hg18KG/blastOut
   ln -s /cluster/bluearc/danRer4/bed/tblastn.hg18KG/blastOut
   for i in `cat kg.lst`; do  mkdir blastOut/`basename $i .fa`; done
   exit # back to tcsh
   cd /cluster/data/danRer4/bed/tblastn.hg18KG
   cat << '_EOF_' > blastGsub
#LOOP
blastSome $(path1) {check in line $(path2)} {check out exists blastOut/$(root2)/q.$(root1).psl }
#ENDLOOP
'_EOF_'

   cat << '_EOF_' > blastSome
#!/bin/sh
BLASTMAT=/cluster/bluearc/blast229/data
export BLASTMAT
g=`basename $2`
f=/tmp/`basename $3`.$g
for eVal in 0.01 0.001 0.0001 0.00001 0.000001 1E-09 1E-11
do
if /cluster/bluearc/blast229/blastall -M BLOSUM80 -m 0 -F no -e $eVal -p tblastn -d $1 -i $2 -o $f.8
then
        mv $f.8 $f.1
        break;
fi
done
if test -f  $f.1
then
    if /cluster/bin/i386/blastToPsl $f.1 $f.2
    then
        liftUp -nosort -type=".psl" -nohead $f.3 /cluster/data/danRer4/jkStuff/subChr.lft carry $f.2
        liftUp -nosort -type=".psl" -nohead $f.4 /cluster/data/danRer4/jkStuff/liftAll.lft carry $f.3
        liftUp -nosort -type=".psl" -pslQ -nohead $3.tmp /cluster/data/hg18/bed/blat.hg18KG/protein.lft warn $f.4

        if pslCheck -prot $3.tmp                                                  
        then                                                                      
            mv $3.tmp $3                                                          
            rm -f $f.1 $f.2 $f.3 $f.4
        fi
        exit 0                                                                    
    fi                                                                            
fi                                                                                
rm -f $f.1 $f.2 $3.tmp $f.8 $f.3 $f.4
exit 1
'_EOF_'
    # << happy emacs
    chmod +x blastSome
    gensub2 query.lst kg.lst blastGsub blastSpec
    
    # then run the Blast cluster jobs
    ssh kk
    cd /cluster/data/danRer4/bed/tblastn.hg18KG
    para create blastSpec
    para try, check, push, check etc.
    # pushed 100,000 jobs at a time so need to do para push again later
    para time
# Completed: 253866 of 253866 jobs
# CPU time in finished jobs:   52410110s  873501.83m 14558.36h  606.60d  1.662 y
# IO & Wait Time:               5508786s   91813.10m  1530.22h   63.76d  0.175 y
# Average job time:                 228s       3.80m     0.06h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            2162s      36.03m     0.60h    0.03d
# Submission to last job:        147825s    2463.75m    41.06h    1.71d

    # Took a while as had to repush some crashed jobs.
    ssh kkstore01
    cd /cluster/data/danRer4/bed/tblastn.hg18KG
    tcsh
    mkdir chainRun
    cd chainRun
    cat << '_EOF_' > chainGsub
#LOOP
chainOne $(path1)
#ENDLOOP
'_EOF_'

    cat << '_EOF_' > chainOne
(cd $1; cat q.*.psl | simpleChain -prot -outPsl -maxGap=75000 stdin /cluster/bluearc/danRer4/bed/tblastn.hg18KG/blastOut/c.`basename $1`.psl)
'_EOF_'
    chmod +x chainOne
    ls -1dS \
      /cluster/bluearc/danRer4/bed/tblastn.hg18KG/blastOut/kg?? > chain.lst
    gensub2 chain.lst single chainGsub chainSpec
    # do the cluster run for chaining
    ssh kk
    cd /cluster/data/danRer4/bed/tblastn.hg18KG/chainRun
    para create chainSpec
    para try, check, push, check etc.
# Completed: 58 of 58 jobs
# CPU time in finished jobs:     759034s   12650.56m   210.84h    8.79d  0.024 y
# IO & Wait Time:                217724s    3628.74m    60.48h    2.52d  0.007 y
# Average job time:               16841s     280.68m     4.68h    0.19d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:          208828s    3480.47m    58.01h    2.42d
# Submission to last job:        208891s    3481.52m    58.03h    2.42d
    ssh kkstore01
    cd /cluster/data/danRer4/bed/tblastn.hg18KG/blastOut
    bash # if using another shell
    for i in kg??
    do
       cat c.$i.psl | awk "(\$13 - \$12)/\$11 > 0.6 {print}" > c60.$i.psl
       sort -rn c60.$i.psl | pslUniq stdin u.$i.psl
       awk "((\$1 / \$11) ) > 0.60 { print   }" c60.$i.psl > m60.$i.psl
       echo $i
    done
    liftUp -nohead -type=.psl stdout \
           /cluster/data/danRer4/jkStuff/liftAll.lft carry u.*.psl m60* | \
           sort -T /tmp -k 14,14 -k 16,16n -k 17,17n | uniq \
           > /cluster/data/danRer4/bed/tblastn.hg18KG/blastHg18KG.psl
    pslCheck blastHg18KG.psl
    # this is ok.
    # load table 
    ssh hgwdev
    cd /cluster/data/danRer4/bed/tblastn.hg18KG
    hgLoadPsl danRer4 blastHg18KG.psl
    # check coverage
    featureBits danRer4 blastHg18KG
    # 21159392 bases of 1626093931 (1.301%) in intersection 
    
    featureBits danRer3 blastHg17KG
    # 21063005 bases of 1630323462 (1.292%) in intersection
    
    featureBits -chrom=chr1 danRer4 refGene:cds blastHg18KG -enrichment
    # refGene:cds 0.732%, blastHg18KG 1.333%, both 0.428%, cover 58.43%, 
    # enrich 43.83x
    featureBits -chrom=chr1 danRer3 refGene:cds blastHg17KG -enrichment
    # refGene:cds 0.774%, blastHg17KG 1.370%, both 0.450%, cover 58.05%, 
    # enrich 42.38x
    # Similar coverage compared to refGene CDS as for hg17 proteins on danRer3.
    # back to kkstore04 to clean up
    ssh kkstore04
    rm -rf /cluster/data/danRer4/bed/tblastn.hg18KG/blastOut
    rm -rf /cluster/bluearc/danRer4/bed/tblastn.hg18KG/blastOut

    # add trackDb.ra entry and html to ~/kent/src/hg/makeDb/trackDb/trackDb.ra
    # also added the blastHg18KG.html here. 
    # blastKGPep04 and blastKGRef04 tables required on hg18 - these have
    # been created - see makeHg18.doc. update of hgc.c, hgTrackUi.c and 
    # hgTracks.c was required - done by Brian.

###########################################################################
# MULTIZ7WAY ALIGNMENTS FOR CONSERVATION TRACK 
# (DONE, 2006-05-04 - 2006-05-10, hartera)
# RE-MAKE WITH DANRER4 RANDOMS FOR MM8 AND ADDED FRAMES TABLE AND 
# MULTIZ7WAY DOWNLOADS (DONE, 2006-05-28 - 2005-05-29, hartera)
#   for tetNig1, fr1, xenTro2, monDom4, mm8 and hg18.
    ssh kkstore04
    mkdir /cluster/data/danRer4/bed/multiz7way.2006-05-28
    cd /cluster/data/danRer4/bed
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28
    # copy MAFs to a cluster-friendly server
    rm -r /san/sanvol1/scratch/danRer4/mafNet
    mkdir /san/sanvol1/scratch/danRer4/mafNet
    foreach s (tetNig1 fr1 xenTro2 monDom4 mm8 hg18)
       echo $s
       rsync -av /cluster/data/danRer4/bed/blastz.$s/mafNet/* \
        /san/sanvol1/scratch/danRer4/mafNet/$s/
    end
    # prune the hg17 17way tree to just these 7 and update db names:
    /cluster/bin/phast/tree_doctor \
      --prune-all-but=mouse_mm8,human_hg18,monodelphis_monDom4,xenopus_xenTro1,tetraodon_tetNig1,fugu_fr1,zebrafish_danRer3 \
      --rename="xenopus_xenTro1 -> xenopus_xenTro2 ; zebrafish_danRer3 -> zebrafish_danRer4" \
      /cluster/data/hg18/bed/multiz17way/17way.nh > 7way.nh
    # carefully edit so that danRer4 is first. copy first to new file
    cp 7way.nh 7way_zfishFirst.nh
  #  /cluster/bin/phast/draw_tree 7way_zfishFirst.nh > 7way.ps
    # also made the ps file for the 7way.nh and compared to make sure
    # that the tree with zebrafish at the top looks correct.
    /cluster/bin/phast/all_dists 7way_zfishFirst.nh > 7way.distances
    grep danRer4 7way.distances | sort -k3,3n | \
        awk '{printf ("%.4f\t%s\n", $3, $2)}' > distances.txt
    cat distances.txt
# 1.4749  tetraodon_tetNig1
# 1.5154  fugu_fr1
# 1.7480  human_hg18
# 1.7782  monodelphis_monDom4
# 1.8771  xenopus_xenTro2
# 2.1058  mouse_mm8
    # the order in the browser display will be by tree topology,
    # not by distance, so they will be:
    # danRer4
    # 1.5154  fugu_fr1
    # 1.4749  tetraodon_tetNig1
    # 1.8771  xenopus_xenTro2
    # 1.7782  monodelphis_monDom4
    # 2.1058  mouse_mm8
    # 1.7480  human_hg18

    # create species list and stripped down tree for autoMZ
    sed -e 's/[a-z][a-z]*_//g; s/:[0-9\.][0-9\.]*//g; s/;//' \
        7way_zfishFirst.nh > tree-commas.nh
    sed -e 's/ //g; s/,/ /g' tree-commas.nh > tree.nh
    sed -e 's/[()]//g; s/,/ /g' tree.nh > species.lst

    ssh pk
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28
    mkdir maf run
    cd run

    # stash binaries
    mkdir penn
    cp -p /cluster/bin/penn/multiz.v11.x86_64/multiz-tba/multiz penn
    cp -p /cluster/bin/penn/multiz.v11.x86_64/multiz-tba/maf_project penn
    cp -p /cluster/bin/penn/multiz.v11.x86_64/multiz-tba/autoMZ penn

cat > autoMultiz.csh << 'EOF'
#!/bin/csh -ef
    set db = danRer4
    set c = $1
    set maf = $2
    set run = `pwd`
    set tmp = /scratch/tmp/$db/multiz.$c
    set pairs = /san/sanvol1/scratch/$db/mafNet
    rm -fr $tmp
    mkdir -p $tmp
    cp ../{tree.nh,species.lst} $tmp
    pushd $tmp
    foreach s (`cat species.lst`)
        set in = $pairs/$s/$c.maf
        set out = $db.$s.sing.maf
        if ($s == $db) then
            continue
        endif
        if (-e $in.gz) then
            zcat $in.gz > $out
        else if (-e $in) then
            cp $in $out
        else
            echo "##maf version=1 scoring=autoMZ" > $out
        endif
    end
    set path = ($run/penn $path); rehash
    $run/penn/autoMZ + T=$tmp E=$db "`cat tree.nh`" $db.*.sing.maf $c.maf
    popd
    cp $tmp/$c.maf $maf
    rm -fr $tmp
'EOF'
    # << emacs
    chmod +x autoMultiz.csh

cat  << 'EOF' > spec
#LOOP
./autoMultiz.csh $(root1) {check out line+ /cluster/data/danRer4/bed/multiz7way.2006-05-28/maf/$(root1).maf}
#ENDLOOP
'EOF'
    # << emacs
    awk '{print $1}' /cluster/data/danRer4/chrom.sizes > chrom.lst
    gensub2 chrom.lst single spec jobList
    para create jobList
    para try, check, push, check etc. ...
    para time
# Completed: 28 of 28 jobs
# CPU time in finished jobs:       7022s     117.03m     1.95h    0.08d  0.000 y
IO & Wait Time:                   142s       2.37m     0.04h    0.00d  0.000 y
Average job time:                 256s       4.26m     0.07h    0.00d
Longest running job:                0s       0.00m     0.00h    0.00d
Longest finished job:             368s       6.13m     0.10h    0.00d
Submission to last job:           705s      11.75m     0.20h    0.01d

   # Make .jpg for tree and install in htdocs/images/phylo/... don't forget
   # to request a push of that file.  The treeImage setting in trackDb.ra 
   # is phylo/danRer4_7way.jpg (relative to htdocs/images).
   ssh hgwdev
   cd /cluster/data/danRer4/bed/multiz7way.2006-05-28
cat << '_EOF_' > species7.nh
((zebrafish,(Fugu,Tetraodon)),(X. tropicalis,(opossum,(mouse,human))))
'_EOF_'

   /cluster/bin/phast/draw_tree species7.nh > species7way.ps
   # ask Bob to resize image for Browser track description page and convert
   # to JPEG and rename as danRer4_7way.jpg

   # Build maf annotation and load dataabase
   ssh kolossus
   mkdir /cluster/data/danRer4/bed/multiz7way.2006-05-28/anno
   cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/anno
   mkdir maf run
   cd run
   rm -f sizes nBeds
 foreach db (`cat /cluster/data/danRer4/bed/multiz7way.2006-05-28/species.lst`)
      ln -s  /cluster/data/$db/chrom.sizes $db.len
      if (! -e /cluster/data/$db/$db.N.bed) then
        twoBitInfo -nBed /cluster/data/$db/$db.{2bit,N.bed}
      endif
      ln -s  /cluster/data/$db/$db.N.bed $db.bed
      echo $db.bed  >> nBeds
      echo $db.len  >> sizes
  end
    echo date > jobs.csh
    # do smaller jobs first:
    foreach f (`ls -1rS ../../maf/*.maf`)
      echo nice mafAddIRows -nBeds=nBeds -sizes=sizes $f \
        /cluster/data/danRer4/danRer4.2bit ../maf/`basename $f` \
        >> jobs.csh
      echo "echo $f" >> jobs.csh
    end
    echo date >> jobs.csh
    csh -efx jobs.csh >&! jobs.log & 
    tail -f jobs.log
    # Took 27 minutes to run.

    # Load anno/maf
    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/anno/maf
    mkdir -p /gbdb/danRer4/multiz7way/anno/maf
    ln -s /cluster/data/danRer4/bed/multiz7way.2006-05-28/anno/maf/*.maf \
      /gbdb/danRer4/multiz7way/anno/maf
    # delete old files from extFile table
    hgsql -e 'delete from extFile where path like "%multiz7way/anno/maf%";' \
          danRer4
    cat > loadMaf.csh << 'EOF'
date
nice hgLoadMaf -pathPrefix=/gbdb/danRer4/multiz7way/anno/maf danRer4 multiz7way
date
'EOF'
    # << emacs
    csh -efx loadMaf.csh >&! loadMaf.log & tail -f loadMaf.log
    # Took about 1 minute.
    # Do the computation-intensive part of hgLoadMafSummary on a workhorse 
    # machine and then load on hgwdev:
    ssh kkr7u00
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/anno/maf
    cat *.maf \
    | nice hgLoadMafSummary danRer4 -minSize=30000 -mergeGap=1500 \
    -maxSize=200000 -test multiz7waySummary stdin
    # Created 820403 summary blocks from 4245668 components and 
    # 2120803 mafs from stdin
    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/anno/maf
    sed -e 's/mafSummary/multiz7waySummary/' ~/kent/src/hg/lib/mafSummary.sql \
      > /tmp/multiz7waySummary.sql
    time nice hgLoadSqlTab danRer4 multiz7waySummary \
         /tmp/multiz7waySummary.sql multiz7waySummary.tab
    # 0.000u 0.000s 2:05.26 0.0%      0+0k 0+0io 209pf+0w
    rm *.tab /tmp/multiz7waySummary.sql
 
    # zip mafs:
    ssh kkstore04
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/maf
cat > zipMafs.csh << 'EOF'
    date
    foreach f (chr*.maf)
        set c = $f:r
        echo $c
        nice gzip -c $f > $c.maf.gz
    end
    date
'EOF'
    time csh -efx zipMafs.csh >&! zip.log
    # 219.706u 1.939s 3:41.75 99.9%   0+0k 0+0io 0pf+0w
    rm *.maf
    # add Frames table:
    mkdir /cluster/data/danRer4/bed/multiz7way.2006-05-28/frames
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/frames
    # The following is adapted from MarkD's Makefile used for mm7...
    # and used in makeRn4.doc.
    #------------------------------------------------------------------------
    # get the genes for all genomes
    # using mrna for danRer4
    # using knownGene for mm8 hg18
    # using mgcGenes for xenTro2
    # using ensGene for fr1
    # no genes for monDom4 and tetNig1
    # targetDb = danRer4
    # queryDbs = mm8 hg18 xenTro2 fr1 (to build frames for)
    # genePreds; (must keep only the first 10 columns for knownGene)
    
    # mRNAs with CDS.  single select to get cds+psl, then split that up and
    # create genePred
    # using mrna table as genes: danRer4
    mkdir genes
    foreach queryDb (danRer4)
      set tmpExt = `mktemp temp.XXXXXX`
      set tmpMrnaCds = ${queryDb}.mrna-cds.${tmpExt}
      set tmpMrna = ${queryDb}.mrna.${tmpExt}
      set tmpCds = ${queryDb}.cds.${tmpExt}
      echo $queryDb
      hgsql -N -e 'select all_mrna.qName,cds.name,all_mrna.* \
                   from all_mrna,gbCdnaInfo,cds \
                   where (all_mrna.qName = gbCdnaInfo.acc) and \
                     (gbCdnaInfo.cds != 0) and (gbCdnaInfo.cds = cds.id)' \
       ${queryDb} > ${tmpMrnaCds}
      cut -f 1-2  ${tmpMrnaCds} > ${tmpCds}
      cut -f 4-100  ${tmpMrnaCds} > ${tmpMrna}
      mrnaToGene -cdsFile=${tmpCds} -smallInsertSize=8 -quiet ${tmpMrna} \
        stdout \
      | genePredSingleCover stdin stdout | gzip -2c \
        > /scratch/tmp/$queryDb.tmp.gz
      rm ${tmpMrnaCds} ${tmpMrna} ${tmpCds}
      mv /scratch/tmp/$queryDb.tmp.gz genes/$queryDb.gp.gz
      rm -f $tmpExt
    end

    # using knownGene for mm8 hg18
    # using mgcGenes for xenTro2
    # using enesGene for fr1
    foreach queryDb (mm8 hg18 xenTro2 fr1)
      if ($queryDb == "xenTro2") then
        set geneTbl = mgcGenes
      else if ($queryDb == "fr1") then
        set geneTbl = ensGene
      else
        set geneTbl = knownGene
      endif
      hgsql -N -e "select * from $geneTbl" ${queryDb} | cut -f 1-10 \
      | genePredSingleCover stdin stdout | gzip -2c \
        > /scratch/tmp/$queryDb.tmp.gz
      mv /scratch/tmp/$queryDb.tmp.gz genes/$queryDb.gp.gz
      rm -f $tmpExt
    end
 
    #------------------------------------------------------------------------
    # create frames
    set clusterDir = /cluster/bluearc/danRer4/multiz7wayFrames
    set multizDir = /cluster/data/danRer4/bed/multiz7way.2006-05-28
    set mafDir = $multizDir/maf
    set geneDir = $multizDir/frames/genes
    set clusterMafDir = ${clusterDir}/maf
    set clusterGeneDir = ${clusterDir}/genes
    set clusterFramesDir = ${clusterDir}/mafFrames.kki

    # copy mafs to cluster storage
    mkdir $clusterDir
    ssh -x kkstore04 "rsync -av $mafDir/*.maf.gz $clusterMafDir/"

    # copy genes to cluster storage
    ssh -x kkstore04 "rsync -av $geneDir/*.gp.gz $clusterGeneDir/"

    # run cluster jobs
    set tmpExt = `mktemp temp.XXXXXX`
    set paraDir = $multizDir/frames/para.${tmpExt}
    mkdir mafFrames $paraDir
    rm -f $paraDir/jobList
    mkdir ${clusterFramesDir}
    foreach queryDb (`cat /cluster/data/danRer4/bed/multiz7way.2006-05-28/species.lst`)
      mkdir ${clusterFramesDir}/${queryDb}
      foreach c (`awk '{print $1;}' /cluster/data/danRer4/chrom.sizes`)
        if (-e ${clusterGeneDir}/${queryDb}.gp.gz) then
          echo /cluster/bin/scripts/mkMafFrames.pl ${queryDb} danRer4 \
            ${clusterGeneDir}/${queryDb}.gp.gz ${clusterMafDir}/$c.maf.gz \
            ${clusterFramesDir}/${queryDb}/$c.mafFrames \
            >> $paraDir/jobList
        endif
      end
    end
    rm -f $tmpExt
    ssh -x kki "cd ${paraDir} && para make jobList && para time"
# Completed: 140 of 140 jobs
# CPU time in finished jobs:        255s       4.25m     0.07h    0.00d  0.000 y
# IO & Wait Time:                   360s       6.00m     0.10h    0.00d  0.000 y
# Average job time:                   4s       0.07m     0.00h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:               8s       0.13m     0.00h    0.00d
# Submission to last job:            55s       0.92m     0.02h    0.00d

    # combine results from cluster
    foreach queryDb (`cat ../species.lst`)
      echo $queryDb
      ssh -x kolossus "cat ${clusterFramesDir}/${queryDb}/*.mafFrames | gzip -2c > ${multizDir}/frames/mafFrames/${queryDb}.mafFrames.gz"
    end
    #------------------------------------------------------------------------
    # load the database
    hgLoadMafFrames danRer4 multiz7wayFrames mafFrames/*.mafFrames.gz


    #------------------------------------------------------------------------
    # clean up
    rm -rf ${clusterDir}

    ###
    # rebuild frames to get bug fix, using 1-pass maf methodology
    # (2006-06-09 markd)
    ssh kkstore04
    cd /cluster/data/danRer4/bed/multiz7way/frames
    mv mafFrames/ mafFrames.old
    nice tcsh # easy way to get process niced
    (zcat  ../maf/*.maf.gz | time genePredToMafFrames danRer4 stdin stdout danRer4 genes/danRer4.gp.gz fr1 genes/fr1.gp.gz hg18 genes/hg18.gp.gz mm8 genes/mm8.gp.gz xenTro2 genes/xenTro2.gp.gz | gzip >multiz7way.mafFrames.gz)>&log&
    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way/frames

    hgLoadMafFrames danRer4 multiz7wayFrames multiz7way.mafFrames.gz >&log&

    # end of multiz7way frames and load

    cd /cluster/data/danRer4/bed
    ln -s multiz7way.2006-05-28 /cluster/data/danRer4/bed/multiz7way
    # create and add the tree image for the description page
    # Make .jpg for tree and install in htdocs/images/phylo/... don't forget
    # to request a push of that file.  The treeImage setting in trackDb.ra 
    # is phylo/danRer4_7way.jpg (relative to htdocs/images).
    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28
cat << '_EOF_' > species7.nh
((zebrafish,(Fugu,Tetraodon)),(X. tropicalis,(opossum,(mouse,human))))
'_EOF_'

    /cluster/bin/phast/draw_tree species7.nh > species7way.ps
    # ask Bob to resize image for Browser track description page and convert
    # to JPEG and rename as danRer4_7way.jpg
    ln -s /cluster/data/danRer4/bed/multiz7way.2006-05-28/danRer4_7way.jpg \
          /usr/local/apache/htdocs/images/phylo/danRer4_7way.jpg
    # change permissions for display if not already readable to all
    chmod +r /usr/local/apache/htdocs/images/phylo/danRer4_7way.jpg

# check for all.joiner entry for 7-way - it is there already.

# add html and trackDb.ra entry for danRer4:
# track multiz7way
# shortLabel Conservation
# longLabel Vertebrate Multiz Alignment & Conservation
# group compGeno
# priority 104
# visibility pack
# color 0, 10, 100
# altColor 0,90,10
# type wigMaf 0.0 1.0
# maxHeightPixels 100:40:11
# wiggle phastCons7way
# pairwiseHeight 12
# spanList 1
# yLineOnOff Off
# autoScaleDefault Off
# windowingFunction mean
# summary multiz7waySummary
# frames multiz7wayFrames
# irows on
# speciesGroups vertebrate mammal
# sGroup_vertebrate fr1 tetNig1 xenTro2
# sGroup_mammal monDom4 mm8 hg18
# treeImage phylo/danRer4_7way.jpg

###########################################################################
# MAF DOWNLOADS FOR MULTIZ7WAY (DONE, 2006-05-29, hartera)
# GZIPPED UPSTREAM FILES AND ADDED TO DOWNLOADS AND RE-MADE md5sum.txt
# (DONE, 2006-06-02, hartera)
   ssh hgwdev
   cd /cluster/data/danRer4/bed/multiz7way.2006-05-28
   mkdir mafDownloads
   cd mafDownloads
    # upstream mafs
cat > mafFrags.csh << 'EOF'
    date
    foreach i (1000 2000 5000)
        echo "making upstream$i.maf"
        nice featureBits danRer4 refGene:upstream:$i -fa=/dev/null -bed=up.bad
        awk -F '\t' '{printf("%s\t%s\t%s\t%s\t%s\t%s\n", $1, $2, $3, substr($4, 0, 9), 0, $5)}' up.bad > up.bed
        rm up.bad
        nice mafFrags danRer4 multiz7way up.bed upstream$i.maf \
                -orgs=../species.lst
        rm up.bed
    end
    date
'EOF'
    time csh mafFrags.csh >&! mafFrags.log & tail -f mafFrags.log
    # 57.823u 105.238s 4:13.15 64.4%  0+0k 0+0io 2pf+0w
    # add maf downloads for annotated mafs
    ssh kkstore04
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/mafDownloads
cat > downloads.csh << 'EOF'
    date
    foreach f (../anno/maf/chr*.maf)
        set c = $f:t:r
        echo $c
        nice gzip -c $f > $c.maf.gz
    end
    md5sum *.gz > md5sum.txt
    date
'EOF'
    # 446.734u 5.629s 7:38.09 98.7%       0+0k 0+0io 2pf+0w
   
    ssh hgwdev
    set dir = /usr/local/apache/htdocs/goldenPath/danRer4/multiz7way
    mkdir $dir
    ln -s \
/cluster/data/danRer4/bed/multiz7way.2006-05-28/mafDownloads/{*.gz,md5sum.txt} \
    $dir
    cp /usr/local/apache/htdocs/goldenPath/danRer3/multiz5way/README.txt $dir
    # edit README.txt

    # gzip the upstream maf downloads and remake md5sum.txt 
    # (2006-06-02, hartera)
    ssh kkstore04
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/mafDownloads
    foreach f (upstream*.maf)
      nice gzip -c $f > $f.maf.gz
    end 
    rm md5sum.txt
    md5sum *.gz > md5sum.txt
    ssh hgwdev
    set dir = /usr/local/apache/htdocs/goldenPath/danRer4/multiz7way
    rm $dir/md5sum.txt
    ln -s \
/cluster/data/danRer4/bed/multiz7way.2006-05-28/mafDownloads/{upstream*.gz,md5sum.txt} $dir 

###########################################################################
# PHYLO-HMM (PHASTCONS) CONSERVATION TRACK FOR 7-WAY ALIGNMENT 
# (DONE, 2006-05-17 - 2006-05-24, hartera) 
# REMAKE CONSERVATION TRACK USING MULTIZ 7-WAY INCLUDING DANRER4 RANDOM CHROMS
# FOR MM8 ALIGNMENTS (DONE, 2006-05-29, hartera)
   ssh kkstore04
   # Need unzipped maf files for this.
   cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/maf
   foreach f (*.maf.gz)
     echo $f
     gunzip -c $f > $f:r
   end 

   mkdir /cluster/data/danRer4/bed/multiz7way.2006-05-28/phastCons
   cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/phastCons

   # create a starting-tree.mod based on chr14 (92 Mb)
   # chr14 is the largest chrom apart from chrNA_random
   /cluster/bin/phast/$MACHTYPE/msa_split ../maf/chr14.maf \
        --refseq ../../../14/chr14.fa --in-format MAF \
        --windows 100000000,1000 --out-format SS \
        --between-blocks 5000 --out-root s1

   /cluster/bin/phast/$MACHTYPE/phyloFit -i SS s1.*.ss \
        --tree "`cat ../tree-commas.nh`" \
        --out-root starting-tree
   # took less than a minute
   rm s1.*ss
   
   # Get genome-wide average GC content (for all species together,
   # not just the reference genome).  If you have a globally
   # estimated tree model, as above, you can get this from the
   # BACKGROUND line in the .mod file.  E.g.,
# ALPHABET: A C G T
# ...
# BACKGROUND: 0.305239 0.194225 0.194292 0.306244
   # add up the C and G:
   grep BACKGROUND starting-tree.mod | awk '{printf "%0.3f\n", $3 + $4;}'
   # 0.389 is the GC content. This is used in the -gc argument below.
   # If you do *not* have a global tree model and you do not know your
   # GC content, you can get it directly from the MAFs with a command
   # like:
   /cluster/bin/phast/$MACHTYPE/msa_view \
    --aggregate danRer4,tetNig1,fr1,xenTro2,monDom4,mm8,hg18 -i MAF \
    -S /cluster/data/danRer4/bed/multiz7way/maf/chr*.maf > maf_summary.txt
   # This gives a GC content of 0.426 so use this as it is from mafs for
   # the whole genome.
   # break up the genome-wide MAFs into pieces on the san filesystem
   ssh pk
   set WINDOWS=/san/sanvol1/scratch/danRer4/multiz7way.2006-05-28/phastCons/ss
   mkdir -p $WINDOWS
   cd $WINDOWS
   cat << 'EOF' > doSplit.csh
#!/bin/csh -ef
set MAFS = /cluster/data/danRer4/bed/multiz7way.2006-05-28/maf
set WINDOWS=/san/sanvol1/scratch/danRer4/multiz7way.2006-05-28/phastCons/ss
cd $WINDOWS
set c = $1
echo $c
rm -fr $c
mkdir $c
set N = `echo $c | sed -e 's/chr//'`
/cluster/bin/phast/$MACHTYPE/msa_split $MAFS/$c.maf -i MAF \
       -M /cluster/data/danRer4/$N/$c.fa \
       -o SS -w 10000000,0 -I 1000 -B 5000 -r $c/$c
echo "Done" >> $c.done
'EOF'
# << emacs
   chmod +x doSplit.csh
   rm -f jobList
   foreach c (`cat /cluster/data/danRer4/chrom.lst`)
    echo "doSplit.csh chr${c} {check out line+ $WINDOWS/chr$c.done}" >> jobList
   end

   para create jobList
   para push, check etc.
   para time
# Completed: 28 of 28 jobs
# CPU time in finished jobs:        831s      13.86m     0.23h    0.01d  0.000 y
# IO & Wait Time:                   634s      10.56m     0.18h    0.01d  0.000 y
# Average job time:                  52s       0.87m     0.01h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:             118s       1.97m     0.03h    0.00d
# Submission to last job:           118s       1.97m     0.03h    0.00d

    # Create a random list of 50 1 mb regions (do not use chrNA and chrUn)

    ls -1l chr*/chr*.ss | grep -v NA | grep -v Un | \
       awk '$5 > 4000000 {print $9;}' | randomLines stdin 50 ../randomSs.list
    
    # Set up parasol directory to calculate trees on these 50 regions
    ssh pk
    set dir = /san/sanvol1/scratch/danRer4/multiz7way.2006-05-28/phastCons
    mkdir -p $dir
    cd $dir
    # now set up cluster job to estimate model parameters.  Parameters
    # will be estimated separately for each alignment fragment then
    # will be combined across fragments. Tuning this loop should come
    # back to here to recalculate. Tuning target-coverage and expected-length.
    # Create little script that calls phastCons with right arguments

    cat > makeTree.csh << 'EOF'
#!/bin/csh -fe
set C = $1:h
set treeRun = $2
set cov = $3
set len = $4
set dir = /san/sanvol1/scratch/danRer4/multiz7way.2006-05-28/phastCons
mkdir -p $dir/$treeRun/log/${C} $dir/$treeRun/tree/${C}
/cluster/bin/phast/x86_64/phastCons $dir/ss/$1 \
  /cluster/data/danRer4/bed/multiz7way.2006-05-28/phastCons/starting-tree.mod \
  --gc 0.426 --nrates 1,1 --no-post-probs --ignore-missing \
  --expected-length $len --target-coverage $cov \
  --quiet --log $dir/$treeRun/log/$1 --estimate-trees $dir/$treeRun/tree/$1
'EOF'
    # << emacs
    chmod a+x makeTree.csh
    # Make sure that the correct GC content is substituted in here. Notice
    # the target coverage of 0.17. Here we are going to aim
    # for 65% coverage of coding regions by conserved elements.
    # Create gensub file
# need to add cov and len parameters
    cat > template << '_EOF_'
#LOOP
makeTree.csh $(path1) $(path2)
#ENDLOOP
'_EOF_'
    #   happy emacs
    # Make cluster job and run it to try out a few parameters close
    # to those used for danRer3 and danRer2 phastCons runs.
    echo "treeRun1 0.17 12" > tree.lst
    echo "treeRun2 0.32 18" >> tree.lst
    echo "treeRun3 0.32 20" >> tree.lst
    echo "treeRun4 0.35 18" >> tree.lst
    gensub2 randomSs.list tree.lst template jobList
    para create jobList
    para try,check,push,check etc.
# para time
# Completed: 200 of 200 jobs
# CPU time in finished jobs:      68652s    1144.20m    19.07h    0.79d  0.002 y
# IO & Wait Time:                  2521s      42.02m     0.70h    0.03d  0.000 y
# Average job time:                 356s       5.93m     0.10h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:             629s      10.48m     0.17h    0.01d
# Submission to last job:          2356s      39.27m     0.65h    0.03d

    # Now combine parameter estimates.  We can average the .mod files
    # using phyloBoot.  This must be done separately for the conserved
    # and nonconserved models
    set dir = /san/sanvol1/scratch/danRer4/multiz7way.2006-05-28/phastCons
    foreach d ($dir/treeRun*)
       cd $d
       ls tree/chr*/*.cons.mod > cons.txt
       /cluster/bin/phast/$MACHTYPE/phyloBoot --read-mods '*cons.txt' \
         --output-average ave.cons.mod > cons_summary.txt
       ls tree/chr*/*.noncons.mod > noncons.txt
       /cluster/bin/phast/$MACHTYPE/phyloBoot --read-mods '*noncons.txt' \
         --output-average ave.noncons.mod > noncons_summary.txt
    end
       #   measuring entropy
    #   consEntropy <target coverage> <expected lengths>
    #            ave.cons.mod ave.noncons.mod --NH 9.78
    #   never stops with the --NH argument
    # target entropy should be L_min*H=9.8 bits, (between 9.5 to 10.5 is ok)
    # the expected length that produces this entropy is the one
    # to use for phastCons.
    # foreach treeRun, set the appropriate coverage and length
    # file: treeRunN cov len
    # use awk to split up
    cd /san/sanvol1/scratch/danRer4/multiz7way.2006-05-28/phastCons
    cp tree.lst entropy.csh 
    perl -pi.bak -e 's/^(treeRun[0-9]+)\s*([0-9\.]+)\s*([0-9]+)/echo \"Coverage = $2 Length = $3\"\ncd $1\n\/cluster\/bin\/phast\/x86_64\/consEntropy $2 $3 ave.cons.mod ave.noncons.mod\ncd \.\./' entropy.csh 
    chmod +x entropy.csh
    entropy.csh >& entropy.out
# entropy.out
#Coverage = 0.17 Length = 12
#Transition parameters:gamma=0.170000,omega=12.000000, mu=0.083333, nu=0.017068
#Relative entropy: H=0.857449 bits/site
#Expected min. length: L_min=12.298748 sites
#Expected max. length: L_max=8.165741 sites
#Phylogenetic information threshold: PIT=L_min*H=10.545544 bits

#### !!! THESE PARAMETERS BELOW WERE THOSE THAT WERE FINALLY USED ####
# These are the same as for danRer2 and give the targeted L_min*H value.
# This is from treeRun2.
#Coverage = 0.32 Length = 18
#Transition parameters:gamma=0.320000,omega=18.000000, mu=0.055556, nu=0.026144
#Relative entropy: H=0.818130 bits/site
#Expected min. length: L_min=12.025818 sites
#Expected max. length: L_max=9.281106 sites
#Phylogenetic information threshold: PIT=L_min*H=9.838688 bits
###

#Coverage = 0.32 Length = 20
#Transition parameters:gamma=0.320000,omega=20.000000, mu=0.050000, nu=0.023529
#Relative entropy: H=0.795926 bits/site
#Expected min. length: L_min=12.724131 sites
#Expected max. length: L_max=9.927736 sites
#Phylogenetic information threshold: PIT=L_min*H=10.127467 bits

#Coverage = 0.35 Length = 18
#Transition parameters:gamma=0.350000,omega=18.000000, mu=0.055556, nu=0.029915
#Relative entropy: H=0.827604 bits/site
#Expected min. length: L_min=11.542637 sites
#Expected max. length: L_max=9.061627 sites
#Phylogenetic information threshold: PIT=L_min*H=9.552732 bits

# need to iterate and get the right coverage and parameters
# try running phastCons below with parameters used above and check the
# coverage of coding regions by the most conserved elements
    # Create cluster dir to do main phastCons run
    ssh pk
    mkdir -p \
       /san/sanvol1/scratch/danRer4/multiz7way.2006-05-28/phastCons/consRun
    cd /san/sanvol1/scratch/danRer4/multiz7way.2006-05-28/phastCons/consRun
    cp -p ../treeRun2/ave.*.mod .
    cp -p ../treeRun2/ave.*.mod \
       /cluster/data/danRer4/bed/multiz7way.2006-05-28/phastCons
    mkdir ppRaw bed
    # Create script to run phastCons with right parameters
    #   This job is I/O intensive in its output files, thus it is all
    #   working over in /scratch/tmp/
    # Use the expected length and target coverage determined above and 
    # the corresponding average conserved and nonconserved models
    cat > doPhast.csh << '_EOF_'
#!/bin/csh -fe
mkdir /scratch/tmp/${2}
cp -p ../ss/${1}/${2}.ss ave.*.mod /scratch/tmp/${2}
pushd /scratch/tmp/${2} > /dev/null
/cluster/bin/phast/x86_64/phastCons ${2}.ss ave.cons.mod,ave.noncons.mod \
        --expected-length 18 --target-coverage 0.32 --quiet \
        --seqname ${1} --idpref ${1} --viterbi ${2}.bed --score > ${2}.pp
popd > /dev/null
mkdir -p ppRaw/${1}
mkdir -p bed/${1}
mv /scratch/tmp/${2}/${2}.pp ppRaw/${1}
mv /scratch/tmp/${2}/${2}.bed bed/${1}
rm /scratch/tmp/${2}/ave.*.mod
rm /scratch/tmp/${2}/${2}.ss
rmdir /scratch/tmp/${2}
'_EOF_'
    # emacs happy
    chmod a+x doPhast.csh

    #   root1 == chrom name, file1 == ss file name without .ss suffix
    # Create gsub file
cat > template << '_EOF_'
#LOOP
doPhast.csh $(root1) $(file1)
#ENDLOOP
'_EOF_'
   #   happy emacs

   # Create parasol batch and run it
   ls -1 ../ss/chr*/chr*.ss | sed 's/.ss$//' > in.list

   gensub2 in.list single template jobList
   para create jobList
   para try/check/push/etc.
   para time
# Completed: 191 of 191 jobs
# CPU time in finished jobs:       4660s      77.67m     1.29h    0.05d  0.000 y
# IO & Wait Time:                  2927s      48.78m     0.81h    0.03d  0.000 y
# Average job time:                  40s       0.66m     0.01h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:              83s       1.38m     0.02h    0.00d
# Submission to last job:          2246s      37.43m     0.62h    0.03d
 
# combine predictions and transform scores to be in 0-1000 interval
   ssh kkstore04
   cd /san/sanvol1/scratch/danRer4/multiz7way.2006-05-28/phastCons/consRun
   #   The sed's and the sort get the file names in chrom,start order 
   # (Hiram tricks -- split into columns on [.-/] with
   #    identifying x,y,z, to allow column sorting and
   #    restoring the filename.  Warning: the sort column
   # will depend on how deep you are in the dir
   find ./bed -type f | sed -e "s#/# x #g; s#\.# y #g; s#-# z #g" \
        | sort -k7,7 -k9,9n \
        | sed -e "s# z #-#g; s# y #\.#g; s# x #/#g" | xargs cat \
        | awk '{printf "%s\t%d\t%d\tlod=%d\t%s\n", $1, $2, $3, $5, $5;}' \
        | /cluster/bin/scripts/lodToBedScore /dev/stdin > mostConserved.bed
    #   ~ 1 minute
    cp -p mostConserved.bed \
        /cluster/data/danRer4/bed/multiz7way.2006-05-28/phastCons
# Figure out how much is actually covered by the mostConserved data as so:
    cd /cluster/data/danRer4
    faSize */chr*.fa
    # 1774660131 bases (175779328 N's 1598880803 real 816338509 upper 
    # 782542294 lower) in 28 sequences in 28 files
    # Total size: mean 63380719.0 sd 33877121.9 min 16596 (chrM) 
    # max 208014280 (chrNA_random) median 59765243
    # The non-N size is 1598880803 bases
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/phastCons
    awk '{sum+=$3-$2}
END{printf "%% %.2f = 100.0*%d/1598880803\n",100.0*sum/1598880803,sum}' \
        mostConserved.bed
    -target-coverage 0.32: % 3.18 = 100.0*50871950/1598880803 length=18 

    # want to aim for 65% coverage of coding regions
    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way/phastCons
    # get an or of refGene and mgcGenes CDS regions
    featureBits danRer4 refGene:cds mgcGenes:cds -or -bed=refSeqOrMgcCds.bed
    # 11770580 bases of 1626093931 (0.724%) in intersection

    featureBits danRer4 refSeqOrMgcCds.bed mostConserved.bed -enrichment
    # refSeqOrMgcCds.bed 0.724%, mostConserved.bed 3.128%, both 0.463%, 
    # cover 63.94%, enrich 20.44x

    # for danRer3:
    featureBits danRer3 refSeqOrMgcCdsDanRer3.bed \
      /cluster/data/danRer3/bed/multiz5way/mostConserved.bed -enrichment
    # refSeqOrMgcCdsDanRer3.bed 0.714%, 
    # /cluster/data/danRer3/bed/multiz5way/mostConserved.bed 2.998%, 
    # both 0.474%, cover 66.40%, enrich 22.14x
    # so use this result for -target-coverage=0.32 -expected-lengths=18
    # with L_min*H entropy (PIT) value of 9.84 (aiming for around 9.8) and
    # 63.9% coverage of coding regions with most conserved elements
    # (aiming for about 65%)
    # Load most conserved track into database
    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/phastCons
    hgsql -e 'drop table phastConsElements;' danRer4
    hgLoadBed danRer4 phastConsElements mostConserved.bed
    # Loaded 676058 elements of size 5
    featureBits danRer4 mgcGenes:cds phastConsElements -enrichment
    # mgcGenes:cds 0.560%, phastConsElements 3.128%, both 0.366%, 
    # cover 65.36%, enrich 20.89x
    # Create merged posterier probability file and wiggle track data files
    # the sed business gets the names sorted by chromName, chromStart
    # so that everything goes in numerical order into wigEncode
    ssh kkstore04
    cd /san/sanvol1/scratch/danRer4/multiz7way.2006-05-28/phastCons/consRun
    find ./ppRaw -type f | sed -e "s#/# x #g; s#\.# y #g; s#-# z #g" \
        | sort -k7,7 -k9,9n \
        | sed -e "s# z #-#g; s# y #\.#g; s# x #/#g" | xargs cat \
        | wigEncode stdin phastCons7way.wig phastCons7way.wib
    # takes a few minutes
    ls -l phastCons*
    # -rw-rw-r--  1 hartera protein 255524779 May 29 19:49 phastCons7way.wib
    # -rw-rw-r--  1 hartera protein  61525690 May 29 19:49 phastCons7way.wig
    cp -p phastCons7way.wi? /cluster/data/danRer4/bed/multiz7way/phastCons

    # Load gbdb and database with wiggle.
    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/phastCons
    mkdir -p /gbdb/danRer4/wib
    rm /gbdb/danRer4/wib/phastCons7way.wib
    ln -s `pwd`/phastCons7way.wib /gbdb/danRer4/wib/phastCons7way.wib
    # use this if need to reload table
    hgsql -e 'drop table phastCons7way;' danRer4
    # load table
    hgLoadWiggle danRer4 phastCons7way phastCons7way.wig

    #  Create histogram to get an overview of all the data
    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/phastCons
    bash
    time hgWiggle -doHistogram \
        -hBinSize=0.001 -hBinCount=1000 -hMinVal=0.0 -verbose=2 \
            -db=danRer4 phastCons7way > histogram.data 2>&1
# real    0m30.234s
# user    0m23.721s
# sys     0m3.234s

        #   create plot of histogram:
    cat << '_EOF_' > histo.gp
set terminal png small color \
        x000000 xffffff xc000ff x66ff66 xffff00 x00ffff xff0000
set size 1.4, 0.8
set key left box
set grid noxtics
set grid ytics
set title " Zebrafish danRer4 Histogram phastCons7 track"
set xlabel " phastCons7 score"
set ylabel " Relative Frequency"
set y2label " Cumulative Relative Frequency (CRF)"
set y2range [0:1]
set y2tics
set yrange [0:0.02]

plot "histogram.data" using 2:5 title " RelFreq" with impulses, \
     "histogram.data" using 2:7 axes x1y2 title " CRF" with lines
'_EOF_'

    #   happy emacs
    gnuplot histo.gp > histo.png
    display histo.png &

# add line: wiggle phastCons7way to trackDb.ra for multiz7way to display the
# wiggle for the conservation track.
# check all.joiner for entries for phastCons7way and phastConsElements7way -ok
# copy over html for multiz and edit.

###########################################################################
# PHASTCONS SCORES DOWNLOADABLES FOR 7WAY (DONE, 2006-05-30, hartera)
    #   prepare compressed copy of ascii data values for downloads
    ssh kolossus
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28
    mkdir phastConsDownloads
    cd phastConsDownloads
cat > downloads.csh << 'EOF'
date
cd /san/sanvol1/scratch/danRer4/multiz7way.2006-05-28/phastCons/consRun/ppRaw
foreach chr (`awk '{print $1}' /cluster/data/danRer4/chrom.sizes`)
  echo $chr
  cat `ls -1 $chr/$chr.*.pp | sort -t\. -k2,2n` \
       | nice gzip -c \
   > /cluster/data/danRer4/bed/multiz7way.2006-05-28/phastConsDownloads/$chr.gz
end
date
'EOF'
    # << emacs
    csh -efx downloads.csh >&! downloads.log & tail -f downloads.log
    # Took ~5 minutes.
    md5sum *.gz > md5sum.txt

    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-28/phastConsDownloads
    set dir = /usr/local/apache/htdocs/goldenPath/danRer4/phastCons7wayScores
    mkdir $dir
    ln -s /cluster/data/danRer4/bed/multiz7way.2006-05-28/phastConsDownloads/{*.gz,md5sum.txt} $dir
    # copy over and edit README.txt
    cd $dir
    cp \
 /usr/local/apache/htdocs/goldenPath/danRer3/phastCons5wayScores/README.txt .
    # Clean up after phastCons run.
    ssh kkstore04
    rm /cluster/data/danRer4/bed/multiz7way.2006-05-28/phastCons/*.tab
    rm -r /san/sanvol1/scratch/danRer4/multiz7way.2006-05-28/phastCons

###########################################################################
# CREATED RECIPROCAL BEST NETS AND MAF NETS FOR ALL SPECIES WITH PAIRWISE
# ALIGNMENTS USED FOR MULTIZ MULTIPLE ALIGNMENT 
# (DONE, 2006-05-12 - 2006-05-15 , hartera)
#   for tetNig1, fr1, xenTro2, monDom4, mm8 and hg18.
    ssh kolossus
    mkdir /cluster/data/danRer4/bed/rBestRunForMultiz/
    cd /cluster/data/danRer4/bed/rBestRunForMultiz
    # need to re-run chainNet and keep first output (target-referenced,
    # target-centric nets) and second output that we usually /dev/null
    # (query-referenced, target-centric nets).
cat > rBestNet.csh << 'EOF'
#!/bin/csh -ef
foreach s (tetNig1 fr1 xenTro2 monDom4 mm8 hg18)
  echo "Creating Reciprocal Best Net for $s..."
  set binDir=/cluster/home/hartera/bin/i386
  set dir=/cluster/data/danRer4/bed/blastz.$s/axtChain
  cd $dir
# Run chainNet again, this time keeping the second output:
  chainPreNet danRer4.$s.all.chain.gz /cluster/data/danRer4/chrom.sizes \
       /cluster/data/$s/chrom.sizes stdout \
       | $binDir/chainNet stdin /cluster/data/danRer4/chrom.sizes \
       /cluster/data/$s/chrom.sizes /dev/null stdout | \
       netSyntenic stdin $dir/$s.danRer4_ref.net
# get the other species chains from the other species-referenced 
# (but danRer4-centric) net:
  chainSwap danRer4.$s.all.chain.gz $s.danRer4.all.chain
  netChainSubset -verbose=0 $s.danRer4_ref.net \
       $s.danRer4.all.chain stdout \
       | chainSort stdin $s.danRer4_ref.subset.chain
# Net those (sorted) danRer4 chains, and keep both outputs, to get
# reciprocal best nets referenced to both species:
  chainPreNet $s.danRer4_ref.subset.chain \
       /cluster/data/$s/chrom.sizes /cluster/data/danRer4/chrom.sizes stdout \
       | $binDir/chainNet stdin /cluster/data/$s/chrom.sizes \
       /cluster/data/danRer4/chrom.sizes tmp1 tmp2

  netSyntenic tmp1 $s.danRer4.rbest.net
  netSyntenic tmp2 danRer4.$s.rbest.net
  rm tmp1 tmp2
  nice gzip *.rbest.net
end
'EOF'
    chmod +x rBestNet.csh
    nice rBestNet.csh >& rBestNet.log &
    # Took about 11 minutes to complete.
    # Then make axtNet and mafNet 
cat > makeMafRBestNet.csh << 'EOF'
#!/bin/csh -ef
foreach s (tetNig1 fr1 xenTro2 monDom4 mm8 hg18)
  echo "Creating mafs for $s ..."
  set dir=/cluster/data/danRer4/bed/blastz.$s/axtChain
  set seqDir=/san/sanvol1/scratch
  cd $dir
# extract recriprocal best chains from the zebrafish-other species rbest.net
  echo "Get reciprocal best chains for best zebrafish-$s"
  netChainSubset danRer4.$s.rbest.net.gz danRer4.$s.all.chain.gz \
      danRer4.$s.rbest.chain
# need to make sure this is sorted and assing unique chain IDs
  chainSort danRer4.$s.rbest.chain stdout | chainMergeSort stdin \
       > danRer4.$s.rbest.newids.chain
# need to re-net with new ids 
  chainNet danRer4.$s.rbest.newids.chain /cluster/data/danRer4/chrom.sizes \
          /cluster/data/$s/chrom.sizes danRer4.$s.rbest.newids.net /dev/null
# split reciprocal best chains and net
  chainSplit rBestChain danRer4.$s.rbest.newids.chain 
  netSplit danRer4.$s.rbest.newids.net rBestNet
  mkdir ../axtRBestNet
# make axtNet for reciprocal best
  echo "Making axtRBestNet for $s ..."
  foreach f (rBestNet/*.net)
    netToAxt $f rBestChain/$f:t:r.chain \
      $seqDir/danRer4/danRer4.2bit $seqDir/$s/$s.2bit stdout \
      | axtSort stdin stdout \
      | gzip -c > ../axtRBestNet/$f:t:r.danRer4.$s.net.axt.gz
  end
# make mafNet for reciprocal best
  cd ..
  mkdir mafRBestNet  
  echo "Making mafRBestNet for $s ..."
  foreach f (axtRBestNet/*.danRer4.$s.net.axt.gz)
  axtToMaf -tPrefix=danRer4. -qPrefix=$s. $f \
     /cluster/data/danRer4/chrom.sizes /cluster/data/$s/chrom.sizes stdout \
     | gzip -c > mafRBestNet/$f:t:r:r:r:r:r.maf.gz
  end
end
'EOF'
    chmod +x makeMafRBestNet.csh 
    nice makeMafRBestNet.csh >& mafRBestNet.log &
    # Took about an hour.
    # NOTE: Must use chainSort and chainMergeSort to reassign unique IDs
    # to the chains extracted from the rbest.net and then re-net the chains
    # with the new IDs ortherwise netToAxt crashes due to duplicate chain IDs. 
    # Now do the multiple alignment using reciprocal best mafNets as input
    # for multiz.
    # Load up nets and chains from rBestChain and rBestNet 
    ssh hgwdev
    cd /cluster/data/danRer4/bed/rBestRunForMultiz
    # Nets from Reciprocal Best have no type field or repeat/gap stats so need
    # to add these.
cat > loadRBest.csh << 'EOF'
#!/bin/csh -ef
foreach s (tetNig1 fr1 xenTro2 monDom4 mm8 hg18)
  set dir=/cluster/data/danRer4/bed/blastz.$s/axtChain
  if ($s == "tetNig1") then
     set g = TetNig1
  else if ($s == "fr1") then
     set g = Fr1
  else if ($s == "xenTro2") then
     set g = XenTro2
  else if ($s == "monDom4") then
     set g = MonDom4 
  else if ($s == "mm8") then
     set g = Mm8
  else if ($s == "hg18") then
     set g = Hg18
  endif
# load chains
  echo "Loading chains for $s ..."
  cd $dir/rBestChain
  foreach f (*.chain)
    set c = $f:r
    hgLoadChain danRer4 ${c}_chainRBest${g} $f 
  end   
# load nets
  cd $dir
  echo "Loading nets for $s ..."
# add type field
  netSyntenic danRer4.${s}.rbest.newids.net noClassRBest.net
# add gap/repeat stats to net file using database tables
  netClass -verbose=0 -noAr noClassRBest.net danRer4 $s \
     danRer4.${s}.rbest.withClass.net
  netFilter -minGap=10 danRer4.${s}.rbest.withClass.net \
     | hgLoadNet -verbose=0 danRer4 netRBest${g} stdin
end
'EOF'

    << emacs
    chmod +x loadRBest.csh
    nohup nice loadRBest.csh >& loadRBest.log &

###########################################################################
# MULTIZ7WAY ALIGNMENTS FOR CONSERVATION TRACK - USING RECIPROCAL BEST NETS
# (DONE, 2006-05-18 - 2006-05-24, hartera)
#   for tetNig1, fr1, xenTro2, monDom4, mm8 and hg18.
    ssh kkstore04
    mkdir /cluster/data/danRer4/bed/multiz7way.2006-05-18
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-18

    # copy MAFs to a cluster-friendly server
    # use bluearc as the san is down
    mkdir /cluster/bluearc/danRer4/mafRBestNet
    foreach s (tetNig1 fr1 xenTro2 monDom4 mm8 hg18)
       echo $s
       rsync -av /cluster/data/danRer4/bed/blastz.$s/mafRBestNet/* \
        /cluster/bluearc/danRer4/mafRBestNet/$s/
    end
    # prune the hg17 17way tree to just these 7 and update db names:
    /cluster/bin/phast/tree_doctor \
      --prune-all-but=mouse_mm8,human_hg18,monodelphis_monDom4,xenopus_xenTro1,tetraodon_tetNig1,fugu_fr1,zebrafish_danRer3 \
      --rename="xenopus_xenTro1 -> xenopus_xenTro2 ; zebrafish_danRer3 -> zebrafish_danRer4" \
      /cluster/data/hg18/bed/multiz17way/17way.nh > 7way.nh
    # carefully edit so that danRer4 is first. copy first to new file
    cp 7way.nh 7way_zfishFirst.nh
    # DO THIS LATER AND CREATE FROM TREE WITHOUT DISTANCES
    /cluster/bin/phast/draw_tree 7way_zfishFirst.nh > 7way.ps
    # also made the ps file for the 7way.nh and compared to make sure
    # that the tree with zebrafish at the top looks correct.
    /cluster/bin/phast/all_dists 7way_zfishFirst.nh > 7way.distances
    grep danRer4 7way.distances | sort -k3,3n | \
        awk '{printf ("%.4f\t%s\n", $3, $2)}' > distances.txt
    cat distances.txt
# 1.4749  tetraodon_tetNig1
# 1.5154  fugu_fr1
# 1.7480  human_hg18
# 1.7782  monodelphis_monDom4
# 1.8771  xenopus_xenTro2
# 2.1058  mouse_mm8
    # the order in the browser display will be by tree topology,
    # not by distance, so they will be:
    # danRer4
    # 1.5154  fugu_fr1
    # 1.4749  tetraodon_tetNig1
    # 1.8771  xenopus_xenTro2
    # 1.7782  monodelphis_monDom4
    # 2.1058  mouse_mm8
    # 1.7480  human_hg18

    # create species list and stripped down tree for autoMZ
    sed -e 's/[a-z][a-z]*_//g; s/:[0-9\.][0-9\.]*//g; s/;//' \
        7way_zfishFirst.nh > tree-commas.nh
    sed -e 's/ //g; s/,/ /g' tree-commas.nh > tree.nh
    sed -e 's/[()]//g; s/,/ /g' tree.nh > species.lst
    cp tree-commas.nh 7way.nh

    ssh pk
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-18
    mkdir maf run
    cd run

    # stash binaries
    mkdir penn
    cp -p /cluster/bin/penn/multiz.v11.x86_64/multiz-tba/multiz penn
    cp -p /cluster/bin/penn/multiz.v11.x86_64/multiz-tba/maf_project penn
    cp -p /cluster/bin/penn/multiz.v11.x86_64/multiz-tba/autoMZ penn

cat > autoMultiz.csh << 'EOF'
#!/bin/csh -ef
    set db = danRer4
    set c = $1
    set maf = $2
    set run = `pwd`
    set tmp = /scratch/tmp/$db/multiz.$c
    set pairs = /cluster/bluearc/$db/mafRBestNet
    rm -fr $tmp
    mkdir -p $tmp
    cp ../{tree.nh,species.lst} $tmp
    pushd $tmp
    foreach s (`cat species.lst`)
        set in = $pairs/$s/$c.maf
        set out = $db.$s.sing.maf
        if ($s == $db) then
            continue
        endif
        if (-e $in.gz) then
            zcat $in.gz > $out
        else if (-e $in) then
            cp $in $out
        else
            echo "##maf version=1 scoring=autoMZ" > $out
        endif
    end
    set path = ($run/penn $path); rehash
    $run/penn/autoMZ + T=$tmp E=$db "`cat tree.nh`" $db.*.sing.maf $c.maf
    popd
    cp $tmp/$c.maf $maf
    rm -fr $tmp
'EOF'
    # << emacs
    chmod +x autoMultiz.csh

cat  << 'EOF' > spec
#LOOP
./autoMultiz.csh $(root1) {check out line+ /cluster/data/danRer4/bed/multiz7way.2006-05-18/maf/$(root1).maf}
#ENDLOOP
'EOF'
    # << emacs
    awk '{print $1}' /cluster/data/danRer4/chrom.sizes > chrom.lst
    gensub2 chrom.lst single spec jobList
    para create jobList
    para try, check, push, check etc. ...
    # Took less than 10 minutes to run 
   # Make .jpg for tree and install in htdocs/images/phylo/... don't forget
   # to request a push of that file.  The treeImage setting in trackDb.ra
   # is phylo/danRer4_7way.jpg (relative to htdocs/images).
 #  ssh hgwdev
  # DO LATER
#   cd /cluster/data/danRer4/bed/multiz7way.2006-05-04
#   pstopnm -stdout 7way.ps | pnmtojpeg > danRer4_7way.jpg
   # ask Bob to resize image for Browser track description page.

   # Build maf annotation and load database
   ssh kolossus
   mkdir /cluster/data/danRer4/bed/multiz7way.2006-05-18/anno
   cd /cluster/data/danRer4/bed/multiz7way.2006-05-18/anno
   mkdir maf run
   cd run
   rm -f sizes nBeds
 foreach db (`cat /cluster/data/danRer4/bed/multiz7way.2006-05-18/species.lst`)
      ln -s  /cluster/data/$db/chrom.sizes $db.len
      if (! -e /cluster/data/$db/$db.N.bed) then
        twoBitInfo -nBed /cluster/data/$db/$db.{2bit,N.bed}
      endif
      ln -s  /cluster/data/$db/$db.N.bed $db.bed
      echo $db.bed  >> nBeds
      echo $db.len  >> sizes
  end
    echo date > jobs.csh
    # do smaller jobs first:
    foreach f (`ls -1rS ../../maf/*.maf`)
      echo nice mafAddIRows -nBeds=nBeds -sizes=sizes $f \
        /cluster/data/danRer4/danRer4.2bit ../maf/`basename $f` \
        >> jobs.csh
      echo "echo $f" >> jobs.csh
    end
    echo date >> jobs.csh
    csh -efx jobs.csh >&! jobs.log &
    tail -f jobs.log

    # Load anno/maf  
    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-18/anno/maf
    mkdir -p /gbdb/danRer4/multiz7wayRBest/anno/maf
    ln -s /cluster/data/danRer4/bed/multiz7way.2006-05-18/anno/maf/*.maf \
      /gbdb/danRer4/multiz7wayRBest/anno/maf
    # Reload as not working correctly. 
    hgsql -e 'drop table multiz7wayRBest;' danRer4
    hgsql -e 'delete from extFile where path like "%multiz7wayRBest%";' \
          danRer4
    cat > loadMaf.csh << 'EOF'
date
nice hgLoadMaf -pathPrefix=/gbdb/danRer4/multiz7wayRBest/anno/maf danRer4 multiz7wayRBest
date
'EOF'
    # << emacs
    csh -efx loadMaf.csh >&! loadMaf.log & tail -f loadMaf.log
    # Do the computation-intensive part of hgLoadMafSummary on a workhorse
    # machine and then load on hgwdev:
    ssh kkr7u00
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-18/anno/maf
    cat *.maf \
    | nice hgLoadMafSummary danRer4 -minSize=30000 -mergeGap=1500 \
    -maxSize=200000 -test multiz7wayRBestSummary stdin
    # Created 526386 summary blocks from 1972659 components and 1105457 mafs 
    # from stdin

    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-18/anno/maf
    sed -e 's/mafSummary/multiz7wayRBestSummary/' \
      ~/kent/src/hg/lib/mafSummary.sql \
      > /tmp/multiz7wayRBestSummary.sql
    time nice hgLoadSqlTab danRer4 multiz7wayRBestSummary \
         /tmp/multiz7wayRBestSummary.sql multiz7wayRBestSummary.tab
    # 0.000u 0.000s 0:07.56 0.0%      0+0k 0+0io 4pf+0w
    rm *.tab /tmp/multiz7wayRBestSummary.sql
  #  ln -s multiz7way.2006-05-18 /cluster/data/danRer4/bed/multiz7way
  #  ln -s /cluster/data/danRer4/bed/multiz7way.2006-05-18/danRer4_7way.jpg \
   #       /usr/local/apache/htdocs/images/phylo/danRer4_7way.jpg
    # change permissions for display if not already readable to all
  #  chmod +r /usr/local/apache/htdocs/images/phylo/danRer4_7way.jpg

# check for all.joiner entry for 7-way - it is there already.
# add trackDb.ra entry for danRer4:

###########################################################################
# PHYLO-HMM (PHASTCONS) CONSERVATION TRACK FOR 7-WAY ALIGNMENT USING MAFS
# FROM RECIPROCAL BEST NET (DONE, 2006-05-19 - 2005-05-24, hartera)
   ssh kkstore04
   mkdir /cluster/data/danRer4/bed/multiz7way.2006-05-18/phastCons
   cd /cluster/data/danRer4/bed/multiz7way.2006-05-18/phastCons

   # create a starting-tree.mod based on chr14 (92 Mb)
   # chr14 is the largest chrom apart from chrNA_random
   /cluster/bin/phast/$MACHTYPE/msa_split ../maf/chr14.maf \
        --refseq ../../../14/chr14.fa --in-format MAF \
        --windows 100000000,1000 --out-format SS \
        --between-blocks 5000 --out-root s1
   /cluster/bin/phast/$MACHTYPE/phyloFit -i SS s1.*.ss \
        --tree "`cat ../tree-commas.nh`" \
        --out-root starting-tree
   # took less than a minute
   rm s1.*ss
   
   # Get genome-wide average GC content (for all species together,
   # not just the reference genome).  If you have a globally
   # estimated tree model, as above, you can get this from the
   # BACKGROUND line in the .mod file.  E.g.,
# ALPHABET: A C G T
# ...
# BACKGROUND: 0.309665 0.189697 0.189720 0.310918
   # add up the C and G:
   grep BACKGROUND starting-tree.mod | awk '{printf "%0.3f\n", $3 + $4;}'
   # 0.379 is the GC content. This is used in the -gc argument below.
   # If you do *not* have a global tree model and you do not know your
   # GC content, you can get it directly from the MAFs with a command
   # like:
   /cluster/bin/phast/$MACHTYPE/msa_view \
    --aggregate danRer4,tetNig1,fr1,xenTro2,monDom4,mm8,hg18 -i MAF \
    -S /cluster/data/danRer4/bed/multiz7way/maf/chr*.maf > maf_summary.txt
   # This gives a GC content of 0.426 so use this as it is from mafs for
   # the whole genome.
   # break up the genome-wide MAFs into pieces on the san filesystem
   ssh pk
   # should use a directory on the san but it is down and para create is
   # not working on kk.
   set WINDOWS=/cluster/bluearc/danRer4/multiz7way.2006-05-18/phastCons/ss
   mkdir -p $WINDOWS
   cd $WINDOWS
   cat << 'EOF' > doSplit.csh
#!/bin/csh -ef
set MAFS = /cluster/data/danRer4/bed/multiz7way.2006-05-18/maf
set WINDOWS=/cluster/bluearc/danRer4/multiz7way.2006-05-18/phastCons/ss
cd $WINDOWS
set c = $1
echo $c
rm -fr $c
mkdir $c
set N = `echo $c | sed -e 's/chr//'`
/cluster/bin/phast/$MACHTYPE/msa_split $MAFS/$c.maf -i MAF \
       -M /cluster/data/danRer4/$N/$c.fa \
       -o SS -w 10000000,0 -I 1000 -B 5000 -r $c/$c
echo "Done" >> $c.done
'EOF'
# << emacs
   chmod +x doSplit.csh
   rm -f jobList
   foreach c (`cat /cluster/data/danRer4/chrom.lst`)
    echo "doSplit.csh chr${c} {check out line+ $WINDOWS/chr$c.done}" >> jobList
   end

   para create jobList
   para push, check etc.
   para time
# Completed: 28 of 28 jobs
# CPU time in finished jobs:        847s      14.12m     0.24h    0.01d  0.000 y
# IO & Wait Time:                  9741s     162.35m     2.71h    0.11d  0.000 y
# Average job time:                 378s       6.30m     0.11h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:             539s       8.98m     0.15h    0.01d
# Submission to last job:           581s       9.68m     0.16h    0.01d

    # Create a random list of 50 1 mb regions (do not use chrNA and chrUn)

    ls -1l chr*/chr*.ss | grep -v NA | grep -v Un | \
       awk '$5 > 4000000 {print $9;}' | randomLines stdin 50 ../randomSs.list
    
    # Set up parasol directory to calculate trees on these 50 regions
    ssh pk
    set dir = /cluster/bluearc/danRer4/multiz7way.2006-05-18/phastCons
    mkdir -p $dir
    cd $dir
    # now set up cluster job to estimate model parameters.  Parameters
    # will be estimated separately for each alignment fragment then
    # will be combined across fragments. Tuning this loop should come
    # back to here to recalculate. Tuning target-coverage and expected-length.
    # Create little script that calls phastCons with right arguments

    cat > makeTree.csh << 'EOF'
#!/bin/csh -fe
set C = $1:h
set treeRun = $2
set cov = $3
set len = $4
set dir = /cluster/bluearc/danRer4/multiz7way.2006-05-18/phastCons
mkdir -p $dir/$treeRun/log/${C} $dir/$treeRun/tree/${C}
/cluster/bin/phast/x86_64/phastCons $dir/ss/$1 \
  /cluster/data/danRer4/bed/multiz7way.2006-05-18/phastCons/starting-tree.mod \
  --gc 0.426 --nrates 1,1 --no-post-probs --ignore-missing \
  --expected-length $len --target-coverage $cov \
  --quiet --log $dir/$treeRun/log/$1 --estimate-trees $dir/$treeRun/tree/$1
'EOF'
    # << emacs
    chmod a+x makeTree.csh
    # Make sure that the correct GC content is substituted in here. Notice
    # the target coverage of 0.17. Here we are going to aim
    # for 65% coverage of coding regions by conserved elements.
    # Create gensub file
# need to add cov and len parameters
    cat > template << '_EOF_'
#LOOP
makeTree.csh $(path1) $(path2)
#ENDLOOP
'_EOF_'
    #   happy emacs
    # Make cluster job and run it
    echo "treeRun1 0.17 12" > tree.lst
    echo "treeRun2 0.32 18" >> tree.lst
    echo "treeRun3 0.32 20" >> tree.lst
    echo "treeRun4 0.35 18" >> tree.lst
    gensub2 randomSs.list tree.lst template jobList
    para create jobList
    para try,check,push,check etc.
# para time
# Completed: 200 of 200 jobs
# CPU time in finished jobs:      45500s     758.33m    12.64h    0.53d  0.001 y
# IO & Wait Time:                 31478s     524.64m     8.74h    0.36d  0.001 y
# Average job time:                 385s       6.41m     0.11h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:             622s      10.37m     0.17h    0.01d
# Submission to last job:           821s      13.68m     0.23h    0.01d
    # try again, mkdir test2. if aim for about 5% coverage and for chr1 on
    # hg18, netDanRer4 covers about 31% of bases then 0.05/0.30 = 0.156
    # want length of about 20 bp to influence the model towards detecting
    # shorter conserved regions such as TFBSs.
    cd test2
    echo "treeRun5 0.156 20" > tree.lst
    gensub2 ../randomSs.list tree.lst template jobList
    para create jobList
    cd test3
    echo "treeRun6 0.156 15" > tree.lst
    gensub2 ../randomSs.list tree.lst template jobList
    para create jobList
    cd test4
    # increase coverage and compensate a bit by lowering the expected length
    echo "treeRun7 0.25 8" > tree.lst
    gensub2 ../randomSs.list tree.lst template jobList
    para create jobList
    cd test5
    echo "treeRun8 0.35 12" > tree.lst
    gensub2 ../randomSs.list tree.lst template jobList
    para create jobList
    cd test6
    echo "treeRun9 0.5 20" > tree.lst
    gensub2 ../randomSs.list tree.lst template jobList
    para create jobList
    cd test7
    echo "treeRun10 0.5 24" > tree.lst
    gensub2 ../randomSs.list tree.lst template jobList
    para create jobList
    cd test8
    echo "treeRun11 0.45 22" > tree.lst
    echo "treeRun12 0.5 26" >> tree.lst
    echo "treeRun13 0.5 28" >> tree.lst
    gensub2 ../randomSs.list tree.lst template jobList
    para create jobList
    cd test9
    echo "treeRun14 0.45 24" > tree.lst
    echo "treeRun15 0.45 20" >> tree.lst
    gensub2 ../randomSs.list tree.lst template jobList
    para create jobList
    cd test10
    echo "treeRun16 0.40 24" > tree.lst
    echo "treeRun17 0.40 20" >> tree.lst
    echo "treeRun18 0.42 20" >> tree.lst
    gensub2 ../randomSs.list tree.lst template jobList
    para create jobList
    cd test11
    echo "treeRun19 0.38 24" > tree.lst
    echo "treeRun20 0.38 22" >> tree.lst
    echo "treeRun21 0.38 20" >> tree.lst
    gensub2 ../randomSs.list tree.lst template jobList
    para create jobList
    # Now combine parameter estimates.  We can average the .mod files
    # Now combine parameter estimates.  We can average the .mod files
    # using phyloBoot.  This must be done separately for the conserved
    # and nonconserved models
    set dir = /cluster/bluearc/danRer4/multiz7way.2006-05-18/phastCons
    foreach d ($dir/treeRun*)
       cd $d
       ls tree/chr*/*.cons.mod > cons.txt
       /cluster/bin/phast/$MACHTYPE/phyloBoot --read-mods '*cons.txt' \
         --output-average ave.cons.mod > cons_summary.txt
       ls tree/chr*/*.noncons.mod > noncons.txt
       /cluster/bin/phast/$MACHTYPE/phyloBoot --read-mods '*noncons.txt' \
         --output-average ave.noncons.mod > noncons_summary.txt
    end
       #   measuring entropy
    #   consEntropy <target coverage> <expected lengths>
    #            ave.cons.mod ave.noncons.mod --NH 9.78
    #   never stops with the --NH argument
    # target entropy should be L_min*H=9.8 bits, (between 9.5 to 10.5 is ok)
    # the expected length that produces this entropy is the one
    # to use for phastCons.
    # foreach treeRun, set the appropriate coverage and length
    # file: treeRunN cov len
    # use awk to split up
    cd /cluster/bluearc/danRer4/multiz7way.2006-05-18/phastCons
    cp tree.lst entropy.csh 
    perl -pi.bak -e 's/^(treeRun[0-9]+)\s*([0-9\.]+)\s*([0-9]+)/echo \"Coverage = $2 Length = $3\"\ncd $1\n\/cluster\/bin\/phast\/x86_64\/consEntropy $2 $3 ave.cons.mod ave.noncons.mod\ncd \.\./' entropy.csh 
    chmod +x entropy.csh
    entropy.csh >& entropy.out
# entropy.out
#Coverage = 0.17 Length = 12
#Transition parameters:gamma=0.170000, omega=12.000000, mu=0.083333, nu=0.017068
#Relative entropy: H=0.782279 bits/site
#Expected min. length: L_min=13.655129 sites
#Expected max. length: L_max=8.801144 sites
#Phylogenetic information threshold: PIT=L_min*H=10.682123 bits

#Coverage = 0.32 Length = 18
#Transition parameters:gamma=0.320000, omega=18.000000, mu=0.055556, nu=0.026144
#Relative entropy: H=0.757117 bits/site
#Expected min. length: L_min=13.055080 sites
#Expected max. length: L_max=9.912578 sites
#Phylogenetic information threshold: PIT=L_min*H=9.884225 bits

#Coverage = 0.32 Length = 20
#Transition parameters:gamma=0.320000, omega=20.000000, mu=0.050000, nu=0.023529
#Relative entropy: H=0.736191 bits/site
#Expected min. length: L_min=13.815340 sites
#Expected max. length: L_max=10.615242 sites
#Phylogenetic information threshold: PIT=L_min*H=10.170732 bits

#Coverage = 0.35 Length = 18
#Transition parameters:gamma=0.350000, omega=18.000000, mu=0.055556, nu=0.029915
#Relative entropy: H=0.768872 bits/site
#Expected min. length: L_min=12.471015 sites
#Expected max. length: L_max=9.642561 sites
#Phylogenetic information threshold: PIT=L_min*H=9.588610 bits

#Coverage = 0.156 Length = 20
#Transition parameters:gamma=0.156000, omega=20.000000, mu=0.050000, nu=0.009242
#Relative entropy: H=0.676147 bits/site
#Expected min. length: L_min=17.857722 sites
#Expected max. length: L_max=12.694666 sites
#Phylogenetic information threshold: PIT=L_min*H=12.074436 bits

#Coverage = 0.156 Length = 15
#Transition parameters:gamma=0.156000, omega=15.000000, mu=0.066667, nu=0.012322
#Relative entropy: H=0.726430 bits/site
#Expected min. length: L_min=15.713919 sites

#Transition parameters: gamma=0.250000, omega=8.000000, mu=0.125000, nu=0.041667
#Relative entropy: H=0.950194 bits/site
#Expected min. length: L_min=8.951612 sites
#Expected max. length: L_max=5.560228 sites
#Phylogenetic information threshold: PIT=L_min*H=8.505767 bits

#Coverage = 0.5 Length = 20
#Transition parameters:gamma=0.500000, omega=20.000000, mu=0.050000, nu=0.050000
#Relative entropy: H=0.817081 bits/site
#Expected min. length: L_min=10.397809 sites
#Expected max. length: L_max=9.006386 sites
#Phylogenetic information threshold: PIT=L_min*H=8.495855 bits

# Coverage = 0.5 Length = 24
#Transition parameters:gamma=0.500000, omega=24.000000, mu=0.041667, nu=0.041667
#Relative entropy: H=0.772807 bits/site
#Expected min. length: L_min=11.706841 sites
#Expected max. length: L_max=10.170845 sites
#Phylogenetic information threshold: PIT=L_min*H=9.047124 bits

# Coverage = 0.5 Length = 26
#Transition parameters:gamma=0.500000,omega=26.000000, mu=0.038462, nu=0.038462
#Relative entropy: H=0.755159 bits/site
#Expected min. length: L_min=12.299010 sites
#Expected max. length: L_max=10.697444 sites
#Phylogenetic information threshold: PIT=L_min*H=9.287712 bits

#Coverage = 0.5 Length = 28
#Transition parameters:gamma=0.500000,omega=28.000000, mu=0.035714, nu=0.035714
#Relative entropy: H=0.739661 bits/site
#Expected min. length: L_min=12.856932 sites
#Expected max. length: L_max=11.193931 sites
#Phylogenetic information threshold: PIT=L_min*H=9.509775 bits

########USED THESE PARAMETERS##################
#Coverage = 0.45 Length = 24
#Transition parameters:gamma=0.450000, omega=24.000000, mu=0.041667, nu=0.034091
#Relative entropy: H=0.749572 bits/site
#Expected min. length: L_min=12.663020 sites
#Expected max. length: L_max=10.634682 sites
#Phylogenetic information threshold: PIT=L_min*H=9.491841 bits

#Coverage = 0.40 Length = 24
#Transition parameters:gamma=0.400000, omega=24.000000, mu=0.041667, nu=0.027778
#Relative entropy: H=0.730161 bits/site
#Expected min. length: L_min=13.607002 sites
#Expected max. length: L_max=11.092981 sites
#Phylogenetic information threshold: PIT=L_min*H=9.935307 bits

#Coverage = 0.38 Length = 20
#Transition parameters:gamma=0.380000, omega=20.000000, mu=0.050000, nu=0.030645
#Relative entropy: H=0.758676 bits/site
#Expected min. length: L_min=12.652818 sites
#Expected max. length: L_max=10.063048 sites
#Phylogenetic information threshold: PIT=L_min*H=9.599385 bits
 
#Coverage = 0.38 Length = 24
#Transition parameters:gamma=0.380000, omega=24.000000, mu=0.041667, nu=0.025538
#Relative entropy: H=0.723105 bits/site
#Expected min. length: L_min=13.987286 sites
#Expected max. length: L_max=11.279443 sites
#Phylogenetic information threshold: PIT=L_min*H=10.114270 bits

    # Create cluster dir to do main phastCons run
    ssh pk
    mkdir -p \
       /cluster/bluearc/danRer4/multiz7way.2006-05-18/phastCons/consRun
    cd /cluster/bluearc/danRer4/multiz7way.2006-05-18/phastCons/consRun
    cp -p ../treeRun1/ave.*.mod .
    cp -p ../treeRun1/ave.*.mod \
       /cluster/data/danRer4/bed/multiz7way.2006-05-18/phastCons
    mkdir ppRaw bed
    # Create script to run phastCons with right parameters
    #   This job is I/O intensive in its output files, thus it is all
    #   working over in /scratch/tmp/
    # Use the expected length and target coverage determined above and 
    # the corresponding average conserved and nonconserved models
    cat > doPhast.csh << '_EOF_'
#!/bin/csh -fe
mkdir /scratch/tmp/${2}
cp -p ../ss/${1}/${2}.ss ave.*.mod /scratch/tmp/${2}
pushd /scratch/tmp/${2} > /dev/null
/cluster/bin/phast/x86_64/phastCons ${2}.ss ave.cons.mod,ave.noncons.mod \
        --expected-length 18 --target-coverage 0.32 --quiet \
        --seqname ${1} --idpref ${1} --viterbi ${2}.bed --score > ${2}.pp
popd > /dev/null
mkdir -p ppRaw/${1}
mkdir -p bed/${1}
mv /scratch/tmp/${2}/${2}.pp ppRaw/${1}
mv /scratch/tmp/${2}/${2}.bed bed/${1}
rm /scratch/tmp/${2}/ave.*.mod
rm /scratch/tmp/${2}/${2}.ss
rmdir /scratch/tmp/${2}
'_EOF_'
    # emacs happy
    chmod a+x doPhast.csh

    #   root1 == chrom name, file1 == ss file name without .ss suffix
    # Create gsub file
cat > template << '_EOF_'
#LOOP
doPhast.csh $(root1) $(file1)
#ENDLOOP
'_EOF_'
   #   happy emacs

   # Create parasol batch and run it
   ls -1 ../ss/chr*/chr*.ss | sed 's/.ss$//' > in.list

   gensub2 in.list single template jobList
   para create jobList
   para try/check/push/etc.
   para time
# Completed: 191 of 191 jobs
# CPU time in finished jobs:       4421s      73.69m     1.23h    0.05d  0.000 y
# IO & Wait Time:                121036s    2017.26m    33.62h    1.40d  0.004 y
# Average job time:                 657s      10.95m     0.18h    0.01d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:             726s      12.10m     0.20h    0.01d
# Submission to last job:           874s      14.57m     0.24h    0.01d
 
# combine predictions and transform scores to be in 0-1000 interval
   ssh kkstore04
   cd /cluster/bluearc/danRer4/multiz7way.2006-05-18/phastCons/consRun
   #   The sed's and the sort get the file names in chrom,start order 
   # (Hiram tricks -- split into columns on [.-/] with
   #    identifying x,y,z, to allow column sorting and
   #    restoring the filename.  Warning: the sort column
   # will depend on how deep you are in the dir
   find ./bed -type f | sed -e "s#/# x #g; s#\.# y #g; s#-# z #g" \
        | sort -k7,7 -k9,9n \
        | sed -e "s# z #-#g; s# y #\.#g; s# x #/#g" | xargs cat \
        | awk '{printf "%s\t%d\t%d\tlod=%d\t%s\n", $1, $2, $3, $5, $5;}' \
        | /cluster/bin/scripts/lodToBedScore /dev/stdin > mostConserved.bed
    cp -p mostConserved.bed \
        /cluster/data/danRer4/bed/multiz7way.2006-05-18/phastCons
# Figure out how much is actually covered by the mostConserved data as so:
    cd /cluster/data/danRer4
    faSize */chr*.fa
    # 1774660131 bases (175779328 N's 1598880803 real 816338509 upper 
    # 782542294 lower) in 28 sequences in 28 files
    # Total size: mean 63380719.0 sd 33877121.9 min 16596 (chrM) 
    # max 208014280 (chrNA_random) median 59765243
    # 782542294 lower) in 28 sequences in 28 files
    # Total size: mean 63380719.0 sd 33877121.9 min 16596 (chrM) 
    # max 208014280 (chrNA_random) median 59765243
    # The non-N size is 1598880803 bases
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-18/phastCons
    awk '{sum+=$3-$2}
END{printf "%% %.2f = 100.0*%d/1598880803\n",100.0*sum/1598880803,sum}' \
        mostConserved.bed
    -target-coverage 0.17: % 1.51 = 100.0*24186350/1598880803 length=12
    -target-coverage 0.156: % 1.44 = 100.0*22973222/1598880803 length=20
    -target-coverage 0.156: % 1.32 = 100.0*21177329/1598880803 length=15
    -target-coverage 0.25: % 1.32 = 100.0*21104503/1598880803 length=8
    -target-coverage 0.32: % 1.88 = 100.0*30014509/1598880803 length=20
    -target-coverage 0.5: % 3.00 = 100.0*47931076/1598880803 length=20
    -target-coverage 0.5: % 2.95 = 100.0*47170018/1598880803 length=24
    -target-coverage 0.5: % 2.24 = 100.0*35801661/1598880803 length=28
    -target-coverage 0.45: % 2.50 = 100.0*39965003/1598880803 length=24
    -target-coverage 0.40: % 2.22 = 100.0*35436744/1598880803 length=24
    -target-coverage 0.38: % 2.12 = 100.0*33911465/1598880803 length=20
    -target-coverage 0.38: % 2.13 = 100.0*33986115/1598880803 length=24

    # want to aim for 65% coverage of coding regions
    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-18/phastCons
    # get an or of refGene and mgcGenes CDS regions
    featureBits danRer4 refGene:cds mgcGenes:cds -or -bed=refSeqOrMgcCds.bed
    # 11753378 bases of 1626093931 (0.723%) in intersection

#    featureBits danRer3 refGene:cds mgcGenes:cds -or \
 #               -bed=refSeqOrMgcCdsDanRer3.bed
    # 11633092 bases of 1630323462 (0.714%) in intersection
    featureBits danRer4 refSeqOrMgcCds.bed mostConserved.bed -enrichment
    # refSeqOrMgcCds.bed 0.723%, mostConserved.bed 1.487%, both 0.332%, 
    # cover 45.97%, enrich 30.90x
    # for length = 12 and cov = 0.17 PIT=10.7
    # refSeqOrMgcCds.bed 0.723%, mostConserved.bed 1.846%, both 0.388%, 
    # cover 53.74%, enrich 29.12x
    # for length = 20 and cov = 0.156 PIT=12.1
    # refSeqOrMgcCds.bed 0.723%, mostConserved.bed 1.413%, both 0.333%, 
    # cover 46.04%, enrich 32.59x
    # for length = 15 and cov = 0.156 PIT=11.4
    # refSeqOrMgcCds.bed 0.723%, mostConserved.bed 1.302%, both 0.313%, 
    # cover 43.36%, enrich 33.30x
    # decrease length and increase coverage to compensate
    # for length = 8 and cov = 0.25 PIT=8.5, PIT is too low
    # refSeqOrMgcCds.bed 0.723%, mostConserved.bed 1.298%, both 0.304%, 
    # cover 42.06%, enrich 32.40x
    # try length = 20 and cov = 0.32 PIT=10.8
    # refSeqOrMgcCds.bed 0.723%, mostConserved.bed 1.846%, both 0.388%, 
    # cover 53.74%, enrich 29.12x
    # length = 20 and cov = 0.5 PIT=8.5
    # refSeqOrMgcCds.bed 0.723%, mostConserved.bed 2.948%, both 0.459%, 
    # cover 63.53%, enrich 21.55x
    # coverage good, need to increase the PIT value so increase the length.
    # length = 24 and cov = 0.5 PIT=9.05
    # refSeqOrMgcCds.bed 0.723%, mostConserved.bed 2.901%, both 0.458%, 
    # cover 63.35%, enrich 21.84x
    # length = 28 and cov = 0.5 PIT=9.5
    # refSeqOrMgcCds.bed 0.724%, mostConserved.bed 2.202%, both 0.431%, 
    # cover 59.57%, enrich 27.06x
    # length = 24 and cov = 0.45 PIT=9.5
    featureBits danRer4 refGene:cds mgcGenes:cds -or -bed=refSeqOrMgcCds.bed
    # 11770580 bases of 1626093931 (0.724%) in intersection
    featureBits danRer4 refSeqOrMgcCds.bed mostConserved.bed -enrichment
    # refSeqOrMgcCds.bed 0.724%, mostConserved.bed 2.458%, both 0.438%, 
    # cover 60.57% enrich 24.64x
    # length = 20 and cov = 0.38 PIT=9.6
    # refSeqOrMgcCds.bed 0.724%, mostConserved.bed 2.085%, both 0.411%, 
    # cover 56.76%, enrich 27.22x
    # length = 24 and cov = 0.38 PIT=10.1
    # refSeqOrMgcCds.bed 0.724%, mostConserved.bed 2.090%, both 0.413%, 
    # cover 57.07%, enrich 27.30x
    # with L_min*H entropy (PIT) value of 9.84 (aiming for around 9.8) and
    # 53.3% coverage of coding regions with most conserved elements
    # (aiming for about 65%)

    # use consRun14 length = 24 cov=0.45
    # Load most conserved track into database
    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-18/phastCons
    hgLoadBed danRer4 phastConsRBestElements mostConserved.bed
    # Loaded  elements of size 5
    featureBits danRer4 mgcGenes:cds phastConsRBestElements -enrichment
    # mgcGenes:cds 0.560%, phastConsRBestElements 2.458%, both 0.349%, 
    # cover 62.23%, enrich 25.32x
    # Create merged posterier probability file and wiggle track data files
    # the sed business gets the names sorted by chromName, chromStart
    # so that everything goes in numerical order into wigEncode
    ssh kkstore04
    cd /cluster/bluearc/danRer4/multiz7way.2006-05-18/phastCons/consRun14
    find ./ppRaw -type f | sed -e "s#/# x #g; s#\.# y #g; s#-# z #g" \
        | sort -k7,7 -k9,9n \
        | sed -e "s# z #-#g; s# y #\.#g; s# x #/#g" | xargs cat \
        | wigEncode stdin phastConsRBest7way.wig phastConsRBest7way.wib
    # Converted stdin, upper limit 1.00, lower limit 0.00
    # takes a few minutes
    ls -l phastCons*
    #-rw-rw-r--  1 hartera protein 133817339 May 24 22:48 phastConsRBest7way.wib
    #-rw-rw-r--  1 hartera protein  36947021 May 24 22:48 phastConsRBest7way.wig 
    cp -p phastConsRBest7way.wi? \
          /cluster/data/danRer4/bed/multiz7way.2006-05-18/phastCons
    
    # Load gbdb and database with wiggle.
    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-18/phastCons
    mkdir -p /gbdb/danRer4/wib
    ln -s `pwd`/phastConsRBest7way.wib /gbdb/danRer4/wib/phastConsRBest7way.wib
    # use this if need to reload table
    hgsql -e 'drop table phastConsRBest7way;' danRer4
    # load table
    hgLoadWiggle danRer4 phastConsRBest7way phastConsRBest7way.wig

    #  Create histogram to get an overview of all the data
    ssh hgwdev
    cd /cluster/data/danRer4/bed/multiz7way.2006-05-04/phastCons
    bash
    time hgWiggle -doHistogram \
        -hBinSize=0.001 -hBinCount=1000 -hMinVal=0.0 -verbose=2 \
            -db=danRer4 phastCons7way > histogram.data 2>&1
# real    2m33.069s
# user    1m58.310s
# sys     0m16.170s

        #   create plot of histogram:
    cat << '_EOF_' > histo.gp
set terminal png small color \
        x000000 xffffff xc000ff x66ff66 xffff00 x00ffff xff0000
set size 1.4, 0.8
set key left box
set grid noxtics
set grid ytics
set title " Zebrafish danRer4 Histogram phastCons7 track"
set xlabel " phastCons7 score"
set ylabel " Relative Frequency"
set y2label " Cumulative Relative Frequency (CRF)"
set y2range [0:1]
set y2tics
set yrange [0:0.02]

plot "histogram.data" using 2:5 title " RelFreq" with impulses, \
     "histogram.data" using 2:7 axes x1y2 title " CRF" with lines
'_EOF_'

    #   happy emacs
    gnuplot histo.gp > histo.png
    display histo.png &

# add line: wiggle phastCons7way to trackDb.ra for multiz7way to display the
# wiggle for the conservation track.
# check all.joiner for entries for phastCons7way and phastConsElements7way -ok
# copy over html for multiz and edit.


###########################################################################
# BACENDS TRACK (DONE, 2006-08-25, hartera)
   # Obtain these from the NCBI Trace archive
   ssh kolossus
   mkdir -p /san/sanvol1/scratch/danRer4/bacEnds/sequences
   cd /cluster/data/danRer4/bed/bacEnds/
   ln -s /san/sanvol1/scratch/danRer4/bacEnds/sequences .
   cd sequences
   # go to NCBI Trace Archive
   # http://www.ncbi.nlm.nih.gov/Traces/trace.cgi?
   cat << '_EOF_' > query_tracedb
#!/usr/bin/perl -w
use strict;
use LWP::UserAgent;
use HTTP::Request::Common 'POST';

$ENV{'LANG'}='C';
$ENV{'LC_ALL'}='C';

my $query = join ' ', @ARGV;
$query = 'help' if $query =~ /^(\-h|\-\-help|\-)$/;
$query = join('', <STDIN>) if ! $query;

my $req = POST 'http://www.ncbi.nlm.nih.gov/Traces/trace.cgi?cmd=raw', [query=>$query];
my $res =  LWP::UserAgent->new->request($req, sub { print $_[0] });
die "Couldn't connect to TRACE server\n" if ! $res->is_success;
'_EOF_'
   chmod +x query_tracedb
   # ./query_tracedb usage
   # command to see the help screen with usage examples
   # count number of entries for zebrafish
   query_tracedb "query count species_code='DANIO RERIO' AND trace_type_code = 'CLONEEND'"
   # 473060
   # 428904 (08-16-06)
   # Therefore this is 11 files of 40000 results each.
   # so get from ftp site:
   
   cat << '_EOF_' > getZfishSeqs.csh
#!/bin/csh -fe
foreach n (0 1 2 3 4 5 6 7 8 9 10)
   echo "Fetching page $n ..."
   (echo -n "retrieve_tgz all 0b"; query_tracedb "query page_size 40000 page_number $n binary species_code='DANIO RERIO' AND trace_type_code = 'CLONEEND'") | query_tracedb > data${n}.tgz
end
'_EOF_'
   chmod +x getZfishSeqs.csh
   mkdir -p downloads
   cp query_tracedb getZfishSeqs.csh ./downloads
   cd downloads
   nohup nice getZfishSeqs.csh >& zfishSeqs.log &
   # Took 5 hours 14 minutes.
   ##  Start: Wed May 10 09:57 Finished: 14:51
   # Start: May  2 21:43 Finish: May 3 03:08
   ssh kkstore04
   # unzip and untar the downloads
   cd /cluster/data/danRer4/bed/bacEnds/sequences/downloads
   gunzip *.tgz
cat << '_EOF_' > unTarBacs.csh
#!/bin/csh -fe
foreach t (0 1 2 3 4 5 6 7 8 9 10 11)
   tar xvf data${t}.tar
end
'_EOF_'
   chmod +x unTarBacs.csh
   nohup unTarBacs.csh >& unTarBacs.log &
foreach d (2006*)
   echo "Processing $d"
   nice cat ${d}/TRACEINFO.xml >> allTraceInfo.xml
   nice catBacs.csh >& catBacs.log &
   # The last archive obtained is empty so try downloading from the ftp site
   # to be sure to get everything.

   # get BAC end sequences from NBCI Trace archive ftp site:
   ssh kkstore04
   mkdir /cluster/data/danRer4/bed/bacEnds/sequences2
   mkdir /cluster/bluearc/danRer4/bacEndsDownloads
   cd /cluster/data/danRer4/bed/bacEnds/sequences2
   ln -s /cluster/bluearc/danRer4/bacEndsDownloads
   cd /cluster/data/danRer4/bed/bacEnds/sequences2/bacEndsDownloads
   # get index page and ftp for the trace server
   wget --timestamping \
        ftp://ftp.ncbi.nih.gov/pub/TraceDB/danio_rerio/
   # grab just the ftp link for each file.
   grep "anc" index.html > ancillary.lst
   perl -pi.bak -e 's/.+<a href=\"(ftp.+)\">[a-zA-Z]+.+/$1/' ancillary.lst
   rm *.bak
   # this contains just the ftp link for each file to get the ancillary
   # information files.
cat << '_EOF_' > getFtpFiles.csh
#!/bin/csh -fe
set s=$1
foreach f (`cat "${s}"`)
   echo $f
   nice wget --timestamping $f 
end
'_EOF_'
   chmod +x getFtpFiles.csh 
   nohup nice getFtpFiles.csh ancillary.lst >& ancillary.log &
   # Took about 25 minutes.
   grep "fasta" index.html > otherFiles.lst
   grep "mate_pairs" index.html >> otherFiles.lst
   grep "xml" index.html >> otherFiles.lst
   perl -pi.bak -e 's/.+<a href=\"(ftp.+)\">[a-zA-Z]+.+/$1/' otherFiles.lst
   rm *.bak
   mkdir otherFiles
   cd otherFiles
   cp ../otherFiles.lst .
   # then get these files by ftp
   nice ../getFtpFiles.csh otherFiles.lst >& otherFiles.log &
   # Took about 6 hours and 50 minutes.
   # There are 181 files as expected.
   foreach f (*.gz)
     nice gunzip $f
   end
   cd ..
   cat ./otherFiles/fasta* > danRerBacEnds.fa
   # Took about 20 minutes
   grep '>' danRerBacEnds.fa | wc -l
   # 14566448
   cat ./otherFiles/xml* > danRer.xml
   # Took 4 hours and 40 minutes.
   # find out which have CLONEEND information in them
cat << '_EOF_' > findCloneEnds.csh
#!/bin/csh -fe
foreach f (otherFiles/xml.*)
   echo $f >> cloneEndsXml.txt
   grep CLONEEND $f >> cloneEndsXml.txt
end
'_EOF_'
   chmod +x findCloneEnds.csh
   nice findCloneEnds.csh & 
   # Took 1.5 hours
   # CLONEEND is only in xml.danio_rerio.024 and xml.danio_rerio.033
   cd /cluster/data/danRer4/bed/bacEnds/sequences2/bacEndsDownloads
   cat otherFiles/xml.danio_rerio.024 otherFiles/xml.danio_rerio.033 \
       > cloneEnds.xml
   # cleanup xml files
   rm otherFiles/xml.*
   # get list of libraries:
   grep "LIBRARY_ID" cloneEnds.xml | sort | uniq > libraries.xml.txt
   
   grep "TRACE_NAME" cloneEnds.xml | wc -l
   # 985980
   grep "TRACE_NAME" cloneEnds.xml | sort | uniq -c > traceName.xml.count
   # Hard to tell which are the BAC clone end sequences. These ftp files
   # contain a mixture of sequences from different sources
   # Try downloading sequences from Sanger instead. Not all of the sequences
   # may have been submitted to NCBI anyway yet. 
   ssh kkstore04
   cd /cluster/data/danRer4/bed/bacEnds
   mkdir -p /san/sanvol1/danRer4/bacEnds/ensemblSeqs
   ln -s /san/sanvol1/danRer4/bacEnds/ensemblSeqs
   cd ensemblSeqs
   wget --timestamping  \
        ftp://ftp.ensembl.org/pub/traces/danio_rerio/fasta/
   # gets index.html page
   # get list of cloneEnd FASTA files
   grep cloneEnd index.html > cloneEndsFile
   perl -pi.bak -e 's/.+<a href=\"(ftp.+)\">[a-zA-Z]+.+/$1/' cloneEndsFile
   rm *.bak
   foreach f (`cat cloneEndsFile`)
     echo $f
     wget --timestamping $f 
   end
   # then do the same to get the trace info xml files:
   wget --timestamping \
      ftp://ftp.ensembl.org/pub/traces/danio_rerio/traceinfo/
   grep cloneEnd index.html > cloneEndsXmlFile
   perl -pi.bak -e 's/.+<a href=\"(ftp.+)\">[a-zA-Z]+.+/$1/' cloneEndsXmlFile
   rm *.bak
   foreach f (`cat cloneEndsXmlFile`)
     echo $f
     wget --timestamping $f
   end
   gunzip *.gz
   # check for multiple occurrences of same sequence ID
   grep trace_name *.xml | sort | uniq -c | sort -nr > traceNames.count
   # top of list has count of 1 so the end names are unique.
   grep clone_id *.xml | sort | uniq -c | sort -nr > cloneIds.count
   # top of list has count of 4. All those clone IDs that appear 3 or 4 times
   # do so in the CHORI-1073 library - this is the fosmid library.
   # move CHORI-1073 out of the way
   mkdir fosmids
   mv sanger-zfish-CHORI-1073-cloneEnd* ./fosmids
   
   # FASTA files have clone end names as sequence names
   # concatenate the 18 fasta files
   cat *.fasta > Zv6BacEnds.fa    
   grep '>' Zv6BacEnds.fa | wc -l
   # 694170
   # Zv5 had 729101 but these were not unique reads for each sequence. 
   faSize Zv6BacEnds.fa >& Zv6.faSize.txt
   # there are 31 sequence names with no sequence.
   awk '{print $10}' Zv6.faSize.txt > cloneEnds.noSeq
   # remove extra lines at end of file 
   # list of FASTA files that they are in. 
   grep -f cloneEnds.noSeq *.fasta > cloneEnds.noSeq.files
   # sent this list of sequence names and files to Kerstin Howe
   # at Sanger: kj2@sanger.ac.uk . Sanger said that these are just missing
   # sequences due to poor quality.
   # invalid FASTA file format
   # remove these from FASTA file:
   grep -v -f cloneEnds.noSeq Zv6BacEnds.fa > tmp.fa
   grep '>' tmp.fa | wc -l
   # 694139
   mv tmp.fa Zv6BacEnds.fa
   faSize Zv6BacEnds.fa
   # 728424771 bases (11822219 N's 716602552 real 716602552 upper 0 lower) in
   # 694139 sequences in 1 files
   # Total size: mean 1049.4 sd 277.3 min 4 (zKp108D7.za) max 5403 (zC259G13.zb)
   # median 982
   # N count: mean 17.0 sd 42.1
   # U count: mean 1032.4 sd 265.3
   # L count: mean 0.0 sd 0.0
   
   # Blat these BAC ends vs the danRer4 genome assembly. Gaps between
   # scaffolds in the NA_random and Un_random chroms are 50,000 so 
   # alignments of BAC ends across adjacent scaffolds are unlikely, 
   # but alignments done separately just in case:
   ssh pk
   mkdir -p /san/sanvol1/scratch/danRer4/bacEnds/sequences
   cd /cluster/data/danRer4/bed/bacEnds/ensemblSeqs
   cp Zv6BacEnds.fa /san/sanvol1/scratch/danRer4/bacEnds/sequences
   mkdir -p /cluster/data/danRer4/bed/bacEnds/chromsRun
   cd /cluster/data/danRer4/bed/bacEnds/chromsRun
   ls -1S /san/sanvol1/scratch/danRer4/bacEnds/sequences/Zv6BacEnds.fa \
          > bacends.lst 
   ls -1S /san/sanvol1/scratch/danRer4/trfFa/chr[0-9M]*.fa > seqs.lst
   # create out dir
   mkdir -p /san/sanvol1/scratch/danRer4/bacEnds/chromsPsl
   # use Blat parameters as for mm5 and hg17
cat << '_EOF_' > template
#LOOP
/cluster/bin/x86_64/blat $(path1) $(path2) -ooc=/san/sanvol1/scratch/danRer4/danRer4_11.ooc {check out line+ /san/sanvol1/scratch/danRer4/bacEnds/chromsPsl/$(root1)_$(root2).psl}
#ENDLOOP 
'_EOF_'
# << this line keeps emacs coloring happy
    gensub2 seqs.lst bacends.lst template jobList
    para create jobList
    para try, check, push, check, ...
# para time
# Completed: 271 of 271 jobs
# CPU time in finished jobs:    1063126s   17718.77m   295.31h   12.30d  0.034 y
# IO & Wait Time:                  2531s      42.18m     0.70h    0.03d  0.000 y
# Average job time:                3932s      65.54m     1.09h    0.05d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            9404s     156.73m     2.61h    0.11d
# Submission to last job:          9891s     164.85m     2.75h    0.11d

   # Repeat for random chroms, but use separate scaffolds:
   mkdir -p /cluster/data/danRer4/bed/bacEnds/randomsRun
   cd /cluster/data/danRer4/bed/bacEnds/randomsRun
   ls -1S /san/sanvol1/scratch/danRer4/bacEnds/sequences/Zv6BacEnds.fa \
          > bacends.lst 
   foreach f (/san/sanvol1/scratch/danRer4/scaffoldsSoftMask/Zv6*.fa)
      ls -1S $f >> seqs.lst
   end
   # create out dir
   mkdir -p /san/sanvol1/scratch/danRer4/bacEnds/randomsPsl
   # use Blat parameters as for mm5 and hg17
cat << '_EOF_' > template
#LOOP
/cluster/bin/x86_64/blat $(path1) $(path2) -ooc=/san/sanvol1/scratch/danRer4/danRer4_11.ooc {check out line+ /san/sanvol1/scratch/danRer4/bacEnds/randomsPsl/$(root1)_$(root2).psl}
#ENDLOOP 
'_EOF_'
# << this line keeps emacs coloring happy
    gensub2 seqs.lst bacends.lst template jobList
    para create jobList
    para try, check, push, check, ...
# para time 
# Completed: 2966 of 2966 jobs
# CPU time in finished jobs:     240259s    4004.31m    66.74h    2.78d  0.008 y
# IO & Wait Time:                 84042s    1400.71m    23.35h    0.97d  0.003 y
# Average job time:                 109s       1.82m     0.03h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:             997s      16.62m     0.28h    0.01d
# Submission to last job:         11925s     198.75m     3.31h    0.14d
    # lift chrom alignments and randoms alignments and then merge and filter.
    ssh kolossus
    cd /cluster/data/danRer4/bed/bacEnds/
    nice pslSort dirs rawChroms.psl tmp \
         /san/sanvol1/scratch/danRer4/bacEnds/chromsPsl >& chromSort.log 
    # Took 2 hours
    # very large output so do the randoms on the san
    cd /san/sanvol1/scratch/danRer4/bacEnds/
    nice pslSort dirs rawRandoms.psl tmp randomsPsl >& randomsSort.log
    # Took 12 minutes
    # move the rawChroms.psl over to the san
    mv /cluster/data/danRer4/bed/bacEnds/rawChroms.psl \
       /san/sanvol1/scratch/danRer4/bacEnds/
    cd /san/sanvol1/scratch/danRer4/bacEnds/
    # for danRer3, hg18 etc.:
    pslReps -nearTop=0.02 -minCover=0.60 -minAli=0.85 -noIntrons \
                rawChroms.psl bacEndsChroms.psl /dev/null
    # Took about 1 hour.
    pslReps -nearTop=0.02 -minCover=0.60 -minAli=0.85 -noIntrons \
                rawRandoms.psl bacEndsRandoms.psl /dev/null
    # Took 2 minutes.
    # merge files. There is a single liftOver file that works for both the
    # pseudocontigs and the scaffolds.
    # remove header for bacEndsRandoms.psl
    tail +6 bacEndsRandoms.psl > tmp.psl 
    cat bacEndsChroms.psl tmp.psl > bacEndsNoLift.psl
    # liftUp file to chrom coordinates.
    liftUp bacEnds.psl \
           /cluster/data/danRer4/jkStuff/liftAll.lft warn bacEndsNoLift.psl 
    # Took 2 minutes
    # Now put togehter the pairs information:
    ssh kkstore04
    cd /cluster/data/danRer4/bed/bacEnds
    mv /san/sanvol1/danRer4/bacEnds/bacEnds.psl .
    # cat together the xml files of BAC clone end information
    cat ensemblSeqs/*.xml > danRerBacEnds.xml
    # get mate-pair information from xml, forward is SP6, reverse is T7
    # edit getBacInfo.pl used for canFam1 and adapt for use with zebrafish
    # BAC ends. Not all entries in the xml file have clone_id or trace_end
    # but sometimes they have trace_direction instead of trace_end.
cat << '_EOF_' > getZfishBacInfo.pl
#!/usr/bin/perl -w
use strict;

my ($file, $outFile, $name, $clone, $library, $dir);
$file = $ARGV[0];
$outFile = $ARGV[1];

open (FILE, $file) || die "Can not open $file : $!\n";
open (OUT, ">$outFile") || die "Can not create $outFile : $!\n";
open (STDERR, ">error.log") || die "Can not create error.log : $!\n";
my %cloneHash = qw {
   zC   CH211-
   zK  DKEY-
   zKp DKEYP-
   bZ  RP71-
   dZ  BUSM1-
   CHORI73_ CH73-
};

$name = "";
$clone = "";
$dir = "";
while (<FILE>)
{
chomp;
my $l = $_;
if ($l =~ /<trace_name>([A-Za-z0-9\_\.]+)/)
   {
   $name = $1;
   }
elsif ($l =~ /<clone_id>([A-Z0-9]+\-[0-9A-Z]+)/)
   {
   $clone = $1;
   }
elsif ($l =~ /<library_id>([A-Z0-9a-z\s]+\-?[0-9A-Z]*)<\/library_id>/)
   {
   $library = $1;
   if ($library eq "Daniokey Pilot")
      {
      $library = "DKEYP";
      }
   }
elsif ($l =~ /<trace_end>(F|R)/)
   {
   $dir = $1;
   }
elsif ($l =~ /<trace_direction>(F|R)/)
   {
   $dir = $1;
   }
# find end of record and print out end information
if ($l =~ /^\s+<\/trace>/)
   {
   printInfo($name, $clone, $library, $dir);
   $name = $clone = $dir = $library = "";
   }
}
close FILE;
close OUT;
close STDERR;

sub printInfo  {
   my ($name, $clone, $lib, $d) = @_; 
   # if no clone name read from file then create from trace name
   if ($clone  eq "")
      {
      foreach my $c (keys(%cloneHash))
         {
         if ($name =~ /$c/)
             {
             if (exists($cloneHash{$c})) 
                {
                my $prefix = $cloneHash{$c};
                $clone = $name;
                # change to clone name
                $clone =~ s/$c/$prefix/;
                # remove suffix
                $clone =~ s/\.[a-z]+|SP6|T7//;
                }
             }
         }
      }
   # convert forward or reverse direction to SP6 or T7
   if ($d ne "")
      {
      if ($d eq "F")
         {
         $d = "SP6";
         }
      elsif ($d eq "R")
         {
         $d = "T7";
         }
      }
   else 
      {
      print STDERR "No direction for $name found\n";
      }
   # print clone end information
   print OUT "$clone\t$name\t0\t$lib\t0\t$d\n";
}
'_EOF_'
    # << for emacs
    chmod +x getZfishBacInfo.pl
    perl getZfishBacInfo.pl danRerBacEnds.xml bacEndInfo.txt
    # check all the names are there 
    grep '>' ./ensemblSeqs/Zv6BacEnds.fa > names
    perl -pi.bak -e 's/>//' names
    sort names | uniq > names.sort
    awk '{print $2}' bacEndInfo.txt  | sort | uniq > bacEndInfo.names.sort
    comm -13 bacEndInfo.names.sort names.sort
    # no difference so all clone ends in the FASTA file are also 
    # in the xml file.
    rm *.bak *.sort names  
    # create mate-pair information
    cp /cluster/bin/scripts/convertBacEndPairInfo convertZfishBacEndInfo
    # comment out line 43 as this removes the suffix after a . from the
    # trace names. In this case, we need to keep those. 
    # line 43:  ($acc, $ver) = split(/\./,$acc);
    convertBacEndPairInfo bacEndInfo.txt
    # 312901 pairs and 35479 singles
    mkdir -p /cluster/data/danRer4/bed/bacEnds/pairs
    cd /cluster/data/danRer4/bed/bacEnds/pairs
    set dir = /cluster/data/danRer4/bed/bacEnds
    # use parameters from REDO of danRer3 BAC ends
    /cluster/bin/x86_64/pslPairs -tInsert=10000 -minId=0.91 -noBin -min=25000 -max=350000 -slopval=10000 -hardMax=500000 -slop -short -long -orphan -mismatch -verbose $dir/bacEnds.psl $dir/bacEndPairs.txt all_bacends bacEnds
    wc -l *
#   1714 bacEnds.long
#  14889 bacEnds.mismatch
# 109213 bacEnds.orphan
# 105294 bacEnds.pairs
#    347 bacEnds.short
#    782 bacEnds.slop

    # create header required by "rdb" tools
    echo 'chr\tstart\tend\tclone\tscore\tstrand\tall\tfeatures\tstarts\tsizes' \
         > ../header
    echo '10\t10N\t10N\t10\t10N\t10\t10\t10N\t10\t10' >> ../header
    # edit header to make sure \t is/become tab character
    cat header bacEnds.pairs | row score ge 300 | sorttbl chr start \
        | headchg -del > bacEndPairs.bed
    # create bad BAC ends set
    cat header bacEnds.slop bacEnds.short bacEnds.long bacEnds.mismatch \
        bacEnds.orphan | row score ge 300 | sorttbl chr start \
        | headchg -del > bacEndPairsBad.bed
    # Also create a bad BAC ends set with no orphans since orphans are
    # already added to the singles track and do not want to add these orphans
    # twice when extracting PSL. Use this bacEndPairsBadNoOrphans.bed
    # file when extracting PSLs for adding to the all_bacends table.
    cat header  bacEnds.slop bacEnds.short bacEnds.long bacEnds.mismatch \
        | row score ge 300 | sorttbl chr start \
        | headchg -del > bacEndPairsBadNoOrphans.bed
    # To create singles set:
    # also need to process bacEndSingles.txt into a database table
    # for singles in bacEndSingles.txt, create a dummy file where they
    # are given zJA11B12T7 as dummy sequence pair. If the single is a forward
    # sequence, put the dummy sequence in the second column, if the single is
    # a reverse sequence put in first column. use a perl script to do this.
    cd /cluster/data/danRer4/bed/bacends
    set bacDir = /cluster/data/danRer4/bed/bacEnds
    mkdir singles
    cd singles
    cp /cluster/data/danRer2/bed/ZonLab/bacends/singles/formatSingles.pl .
    perl formatSingles.pl $bacDir/bacEndSingles.txt > \
                           $bacDir/bacEndSingles.format
    # then run pslPairs on this formatted file
    /cluster/bin/x86_64/pslPairs -tInsert=10000 -minId=0.91 -noBin -min=25000 \
     -max=350000 -slopval=10000 -hardMax=500000 -slop -short -long -orphan \
     -mismatch -verbose ../bacEnds.psl $bacDir/bacEndSingles.format \
     all_bacends bacEnds
    wc -l bacEnds.*
    #     0 bacEnds.long
    #     0 bacEnds.mismatch
    # 22036 bacEnds.orphan
    #     0 bacEnds.pairs
    #     0 bacEnds.short
    #     0 bacEnds.slop
    cat bacEnds.orphan ../pairs/bacEnds.orphan > bacEnds.singles 
    wc -l bacEnds.singles
    # 131249 bacEnds.singles
    # Of these, 109213 are from pair analysis and 22036 from singles.
    # For danRer3: there are 11439 orphans from singles and 242235 from 
    # pair analysis so a total of 253674 orphans so this has improved.
    # Although for danRer3, some of these could be replicate reads for the
    # same BAC clone end.
    # make singles bed file
    cat ../header bacEnds.singles | row score ge 300 | sorttbl chr start \
                  | headchg -del > bacEndSingles.bed

    # check if there are any overlapping alignments that can be removed.
    cd /cluster/data/danRer4/bed/bacEnds
    mkdir -p duplicates/overlapRun
    cd duplicates/overlapRun
    sort -k1,2 /cluster/data/danRer4/bed/bacEnds/pairs/bacEndPairs.bed \
         > bacEndPairs.lfs 
    wc -l *.lfs
    # 104546 bacEndPairs.lfs
    nice /cluster/bin/x86_64/lfsOverlap bacEndPairs.lfs bacEndPairs.bed \
         -name -minOverlap=0.999 -notBlocks
    # Loaded 104546 elements of size 11
    # only 5 lines removed
    sort -k1,2 /cluster/data/danRer4/bed/bacEnds/singles/bacEndSingles.bed \
         > bacEndSingles.lfs
    nice /cluster/bin/x86_64/lfsOverlap bacEndSingles.lfs bacEndSingles.bed \
        -name -minOverlap=0.999 -notBlocks
    # Loaded 125695 elements of size 11
    # No lines removed.
    sort -k1,2 \
         /cluster/data/danRer4/bed/bacEnds/pairs/bacEndPairsBadNoOrphans.bed \
         > bacEndPairsBadNoOrphans.lfs 
    wc -l *.lfs
    # 17611 bacEndPairsBadNoOrphans.lfs
    nice /cluster/bin/x86_64/lfsOverlap bacEndPairsBadNoOrphans.lfs \
         bacEndPairsBadNoOrphans.bed -name -minOverlap=0.999 -notBlocks
    # Loaded 17611 elements of size 11
    # Saving 17608 records to bacEndPairsBadNoOrphans.bed
    # Only 3 alignments were removed. 
    # Therefore no point in doing using these files. Use the original bed 
    # files for pairs and singles. No further processing of BED files is 
    # needed as they have not been changed in any way.
    # Remove duplicates directory.
    rm -r /cluster/data/danRer4/bed/bacEnds/duplicates
    # use new extract program that extracts PSLs using name and position:
    ssh kkstore04
    set bacDir=/cluster/data/danRer4/bed/bacEnds
    cd $bacDir/pairs

    nice /cluster/home/hartera/bin/x86_64/extractPslForLfs -verbose=1 \
        $bacDir/bacEnds.psl bacEndPairs.bed bacPairs.psl
    # for this, use bacEndPairsBadNoOrphans since pairs orphans are already
    # included in bacEndSingles
    nice /cluster/home/hartera/bin/x86_64/extractPslForLfs -verbose=1 \
      $bacDir/bacEnds.psl bacEndPairsBadNoOrphans.bed bacPairsBadNoOrphans.psl
    # then for singles
    cd $bacDir/singles
    nice /cluster/home/hartera/bin/x86_64/extractPslForLfs -verbose=1 \
      $bacDir/bacEnds.psl bacEndSingles.bed bacSingles.psl
    cd $bacDir
    cat pairs/*.psl singles/bacSingles.psl > allBacends.load.psl
    # try old program and compare
    extractPslLoad -noBin bacEnds.psl pairs/bacEndPairs.bed \
        pairs/bacEndPairsBadNoOrphans.bed singles/bacEndSingles.bed \
        | sorttbl tname tstart | headchg -del > bacEnds.load.psl
    wc -l *.load.psl
    # 364457 allBacends.load.psl
    # 4568907 bacEnds.load.psl
    # Much reduced by using only BAC end alignments that are in BED files.
    # load into database
    ssh hgwdev
    cd /cluster/data/danRer4/bed/bacEnds/pairs
    hgLoadBed danRer4 bacEndPairs bacEndPairs.bed -notItemRgb \
               -sqlTable=$HOME/kent/src/hg/lib/bacEndPairs.sql
    # Loaded 104546 elements of size 11
    # note - this next track isn't pushed to RR, just used for assembly QA
    hgLoadBed danRer4 bacEndPairsBad bacEndPairsBad.bed -notItemRgb \
              -sqlTable=$HOME/kent/src/hg/lib/bacEndPairsBad.sql
    # Loaded 121728 elements of size 11
    cd /cluster/data/danRer4/bed/bacEnds/singles
    cp /cluster/data/danRer3/bed/bacends/singles/bacEndSingles.sql .
    hgLoadBed danRer4 bacEndSingles bacEndSingles.bed -notItemRgb \
              -sqlTable=bacEndSingles.sql
    # Loaded 125695 elements of size 11
    cd /cluster/data/danRer4/bed/bacEnds
    hgLoadPsl danRer4 -table=all_bacends allBacends.load.psl
    # All alignments were loaded into the table - no problems.
    # load BAC end sequences into seq table so alignments may be viewed
    # symlink to FASTA sequence file in ncbi directory
    # move BAC ends to the ncbi directory
    mkdir -p /cluster/data/ncbi/bacends/zebrafish/bacends.1
    # remove some files
    cd ensemblSeqs 
    rm tmp clone* index.html
    cd /cluster/data/danRer4/bed/bacEnds
    mv /cluster/data/danRer4/bed/bacEnds/ensemblSeqs/* \
       /cluster/data/ncbi/bacends/zebrafish/bacends.1
    rm -r ensemblSeqs
    mkdir -p /gbdb/danRer4/bacends
    ln -s /cluster/data/ncbi/bacends/zebrafish/bacends.1/Zv6BacEnds.fa \
          /gbdb/danRer4/bacends/Zv6BacEnds.fa
    hgLoadSeq danRer4 /gbdb/danRer4/bacends/Zv6BacEnds.fa
    # check trackDb.ra entry and description
    # cleanup:
    ssh kkstore04
    cd /cluster/data/danRer4/bed/bacEnds/
    rm -r sequences
    rm -r /san/sanvol1/scratch/danRer4/bacEnds/sequences
    rm -r sequences2
    rm changes.txt bacEnds.load.psl *.log
    du -sh /cluster/data/danRer4/bed/bacEnds
    # 2.4G    /cluster/data/danRer4/bed/bacEnds
    gzip *.psl *.txt danRerBacEnds.xml
    du -sh /cluster/data/danRer4/bed/bacEnds
    # 599M    /cluster/data/danRer4/bed/bacEnds
 
###########################################################################
# SPLIT SEQUENCE FOR LIFTOVER CHAINS FROM OTHER DANRER ASSEMBLIES
# (DONE, 2006-06-27, hartera)
# ADD TO SAN FOR PK RUNS (DONE, 2006-05-30, hartera)
   ssh kkr3u00
   # change script to do this and only rsync to 4,5,6,7, and 8 as
   # kkr1u00 and kkr2u00 are down.
   cd /cluster/data/danRer4/bed
   mkdir -p liftOver
   cd liftOver
   # commented out lines in local copy that makes the script abort if 
   # kkr1u00 not used. can not connect to kkr1u00 at the moment.
   ~/kent/src/hg/makeDb/makeLoChain/makeLoChain-split.csh danRer4 \
         /cluster/data/danRer4/nib >&! split.log &
   # rsync didn't work properly so do manually
   foreach R (4 5 6 7 8)
    rsync -a --progress /iscratch/i/danRer4/ kkr${R}u00:/iscratch/i/danRer4/
   end
   ssh kk
   # add split10k to san for pk runs (2006-05-30, hartera)
   rsync -a --progress /iscratch/i/danRer4/split10k \
         /san/sanvol1/scratch/danRer4/
   
###########################################################################
# LIFTOVER CHAINS TO DANRER3 (DONE, 2006-05-30 = 2006-05-31, hartera)
   # Split (using makeLoChain-split) of danRer3 is doc'ed in makeDanRer3.doc
   # Do what makeLoChain-split says to do next (start blat alignment)
   # Took too long on kk. Try pk. Scripts only run on kk so run manually.
   ssh pk
   mkdir -p /cluster/data/danRer4/bed/liftOver
   cd /cluster/data/danRer4/bed/liftOver
cat << '_EOF_' > align.csh
#!/bin/csh -fe
set oldAssembly = $1
set oldNibDir = $2
set newAssembly = $3
set newSplitDir = $4
set ooc = $5
if ("$ooc" != "") then
    set ooc = '-ooc='$ooc
endif

set blatDir = /cluster/data/$oldAssembly/bed/blat.$newAssembly.`date +%Y-%m-%d`
echo "Setting up blat in $blatDir"
rm -fr $blatDir
mkdir $blatDir
cd $blatDir
mkdir raw psl run
cd run

echo '#LOOP' > gsub
echo 'blat $(path1) $(path2) {check out line+ ../raw/$(root1)_$(root2).psl} ' \
       '-tileSize=11 '$ooc' -minScore=100 -minIdentity=98 -fastMap' \
  >> gsub
echo '#ENDLOOP' >> gsub

# target
ls -1S $oldNibDir/*.{nib,2bit} > old.lst
# query
ls -1S $newSplitDir/*.{nib,fa} > new.lst

gensub2 old.lst new.lst gsub spec
/parasol/bin/para create spec

echo ""
echo "First two lines of para spec:"
head -2 spec
echo ""
echo "DO THIS NEXT:"
echo "    cd $blatDir/run"
echo "    para try, check, push, check, ..."
echo ""
exit 0
'_EOF_'
   # << emacs
   chmod +x align.csh
   align.csh danRer4 /san/sanvol1/scratch/danRer4/nib danRer3 \
       /san/sanvol1/scratch/danRer3/split10k \
       /san/sanvol1/scratch/danRer3/danRer3_11.ooc >&! align.log &
   # Took a few seconds.
   # Do what its output says to do next (start cluster job)
   cd /cluster/data/danRer4/bed/blat.danRer3.2006-05-30/run
   para try, check, push, check, ...
   para time >&! run.time
# Completed: 784 of 784 jobs
# CPU time in finished jobs:    1482693s   24711.54m   411.86h   17.16d  0.047 y
# IO & Wait Time:                  2873s      47.89m     0.80h    0.03d  0.000 y
# Average job time:                1895s      31.58m     0.53h    0.02d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:           11350s     189.17m     3.15h    0.13d
# Submission to last job:         13914s     231.90m     3.87h    0.16d

   ssh pk
   cd /cluster/data/danRer4/bed/liftOver
 
cat << '_EOF_' > lift.csh
#!/bin/csh -ef
set oldAssembly = $1
set newAssembly = $2
set newLiftDir = /san/sanvol1/scratch/$newAssembly/split10k

set prefix = /cluster/data/$oldAssembly/bed/blat.$newAssembly
set blatDir = `ls -td $prefix.20* | head -1`
echo "using dir $blatDir"

if ( ! -e $blatDir/raw ) then
    echo "Can't find $blatDir/raw"
endif

if (`ls -1 $newLiftDir/*.lft | wc -l` < 1) then
    echo "Can't find any .lft files in $newLiftDir"
    exit 1
endif
cd $blatDir/raw

foreach chr (`awk '{print $1;}' /cluster/data/$newAssembly/chrom.sizes`)
    echo $chr
    liftUp -pslQ ../psl/$chr.psl $newLiftDir/$chr.lft warn chr*_$chr.psl
end

set execDir = $0:h
echo ""
echo "DO THIS NEXT:"
echo "    ssh pk"
echo "    $execDir/makeLoChain-chain $oldAssembly <$oldAssembly-nibdir> $newAssembly <$newAssembly-nibdir>"
echo ""
exit 0
'_EOF_'
   # << emacs
   chmod +x lift.csh
   lift.csh danRer4 danRer3 >&! lift.log &
   # makeLoChain-chain can be run on pk. chain alignments
   
   makeLoChain-chain danRer4 /san/sanvol1/scratch/danRer4/nib \
                     danRer3 /san/sanvol1/scratch/danRer3/nib >&! chain.log &
   cd /cluster/data/danRer4/bed/blat.danRer3.2006-05-30/chainRun
   para try, check, push, check, ...
   para time
# Completed: 28 of 28 jobs
# CPU time in finished jobs:       4030s      67.16m     1.12h    0.05d  0.000 y
# IO & Wait Time:                   939s      15.66m     0.26h    0.01d  0.000 y
# Average job time:                 177s       2.96m     0.05h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:             797s      13.28m     0.22h    0.01d
# Submission to last job:           953s      15.88m     0.26h    0.01d
   # net alignment chains
   ssh kkstore04
   cd /cluster/data/danRer4/bed/liftOver
   makeLoChain-net danRer4 danRer3 >&! net.log &
   # load reference to over.chain into database table,
   # and create symlinks  /gbdb  and download area
   ssh hgwdev
   cd /cluster/data/danRer4/bed/liftOver
   makeLoChain-load danRer4 danRer3 >&! load.log &
   # clean up
   rm *.log
   # add md5sum.txt to include this new liftOver file
   cd /usr/local/apache/htdocs/goldenPath/danRer4/liftOver
   md5sum *.gz > md5sum.txt
   # copy README.txt from another liftOver directory.
   # test by converting a region using the "convert" link on
   # the browser, and comparing to blat of the same region

###########################################################################
# PRODUCING GENSCAN PREDICTIONS (DONE, 2006-05-27, hartera)
   # Use scaffolds for random chroms to avoid getting false predictions
   # spanning scaffolds in chrNA_random and chrUn_random.
   ssh kkstore04
   cd /cluster/data/danRer4
   # already have a file of soft-masked scaffolds for chrNA_random and
   # chrUn_random. Use this to create hard-masked scaffolds FASTA file
   # for Genscan run.
   foreach c (NA_random Un_random)
      cd /cluster/data/danRer4/$c
      mkdir scaffoldsHardMask
      echo "Hard-masking scaffolds for $c ..."
      cd scaffoldsSoftMask
      foreach f (*.fa)
        maskOutFa $f hard ../scaffoldsHardMask/${f}.masked
      end
   end
  
   ssh hgwdev
   mkdir /cluster/data/danRer4/bed/genscan
   cd /cluster/data/danRer4/bed/genscan
   cvs co hg3rdParty/genscanlinux

   ssh pk
   cd /cluster/data/danRer4/bed/genscan
   # Make 3 subdirectories for genscan to put their output files in
   mkdir gtf pep subopt
   # Generate a list file, genome.list, of all the hard-masked contigs that 
   # *do not* consist of all-N's (which would cause genscan to blow up)
   cp /dev/null genome.list
   foreach c (`cat /cluster/data/danRer4/chrom.lst`)
    echo $c
    if (($c == "NA_random") || ($c == "Un_random")) then
     foreach s (/cluster/data/danRer4/${c}/scaffoldsHardMask/Zv6_*.fa.masked)
      egrep '[ACGT]' $s > /dev/null
      if ($status == 0) echo $s >> genome.list
     end
    else
     foreach f ( `ls -1S /cluster/data/danRer4/$c/chr*_*/chr*_?{,?}.fa.masked` )
       egrep '[ACGT]' $f > /dev/null
       if ($status == 0) echo $f >> genome.list
     end
    endif
   end
   wc -l genome.list
   # 3237 genome.list
   # Create template file, gsub, for gensub2.  For example (3-line file):
   cat << '_EOF_' > gsub
#LOOP
/cluster/bin/x86_64/gsBig {check in line+ $(path1)} {check out line gtf/$(root1).gtf} -trans={check out line pep/$(root1).pep} -subopt={check out line subopt/$(root1).bed} -exe=hg3rdParty/genscanlinux/genscan -par=hg3rdParty/genscanlinux/HumanIso.smat -tmp=/tmp -window=2400000
#ENDLOOP
'_EOF_'
   # << this line makes emacs coloring happy
   gensub2 genome.list single gsub jobList
   para create jobList
   para try, check, push, check ... etc.
   para time
# Completed: 3236 of 3237 jobs
# Crashed: 1 jobs
# CPU time in finished jobs:      46601s     776.69m    12.94h    0.54d  0.001 y
# IO & Wait Time:                 10409s     173.48m     2.89h    0.12d  0.000 y
# Average job time:                  18s       0.29m     0.00h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:             363s       6.05m     0.10h    0.00d
# Submission to last job:           445s       7.42m     0.12h    0.01d

   # If there are crashes, diagnose with "para problems" / "para crashed".  
   # If a job crashes due to genscan running out of memory, re-run it 
   # manually with "-window=1200000" instead of "-window=2400000".
   para problems > problems
   nice /cluster/bin/x86_64/gsBig /cluster/data/danRer4/8/chr8_5/chr8_5.fa.masked gtf/chr8_5.fa.gtf -trans=pep/chr8_5.fa.pep -subopt=subopt/chr8_5.fa.bed -exe=hg3rdParty/genscanlinux/genscan -par=hg3rdParty/genscanlinux/HumanIso.smat -tmp=/tmp -window=1200000 >& chr8_5.fa.log & 
   # Took about 5 minutes to run
   # check log and then remove it
   rm chr8_5.fa.log

   ssh kkstore04
   cd /cluster/data/danRer4/bed/genscan
   liftUp genscan.gtf ../../jkStuff/liftAll.lft warn gtf/*.gtf
   liftUp genscanSubopt.bed ../../jkStuff/liftAll.lft warn subopt/*.bed
   cat pep/*.pep > genscan.pep

   # Load into the database as so:
   ssh hgwdev
   cd /cluster/data/danRer4/bed/genscan
   ldHgGene danRer4 genscan genscan.gtf
   # Read 44534 transcripts in 325488 lines in 1 files
   # 44534 groups 28 seqs 1 sources 1 feature types
   # 44534 gene predictions
   hgPepPred danRer4 generic genscanPep genscan.pep
   hgLoadBed danRer4 genscanSubopt genscanSubopt.bed
   # Loaded 332782 elements of size 6
   # compare to other assemblies:
   featureBits danRer4 genscan
   # 64448019 bases of 1626093931 (3.963%) in intersection
   featureBits rn4 genscan
   # 54781052 bases of 2571531505 (2.130%) in intersection
   featureBits monDom4 genscan
   # 45991425 bases of 3501643220 (1.313%) in intersection
   featureBits tetNig1 genscan
   # 30459626 bases of 342403326 (8.896%) in intersection

   featureBits -chrom=chr1 refGene genscan -enrichment
   # refGene 1.129%, genscan 4.195%, both 0.653%, cover 57.80%, enrich 13.78x 
   # check CDS only
   featureBits -chrom=chr1 danRer4 refGene:cds genscan:cds -enrichment
   # refGene:cds 0.746%, genscan:cds 4.195%, both 0.631%, cover 84.52%, 
   # enrich 20.15x 


###########################################################################
# BLASTZ/CHAIN/NET GALGAL3 (DONE 5/30/06 angie)
    ssh pk
    mkdir /cluster/data/danRer4/bed/blastz.galGal3.2006-05-30
    cd /cluster/data/danRer4/bed/blastz.galGal3.2006-05-30
    cat << '_EOF_' > DEF
# zebrafish vs. chicken
BLASTZ=/cluster/bin/penn/i386/blastz

# Use same params as used for danRer1-xenTro1 (see makeXenTro1.doc)
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q

# TARGET: Zebrafish danRer4
SEQ1_DIR=/san/sanvol1/scratch/danRer4/danRer4.2bit
SEQ1_CTGDIR=/san/sanvol1/scratch/danRer4/danRer4ChrUnNAScafs.2bit
SEQ1_LIFT=/san/sanvol1/scratch/danRer4/liftNAandUnScaffoldsToChrom.lft
SEQ1_LEN=/cluster/data/danRer4/chrom.sizes
SEQ1_CTGLEN=/san/sanvol1/scratch/danRer4/chromsUnNAScafs.sizes
SEQ1_CHUNK=50000000
SEQ1_LAP=10000
SEQ1_LIMIT=100

# QUERY: Chicken galGal3 - single chunk big enough to run while chrom
SEQ2_DIR=/san/sanvol1/galGal3/nib
SEQ2_LEN=/cluster/data/galGal3/chrom.sizes
SEQ2_CHUNK=20000000
SEQ2_LAP=0
SEQ2_LIMIT=100

BASE=/cluster/data/danRer4/bed/blastz.galGal3.2006-05-30
'_EOF_'
    # << emacs
    doBlastzChainNet.pl -blastzOutRoot=/san/sanvol1/scratch/danRer4GalGal3 \
      -bigClusterHub=pk -smallClusterHub=pk \
      -chainMinScore=5000 -chainLinearGap=loose DEF \
      >& do.log & tail -f do.log
    ln -s blastz.galGal3.2006-05-30 /cluster/data/danRer4/bed/blastz.galGal3

###########################################################################
# CREATE MICROARRAY DATA TRACK BY ADDING MICROARRAY DATA TO AFFY ZEBRAFISH 
# ALIGNMENTS (DONE, 2006-06-10, hartera)
# Array data is for whole embryos of five wild type zebrafish strains. 
# Data is in hgFixed (see hgFixed.doc) - from Len Zon's lab at Children's 
# Hospital Boston. Contact: adibiase@enders.tch.harvard.edu
# Zon Lab WT Affy data tables in hgFixed renamed to reflect that the data 
# is log2 transformed (DONE, 2006-07-30, hartera)
    ssh hgwdev
    mkdir /cluster/data/danRer4/bed/ZonLab/wtArray
    cd /cluster/data/danRer4/bed/ZonLab/wtArray
    hgMapMicroarray zebrafishZonWT.bed hgFixed.zebrafishZonWTMedian \
         /cluster/data/danRer4/bed/affyZebrafish/affyZebrafish.psl
    # Loaded 15617 rows of expression data from hgFixed.zebrafishZonWTMedian
    # Mapped 14819,  multiply-mapped 6667, missed 0, unmapped 798
    # hgFixed.zebrafishZonWTMedian table renamed as 
    # hgFixed.zebrafishZonWTMedianLog and hgFixed.zebrafishZonWTAll table
    # renamed as hgFixed.zebrafishZonWTAllLog (hartera, 2006-07-30)

    hgLoadBed danRer4 affyZonWildType zebrafishZonWT.bed
    # Loaded 21486 elements of size 15
    # add trackDb.ra entry at trackDb/zebrafish level
# track affyZonWildType
# shortLabel Wild Type Array
# longLabel Zon Lab Expression data for Wild Type Zebrafish strains
# group regulation
# priority 80
# visibility hide
# type expRatio
# expScale 8.0
# expStep 1.0
# expTable zebrafishZonWTMedianExps
# chip Zebrafish

###########################################################################
# HUMAN ORTHOLOGS ADDED TO AFFY ZEBRAFISH TRACK DETAILS
# (DONE, 2006-06-08, hartera)
    # Human orthologs were mapped to Affy Zebrafish probes by 
    # Tony DiBiase (adibiase@enders.tch.harvard.edu) from Len Zon's group
    # at Children's Hospital, Boston. They map to human hg16.
    ssh kkstore04
    mkdir -p /cluster/data/danRer4/bed/affyZebrafish/humanOrthologs
    cd /cluster/data/danRer4/bed/affyZebrafish/humanOrthologs
    sed -e 's/"//g' cumuList.gedi.2005oct12.txt > hg16Orthologs.txt
    awk \
    'BEGIN {FS="\t"} {OFS="\t"} {if ($2 == $1) print $1,"",""; else print;}' \
        hg16Orthologs.txt > hg16Orthologs.tab 
    # create a table definition for this set:
cat << 'EOF' > orthologs.sql
# Link together an item with an ortholog
CREATE TABLE affyToHg16Orthologs (
   name varchar(255) not null,        # Item ID
   geneSymbol longblob not null,  # Gene Symbol of ortholog
   description longblob not null, # Description of ortholog
       # Indices
   INDEX(name(20)),
   INDEX(geneSymbol(20))
);
'EOF'    
   # load table
   ssh hgwdev 
   cd /cluster/data/danRer4/bed/affyZebrafish/humanOrthologs
   hgsql -e 'drop table affyToHg16Orthologs;' danRer4
   hgLoadSqlTab danRer4 affyToHg16Orthologs orthologs.sql hg16Orthologs.tab
   # edit hgc.c to use this table on affyZebrafish details page and add
   # a search to use the human ortholog gene symbol in a search:
   # affyZebrafishHg16Ortholog, put in trackDb/zebrafish/trackDb.ra

###########################################################################
#  SWAP rn4 BLASTZ CHAIN/NET (DONE, 2006-06-19, hartera)
#  See also makeRn4.doc
    ssh pk
    cd /cluster/data/rn4/bed/blastzDanRer4.2006-06-19
    # blastz parameters used in blastz alignment of danRer4 on mm8:
    # BLASTZ_ABRIDGE_REPEATS=0
    # BLASTZ_H=2000
    # BLASTZ_Y=3400
    # BLASTZ_L=6000
    # BLASTZ_K=2200
    # BLASTZ_M=50
    # BLASTZ_Q=/cluster/data/blastz/HoxD55.q
    nice /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
	-swap `pwd`/DEF >& swap.log &
    
    ssh hgwdev
    featureBits danRer4 chainRn4Link 
    # 68978593 bases of 1626093931 (4.242%) in intersection
    featureBits danRer4 refGene:cds chainRn4Link -chrom=chr1 -enrichment
    # refGene:cds 0.746%, chainRn4Link 4.333%, both 0.564%, 
    # cover 75.55%, enrich 17.43x
    featureBits danRer3 refGene:cds chainRn4Link -chrom=chr1 -enrichment
    # refGene:cds 0.786%, chainRn4Link 4.320%, both 0.604%, 
    # cover 76.87%, enrich 17.80x
    featureBits danRer4 refGene:cds netRn4 -chrom=chr1 -enrichment
    # refGene:cds 0.746%, netRn4 29.601%,both 0.623%,cover 83.49%,enrich 2.82x
    featureBits danRer3 refGene:cds netRn4 -chrom=chr1 -enrichment
    # refGene:cds 0.786%, netRn4 33.103%, both 0.671%,cover 85.33%,enrich 2.58x
    # Add symbolic link to new swap directory 
    ssh kkstore04
    cd /cluster/data/danRer4/bed
    ln -s blastz.rn4.swap blastz.rn4
    # Check README.txt for downloads. 

#######################################################################
# VEGA GENES (DONE, 2006-08-14 - 2006-08-25, hartera)
# Data provided by Kerstin Howe from Sanger: kj2@sanger.ac.uk
    ssh kkstore04
    mkdir /cluster/data/danRer4/bed/vega
    cd /cluster/data/danRer4/bed/vega
    wget --timestamping \
         ftp://ftp.sanger.ac.uk/pub/kj2/gff/vega_in_ensembl.gff
    wget --timestamping \
         ftp://ftp.sanger.ac.uk/pub/kj2/ZFIN/genes_for_tom_new.txt
    # checked list of genes found in vega_in_ensembl.gff but not in 
    # genes_for_tom_new.txt against this file
    grep -f genesWithNoInfo.txt genes_for_tom_20060725.txt
    # got a list of 20 that were not in this file: genesWithNoInfo2.txt
    # e-mailed Kerstin at Sanger and got the information for these 20 genes:
    # moreInfo.txt
    # Need to rewrite this file using tabs:
    # checked format for VEGA genes in hg17. Includes an alternate name.
    cd /cluster/data/hg17/bed/vega30
    # to look at human VEGA
    # vegaInfo is transcriptId, otterId, geneId, method and geneDesc
    awk '{if ((($9 ~ /^ID=OTTDART/) && ($9 ~ /Parent=OTTDARG/)) || \
        (($9 ~ /^ID=OTTDART/) && ($9 ~ /Parent=OTTDART/))) print $9;}' \
        vega_in_ensembl.gff | sort | uniq > vegaIDs.txt
    perl -pi.bak -e 's/ID=//' vegaIDs.txt
    # list of transcript ID and corresponding gene ID for Vega
    perl -pi.bak -e 's/;Parent=/\t/' vegaIDs.txt
    perl -pi.bak -e 's/;Note=Only//' vegaIDs.txt

    # write a script to reformat the GFF3 file to GFF format.
    # some exon and CDS items belong to more than one transcript ID so these
    # lines can just be duplicated. Those items that are labelled as mRNA or
    # gene can be ignored and not added to the GFF file. Some of these lines
    # have an extra comment e.g. Note="   . These will be ignored anyway as
    # they are on the lines with mRNA or gene in them so they will not be in
    # the final GFF file.  
cat << '_EOF_' > formatGff3ToGff.pl
#!/usr/bin/perl -w
use strict;

my (%idsHash, $gffFile, $idsFile);
$gffFile = $ARGV[0];
open(GFF, $gffFile) || die "Can not open $gffFile\n";

while (<GFF>)
{
my ($line, @f, $t, @trans, $r, $chr);
$line = $_;
if ($line !~ /^#/)
   {
   @f = split(/\t/, $line);
   $chr = "chr" . $f[0];
   if (($f[2] ne "gene") && ($f[2] ne "mRNA"))
      {
      $f[8] =~ /Parent=(OTTDART[0-9]+[A-Z0-9,]+)/;
      $t = $1;
      @trans = split(/,/, $t);
      foreach $r (@trans)
         {
         print "$chr\t$f[1]\t$f[2]\t$f[3]\t$f[4]\t$f[5]\t$f[6]\t$f[7]\t$r\n";
         }
      }
   }
else 
   {
   # print lines beginning with "#"
   print $line;
   }
}
close GFF;
'_EOF_'
    chmod +x formatGff3ToGff.pl
    # Use script to format the GFF3 file to GFF format in order to load 
    # using ldHgGene
    perl formatGff3ToGff.pl vega_in_ensembl.gff > vega.gff
    # then use the info file to grab those genes that are pseudogenes, get the
    # transcript ID from the vegaIDs.txt file. Then grep out the pseudogenes
    # to a separate file. Create an info file. Remove the .NOVEL or .PUTATIVE 
    # or .KNOWN or .NOVEL from the method column and add as a separate 
    # confidence column. 
    # check number of items on each line: there are 4 or 6.
    # Some genes have more than one clone ID in a comma separated list
    # so create two files for loading into two tables. 
    # Found that some of the clone ID fields have comma separated lists 
    # and for OTTDARG00000006367, there are 30. Therefore create two info 
    # tables where one is just for clone IDs.  
cat << '_EOF_' > formatVegaInfo.pl
#!/usr/bin/perl -w
use strict;

# format Vega additional information into one file for vegaInfoZfish table
# and another for the vegaToCloneIdZfish table which contains the
# geneId and cloneId for each gene since there are multiple clone IDs for
# some of the genes.
my ($idsFile, $infoFile, $outFile1, $outFile2, %idsHash);
$idsFile = $ARGV[0];
$infoFile = $ARGV[1];
$outFile1 = $ARGV[2];
$outFile2 = $ARGV[3];

open (IDS, $idsFile) || die "Can not open $idsFile: $!\n";
open (INFO, $infoFile) || die "Can not open $infoFile: $!\n";
open (OUT1, ">$outFile1") || die "Can not create $outFile1: $!\n";
open (OUT2, ">$outFile2") || die "Can not create $outFile2: $!\n";
open (STDERR, ">info.log") || die "Can not create info.log: $!\n";

while (<IDS>)
{
my ($line, @f);
chomp;
$line = $_;
@f = split(/\t/, $line);

$idsHash{$f[1]} = $f[0];
}
close IDS;

while (<INFO>)
{
my ($line,@fi,$id,$gene,$trans,@transIds, $tr,@clones, $c,@t, $method, $conf);
chomp;
$gene = "";
$line = $_;
@fi = split(/\t/, $line);
$id = $gene = $fi[1];
# get all the transcript IDs for a gene
while (exists($idsHash{$id}))
   {
   $trans = $idsHash{$id};
   push(@transIds, $trans);
   $id = $trans;
   }
# push clone IDs into an array:
@clones = split(/,/, $fi[2]);
@t = split(/\./, $fi[3]);
$method = $t[0];
if ($#t > 0)
{
$conf = $t[1];
}
elsif ($#t == 0)
   {
   $conf = "";
   }
else
   {
   print STDERR "Should be 4 or 6 items per row, found $#fi \n";
   }
foreach $tr (@transIds)
   {
   print OUT1 "$tr\t$fi[1]\t$fi[0]";
   if ($#fi == 5)
      {
      print OUT1 "\t$fi[4]\t$fi[5]\t$method\t\t$conf\n";
      }
   elsif ($#fi == 3)
      {
      print OUT1 "\t\t\t$method\t\t$conf\n";
      }
   # print out clone IDs for each transcript
   foreach $c (@clones)
      {
      print OUT2 "$tr\t$c\n";
      }
   }
if($gene && !exists($idsHash{$gene})) 
   {
   print STDERR "$gene\n";
   }
}
close IDS;
close INFO;
close OUT1;
close OUT2;
close STDERR;
'_EOF_'
    chmod +x formatVegaInfo.pl 
    wc -l genes_for_tom_new.txt
    # 4822 genes_for_tom_new.txt
    awk '{print $2}' genes_for_tom_new.txt | sort | uniq > genesWithInfo.txt
    awk '{if ($2 ~ /OTTDARG/) print $2;}' vegaIDs.txt \
        | sort | uniq > genesFromGff.txt
    wc -l genesFromGff.txt
    # 4947 genesFromGff.txt
    comm -12 genesWithInfo.txt genesFromGff.txt | wc -l
    # 4033 
    comm -13 genesWithInfo.txt genesFromGff.txt | wc -l
    # 914
    comm -13 genesWithInfo.txt genesFromGff.txt > genesWithNoInfo.txt
    # sent this list to Sanger to ask about getting additional information
    # for these genes.
    comm -23 genesWithInfo.txt genesFromGff.txt | wc -l
    # 789
    # got another file from Sanger that should contain the information 
    # for the 914 genes missing information above.
    ftp://ftp.sanger.ac.uk/pub/kj2/ZFIN/060725/genes_for_tom_20060725.txt
    # check if this contains all of the list missing before
    sort genesWithNoInfo.txt | uniq > genesWithNoInfo.sort
    awk '{print $2}' genes_for_tom_20060725.txt | sort | uniq > genes.txt
    comm -13 genes.txt genesWithNoInfo.uniq > genesWithNoInfo2.txt
    # there are 20 of these. Sent these to Sanger and received 
    # information for these. Copied and pasted these from e-mail into
    # moreInfo.txt. Write script to reformat: addTabs.pl
    perl addTabs.pl < moreInfo.txt > geneInfo3.txt
    grep -f genesWithInfo.txt genes_for_tom_20060725.txt > tmp
    wc -l tmp
    # 4738
    wc -l genesWithInfo.txt
    # 4822 genesWithInfo.txt
    # Not all of these are in genes_for_tom_20060725.txt so merge all the 
    # info files and uniq:
    cat genes_for_tom_new.txt genes_for_tom_20060725.txt geneInfo3.txt \
        | sort | uniq > allGeneInfo.txt
    awk '{print $2}' allGeneInfo.txt | sort | uniq -c | sort -nr > count
    # counts gene names - often occur twice but with more information in 
    # one case than the other. Seems like newer file has most information for
    # each gene.
    grep -f genesFromGff.txt genes_for_tom_20060725.txt > info1.txt
    # then list genes in info1.txt
    comm -13 genesInInfo1.sort genesFromGff.txt > genes1
    wc -l genes1
    # 55 genes1
    grep -f genes1 genes_for_tom_new.txt > info2.txt
    awk '{print $2}' info2.txt | sort | uniq > genesInInfo2.txt
    comm -13 genesInInfo2.sort genes1 > genes2
    wc -l genes2
    # 20 genes2
    # genes2 is list of genes not found in either file. Should be 20 left.
    awk '{print $2}' geneInfo3.txt | sort | uniq > genes3
    comm -12 genes2 genes3 | wc -l
    # 20 - so these are the same 20 that are in geneInfo3.txt
    # These are in geneInfo3.txt. cat all these files together
    cat info1.txt info2.txt geneInfo3.txt > allGeneInfo2.txt
    # Recreate the tab file for loading into the vegaInfoZfish table:
    rm vegaInfoZfish.txt
    # Use new version that prints out one row for each accession in field 3.
    perl formatVegaInfo.pl vegaIDs.txt allGeneInfo2.txt vegaInfoZfish.txt \  
         vegaToCloneId.txt
    # info.log contains genes for which are not in the gff file of VEGA
    # and this is empty as it should be.
    wc -l vegaInfoZfish.txt
    # 6606 vegaInfoZfish.txt
    wc -l vegaToCloneId.txt
    # 7245 vegaToCloneId.txt
    awk '{print $1}' vegaInfoZfish.txt | sort | uniq -c | sort -nr > out2
    # transcripts only have 1 entry
    awk '{print $2}' vegaInfoZfish.txt | sort | uniq > infogenes.txt
    comm -13 infogenes.txt genesFromGff.txt 
    # There are no genes in the GFF file that are not in vegaInfoZfish.txt
    # Then remake the pseudogenes track from this.
    # Next step is to find which transcripts are pseudogenes.
    grep pseudogene vegaInfoZfish.txt | sort | uniq | wc -l
    # There are only 51 in the info file, and all of these are in the GFF
    # file. Anyway, this is too sparse for a separate track, but
    # a subtrack could be created.
    # Get transcript IDs for pseudogenes.
    grep pseudogene vegaInfoZfish.txt | awk '{print $1}' > pseudogenes.ids 
    grep -f pseudogenes.ids vega.gff > vegaPseudoGene.gff 
    awk '{print $9}' vegaPseudoGene.gff |sort | uniq | wc -l
    # 51 
    grep -v -f pseudogenes.ids vega.gff > vegaGene.gff
    wc -l vega*ff
    #  98170 vega.gff
    #  97999 vegaGene.gff
    #    171 vegaPseudoGene.gff
    
    # load gff files:
    ssh hgwdev
    cd /cluster/data/danRer4/bed/vega
    hgsql -e 'drop table vegaGene;' danRer4
    hgsql -e 'drop table vegaPseudoGene;' danRer4
    ldHgGene danRer4 vegaGene vegaGene.gff
    # Read 6555 transcripts in 88104 lines in 1 files
    # 6555 groups 25 seqs 1 sources 2 feature types
    # 6555 gene predictions
 
    ldHgGene danRer4 vegaPseudoGene vegaPseudoGene.gff
    # Read 51 transcripts in 171 lines in 1 files
    # 51 groups 9 seqs 1 sources 1 feature types
    # 51 gene predictions

    # Then create SQL table for adding the zebrafish-specific information
    # Add clone_id to a separate table instead of this one. 
cat << '_EOF_' > ~/kent/src/hg/lib/vegaInfoZfish.as
table vegaInfoZfish
"Vega Genes track additional information"
    (
    string transcriptId;	"Vega transcript ID"
    string geneId;		"Vega gene ID (OTTER ID)"
    string sangerName;		"Sanger gene name"
    string zfinId;		"ZFIN ID"
    string zfinSymbol;		"ZFIN gene symbol"
    string method;		"GTF method field"
    string geneDesc; 		"Vega gene description"
    string confidence;		"Status (KNOWN, NOVEL, PUTATIVE, PREDICTED)"
    )
'_EOF_'
    cd ~/kent/src/hg/lib/
    autoSql vegaInfoZfish.as vegaInfoZfish
    mv vegaInfoZfish.h ../inc/
    # commit vegaInfoZfish{.h,.c,.as,.sql} files to CVS
    # add INDEX(geneId) to vegaInfoZfish.sql
    # create a second table for the cloneId accessions since there
    # are multiple ids for some VEGA genes. Otherwise, there would be 
    # a comma separated list in this field or many rows repeated but just
    # different in the cloneId field. Associate transcript ID to clone IDs.  
    grep ',' allGeneInfo2.txt | wc -l
    # 378
cat << '_EOF_' > ~/kent/src/hg/lib/vegaToCloneId.as
table vegaToCloneId
"Vega Genes track cloneId information"
    (
    string transcriptId;        "Vega transcript ID"
    string cloneId;             "clone ID"
    )
'_EOF_'
    cd ~/kent/src/hg/lib/
    autoSql vegaToCloneId.as vegaToCloneId
    # replace PRIMARY KEY(transcriptId) with Indices on genId and cloneId:
    perl -pi.bak -e \
    's/PRIMARY KEY\(transcriptId\)/INDEX\(transcriptId\),\nINDEX\(cloneId\)/' \
       vegaToCloneId.sql
    rm *.bak
  #  mv vegaInfoZfish.h ../inc/
    
    cd /cluster/data/danRer4/bed/vega
    hgsql -e 'drop table vegaInfoZfish;' danRer4
    hgLoadSqlTab danRer4 vegaInfoZfish ~/kent/src/hg/lib/vegaInfoZfish.sql \
                 vegaInfoZfish.txt
    hgsql -e 'drop table vegaToCloneId;' danRer4
    hgLoadSqlTab danRer4 vegaToCloneId ~/kent/src/hg/lib/vegaToCloneId.sql \
                 vegaToCloneId.txt

    # Add code to hgc.c so that this works for Zebrafish and creates the
    # relevant links. Add seraches by vega transcript ID, ZFIN ID and 
    # clone ID.  Add a Vega zebrafish-specific description to
    # trackDb/zebrafish. 
# Added entry in zebrafish/trackDb.ra to create these tracks as subtracks of
# a Vega Genes track.
# track vegaGeneZfish
# compositeTrack on
# shortLabel Vega Genes 
# longLabel Vega Annotations 
# group genes
# priority 37
# visibility hide
# chromosomes chr1,chr2,chr3,chr4,chr5,chr6,chr7,chr8,chr9,chr10,chr11,chr12,chr13,chr14,chr15,chr16,chr17,chr18,chr19,chr20,chr21,chr22,chr23,chr24,chr25
# type genePred
# url http://vega.sanger.ac.uk/Danio_rerio/geneview?transcript=$$
    
#    track vegaGene
#    subTrack vegaGeneZfish
#    shortLabel Vega Genes
#    longLabel Vega Gene Annotations
#    priority 1
#    color 0,100,180
    
#    track vegaPseudoGene
#    subTrack vegaGeneZfish
#    shortLabel Vega Pseudogenes
#    longLabel Vega Annotated Pseudogenes
#    priority 2
#    color 155,0,125

##########################################################################
# N-SCAN gene predictions (nscanGene) - (2006-08-30 markd)
    cd /cluster/data/danRer4/bed/nscan/

    # obtained NSCAN predictions from michael brent's group
    # at WUSTL
    wget -nv -r -np http://ardor.wustl.edu/jeltje/zebrafish/chr_gtf
    wget -nv -r -np http://ardor.wustl.edu/jeltje/zebrafish/chr_ptx
    # clean up and rename downloaded directorys:
    mv ardor.wustl.edu/jeltje/zebrafish/chr_gtf .
    mv ardor.wustl.edu/jeltje/zebrafish/chr_ptx .
    rm -rf ardor.wustl.edu
    rm chr_*/index.html*
    gzip chr_*/*
    chmod a-w chr_*/*.gz

    # load tracks.  Note that these have *utr features, rather than
    # exon features.  currently ldHgGene creates separate genePred exons
    # for these.
    ldHgGene -bin -gtf -genePredExt danRer4 nscanGene chr_gtf/chr*.gtf.gz

    # load protein, add .1 suffix to match transcript id
    hgPepPred -suffix=.1 danRer4 generic nscanPep chr_ptx/chr*.fa.gz
    rm *.tab

    # update trackDb; need a danRer4-specific page to describe informants
    zebrafish/danRer4/nscanGene.html   (copy from mm8 and edit)
    zebrafish/danRer4/trackDb.ra
    # changed search regex to
        termRegex chr[0-9a-zA-Z_].*\.[0-9]+\.[0-9]
