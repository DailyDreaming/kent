#!/bin/csh -f # set emacs mode
exit; # don't actually run this like a script :)

# This file describes how we made the browser database on the 
# Dog (Canis familiaris) May 2005 release.


# CREATE BUILD DIRECTORY (DONE 6/1/05 angie)
    ssh kkstore01
    mkdir /cluster/store9/canFam2
    ln -s /cluster/store9/canFam2 /cluster/data/canFam2


# DOWNLOAD MITOCHONDRION GENOME SEQUENCE (DONE 6/1/05 angie)
    mkdir /cluster/data/canFam2/M
    cd /cluster/data/canFam2/M
    # go to http://www.ncbi.nih.gov/ and search Nucleotide for 
    # "canis familiaris mitochondrion genome".  That shows the gi number:
    # 17737322
    # Use that number in the entrez linking interface to get fasta:
    wget -O chrM.fa \
      'http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Text&db=Nucleotide&uid=17737322&dopt=FASTA'
    # Edit chrM.fa: make sure the long fancy header line says it's the 
    # Canis familiaris mitochondrion complete genome, and then replace the 
    # header line with just ">chrM".


# MAKE JKSTUFF AND BED DIRECTORIES (DONE 6/1/05 angie)
    # This used to hold scripts -- better to keep them inline in the .doc 
    # so they're in CVS.  Now it should just hold lift file(s) and 
    # temporary scripts made by copy-paste from this file.  
    mkdir /cluster/data/canFam2/jkStuff
    # This is where most tracks will be built:
    mkdir /cluster/data/canFam2/bed


# DOWNLOAD AGP, FASTA & QUAL (DONE 6/2/05 angie)
    ssh kkstore01
    mkdir /cluster/data/canFam2/broad
    cd /cluster/data/canFam2/broad
    ftp ftp.broad.mit.edu
      prompt
      bin
      cd pub/assemblies/mammals/canFam2
      mget contigs.bases.gz AAEX02.full_AGP Dog2.0.agp supercontigs
      mget assembly.agp assembly.format contigs.quals.gz supercontigs.summary
      bye
    # Jean Chang is going to remove AAEX02.full_AGP to avoid confusion,
    # so make sure we can tell people how to quickly regenerate it from 
    # Dog2.0.agp:
    perl -wpe 'if (/contig_(\d+)/) { \
                 $a = sprintf "AAEX020%05d", $1+1;  s/contig_\d+/$a/; }' \
      Dog2.0.agp > /tmp/1.agp 
    diff /tmp/1.agp AAEX02.full_AGP | wc -l
#0
    # Meanwhile, the AGP has chr01, chr02 instead of chr1, chr2... yecch.
    # Substitute those out:
    sed -e 's/^chr0/chr/' Dog2.0.agp > UCSC_Dog2.0.agp


# BUILD CHROM FA (DONE 6/28/05 angie)
    ssh kkstore01
    cd /cluster/data/canFam2/broad
    awk '{print $1;}' UCSC_Dog2.0.agp | uniq > ../chrom.lst
    nice gunzip contigs.bases.gz 
    foreach chr (`cat ../chrom.lst`)
      set c = `echo $chr | sed -e 's/chr//'`
      mkdir ../$c
      awk '$1 == "'$chr'" {print;}' UCSC_Dog2.0.agp > ../$c/$chr.agp
      agpToFa -simpleMultiMixed ../$c/$chr.agp $chr ../$c/$chr.fa contigs.bases
    end
    faSize ../*/chr*.fa
#2531673953 bases (146677410 N's 2384996543 real 2384996543 upper 0 lower) in 41 sequences in 41 files
#Total size: mean 61748145.2 sd 25246010.8 min 16727 (chrM) max 126883977 (chrX) median 61280721
#N count: mean 3577497.8 sd 1401887.8
#U count: mean 58170647.4 sd 24664411.9
#L count: mean 0.0 sd 0.0
    # checkAgpAndFa prints out way too much info -- keep the end/stderr only:
    cd /cluster/data/canFam2
    foreach agp (?{,?}/chr*.agp)
      set fa = $agp:r.fa
      echo checking consistency of $agp and $fa
      checkAgpAndFa $agp $fa | tail -1
    end
    nice gzip broad/contigs.bases
    echo "chrM" >> chrom.lst


# BREAK UP SEQUENCE INTO 5 MB CHUNKS AT CONTIGS/GAPS (DONE 6/28/05 angie)
    ssh kkstore01
    cd /cluster/data/canFam2
    foreach agp (?{,?}/chr*.agp)
      set fa = $agp:r.fa
      echo splitting $agp and $fa
      cp -p $agp $agp.bak
      cp -p $fa $fa.bak
      nice splitFaIntoContigs $agp $fa . -nSize=5000000
    end
    # No _random's in this assembly, so no need to clean up after them.
    # Make a "pseudo-contig" for processing chrM too:
    mkdir M/chrM_1
    sed -e 's/chrM/chrM_1/' M/chrM.fa > M/chrM_1/chrM_1.fa
    mkdir M/lift
    echo "chrM_1/chrM_1.fa.out" > M/lift/oOut.lst
    echo "chrM_1" > M/lift/ordered.lst
    set msize = `faSize M/chrM.fa | awk '{print $1;}'`
    echo "0\tM/chrM_1\t$msize\tchrM\t$msize" > M/lift/ordered.lft
    foreach f ( ?{,?}/chr*.fa.bak )
      nice faCmp $f $f:r
    end


# MAKE LIFTALL.LFT (DONE 6/28/05 angie)
    ssh kkstore01
    cd /cluster/data/canFam2
    cat */lift/{ordered,random}.lft > jkStuff/liftAll.lft


# CREATING DATABASE (DONE 6/28/05 angie)
    ssh hgwdev
    hgsql '' -e 'create database canFam2'
    # Use df to make sure there is at least 75G free on hgwdev:/var/lib/mysql
    df -h /var/lib/mysql
#/dev/sdc1             1.8T  940G  720G  57% /var/lib/mysql


# CREATING GRP TABLE FOR TRACK GROUPING (DONE 6/28/05 angie)
    ssh hgwdev
    hgsql canFam2 -e \
      "create table grp (PRIMARY KEY(NAME)) select * from hg17.grp"


# MAKE CHROMINFO TABLE WITH (TEMPORARILY UNMASKED) 2BIT (DONE 6/28/05 angie)
    # Make .2bit, unmasked until RepeatMasker and TRF steps are done.
    # Do this now so we can load up RepeatMasker and run featureBits; 
    # can also load up other tables that don't depend on masking.  
    ssh kkstore01
    cd /cluster/data/canFam2
    nice faToTwoBit ?{,?}/chr*.fa canFam2.2bit
    mkdir bed/chromInfo
    twoBitInfo canFam2.2bit stdout \
    | awk '{print $1 "\t" $2 "\t/gbdb/canFam2/canFam2.2bit";}' \
      > bed/chromInfo/chromInfo.tab

    # Make symbolic links from /gbdb/canFam2/ to the real .2bit.
    ssh hgwdev
    mkdir /gbdb/canFam2
    ln -s /cluster/data/canFam2/canFam2.2bit /gbdb/canFam2/
    # Load /gbdb/canFam2/canFam2.2bit paths into database and save size info.
    cd /cluster/data/canFam2
    hgsql canFam2  < $HOME/kent/src/hg/lib/chromInfo.sql
    hgsql canFam2 -e 'load data local infile \
      "/cluster/data/canFam2/bed/chromInfo/chromInfo.tab" \
      into table chromInfo;'
    echo "select chrom,size from chromInfo" | hgsql -N canFam2 > chrom.sizes
    # take a look at chrom.sizes, should be 41 lines
    wc chrom.sizes
#     41      82     603 chrom.sizes


# GOLD AND GAP TRACKS (DONE 6/28/05 angie)
    ssh hgwdev
    cd /cluster/data/canFam2
    hgGoldGapGl -noGl -chromLst=chrom.lst canFam2 /cluster/data/canFam2 .
    # featureBits fails if there's no chrM_gap, so make one:
    # echo "create table chrM_gap like chr1_gap" | hgsql canFam2
    # oops, that won't work until v4.1, so do this for the time being:
    hgsql canFam2 -e "create table chrM_gap select * from chr1_gap where 0=1"


# MAKE HGCENTRALTEST ENTRY AND TRACKDB TABLE FOR CANFAM2 (DONE 8/1/05 angie)
    ssh hgwdev
    cd $HOME/kent/src/hg/makeDb/trackDb
    cvs up -d -P
    # Edit that makefile to add canFam2 in all the right places and do
    make update
    cvs commit makefile
    mkdir -p dog/canFam2
    cvs add dog dog/canFam2
    cvs ci -m "trackDb dir for dog genome(s)" dog/canFam2
    # Do this in a clean (up-to-date, no edits) tree:
    make alpha

    # Add dbDb entry (not a new organism so defaultDb and genomeClade already 
    # have entries):
    hgsql -h genome-testdb hgcentraltest \
      -e 'insert into dbDb (name, description, nibPath, organism,  \
          defaultPos, active, orderKey, genome, scientificName,  \
          htmlPath, hgNearOk, hgPbOk, sourceName)  \
          values("canFam2", "May 2005", \
          "/gbdb/canFam2/nib", "Dog", "chr14:11072309-11078928", 1, \
          18, "Dog", "Canis familiaris", \
          "/gbdb/canFam2/html/description.html", 0, 0, \
          "Broad Institute v. 2.0");'


# REPEAT MASKING (DONE braney/angie 7-10-05)
    #- Split contigs into 500kb chunks, at gaps if possible:
    ssh kkstore01
    cd /cluster/data/canFam2
    foreach chr (`cat chrom.lst`)
      set c = `echo $chr | sed -e 's/chr//'`
      foreach d ($c/chr${c}*_?{,?})
        cd $d
        echo "splitting $d"
        set contig = $d:t
        faSplit gap $contig.fa 500000 ${contig}_ -lift=$contig.lft \
          -minGapSize=100
        cd ../..
      end
    end

    #- Make the run directory and job list:
    cd /cluster/data/canFam2
    cat << '_EOF_' > jkStuff/RMDog
#!/bin/csh -fe

cd $1
pushd .
/bin/mkdir -p /tmp/canFam2/$2
/bin/cp $2 /tmp/canFam2/$2/
cd /tmp/canFam2/$2
/cluster/bluearc/RepeatMasker/RepeatMasker -s -spec dog $2
popd
/bin/cp /tmp/canFam2/$2/$2.out ./
if (-e /tmp/canFam2/$2/$2.align) /bin/cp /tmp/canFam2/$2/$2.align ./
if (-e /tmp/canFam2/$2/$2.tbl) /bin/cp /tmp/canFam2/$2/$2.tbl ./
if (-e /tmp/canFam2/$2/$2.cat) /bin/cp /tmp/canFam2/$2/$2.cat ./
/bin/rm -fr /tmp/canFam2/$2/*
/bin/rmdir --ignore-fail-on-non-empty /tmp/canFam2/$2
/bin/rmdir --ignore-fail-on-non-empty /tmp/canFam2
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x jkStuff/RMDog
    mkdir RMRun
    cp /dev/null RMRun/RMJobs
    foreach chr (`cat chrom.lst`)
      set c = `echo $chr | sed -e 's/chr//'`
      foreach d ($c/chr${c}_?{,?})
          set ctg = $d:t
          foreach f ( $d/${ctg}_?{,?}.fa )
            set f = $f:t
            echo /cluster/data/canFam2/jkStuff/RMDog \
                 /cluster/data/canFam2/$d $f \
               '{'check out line+ /cluster/data/canFam2/$d/$f.out'}' \
              >> RMRun/RMJobs
          end
      end
    end

    #- Do the run
    ssh kk
    cd /cluster/data/canFam2/RMRun
    para create RMJobs
    para try, para check, para check, para push, para check,...

# Completed: 6149 of 6149 jobs
# CPU time in finished jobs:   32138805s  535646.75m  8927.45h  371.98d  1.019 y
# IO & Wait Time:                346449s    5774.15m    96.24h    4.01d  0.011 y
# Average job time:                5283s      88.05m     1.47h    0.06d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            8642s     144.03m     2.40h    0.10d
# Submission to last job:        147094s    2451.57m    40.86h    1.70d

    #- Lift up the 500KB chunk .out's to 5MB ("pseudo-contig") level
    ssh kkstore01
    cd /cluster/data/canFam2
    foreach d (*/chr*_?{,?})
      set contig = $d:t
      echo $contig
      liftUp $d/$contig.fa.out $d/$contig.lft warn $d/${contig}_*.fa.out > /dev/null
    end

    #- Lift pseudo-contigs to chromosome level
    foreach c (`cat chrom.lst`)
      echo lifting $c
      set dir=`echo $c | sed "s/chr//" `
      cd $dir
      liftUp $c.fa.out lift/ordered.lft warn `cat lift/oOut.lst` > /dev/null
      cd ..
    end

    #- Load the .out files into the database with:
    ssh hgwdev
    cd /cluster/data/canFam2
    hgLoadOut canFam2 */chr*.fa.out


# VERIFY REPEATMASKER RESULTS  (DONE 2005-07-11 braney)
    # Eyeball some repeat annotations in the browser, compare to lib seqs.
    # Run featureBits on canFam2 and on a comparable genome build, and compare:
    ssh hgwdev
    featureBits canFam2 rmsk
# 968054174 bases of 2384996543 (40.589%) in intersection
#canFam1 is
#896773874 bases of 2359845093 (38.001%) in intersection

# SIMPLE REPEATS (TRF)  (DONE 2005-07-11 braney)
    ssh kkstore01
    mkdir /cluster/data/canFam2/bed/simpleRepeat
    cd /cluster/data/canFam2/bed/simpleRepeat
    mkdir trf
    cp /dev/null jobs.csh
    foreach d (/cluster/data/canFam2/*/chr*_?{,?})
      set ctg = $d:t
      foreach f ($d/${ctg}.fa)
        set fout = $f:t:r.bed
        echo $fout
        echo "/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $f /dev/null -bedAt=trf/$fout -tempDir=/tmp" >> jobs.csh
      end
    end
    csh -ef jobs.csh >&! jobs.log &
    # check on this with
    tail -f jobs.log
    wc -l jobs.csh
    ls -1 trf | wc -l
    endsInLf trf/*
    # When job is done do:
    liftUp simpleRepeat.bed /cluster/data/canFam2/jkStuff/liftAll.lft warn trf/*.bed

    # Load into the database:
    ssh hgwdev
    hgLoadBed canFam2 simpleRepeat \
      /cluster/data/canFam2/bed/simpleRepeat/simpleRepeat.bed \
      -sqlTable=$HOME/kent/src/hg/lib/simpleRepeat.sql
# load of simpleRepeat did not go as planned: 637318 record(s), 0 row(s) skipped, 1172 warning(s) loading bed.tab

    featureBits canFam2 simpleRepeat
# 52855902 bases of 2384996543 (2.216%) in intersection
# canFam1 is
#36509895 bases of 2359845093 (1.547%) in intersection


# PROCESS SIMPLE REPEATS INTO MASK  (DONE 2005-07-11 braney)
    # After the simpleRepeats track has been built, make a filtered version 
    # of the trf output: keep trf's with period <= 12:
    ssh kkstore01
    cd /cluster/data/canFam2/bed/simpleRepeat
    mkdir -p trfMask
    foreach f (trf/chr*.bed)
      awk '{if ($5 <= 12) print;}' $f > trfMask/$f:t
    end
    # Lift up filtered trf output to chrom coords as well:
    cd /cluster/data/canFam2
    mkdir bed/simpleRepeat/trfMaskChrom
    foreach c (`cat chrom.lst`)
      set dir=`echo $c | sed "s/chr//" `
      if (-e $dir/lift/ordered.lst) then
        perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' $dir/lift/ordered.lst > $dir/lift/oTrf.lst
        liftUp bed/simpleRepeat/trfMaskChrom/chr$dir.bed jkStuff/liftAll.lft warn `cat $dir/lift/oTrf.lst`
      endif
    end
    # Here's the coverage for the filtered TRF:
    ssh hgwdev
    cat /cluster/data/canFam2/bed/simpleRepeat/trfMaskChrom/*.bed > /tmp/filtTrf.bed
    featureBits canFam2 /tmp/filtTrf.bed
#23111877 bases of 2384996543 (0.969%) in intersection
# canFam1 was
#23017541 bases of 2359845093 (0.975%) in intersection
    featureBits canFam2 /tmp/filtTrf.bed \!rmsk
# 1205611 bases of 2384996543 (0.051%) in intersection
#canFam1 was
#1275941 bases of 2359845093 (0.054%) in intersection

# MASK SEQUENCE WITH REPEATMASKER AND SIMPLE REPEAT/TRF (DONE 2005-07-11 braney)
    ssh kkstore01
    cd /cluster/data/canFam2
    # Soft-mask (lower-case) the contig and chr .fa's, 
    # then make hard-masked versions from the soft-masked.  
    set trfCtg=bed/simpleRepeat/trfMask
    set trfChr=bed/simpleRepeat/trfMaskChrom
    foreach f (*/chr*.fa)
      echo "repeat- and trf-masking $f"
      maskOutFa -soft $f $f.out $f
      set chr = $f:t:r
      maskOutFa -softAdd $f $trfChr/$chr.bed $f
      echo "hard-masking $f"
      maskOutFa $f hard $f.masked
    end
    # Tons of warnings like this, mostly for L1M*:
#WARNING: negative rEnd: -189 chrX:117586389-117586475 L1M3e
    foreach c (`cat chrom.lst`)
      set c=`echo $c | sed "s/chr//" `
      echo "repeat- and trf-masking contigs of chr$c"
      foreach d ($c/chr*_?{,?})
        set ctg=$d:t
        set f=$d/$ctg.fa
        maskOutFa -soft $f $f.out $f
        maskOutFa -softAdd $f $trfCtg/$ctg.bed $f
        maskOutFa $f hard $f.masked
      end
    end
    #- Rebuild the nib files, using the soft masking in the fa:
    foreach f (*/chr*.fa)
      faToNib -softMask $f nib/$f:t:r.nib
    end
    # Make one big 2bit file as well, and make a link to it in 
    # /gbdb/canFam2/nib because hgBlat looks there:
    faToTwoBit */chr*.fa canFam2.2bit
    ssh hgwdev
    ln -s /cluster/data/canFam2/canFam2.2bit /gbdb/canFam2/nib/


# MAKE DESCRIPTION/SAMPLE POSITION HTML PAGE (DONE 8/1/05 angie)
    ssh hgwdev
    mkdir /gbdb/canFam2/html
    # Write ~/kent/src/hg/makeDb/trackDb/dog/canFam2/description.html 
    # with a description of the assembly and some sample position queries.  
    chmod a+r $HOME/kent/src/hg/makeDb/trackDb/dog/canFam2/description.html
    # Check it in and copy (ideally using "make alpha" in trackDb) to 
    # /gbdb/canFam2/html


# PUT MASKED SEQUENCE OUT FOR CLUSTER RUNS (DONE 8/1/05 angie)
    # pitakluster (blastz etc):
    ssh pk
    mkdir /san/sanvol1/scratch/canFam2
    rsync -av /cluster/data/canFam2/nib /san/sanvol1/scratch/canFam2/
    mkdir /san/sanvol1/scratch/canFam2/rmsk
    cp -p /cluster/data/canFam2/*/chr*.fa.out \
       /san/sanvol1/scratch/canFam2/rmsk
    # small cluster (chaining etc):
    ssh kkr1u00
    mkdir -p /iscratch/i/canFam2/nib
    rsync -av /cluster/data/canFam2/nib /iscratch/i/canFam2/
    iSync
    # big cluster (genbank):
    ssh kkstore01
    mkdir /cluster/bluearc/scratch/hg/canFam2
    rsync -av /cluster/data/canFam2/nib /cluster/bluearc/scratch/hg/canFam2/
    # ask cluster-admin to rsync that to all big cluster nodes' /scratch/...


# MAKE LINEAGE-SPECIFIC REPEATS VS. HUMAN, MOUSE (DONE 8/1/05 angie)
    ssh kolossus
    cd /san/sanvol1/scratch/canFam2/rmsk
    # Run Arian's DateRepsinRMoutput.pl to add extra columns telling 
    # whether repeats in -query are also expected in -comp species.  
    # Human in extra column 1, Mouse in extra column 2
    foreach outfl ( *.out )
        echo "$outfl"
        /cluster/bluearc/RepeatMasker/DateRepeats \
          ${outfl} -query dog -comp human -comp mouse
    end
    # Now extract human (extra column 1), mouse (extra column).
    cd ..
    mkdir linSpecRep.notInHuman
    mkdir linSpecRep.notInMouse
    foreach f (rmsk/*.out_homo-sapiens_mus-musculus)
        set base = $f:t:r:r
        echo $base.out.spec
        /cluster/bin/scripts/extractLinSpecReps 1 $f > \
                        linSpecRep.notInHuman/$base.out.spec
        /cluster/bin/scripts/extractLinSpecReps 2 $f > \
                        linSpecRep.notInMouse/$base.out.spec
    end
    wc -l rmsk/*.out
#4533630 total
    wc -l linSpecRep.notInHuman/*
#1542788 total
    wc -l linSpecRep.notInMouse/*
#1546408 total
    # Clean up.
    rm rmsk/*.out_h*


# PRODUCING GENSCAN PREDICTIONS (DONE 8/11/05 angie)
    ssh hgwdev
    mkdir /cluster/data/canFam2/bed/genscan
    cd /cluster/data/canFam2/bed/genscan
    # Check out hg3rdParty/genscanlinux to get latest genscan:
    cvs co hg3rdParty/genscanlinux
    # Run on small cluster (more mem than big cluster).
    ssh kki
    cd /cluster/data/canFam2/bed/genscan
    # Make 3 subdirectories for genscan to put their output files in
    mkdir gtf pep subopt
    # Generate a list file, genome.list, of all the hard-masked contigs that 
    # *do not* consist of all-N's (which would cause genscan to blow up)
    rm -f genome.list
    touch genome.list
    foreach f ( `ls -1S /cluster/data/canFam2/*/chr*_*/chr*_?{,?}.fa.masked` )
      egrep '[ACGT]' $f > /dev/null
      if ($status == 0) echo $f >> genome.list
    end
    wc -l genome.list
#495
    # Create template file, gsub, for gensub2.  For example (3-line file):
    cat << '_EOF_' > gsub
#LOOP
/cluster/bin/x86_64/gsBig {check in line+ $(path1)} {check out line gtf/$(root1).gtf} -trans={check out line pep/$(root1).pep} -subopt={check out line subopt/$(root1).bed} -exe=hg3rdParty/genscanlinux/genscan -par=hg3rdParty/genscanlinux/HumanIso.smat -tmp=/tmp -window=2400000
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
    gensub2 genome.list single gsub jobList
    para make jobList
#Completed: 493 of 495 jobs
#Crashed: 2 jobs
#Average job time:                 918s      15.30m     0.25h    0.01d
#Longest finished job:           30383s     506.38m     8.44h    0.35d
#Submission to last job:        132236s    2203.93m    36.73h    1.53d
    # If there are crashes, diagnose with "para problems" and "para crashed".  
    # If a job crashes due to genscan running out of memory, re-run it 
    # manually with "-window=1200000" instead of "-window=2400000".
    ssh kkr7u00
    cd /cluster/data/canFam2/bed/genscan
    /cluster/bin/x86_64/gsBig /cluster/data/canFam2/1/chr1_23/chr1_23.fa.masked gtf/chr1_23.fa.gtf -trans=pep/chr1_23.fa.pep -subopt=subopt/chr1_23.fa.bed -exe=hg3rdParty/genscanlinux/genscan -par=hg3rdParty/genscanlinux/HumanIso.smat -tmp=/tmp -window=1200000
    /cluster/bin/x86_64/gsBig /cluster/data/canFam2/27/chr27_1/chr27_1.fa.masked gtf/chr27_1.fa.gtf -trans=pep/chr27_1.fa.pep -subopt=subopt/chr27_1.fa.bed -exe=hg3rdParty/genscanlinux/genscan -par=hg3rdParty/genscanlinux/HumanIso.smat -tmp=/tmp -window=1200000   
    ls -1 gtf | wc -l
#    495
    endsInLf gtf/*

    # Convert these to chromosome level files as so:
    ssh kkstore01
    cd /cluster/data/canFam2/bed/genscan
    liftUp genscan.gtf ../../jkStuff/liftAll.lft warn gtf/*.gtf
    liftUp genscanSubopt.bed ../../jkStuff/liftAll.lft warn subopt/*.bed
    cat pep/*.pep > genscan.pep

    # Load into the database as so:
    ssh hgwdev
    cd /cluster/data/canFam2/bed/genscan
    ldHgGene -gtf canFam2 genscan genscan.gtf
    hgPepPred canFam2 generic genscanPep genscan.pep
    hgLoadBed canFam2 genscanSubopt genscanSubopt.bed


# MAKE 10.OOC, 11.OOC FILES FOR BLAT (DONE 8/1/05 angie)
    ssh kolossus
    # numerator is canFam2 gapless bases as reported by featureBits, 
    # denominator is hg17 gapless bases as reported by featureBits,
    # 1024 is threshold used for human -repMatch:
    calc \( 2384996543 / 2867328468 \) \* 1024
# ( 2384996543 / 2867328468 ) * 1024 = 851.746316
    # ==> use -repMatch=852 according to size scaled down from 1024 for human.
    mkdir /cluster/bluearc/canFam2
    mkdir /cluster/data/canFam2/bed/ooc
    cd /cluster/data/canFam2/bed/ooc
    ls -1 /cluster/data/canFam2/nib/chr*.nib > nib.lst
    blat nib.lst /dev/null /dev/null -tileSize=11 \
      -makeOoc=/cluster/bluearc/canFam2/11.ooc -repMatch=852
#Wrote 27388 overused 11-mers to /cluster/bluearc/canFam2/11.ooc


# AUTO UPDATE GENBANK MRNA RUN  (DONE 8/8/05 angie)
    ssh hgwdev
    # Update genbank config and source in CVS:
    cd ~/kent/src/hg/makeDb/genbank
    cvsup .

    # Edit etc/genbank.conf and add these lines:
# canFam2 (dog)
canFam2.genome = /scratch/hg/canFam2/nib/chr*.nib
canFam2.lift = /cluster/data/canFam2/jkStuff/liftAll.lft
canFam2.refseq.mrna.native.load = no
canFam2.refseq.mrna.xeno.load = yes
canFam2.refseq.mrna.xeno.pslReps = -minCover=0.15 -minAli=0.75 -nearTop=0.005
canFam2.genbank.mrna.xeno.load = yes
canFam2.genbank.est.xeno.load = no
canFam2.downloadDir = canFam2

    cvs ci etc/genbank.conf
    # Edit src/align/gbBlat to add /cluster/bluearc/canFam2/11.ooc
    cvs diff src/align/gbBlat
    make
    cvs ci src/align/gbBlat
    # Install to /cluster/data/genbank:
    make install-server

    ssh eieio
    cd /cluster/data/genbank
    # This is an -initial run, (xeno) refseq only:
    nice bin/gbAlignStep -srcDb=refseq -type=mrna -initial canFam2 &
    # Load results:
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 -drop -initialLoad canFam2
    featureBits canFam2 xenoRefGene
#41278562 bases of 2384996543 (1.731%) in intersection
    # Clean up:
    rm -rf work/initial.canFam2

    ssh eieio
    cd /cluster/data/genbank
    # This is an -initial run, mRNA only:
    nice bin/gbAlignStep -srcDb=genbank -type=mrna -initial canFam2 &
    # Load results:
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 -drop -initialLoad canFam2
    featureBits canFam2 mrna
#2061533 bases of 2384996543 (0.086%) in intersection
    featureBits canFam2 xenoMrna
#63057460 bases of 2384996543 (2.644%) in intersection
    # Clean up:
    rm -rf work/initial.canFam2

    ssh eieio
    # -initial for ESTs:
    nice bin/gbAlignStep -srcDb=genbank -type=est -initial canFam2 &
    # Load results:
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 canFam2 &
    featureBits canFam2 intronEst
#16241242 bases of 2384996543 (0.681%) in intersection
    featureBits canFam2 est
#41719045 bases of 2384996543 (1.749%) in intersection
    # Clean up:
    rm -rf work/initial.canFam2


# SWAP CHAINS FROM HG17, BUILD NETS ETC. (DONE 8/3/05 angie)
    mkdir /cluster/data/canFam2/bed/blastz.hg17.swap
    cd /cluster/data/canFam2/bed/blastz.hg17.swap
    doBlastzChainNet.pl -swap /cluster/data/hg17/bed/blastz.canFam2/DEF \
      >& do.log &
    tail -f do.log
    # Add {chain,net}Hg17 to trackDb.ra if necessary.

# RE-RUN NETTOAXT, AXTTOMAF FOR HG17 (DONE 10/31/05 angie)
    # Kate fixed netToAxt to avoid duplicated blocks, which is important 
    # for input to multiz.  Regenerate maf using commands from sub-script 
    # netChains.csh generated by doBlastzChainNet.pl above.  
    ssh kolossus
    cd /cluster/data/canFam2/bed/blastz.hg17.swap/axtChain
    netSplit canFam2.hg17.net.gz net
    chainSplit chain canFam2.hg17.all.chain.gz
    cd ..
    mv axtNet axtNet.orig
    mkdir axtNet
    foreach f (axtChain/net/*.net)
      netToAxt $f axtChain/chain/$f:t:r.chain \
        /cluster/data/canFam2/nib /iscratch/i/hg17/nib stdout \
      | axtSort stdin stdout \
      | gzip -c > axtNet/$f:t:r.canFam2.hg17.net.axt.gz
    end
    rm -r mafNet
    mkdir mafNet
    foreach f (axtNet/*.canFam2.hg17.net.axt.gz)
      axtToMaf -tPrefix=canFam2. -qPrefix=hg17. $f \
            /cluster/data/canFam2/chrom.sizes /cluster/data/hg17/chrom.sizes \
            stdout \
      | gzip -c > mafNet/$f:t:r:r:r:r:r.maf.gz
    end
    rm -r axtChain/{chain,net}/ axtNet.orig


# QUALITY SCORES (DONE 8/11/05 angie)
    ssh kkstore01
    mkdir /cluster/data/canFam2/bed/quality
    cd /cluster/data/canFam2/bed/quality
    qaToQac ../../broad/contigs.quals.gz stdout \
    | qacAgpLift ../../broad/UCSC_Dog2.0.agp stdin chrom.qac
    mkdir wigData
    # Build .wig, .wib files in current directory so that "wigData/" doesn't 
    # appear in the .wig's:
    cd wigData
    foreach agp (../../../*/chr*.agp)
      set chr = $agp:t:r
      set abbrev = `echo $chr | sed -e 's/^chr//;  s/_random/r/;'`
      echo $chr to qual_$abbrev wiggle
      qacToWig -fixed ../chrom.qac -name=$chr stdout \
      | wigEncode stdin qual_$abbrev.{wig,wib}
    end
    # Verify size of .wib file = chrom length
    foreach f (*.wib)
      set abbrev = $f:t:r
      set chr = `echo $abbrev | sed -e 's/^qual_/chr/;  s/r$/_random/;'`
      set wibLen = `ls -l $f | awk '{print $5;}'`
      set chromLen = `grep -w $chr ../../../chrom.sizes | awk '{print $2;}'`
      if ($wibLen != $chromLen) then
        echo "ERROR: $chr size is $chromLen but wib size is $wibLen"
      else
        echo $chr OK.
      endif
    end

    # /gbdb & load:
    ssh hgwdev
    cd /cluster/data/canFam2/bed/quality/wigData
    mkdir -p /gbdb/canFam2/wib
    ln -s `pwd`/*.wib /gbdb/canFam2/wib
    hgLoadWiggle canFam2 quality *.wig


# GC 5 BASE WIGGLE TRACK (DONE 8/11/05 angie)
    ssh kki
    mkdir /cluster/data/canFam2/bed/gc5Base
    cd /cluster/data/canFam2/bed/gc5Base
    cat > doGc5Base.csh <<'_EOF_'
#!/bin/csh -fe
set chr = $1
set c = `echo $chr | sed -e 's/^chr//;  s/_random/r/;'`
/cluster/bin/x86_64/hgGcPercent \
  -chr=${chr} -wigOut -doGaps -file=stdout -win=5 canFam2 \
  /iscratch/i/canFam2/nib \
| /cluster/bin/x86_64/wigEncode stdin gc5Base_${c}{.wig,.wib}
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doGc5Base.csh
    cp /dev/null spec
    foreach c (`cat ../../chrom.lst`)
      echo "./doGc5Base.csh $c" >> spec
    end
    para make spec
    para time
#Completed: 41 of 41 jobs
#Average job time:                  23s       0.38m     0.01h    0.00d
#Longest finished job:              44s       0.73m     0.01h    0.00d
#Submission to last job:           259s       4.32m     0.07h    0.00d
    # /gbdb and load track on hgwdev
    ssh hgwdev
    cd /cluster/data/canFam2/bed/gc5Base
    mkdir -p /gbdb/canFam2/wib
    ln -s `pwd`/*.wib /gbdb/canFam2/wib
    hgLoadWiggle canFam2 gc5Base *.wig


# EXTRACT LINEAGE-SPECIFIC REPEATS FOR RAT (DONE 8/12/05 angie)
    ssh kolossus
    cd /panasas/store/canFam2/rmsk
    # Run Arian's DateRepsinRMoutput.pl to add extra columns telling 
    # whether repeats in -query are also expected in -comp species.  
    foreach outfl ( *.out )
        echo "$outfl"
        /cluster/bluearc/RepeatMasker/DateRepeats \
          ${outfl} -query dog -comp rat
    end
    # Now extract rat (extra column 1):
    cd ..
    mkdir linSpecRep.notInRat
    foreach f (rmsk/*.out_rattus)
        set base = $f:t:r:r
        echo $base.out.spec
        /cluster/bin/scripts/extractRepeats 1 $f > \
		linSpecRep.notInRat/$base.out.spec
    end
    # Clean up.
    rm rmsk/*.out_rat*


# LOAD CPGISSLANDS (DONE 8/12/05 angie)
    ssh hgwdev
    mkdir -p /cluster/data/canFam2/bed/cpgIsland
    cd /cluster/data/canFam2/bed/cpgIsland
    # Build software from Asif Chinwalla (achinwal@watson.wustl.edu)
    cvs co hg3rdParty/cpgIslands
    cd hg3rdParty/cpgIslands
    make
    mv cpglh.exe /cluster/data/canFam2/bed/cpgIsland/
    
    ssh kolossus
    cd /cluster/data/canFam2/bed/cpgIsland
    foreach f (../../*/chr*.fa.masked)
      set fout=$f:t:r:r.cpg
      echo running cpglh on $f to $fout
      ./cpglh.exe $f > $fout
    end
    # Transform cpglh output to bed +
    cat << '_EOF_' > filter.awk
/* Input columns: */
/* chrom, start, end, len, CpG: cpgNum, perGc, cpg:gpc, observed:expected */
/* chr1\t 41776\t 42129\t 259\t CpG: 34\t 65.8\t 0.92\t 0.94 */
/* Output columns: */
/* chrom, start, end, name, length, cpgNum, gcNum, perCpg, perGc, obsExp */
/* chr1\t41775\t42129\tCpG: 34\t354\t34\t233\t19.2\t65.8\to0.94 */
{
$2 = $2 - 1;
width = $3 - $2;
printf("%s\t%d\t%s\t%s %s\t%s\t%s\t%0.0f\t%0.1f\t%s\t%s\n",
       $1, $2, $3, $5,$6, width,
       $6, width*$7*0.01, 100.0*2*$6/width, $7, $9);
}
'_EOF_'
    # << this line makes emacs coloring happy
    awk -f filter.awk chr*.cpg > cpgIsland.bed

    # load into database:
    ssh hgwdev
    cd /cluster/data/canFam2/bed/cpgIsland
    hgLoadBed canFam2 cpgIslandExt -tab -noBin \
      -sqlTable=$HOME/kent/src/hg/lib/cpgIslandExt.sql cpgIsland.bed
    wc -l cpgIsland.bed 
#  47804 cpgIsland.bed
    featureBits canFam2 cpgIslandExt
#38974374 bases of 2384996543 (1.634%) in intersection


# ANDY LAW CPGISSLANDS (DONE 8/12/05 angie)
    # See notes in makeGalGal2.doc.
    ssh kolossus
    mkdir /cluster/data/canFam2/bed/cpgIslandGgfAndy
    cd /cluster/data/canFam2/bed/cpgIslandGgfAndy
    # Use masked sequence since this is a mammal...
    cp /dev/null cpgIslandGgfAndyMasked.bed
    foreach f (../../*/chr*.fa.masked)
      set chr = $f:t:r:r
      echo preproc and run on masked $chr
      /cluster/home/angie/bin/i386/preProcGgfAndy $f \
      | /cluster/home/angie/ggf-andy-cpg-island.pl \
      | perl -wpe 'chomp; ($s,$e,$cpg,$n,$c,$g,$oE) = split("\t"); $s--; \
                   $gc = $c + $g;  $pCpG = (100.0 * 2 * $cpg / $n); \
                   $pGc = (100.0 * $gc / $n); \
                   $_ = "'$chr'\t$s\t$e\tCpG: $cpg\t$n\t$cpg\t$gc\t" . \
                        "$pCpG\t$pGc\t$oE\n";' \
      >> cpgIslandGgfAndyMasked.bed
    end
    # load into database:
    ssh hgwdev
    cd /cluster/data/canFam2/bed/cpgIslandGgfAndy
    sed -e 's/cpgIslandExt/cpgIslandGgfAndyMasked/g' \
      $HOME/kent/src/hg/lib/cpgIslandExt.sql > cpgIslandGgfAndyMasked.sql
    hgLoadBed canFam2 cpgIslandGgfAndyMasked -tab -noBin \
      -sqlTable=cpgIslandGgfAndyMasked.sql cpgIslandGgfAndyMasked.bed
    featureBits canFam2 cpgIslandExt
#38974374 bases of 2384996543 (1.634%) in intersection
    featureBits canFam2 cpgIslandGgfAndyMasked
#99187178 bases of 2384996543 (4.159%) in intersection
    wc -l ../cpgIsland/cpgIsland.bed *bed
#   47804 ../cpgIsland/cpgIsland.bed
#  138037 cpgIslandGgfAndyMasked.bed


# MAKE LINEAGE-SPECIFIC REPEATS FOR CHICKEN & BEYOND (DONE 8/15/05 angie)
    # In an email 2/13/04, Arian said we could treat all human repeats as 
    # lineage-specific for human-chicken blastz.  Do the same for dog.  
    # Scripts expect *.out.spec filenames, so set that up:
    ssh kkstore01
    cd /cluster/data/canFam2
    mkdir /panasas/store/canFam2/linSpecRep.notInNonMammal
    foreach f (/panasas/store/canFam2/rmsk/chr*.fa.out)
      ln $f /panasas/store/canFam2/linSpecRep.notInNonMammal/$f:t:r:r.out.spec
    end


# SWAP CHAINS FROM MM6, BUILD NETS ETC. (DONE 8/15/05 angie)
    mkdir /cluster/data/canFam2/bed/blastz.mm6.swap
    cd /cluster/data/canFam2/bed/blastz.mm6.swap
    doBlastzChainNet.pl -swap /cluster/data/mm6/bed/blastz.canFam2/DEF \
      >& do.log
    echo "check /cluster/data/canFam2/bed/blastz.mm6.swap/do.log" \
    | mail -s "check do.log" $USER
    # Add {chain,net}Mm6 to trackDb.ra if necessary.

# RE-RUN NETTOAXT, AXTTOMAF FOR MM6 (DONE 11/1/05 angie)
    # Kate fixed netToAxt to avoid duplicated blocks, which is important 
    # for input to multiz.  Regenerate maf using commands from sub-script 
    # netChains.csh generated by doBlastzChainNet.pl above.  
    ssh kolossus
    cd /cluster/data/canFam2/bed/blastz.mm6.swap/axtChain
    netSplit canFam2.mm6.net.gz net
    chainSplit chain canFam2.mm6.all.chain.gz
    cd ..
    mv axtNet axtNet.orig
    mkdir axtNet
    foreach f (axtChain/net/*.net)
      netToAxt $f axtChain/chain/$f:t:r.chain \
        /cluster/data/canFam2/nib /cluster/data/mm6/nib stdout \
      | axtSort stdin stdout \
      | gzip -c > axtNet/$f:t:r.canFam2.mm6.net.axt.gz
    end
    rm -r mafNet
    mkdir mafNet
    foreach f (axtNet/*.canFam2.mm6.net.axt.gz)
      axtToMaf -tPrefix=canFam2. -qPrefix=mm6. $f \
            /cluster/data/canFam2/chrom.sizes /cluster/data/mm6/chrom.sizes \
            stdout \
      | gzip -c > mafNet/$f:t:r:r:r:r:r.maf.gz
    end
    rm -r axtChain/{chain,net}/ axtNet.orig


# SWAP CHAINS FROM RN3, BUILD NETS ETC. (DONE 8/16/05 angie)
    mkdir /cluster/data/canFam2/bed/blastz.rn3.swap
    cd /cluster/data/canFam2/bed/blastz.rn3.swap
    doBlastzChainNet.pl -swap /cluster/data/rn3/bed/blastz.canFam2/DEF \
      >& do.log
    echo "check /cluster/data/canFam2/bed/blastz.rn3.swap/do.log" \
    | mail -s "check do.log" $USER
    # Add {chain,net}Rn3 to trackDb.ra if necessary.

# RE-RUN NETTOAXT, AXTTOMAF FOR RN3 (DONE 11/2/05 angie)
    # Kate fixed netToAxt to avoid duplicated blocks, which is important 
    # for input to multiz.  Regenerate maf using commands from sub-script 
    # netChains.csh generated by doBlastzChainNet.pl above.  
    ssh kolossus
    cd /cluster/data/canFam2/bed/blastz.rn3.swap/axtChain
    netSplit canFam2.rn3.net.gz net
    chainSplit chain canFam2.rn3.all.chain.gz
    cd ..
    mv axtNet axtNet.orig
    mkdir axtNet
    foreach f (axtChain/net/*.net)
      netToAxt $f axtChain/chain/$f:t:r.chain \
        /cluster/data/canFam2/nib /cluster/data/rn3/nib stdout \
      | axtSort stdin stdout \
      | gzip -c > axtNet/$f:t:r.canFam2.rn3.net.axt.gz
    end
    rm -r mafNet
    mkdir mafNet
    foreach f (axtNet/*.canFam2.rn3.net.axt.gz)
      axtToMaf -tPrefix=canFam2. -qPrefix=rn3. $f \
            /cluster/data/canFam2/chrom.sizes /cluster/data/rn3/chrom.sizes \
            stdout \
      | gzip -c > mafNet/$f:t:r:r:r:r:r.maf.gz
    end
    rm -r axtChain/{chain,net}/ axtNet.orig


# MAKE THIS THE DEFAULT ASSEMBLY WHEN THERE ARE ENOUGH TRACKS (DONE 11/28/05 angie)
    hgsql -h genome-testdb hgcentraltest \
      -e 'update defaultDb set name = "canFam2" where genome = "Dog";'


# MAKE Human Proteins track (INPROGRESS 8/17 braney)
    ssh kkstore01
    cd /cluster/data/canFam2
    mkdir blastDb
    for i in */*/*_*_*.fa; do ln `pwd`/$i blastDb; done 
    cd blastDb
    for i in *.fa; do formatdb -p F -i $i; rm $i; done

    ssh pk
    destDir=/san/sanvol1/scratch/canFam2/blastDb
    mkdir -p $destDir
    cd /cluster/data/canFam2/blastDb
    for i in nin nsq nhr; do cp *.$i $destDir; done
    mkdir -p /cluster/data/canFam2/bed/tblastn.hg17KG
    cd /cluster/data/canFam2/bed/tblastn.hg17KG
    ls -1S $destDir/*.nsq | sed "s/\.nsq//" > target.lst

    mkdir kgfa
    # calculate a reasonable number of jobs
    calc `wc /cluster/data/hg17/bed/blat.hg17KG/hg17KG.psl | awk "{print \\\$1}"`/\(150000/`wc target.lst | awk "{print \\\$1}"`\)
# 37365/(150000/498) = 124.051800
    split -l 124 /cluster/data/hg17/bed/blat.hg17KG/hg17KG.psl kgfa/kg
    cd kgfa
    for i in *; do pslxToFa $i $i.fa; rm $i; done
    cd ..
    ls -1S kgfa/*.fa > kg.lst
    rm -rf  /cluster/bluearc/canFam2/bed/tblastn.hg17KG/blastOut
    mkdir -p /cluster/bluearc/canFam2/bed/tblastn.hg17KG/blastOut
    ln -s  /cluster/bluearc/canFam2/bed/tblastn.hg17KG/blastOut
    for i in `cat kg.lst`; do  mkdir blastOut/`basename $i .fa`; done
    tcsh
    cat << '_EOF_' > blastGsub
#LOOP
blastSome $(path1) {check in line $(path2)} {check out exists blastOut/$(root2)/q.$(root1).psl } 
#ENDLOOP
'_EOF_'
    cat << '_EOF_' > blastSome
#!/bin/sh
BLASTMAT=/iscratch/i/blast/data
export BLASTMAT
g=`basename $2`
f=/tmp/`basename $3`.$g
for eVal in 0.01 0.001 0.0001 0.00001 0.000001 1E-09 1E-11
do
if /scratch/blast/blastall -M BLOSUM80 -m 0 -F no -e $eVal -p tblastn -d $1 -i $2 -o $f.8
then
        mv $f.8 $f.1
        break;
fi
done
if test -f  $f.1
then
if /cluster/bin/i386/blastToPsl $f.1 $f.2
then
        liftUp -nosort -type=".psl" -pslQ -nohead $3.tmp /cluster/data/hg17/bed/blat.hg17KG/protein.lft warn $f.2
        mv $3.tmp $3
        rm -f $f.1 $f.2 
        exit 0
    fi
fi
rm -f $f.1 $f.2 $3.tmp $f.8
exit 1
'_EOF_'

    chmod +x blastSome
    gensub2 target.lst kg.lst blastGsub blastSpec

    ssh kk
    cd /cluster/data/canFam2/bed/tblastn.hg17KG
    para create blastSpec
    para push

# Completed: 150396 of 150396 jobs
# CPU time in finished jobs:   17288756s  288145.93m  4802.43h  200.10d  0.548 y
# IO & Wait Time:               1432927s   23882.12m   398.04h   16.58d  0.045 y
# Average job time:                 124s       2.07m     0.03h    0.00d
# Longest finished job:             644s      10.73m     0.18h    0.01d
# Submission to last job:        235403s    3923.38m    65.39h    2.72d

    ssh kki
    cd /cluster/data/canFam2/bed/tblastn.hg17KG
    tcsh
    cat << '_EOF_' > chainGsub
#LOOP
chainSome $(path1)
#ENDLOOP
'_EOF_'

    cat << '_EOF_' > chainSome
(cd $1; cat q.*.psl | simpleChain -prot -outPsl -maxGap=100000 stdin ../c.`basename $1`.psl)
'_EOF_'

    chmod +x chainSome
    ls -1dS `pwd`/blastOut/kg?? > chain.lst
    gensub2 chain.lst single chainGsub chainSpec
    para create chainSpec
    para push

# Completed: 302 of 302 jobs
# CPU time in finished jobs:        725s      12.08m     0.20h    0.01d  0.000 y
# IO & Wait Time:                 22191s     369.85m     6.16h    0.26d  0.001 y
# Average job time:                  76s       1.26m     0.02h    0.00d
# Longest finished job:             228s       3.80m     0.06h    0.00d
# Submission to last job:          2348s      39.13m     0.65h    0.03d

    ssh kkstore01
    cd /cluster/data/canFam2/bed/tblastn.hg17KG/blastOut
    for i in kg??
    do 
	awk "(\$13 - \$12)/\$11 > 0.6 {print}" c.$i.psl > c60.$i.psl
	sort -rn c60.$i.psl | pslUniq stdin u.$i.psl
	awk "((\$1 / \$11) ) > 0.60 { print   }" c60.$i.psl > m60.$i.psl
	echo $i
    done

    sort -u -k 14,14 -k 16,16n -k 17,17n u.*.psl m60* > /cluster/data/canFam2/bed/tblastn.hg17KG/blastHg17KG.psl
    cd ..

    ssh hgwdev
    cd /cluster/data/canFam2/bed/tblastn.hg17KG
    hgLoadPsl canFam2 blastHg17KG.psl
    exit

    # back to kksilo
    rm -rf blastOut

# End tblastn

# BLASTZ SELF  (DONE braney 8-29-2005)
# For future reference -- doBlastzChainNet.pl should have been used
    ssh pk
    mkdir -p /cluster/data/canFam2/bed/blastz.canFam2
    cd /cluster/data/canFam2/bed/blastz.canFam2
    cat << '_EOF_' > DEF
# dog vs. dog
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
BLASTZ_ABRIDGE_REPEATS=0

# TARGET
# Dog
SEQ1_DIR=/scratch/hg/canFam2/nib
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY
# Dog
SEQ2_DIR=/scratch/hg/canFam2/nib
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=10000

BASE=/cluster/data/canFam2/bed/blastz.canFam2

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len
'_EOF_'
    # << this line makes emacs coloring happy

    cp /cluster/data/canFam2/chrom.sizes S1.len
    cp /cluster/data/canFam2/chrom.sizes S2.len
    mkdir run
    cd run
    partitionSequence.pl 10000000 10000 /cluster/data/canFam2/nib ../S1.len \
      -xdir xdir.sh -rawDir ../lav \
    > canFam2.lst
    csh -ef xdir.sh
    cat << '_EOF_' > gsub
#LOOP
/cluster/bin/scripts/blastz-run-ucsc $(path1) $(path2) ../DEF {check out line ../lav/$(file1)/$(file1)_$(file2).lav}
#ENDLOOP
'_EOF_'
    # << this line keeps emacs coloring happy
    gensub2 canFam2.lst canFam2.lst gsub jobList
    para create jobList
    para try, check, push, check, ...

# Completed: 70756 of 70756 jobs
# CPU time in finished jobs:    5743034s   95717.24m  1595.29h   66.47d  0.182 y
# IO & Wait Time:                199800s    3330.00m    55.50h    2.31d  0.006 y
# Average job time:                  84s       1.40m     0.02h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            2758s      45.97m     0.77h    0.03d
# Submission to last job:         17852s     297.53m     4.96h    0.21d

    # convert lav files to psl 
    # First, combine all files per partition (too many files per chrom
    # to do in one swoop!).  Then do another round to collect all files 
    # per chrom.  
    ssh kki
    cd /cluster/data/canFam2/bed/blastz.canFam2.2
    mkdir pslChrom pslParts run.lavToPsl
    cd run.lavToPsl
    ls -1d ../lav/* | sed -e 's@/$@@' > parts.lst
    # For self alignments, we need lavToAxt's -dropSelf behavior, 
    # so go lav->axt->psl... could add -dropSelf to lavToPsl, but that 
    # might be somewhat invasive and perhaps not really much faster (?) 
    # because the sequence must be dug up in order to rescore...
    cat << '_EOF_' > do.csh
#!/bin/csh -ef
cat $1/*.lav \
| lavToAxt -dropSelf stdin /iscratch/i/canFam2/nib \
                           /iscratch/i/canFam2/nib stdout \
| axtToPsl stdin ../S1.len ../S2.len stdout \
| sort -k 14,14 -k 16n,17n \
| gzip -c > $2
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x do.csh
    cat << '_EOF_' > gsub
#LOOP
./do.csh $(path1) {check out exists ../pslParts/$(file1).psl.gz }
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
    gensub2 parts.lst single gsub jobList
    para create jobList
    para try, check, push, check, ...
# Completed: 266 of 275 jobs
# Crashed: 9 jobs
# CPU time in finished jobs:       1228s      20.47m     0.34h    0.01d  0.000 y
# IO & Wait Time:                  7117s     118.61m     1.98h    0.08d  0.000 y
# Average job time:                  31s       0.52m     0.01h    0.00d
# Longest finished job:              74s       1.23m     0.02h    0.00d
# Submission to last job:           641s      10.68m     0.18h    0.01d

    awk '{print $1;}' /cluster/data/canFam2/chrom.sizes > chroms.lst
    cat << '_EOF_' > doChrom.csh
#!/bin/csh -ef
zcat ../pslParts/$1.*.psl.gz \
| sort -k 14,14 -k 16n,17n \
| gzip -c > $2
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doChrom.csh
    cat << '_EOF_' > gsub.chroms
#LOOP
./doChrom.csh $(root1) {check out exists ../pslChrom/$(root1).psl.gz }
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
    gensub2 chroms.lst single gsub.chroms jobList
    para create jobList
    para try, check, push, check, ...

# Completed: 40 of 40 jobs
# CPU time in finished jobs:        463s       7.71m     0.13h    0.01d  0.000 y
# IO & Wait Time:                   178s       2.97m     0.05h    0.00d  0.000 y
# Average job time:                  16s       0.27m     0.00h    0.00d
# Longest finished job:              38s       0.63m     0.01h    0.00d
# Submission to last job:            61s       1.02m     0.02h    0.00d

# CHAIN SELF BLASTZ 
# For future reference -- doBlastzChainNet.pl should have been used
    # Run axtChain on little cluster
    ssh kki
    cd /cluster/data/canFam2/bed/blastz.canFam2.2
    mkdir -p axtChain/run1
    cd axtChain/run1
    mkdir out chain
    ls -1S ../../pslChrom/*.psl.gz > input.lst
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    cat << '_EOF_' > doChain
#!/bin/csh -ef
setenv PATH /cluster/bin/x86_64:$PATH
axtChain -verbose=0 -psl $1 /iscratch/i/canFam2/nib \
                            /iscratch/i/canFam2/nib stdout \
| chainAntiRepeat /iscratch/i/canFam2/nib /iscratch/i/canFam2/nib \
    stdin $2
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doChain
    gensub2 input.lst single gsub jobList
    para create jobList
    para try, check, push, check...
#Completed: 40 of 41 jobs
#Crashed: 1 jobs
#Average job time:                 185s       3.09m     0.05h    0.00d
#Longest job:                      490s       8.17m     0.14h    0.01d
#Submission to last job:         74789s    1246.48m    20.77h    0.87d

    # now on the cluster server, sort chains
    ssh kolossus
    cd /cluster/data/canFam2/bed/blastz.canFam2.2/axtChain
    chainMergeSort run1/chain/*.chain > all.chain
    chainSplit chain all.chain
    rm run1/chain/*.chain

    # take a look at score distr's
    foreach f (chain/*.chain)
      grep chain $f | awk '{print $2;}' | sort -nr > /tmp/score.$f:t:r
      echo $f:t:r
      textHistogram -binSize=10000 /tmp/score.$f:t:r
      echo ""
    end

    # trim to minScore=20000 to cut some of the fluff
    mkdir chainFilt
    foreach f (chain/*.chain)
      chainFilter -minScore=20000 $f > chainFilt/$f:t
    end
    gzip -c all.chain > all.chain.unfiltered.gz
    rm -r chain
    mv chainFilt chain
    chainMergeSort -saveId chain/*.chain > all.chain

    # Load chains into database
    ssh hgwdev
    cd /cluster/data/canFam2/bed/blastz.canFam2.2005-01-27/axtChain/chain
    foreach i (*.chain)
        set c = $i:r
        echo loading $c
        hgLoadChain canFam2 ${c}_chainSelf $i
    end


# NET SELF BLASTZ 
# For future reference -- doBlastzChainNet.pl should have been used
    ssh kolossus
    cd /cluster/data/canFam2/bed/blastz.canFam2/axtChain
    chainPreNet all.chain.gz ../S1.len ../S2.len stdout \
    | chainNet stdin -minSpace=1 ../S1.len ../S2.len stdout /dev/null \
    | netSyntenic stdin noClass.net

    # Add classification info using db tables:
    ssh hgwdev
    cd /cluster/data/canFam2/bed/blastz.canFam2/axtChain
    netClass -noAr noClass.net canFam2 canFam2 self.net
    rm noClass.net
    # Load the nets into database 
    netFilter -minGap=10 self.net |  hgLoadNet canFam2 netSelf stdin
    # Add entries for chainSelf, netSelf to dog/canFam2 trackDb


# ADD SELF DOWNLOADABLE FILES (DONE 11/21/05 angie)
# For future reference -- doBlastzChainNet.pl should have been used for 
# all steps from blastz up to this point.
    cd /cluster/data/canFam2/bed/blastz.canFam2/axtChain
    mv self.net canFam2.canFam2.net
    gzip canFam2.canFam2.net
    cd ..
    doBlastzChainNet.pl -continue download -stop download DEF \
      >& doDownload.log
    tail -f doDownload.log


# MULTIZ.V10 4WAY (DOG/H/M/R) (DONE 11/7/05 angie)
    # Tree: ((canFam2 hg17) (mm6 rn3))
    ssh kkstore01
    mkdir /cluster/data/canFam2/bed/multiz4way.2005-11-07
    ln -s /cluster/data/canFam2/bed/multiz4way.2005-11-07 \
      /cluster/data/canFam2/bed/multiz4way
    cd /cluster/data/canFam2/bed/multiz4way
    # Setup: Copy pairwise MAF to /san/sanvol1/scratch:
    mkdir /san/sanvol1/scratch/dogMultiz4way
    foreach db (hg17 mm6 rn3)
      echo $db
      cp -pR /cluster/data/canFam2/bed/blastz.$db/mafNet \
        /san/sanvol1/scratch/dogMultiz4way/$db
    end
    ls -lLR /san/sanvol1/scratch/dogMultiz4way
    # Make output dir and script to use /cluster/bin/scripts/runMultizV10.csh:
    mkdir maf
    cat << '_EOF_' > doMultizAll.csh
#!/bin/csh -fex
set chr = $1
set tmpDir = /scratch/dogMultiz4way.$chr
mkdir $tmpDir

set mafScratch = /san/sanvol1/scratch/dogMultiz4way

# Really should write a perl script to take a tree like this and generate 
# commands like the ones below:
# ((canFam2 hg17) (mm6 rn3))

/cluster/bin/scripts/runMultizV10.csh \
  $mafScratch/mm6/$chr.maf.gz \
  $mafScratch/rn3/$chr.maf.gz \
  0 canFam2.$chr $tmpDir $tmpDir/$chr.MusRat.maf
/cluster/bin/scripts/runMultizV10.csh \
  $mafScratch/hg17/$chr.maf.gz \
  $tmpDir/$chr.MusRat.maf \
  1 canFam2.$chr $tmpDir maf/$chr.maf

rm -f $tmpDir/*.maf
rmdir $tmpDir
'_EOF_'
    # << for emacs
    chmod 775 doMultizAll.csh
    awk '{print "./doMultizAll.csh " $1;}' /cluster/data/canFam2/chrom.sizes \
      > jobs.lst
    # Run on brawny cluster
    ssh pk
    cd /cluster/data/canFam2/bed/multiz4way
    para make jobs.lst
    para time
#Completed: 41 of 41 jobs
#Average job time:                1279s      21.32m     0.36h    0.01d
#Longest finished job:            2763s      46.05m     0.77h    0.03d
#Submission to last job:          2763s      46.05m     0.77h    0.03d
    ls -1 missing*
#ls: No match.

    # make /gbdb/ links to 4way maf files:
    ssh hgwdev
    mkdir -p /gbdb/canFam2/multiz4way/maf/multiz4way
    ln -s /cluster/data/canFam2/bed/multiz4way/maf/chr*.maf \
      /gbdb/canFam2/multiz4way/maf/multiz4way/
    # load into database
    cd /tmp
    hgLoadMaf -warn canFam2 multiz4way \
      -pathPrefix=/gbdb/canFam2/multiz4way/maf/multiz4way
    # load summary table to replace pairwise
    cat /cluster/data/canFam2/bed/multiz4way/maf/chr*.maf \
    | nice hgLoadMafSummary canFam2 multiz4waySummary stdin

    # put 4way MAF out for download
    ssh kolossus
    cd /cluster/data/canFam2/bed/multiz4way
    mkdir mafDownload
    foreach f (maf/*.maf)
      nice gzip -c $f > mafDownload/$f:t.gz
    end
    cd mafDownload
    md5sum *.maf.gz > md5sum.txt
    # make a README.txt
    ssh hgwdev
    mkdir /usr/local/apache/htdocs/goldenPath/canFam2/multiz4way
    ln -s /cluster/data/canFam2/bed/multiz4way/mafDownload/{*.maf.gz,*.txt} \
      /usr/local/apache/htdocs/goldenPath/canFam2/multiz4way

    # Cleanup
    rm -rf /san/sanvol1/scratch/dogMultiz4way/


# PHASTCONS 4WAY WITH METHODS FROM PAPER (DONE 11/9/05 angie)
# ((canFam2,hg17),(mm6,rn3))
    ssh kkstore01
    mkdir -p /san/sanvol1/scratch/canFam2/chrom
    cp -p /cluster/data/canFam2/?{,?}/chr*.fa /san/sanvol1/scratch/canFam2/chrom/
    # Split chrom fa into smaller windows for phastCons:
    ssh pk
    mkdir /cluster/data/canFam2/bed/multiz4way/phastCons
    mkdir /cluster/data/canFam2/bed/multiz4way/phastCons/run.split
    cd /cluster/data/canFam2/bed/multiz4way/phastCons/run.split
    set WINDOWS = /san/sanvol1/scratch/canFam2/phastCons/WINDOWS
    rm -fr $WINDOWS
    mkdir -p $WINDOWS
    cat << 'EOF' > doSplit.sh
#!/bin/csh -ef

set PHAST=/cluster/bin/phast
set FA_SRC=/san/sanvol1/scratch/canFam2/chrom
set WINDOWS=/san/sanvol1/scratch/canFam2/phastCons/WINDOWS

set maf=$1
set c = $maf:t:r
set tmpDir = /scratch/msa_split/$c
rm -rf $tmpDir
mkdir -p $tmpDir
${PHAST}/msa_split $1 -i MAF -M ${FA_SRC}/$c.fa \
   -O canFam2,hg17,mm6,rn3 \
   -w 1000000,0 -r $tmpDir/$c -o SS -I 1000 -B 5000
cd $tmpDir
foreach file ($c.*.ss)
  gzip -c $file > ${WINDOWS}/$file.gz
end
rm -f $tmpDir/$c.*.ss
rmdir $tmpDir
'EOF'
# << for emacs
    chmod a+x doSplit.sh
    rm -f jobList
    foreach file (/cluster/data/canFam2/bed/multiz4way/maf/*.maf)
      if (-s $file) then
        echo "doSplit.sh {check in exists+ $file}" >> jobList
      endif
    end
    para make jobList
    para time
#Completed: 41 of 41 jobs
#Average job time:                 143s       2.39m     0.04h    0.00d
#Longest finished job:             240s       4.00m     0.07h    0.00d
#Submission to last job:           240s       4.00m     0.07h    0.00d

    ############### FIRST ITERATION OF PARAMETER ESTIMATION ONLY #############
    # Use consEntropy --NH to make it suggest a --expected-lengths param 
    # that we should try next.  Adam ran this on hg17 to find out the 
    # total entropy of the hg17 model:
    # consEntropy 0.265 12 ave.cons.mod ave.noncons.mod
#Transition parameters: gamma=0.265000, omega=12.000000, mu=0.083333, nu=0.030045
#Relative entropy: H=0.608216 bits/site
#Required length: N=16.085437 sites
#Total entropy: NH=9.783421 bits
    # Our target is that NH result: 9.7834 bits.  
    # Use phyloFit to make an initial model:
    ssh kolossus
    cd /cluster/data/canFam2/bed/multiz4way/phastCons
    /cluster/bin/phast/msa_view ../maf/chr{2,20,37}.maf \
        --aggregate canFam2,hg17,mm6,rn3 \
        -i MAF -o SS > all.ss
    /cluster/bin/phast/phyloFit all.ss \
      --tree "((canFam2,hg17),(mm6,rn3))" \
      -i SS --out-root starting-tree
    cat starting-tree.mod 
#ALPHABET: A C G T 
#ORDER: 0
#SUBST_MOD: REV
#TRAINING_LNL: -376795316.708219
#BACKGROUND: 0.285384 0.213893 0.214062 0.286660 
#RATE_MAT:
#  -0.890529    0.170099    0.563707    0.156723 
#   0.226953   -1.148695    0.168868    0.752875 
#   0.751527    0.168735   -1.147776    0.227515 
#   0.156025    0.561763    0.169895   -0.887683 
#TREE: ((canFam2:0.160873,hg17:0.166317):0.114048,(mm6:0.071352,rn3:0.076741):0.114048);
# also get GC content from model -- if similar enough, no need to extract it 
# separately above.
awk '$1 == "BACKGROUND:" {print $3 + $4;}' starting-tree.mod 
#0.427955
    # OK, use .428 for --gc below.

    # Use values of --target-coverage and --expected-lengths sort of like 
    # those in the latest run on human, 0.25 and 12, just as a starting point.
    # Multiply each subst rate on the TREE line by 3.75 which is roughly the 
    # ratio of noncons to cons in 
    # /cluster/data/canFam1/bed/multiz.canFam1hg17mm5/phastCons/run.estimate/ave.*
    /cluster/bin/phast/tree_doctor -s 3.75 starting-tree.mod \
      > starting-tree.noncons.mod
    /cluster/bin/phast/consEntropy --NH 9.7834 0.25 12 \
      starting-tree{,.noncons}.mod
#( Solving for new omega: 12.000000 13.074286 13.006290 13.006032 )
#Transition parameters: gamma=0.250000, omega=12.000000, mu=0.083333, nu=0.027778
#Relative entropy: H=0.803665 bits/site
#Expected min. length: L_min=11.949090 sites
#Expected max. length: L_max=9.263256 sites
#Phylogenetic information threshold: PIT=L_min*H=9.603060 bits
#Recommended expected length: omega=13.006032 sites (for L_min*H=9.783400)
    # OK, use --expected-lengths 13.

    ############## SUBSEQUENT ITERATIONS OF PARAM ESTIMATION ONLY ###########
    # We're here because the actual target coverage was not satisfactory,
    # so we're changing the --target-coverage param.  Given that we're 
    # changing that, take a guess at how we should change --expected-lengths
    # in order to also hit the total entropy target.
    cd /cluster/data/canFam2/bed/multiz4way/phastCons/run.estimate
    # SECOND ITERATION:
    /cluster/bin/phast/consEntropy --NH 9.7834 0.18 13 ave.{cons,noncons}.mod
#ERROR: too many iterations, not converging; try without --NH.
    # -- it gets that error unless I raise coverage to 0.34.  Well, 
    # stick with 13 for now...
    # THIRD ITERATION:
    /cluster/bin/phast/consEntropy --NH 9.7834 0.15 13 ave.{cons,noncons}.mod
#ERROR: too many iterations, not converging; try without --NH.
    # -- it gets that error unless I raise coverage to 0.31.  Well, 
    # stick with 13 for now...

    # Now set up cluster job to estimate model parameters given free params 
    # --target-coverage and --expected-lengths and the data.  
    ssh pk
    mkdir /cluster/data/canFam2/bed/multiz4way/phastCons/run.estimate
    cd /cluster/data/canFam2/bed/multiz4way/phastCons/run.estimate
    # FIRST ITERATION: Use ../starting-tree.mod:
    cat << '_EOF_' > doEstimate.sh
#!/bin/csh -ef
zcat $1 \
| /cluster/bin/phast/phastCons - ../starting-tree.mod --gc 0.428 --nrates 1,1 \
    --no-post-probs --ignore-missing \
    --expected-lengths 13 --target-coverage 0.25 \
    --quiet --log $2 --estimate-trees $3
'_EOF_'
# << for emacs
    # SUBSEQUENT ITERATIONS: Use last iteration's estimated noncons model.
    cat << '_EOF_' > doEstimate.sh
#!/bin/csh -ef
zcat $1 \
| /cluster/bin/phast/phastCons - ave.noncons.mod --gc 0.428 --nrates 1,1 \
    --no-post-probs --ignore-missing \
    --expected-lengths 13 --target-coverage 0.15 \
    --quiet --log $2 --estimate-trees $3
'_EOF_'
# << for emacs
    chmod a+x doEstimate.sh
    rm -fr LOG TREES
    mkdir -p LOG TREES
    rm -f jobList
    foreach f (/san/sanvol1/scratch/canFam2/phastCons/WINDOWS/*.ss.gz)
      set root = $f:t:r:r
      echo doEstimate.sh $f LOG/$root.log TREES/$root >> jobList
    end
    para make jobList
    para time
#Completed: 2434 of 2434 jobs
#Average job time:                  32s       0.54m     0.01h    0.00d
#Longest finished job:              66s       1.10m     0.02h    0.00d
#Submission to last job:           285s       4.75m     0.08h    0.00d

    # Now combine parameter estimates.  We can average the .mod files
    # using phyloBoot.  This must be done separately for the conserved
    # and nonconserved models
    ssh kolossus
    cd /cluster/data/canFam2/bed/multiz4way/phastCons/run.estimate
    ls -1 TREES/*.cons.mod > cons.txt
    /cluster/bin/phast/phyloBoot --read-mods '*cons.txt' \
      --output-average ave.cons.mod > cons_summary.txt
    ls -1 TREES/*.noncons.mod > noncons.txt
    /cluster/bin/phast/phyloBoot --read-mods '*noncons.txt' \
      --output-average ave.noncons.mod > noncons_summary.txt
    grep TREE ave*.mod
    # FIRST ITERATION:
#ave.cons.mod:TREE: ((canFam2:0.063241,hg17:0.067179):0.047808,(mm6:0.029756,rn3:0.031924):0.047808);
#ave.noncons.mod:TREE: ((canFam2:0.201135,hg17:0.213229):0.151937,(mm6:0.094208,rn3:0.101249):0.151937);
    # SECOND ITERATION:
#ave.cons.mod:TREE: ((canFam2:0.056147,hg17:0.059693):0.042487,(mm6:0.026610,rn3:0.028533):0.042487);
#ave.noncons.mod:TREE: ((canFam2:0.192186,hg17:0.203958):0.145480,(mm6:0.090727,rn3:0.097451):0.145480);
    # THIRD ITERATION:
#ave.cons.mod:TREE: ((canFam2:0.052533,hg17:0.055873):0.039758,(mm6:0.024967,rn3:0.026765):0.039758);
#ave.noncons.mod:TREE: ((canFam2:0.188439,hg17:0.200074):0.142743,(mm6:0.089242,rn3:0.095829):0.142743);

    cat cons_summary.txt 
    # look over the files cons_summary.txt and noncons_summary.txt.
    # The means and medians should be roughly equal and the stdevs
    # should be reasonably small compared to the means, particularly
    # for rate matrix parameters (at bottom) and for branches to the
    # leaves of the tree.  The stdevs may be fairly high for branches
    # near the root of the tree; that's okay.  Some min values may be
    # 0 for some parameters.  That's okay, but watch out for very large
    # values in the max column, which might skew the mean.  If you see
    # any signs of bad outliers, you may have to track down the
    # responsible .mod files and throw them out.  I've never had to do
    # this; the estimates generally seem pretty well behaved.

    # NOTE: Actually, a random sample of several hundred to a thousand
    # alignment fragments (say, a number equal to the number of
    # available cluster nodes) should be more than adequate for
    # parameter estimation.  If pressed for time, use this strategy.

    # FIRST ITERATION ONLY:
    # Check the total entropy figure to see if we're way off.
    # This takes an hour for 4way (exponential in #species) and has never 
    # produced a different answer from the input after the first iteration,
    # so do this for the first iteration only:
    /cluster/bin/phast/consEntropy --NH 9.7834 0.25 13 ave.{cons,noncons}.mod
#ERROR: too many iterations, not converging; try without --NH
    # Dang.  That is a new one.  Oh well, I'll proceed with 13.

    # Now we are ready to set up the cluster job for computing the
    # conservation scores and predicted elements.  The we measure the 
    # conserved elements coverage, and if that's not satisfactory then we 
    # adjust parameters and repeat.  
    ssh pk
    mkdir /cluster/data/canFam2/bed/multiz4way/phastCons/run.phast
    cd /cluster/data/canFam2/bed/multiz4way/phastCons/run.phast
    cat << 'EOF' > doPhastCons.sh
#!/bin/csh -ef
set pref = $1:t:r:r
set chr = `echo $pref | awk -F\. '{print $1}'`
set tmpfile = /scratch/phastCons.$$
zcat $1 \
| /cluster/bin/phast/phastCons - \
    ../run.estimate/ave.cons.mod,../run.estimate/ave.noncons.mod \
    --expected-lengths 13 --target-coverage 0.15 \
    --quiet --seqname $chr --idpref $pref \
    --viterbi /san/sanvol1/scratch/canFam2/phastCons/ELEMENTS/$chr/$pref.bed --score \
    --require-informative 0 \
  > $tmpfile
gzip -c $tmpfile > /san/sanvol1/scratch/canFam2/phastCons/POSTPROBS/$chr/$pref.pp.gz
rm $tmpfile
'EOF'
# << for emacs
    chmod a+x doPhastCons.sh
    rm -fr /san/sanvol1/scratch/canFam2/phastCons/{POSTPROBS,ELEMENTS}
    mkdir -p /san/sanvol1/scratch/canFam2/phastCons/{POSTPROBS,ELEMENTS}
    foreach chr (`awk '{print $1;}' /cluster/data/canFam2/chrom.sizes`)
      mkdir /san/sanvol1/scratch/canFam2/phastCons/{POSTPROBS,ELEMENTS}/$chr
    end
    rm -f jobList
    foreach f (/san/sanvol1/scratch/canFam2/phastCons/WINDOWS/*.ss.gz)
      echo doPhastCons.sh $f >> jobList
    end
    para make jobList
    para time
#Completed: 2434 of 2434 jobs
#Average job time:                   8s       0.13m     0.00h    0.00d
#Longest finished job:              12s       0.20m     0.00h    0.00d
#Submission to last job:            69s       1.15m     0.02h    0.00d

    # back on kolossus:
    # combine predictions and transform scores to be in 0-1000 interval
    cd /cluster/data/canFam2/bed/multiz4way/phastCons
    cp /dev/null all.bed
    foreach d (/san/sanvol1/scratch/canFam2/phastCons/ELEMENTS/*)
      echo $d:t
      awk '{printf "%s\t%d\t%d\tlod=%d\t%s\n", $1, $2, $3, $5, $5;}' \
        $d/*.bed \
      | /cluster/bin/scripts/lodToBedScore >> all.bed
    end

    ssh hgwdev
    # Now measure coverage of CDS by conserved elements. 
    # We want the "cover" figure to be close to 68.9%.
    # However we don't have dog gene annotations -- we just have xenoRefGene 
    # which is of course biased towards conserved genes.  So cover will be 
    # artificially high.  Shoot for "all.bed 5%" and make sure cover is 
    # somewhat higher than 68.9%.  
    cd /cluster/data/canFam2/bed/multiz4way/phastCons
    featureBits -enrichment canFam2 xenoRefGene:cds all.bed
    # FIRST ITERATION: too high, reduce target-coverage:
#xenoRefGene:cds 1.268%, all.bed 6.812%, both 1.091%, cover 86.06%, enrich 12.63x
    # SECOND ITERATION: still too high.  
#xenoRefGene:cds 1.268%, all.bed 5.432%, both 1.048%, cover 82.67%, enrich 15.22x
    # THIRD ITERATION: close enough.
#xenoRefGene:cds 1.268%, all.bed 4.960%, both 1.025%, cover 80.83%, enrich 16.29x

    # Having met the CDS coverage target, load up the results.
    hgLoadBed canFam2 phastConsElements4way all.bed

    # Create wiggle
    ssh pk
    mkdir /cluster/data/canFam2/bed/multiz4way/phastCons/run.wib
    cd /cluster/data/canFam2/bed/multiz4way/phastCons/run.wib
    rm -rf /san/sanvol1/scratch/canFam2/phastCons/wib
    mkdir -p /san/sanvol1/scratch/canFam2/phastCons/wib
    cat << 'EOF' > doWigEncode
#!/bin/csh -ef
set chr = $1
cd /san/sanvol1/scratch/canFam2/phastCons/wib
zcat `ls -1 /san/sanvol1/scratch/canFam2/phastCons/POSTPROBS/$chr/*.pp.gz \
      | sort -t\. -k2,2n` \
| wigEncode stdin ${chr}_phastCons.wi{g,b}
'EOF'
# << for emacs
    chmod a+x doWigEncode
    rm -f jobList
    foreach chr (`ls -1 /san/sanvol1/scratch/canFam2/phastCons/POSTPROBS \
                  | sed -e 's/\/$//'`)
      echo doWigEncode $chr >> jobList
    end
    para make jobList
    para time
#Completed: 41 of 41 jobs
#Average job time:                  31s       0.52m     0.01h    0.00d
#Longest finished job:              63s       1.05m     0.02h    0.00d
#Submission to last job:            63s       1.05m     0.02h    0.00d

    # back on kkstore01, copy wibs, wigs and POSTPROBS (people sometimes want 
    # the raw scores) from san/sanvol1
    cd /cluster/data/canFam2/bed/multiz4way/phastCons
    rm -rf wib POSTPROBS
    rsync -av /san/sanvol1/scratch/canFam2/phastCons/wib .
    rsync -av /san/sanvol1/scratch/canFam2/phastCons/POSTPROBS .

    # load wiggle component of Conservation track
    ssh hgwdev
    mkdir /gbdb/canFam2/multiz4way/wib
    cd /cluster/data/canFam2/bed/multiz4way/phastCons
    chmod 775 . wib
    chmod 664 wib/*.wib
    ln -s `pwd`/wib/*.wib /gbdb/canFam2/multiz4way/wib/
    hgLoadWiggle canFam2 phastCons4way \
      -pathPrefix=/gbdb/canFam2/multiz4way/wib wib/*.wig
    rm wiggle.tab

    # and clean up san/sanvol1.
    rm -r /san/sanvol1/scratch/canFam2/phastCons/{ELEMENTS,POSTPROBS,wib}
    rm -r /san/sanvol1/scratch/canFam2/chrom
    # Offer raw scores for download since fly folks are likely to be interested:
    # back on kolossus
    cd /cluster/data/canFam2/bed/multiz4way/phastCons/POSTPROBS
    mkdir ../postprobsDownload
    foreach chr (`awk '{print $1;}' ../../../../chrom.sizes`)
      zcat `ls -1 $chr/$chr.*.pp.gz | sort -t\. -k2,2n` | gzip -c \
        > ../postprobsDownload/$chr.pp.gz
    end
    cd ../postprobsDownload
    md5sum *.gz > md5sum.txt
    # Make a README.txt there too.
    ssh hgwdev
    mkdir /usr/local/apache/htdocs/goldenPath/canFam2/phastCons4way
    cd /usr/local/apache/htdocs/goldenPath/canFam2/phastCons4way
    ln -s /cluster/data/canFam2/bed/multiz4way/phastCons/postprobsDownload/* .


# UNCERTIFIED ASSEMBLY REGIONS (DONE 11/9/05 angie)
    ssh hgwdev
    mkdir /cluster/data/canFam2/bed/uncertified
    cd /cluster/data/canFam2/bed/uncertified
    mv ../../broad/canFam2.0uncertifiedRegions.txt .
    tail +2 canFam2.0uncertifiedRegions.txt \
    | awk -F"\t" '{print "chr" $1 "\t" $2 "\t" $3 "\t" $7 "\t" $6;}' \
    | sed -e 's/Missing read partners/MRP/; s/Haplotype inconsistency/HI/; \
              s/Negative gap/NG/; s/Linking inconsistency/LI/; \
              s/Illogical links/IL/; s/; /+/g;' \
    > uncertified.bed
    hgLoadBed -tab canFam2 uncertified uncertified.bed


# MAKE DOWNLOADABLE SEQUENCE FILES (DONE 11/21/05 angie)
    ssh kolossus
    cd /cluster/data/canFam2
    #- Build the .tar.gz files -- no genbank for now.
    cat << '_EOF_' > jkStuff/zipAll.csh
rm -rf bigZips
mkdir bigZips
tar cvzf bigZips/chromAgp.tar.gz ?{,?}/chr*.agp
tar cvzf bigZips/chromOut.tar.gz ?{,?}/chr*.fa.out
tar cvzf bigZips/chromFa.tar.gz ?{,?}/chr*.fa
tar cvzf bigZips/chromFaMasked.tar.gz ?{,?}/chr*.fa.masked
cd bed/simpleRepeat
tar cvzf ../../bigZips/chromTrf.tar.gz trfMaskChrom/chr*.bed
cd ../..
'_EOF_'
    # << this line makes emacs coloring happy
    csh -efx ./jkStuff/zipAll.csh |& tee zipAll.log
    #- Look at zipAll.log to make sure all file lists look reasonable.  
    cd bigZips
    md5sum *.gz > md5sum.txt
    # Make a README.txt
    cd ..
    mkdir chromGz
    foreach f ( ?{,?}/chr*.fa )
      echo $f:t:r
      gzip -c $f > chromGz/$f:t.gz
    end
    cd chromGz
    md5sum *.gz > md5sum.txt
    # Make a README.txt

    #- Link the .gz and .txt files to hgwdev:/usr/local/apache/...
    ssh hgwdev
    set gp = /usr/local/apache/htdocs/goldenPath/canFam2
    mkdir -p $gp/bigZips
    ln -s /cluster/data/canFam2/bigZips/{chrom*.tar.gz,*.txt} $gp/bigZips
    mkdir -p $gp/chromosomes
    ln -s /cluster/data/canFam2/chromGz/{chr*.gz,*.txt} $gp/chromosomes
    # Take a look at bigZips/* and chromosomes/*
    # Can't make refGene upstream sequence files - no refSeq for dog.
    mkdir database
    # Create README.txt files in database/ to explain the files.


# CHORI BAC END PAIRS  (DONE 11/22/05 angie)
    # Rachel downloaded and parsed BAC end sequences and pair info from 
    # CHORI and NCBI -- seee makeCanFam1.doc "BAC END PAIRS".  Use those 
    # files and align to canFam2.  

    # Do BLAT alignments of BAC ends to the genome on the pitakluster.
    # copy over masked contigs to the san
    ssh kkstore01
    cd /cluster/data/canFam2
    mkdir /san/sanvol1/scratch/canFam2/maskedContigs
    foreach d (?{,?}/chr*_?{,?})
      echo $d:t
      cp -p $d/$d:t.fa /san/sanvol1/scratch/canFam2/maskedContigs/
    end
    # copy over 11.ooc file to the san
    cp -p /cluster/bluearc/canFam2/11.ooc /san/sanvol1/scratch/canFam2/
    # make output directory, run directory and bed/ directory
    mkdir -p /san/sanvol1/scratch/canFam2/bacendsRun/psl
    mkdir /cluster/data/canFam2/bed/bacends
    ssh pk
    cd /san/sanvol1/scratch/canFam2/bacendsRun
    echo '#LOOP\n/cluster/bin/x86_64/blat $(path1) $(path2) -ooc=/san/sanvol1/scratch/canFam2/11.ooc {check out line+ /san/sanvol1/scratch/canFam2/bacendsRun/psl/$(root1).$(root2).psl}\n#ENDLOOP' > gsub
    ls -1S /san/sanvol1/scratch/canFam2/maskedContigs/*.fa > contigs.lst
    ls -1S /san/sanvol1/scratch/canFam1/bacends/bacends*.fa > bacends.lst
    gensub2 contigs.lst bacends.lst gsub jobList
    para make jobList
    para time
#Completed: 49005 of 49005 jobs
#Average job time:                   7s       0.12m     0.00h    0.00d
#Longest finished job:             105s       1.75m     0.03h    0.00d
#Submission to last job:          1884s      31.40m     0.52h    0.02d

    # back on kkstore01, retrieve pitakluster results
    cd /cluster/data/canFam2/bed/bacends
    rsync -av /san/sanvol1/scratch/canFam2/bacendsRun/{*.lst,batch*,g*,j*,p*} .
    
    # lift alignments
    ssh kolossus
    cd /cluster/data/canFam2/bed/bacends
    pslSort dirs raw.psl tmp psl
    # started 08:15, PID: 16561
    pslCheck raw.psl >& check.log 
    wc -l check.log
#4379 check.log
    grep '< previous block' check.log | wc -l
#2194
    grep -v 'Error: invalid PSL' check.log | grep -v '< previous block' | wc -l
#0
    wc -l raw.psl
#27352955 raw.psl -- 2194 / 27352955 = 0.000080 = 0.008% overlapping
    pslReps -nearTop=0.02 -minCover=0.60 -minAli=0.85 -noIntrons \
                raw.psl  bacEnds.psl /dev/null
    # Processed 27147454 alignments
    wc -l bacEnds.psl
#769149 bacEnds.psl
    liftUp bacEnds.lifted.psl /cluster/data/canFam2/jkStuff/liftAll.lft \
           warn bacEnds.psl
    awk '{print $10}' bacEnds.lifted.psl | sort | uniq | wc -l
#317042
    faSize /san/sanvol1/scratch/canFam1/bacends/bacends*.fa
#290482800 bases (10817785 N's 279665015 real 279665015 upper 0 lower) in 393408 sequences in 99 files
    calc 317042 / 393408
#317042 / 393408 = 0.805886
    # Make BAC end pairs track: 
    mkdir pairs
    cd pairs
    set ncbiDir = /cluster/data/ncbi/bacends/dog/bacends.1
    /cluster/bin/x86_64/pslPairs -tInsert=10000 -minId=0.91 -noBin -min=25000 -max=350000 -slopval=10000 -hardMax=500000 -slop -short -long -orphan -mismatch -verbose ../bacEnds.lifted.psl $ncbiDir/bacEndPairs.txt all_bacends bacEnds
    wc -l *
#      40 bacEnds.long
#     460 bacEnds.mismatch
#   44366 bacEnds.orphan
#  133248 bacEnds.pairs
#     478 bacEnds.short
#     816 bacEnds.slop
#  179408 total

    # Filter by score and sort by {chrom,chromStart}:
    awk '$5 >= 300 {print;}' bacEnds.pairs | sort -k1,2n > bacEndPairs.bed
    cat bacEnds.{slop,short,long,mismatch,orphan} \
    | awk '$5 >= 300 {print;}' | sort -k1,2n > bacEndPairsBad.bed
    wc -l *.bed
#  133241 bacEndPairs.bed
#   46057 bacEndPairsBad.bed
#  179298 total
    extractPslLoad -noBin ../bacEnds.lifted.psl bacEndPairs.bed   \
      bacEndPairsBad.bed \
    | sorttbl tname tstart | headchg -del \
    > bacEnds.load.psl

    # load into database
    ssh hgwdev
    cd /cluster/data/canFam2/bed/bacends/pairs
    hgLoadBed canFam2 bacEndPairs bacEndPairs.bed -notItemRgb \
               -sqlTable=$HOME/kent/src/hg/lib/bacEndPairs.sql
#Loaded 133241 elements of size 11
    # note - this next track isn't pushed to RR, just used for assembly QA
    hgLoadBed canFam2 bacEndPairsBad bacEndPairsBad.bed -notItemRgb \
              -sqlTable=$HOME/kent/src/hg/lib/bacEndPairsBad.sql
#Loaded 46057 elements of size 11
    hgLoadPsl canFam2 -table=all_bacends bacEnds.load.psl
#load of all_bacends did not go as planned: 748544 record(s), 0 row(s) skipped, 928 warning(s) loading psl.tab
    # Diagnose...
    echo select \* from all_bacends | hgsql -N canFam2 > /tmp/1
    diff psl.tab /tmp/1 | less
    # Looks like some rows of psl.tab have negative numbers in the 
    # qBaseInsert column!

    # load BAC end sequences into seq table so alignments may be viewed
    # symlink to FASTA sequence file in ncbi directory
    mkdir -p /gbdb/canFam2/bacends
    ln -s /cluster/data/ncbi/bacends/dog/bacends.1/canFamBacends.fa \
          /gbdb/canFam2/bacends/canFamBacends.fa
    hgLoadSeq canFam2 /gbdb/canFam2/bacends/canFamBacends.fa
#393408 sequences
# featureBits canFam1 all_bacends
#211644790 bases of 2359845093 (8.969%) in intersection
    featureBits canFam2 all_bacends
#218709219 bases of 2384996543 (9.170%) in intersection

# featureBits canFam1 bacEndPairs
#2334084046 bases of 2359845093 (98.908%) in intersection
    featureBits canFam2 bacEndPairs
#2353239742 bases of 2384996543 (98.668%) in intersection

# featureBits canFam1 bacEndPairsBad
# 548130287 bases of 2359845093 (23.227%) in intersection
    featureBits canFam2 bacEndPairsBad
#534195657 bases of 2384996543 (22.398%) in intersection

# add trackDb entry and html
    # Clean up
    ssh pk
    rm -r /san/sanvol1/scratch/canFam2/bacendsRun


