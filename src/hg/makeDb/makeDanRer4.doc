#!/bin/csh -f # set emacs mode
exit; # don't actually run this like a script :)

# Danio Rerio (zebrafish) from Sanger, version Zv5 (released 5/20/05)
#  Project website:
#    http://www.sanger.ac.uk/Projects/D_rerio/
#  Assembly notes:
#    http://www.sanger.ac.uk/Projects/D_rerio/
#    ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6_assembl_information.shmtl

###########################################################################
# DOWNLOAD SEQUENCE (DONE, 2006-03-29, hartera)
# CHANGED NAME OF SCAFFOLDS AGP FILE (DONE, 2006-04-13, hartera)
     ssh kkstore01
     mkdir /cluster/store8/danRer4 
     ln -s /cluster/store8/danRer4 /cluster/data
     cd /cluster/data/danRer4
     wget --timestamp \
      ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/README
     wget --timestamp \
      ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6.chunks.agp
     wget --timestamp \
      ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6.scaffold.agp
     wget --timestamp \
      ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6_scaffolds.fa
     wget --timestamp \
    ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6_scaffolds.stats
     # keep agp file name consistent with Zv5 (hartera, 2006-04-13)
     mv Zv6.scaffold.agp Zv6.scaffolds.agp

###########################################################################
# DOWNLOAD MITOCHONDRION GENOME SEQUENCE (DONE, 2006-03-29, hartera)
# ADDED CHUNKS AGP FILE (DONE, 2006-04-13, hartera)
     ssh kkstore01
     mkdir -p /cluster/data/danRer4/M
     cd /cluster/data/danRer4/M
     # go to http://www.ncbi.nih.gov/ and search the Nucleotide database for
     # "Danio mitochondrion genome".  That shows the gi number:
     # 8576324 for the accession, AC024175
 # Use that number in the entrez linking interface to get fasta:
     wget -O chrM.fa \
      'http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Text&db=Nucleotide&uid=8576324&dopt=FASTA'
     # Edit chrM.fa: make sure the header line says it is the
     # Danio Rerio mitochondrion complete genome, and then replace the
     # header line with just ">chrM".
     perl -pi.bak -e 's/>.+/>chrM/' chrM.fa
     rm *.bak
     # Make a "pseudo-contig" for processing chrM too:
     mkdir ./chrM_1
     sed -e 's/chrM/chrM_1/' ./chrM.fa > ./chrM_1/chrM_1.fa
     mkdir ./lift
     echo "chrM_1/chrM_1.fa.out" > ./lift/oOut.lst
     echo "chrM_1" > ./lift/ordered.lst
     # make sure this is tab delimited:
     echo "0\tM/chrM_1\t16596\tchrM\t16596" > ./lift/ordered.lft
# create a .agp file for chrM as hgGoldGapGl and other
# programs require a .agp file so create chrM.agp
     echo "chrM\t1\t16596\t1\tF\tAC024175.3\t1\t16596\t+" \
          > chrM.agp
     # Create a chrM.chunks.agp (hartera, 2006-04-13)
     mkdir -p /cluster/data/danRer4/M/agps
     cd /cluster/data/danRer4/M/agps
     awk 'BEGIN {OFS="\t"} \
        {print $1, $2, $3, $4, $5, $6, $7, $8, $1, $7, $8}' \
        ../chrM.agp > chrM.chunks.agp
     # make sure that all above *.agp files are tab delimited

###########################################################################
# CREATE LIST OF CHROMOSOMES (DONE, 2006-04-12, hartera)
     ssh kkstore01
     cd /cluster/data/danRer4
     awk '{if ($1 !~ /Zv6/) print $1;}' Zv6.scaffolds.agp \
         | sort -n | uniq > chrom.lst
     cp chrom.lst chrom1to25.lst
     # add chrM, chrUn and chrNA
     echo "M" >> chrom.lst
     echo "NA" >> chrom.lst
     echo "Un" >> chrom.lst

###########################################################################
# MAKE JKSTUFF AND BED DIRECTORIES (DONE, 2006-04-12, hartera)
    ssh kkstore01
    cd /cluster/data/danRer4
    # This used to hold scripts -- better to keep them inline here 
    # Now it should just hold lift file(s) and
    # temporary scripts made by copy-paste from this file.
    mkdir /cluster/data/danRer4/jkStuff
    # This is where most tracks will be built:
    mkdir /cluster/data/danRer4/bed

###########################################################################
# CHECK AGP FILES AND FASTA SIZE CONSISTENCY (DONE, 2006-04-13, hartera)
# 
    ssh kkstore01
    cd /cluster/data/danRer4
    mkdir -p /cluster/data/danRer4/scaffolds
    cd /cluster/data/danRer4/scaffolds
    faSize detailed=on ../Zv6_scaffolds.fa > Zv6.scaffolds.sizes
    # Check that these sizes correspond to the sizes in the scaffolds agp file
    # use script compareSizes2.pl
    cat << '_EOF_' > ../jkStuff/compareSizes2.pl
#!/usr/bin/perl -w
use strict;

my ($file, $agp);

$file = $ARGV[0];
$agp = $ARGV[1];

open(FILE, $file) || die "Can not open $file: $!\n";
open(AGP, $agp) || die "Can not open $agp: $!\n";
open(OUT, ">log.txt") || die "Can not create log.txt: $!\n";

my ($l, @f, $name, $size, %scafsHash);
while (<FILE>)
{
$l = $_;
@f = split(/\t/, $l);

$name = $f[0]; 
$size = $f[1];
$scafsHash{$name} = $size;
}
close FILE;

while (<AGP>)
{
my ($line, @fi, $scaf, $end);
$line = $_;

if ($line =~ /Zv/)
   {
   @fi = split(/\t/, $line);
   $scaf = $fi[5];
   $end = $fi[7];

   if (exists($scafsHash{$scaf}))
      {
      if ($scafsHash{$scaf} == $end)
         {
         print OUT "$scaf - ok\n";
         }
      else
         {
         print OUT "$scaf - different size to sequence\n";
         }
      }
   else
      {
      print OUT "$scaf - does not exist in list of sizes\n";
      }
   }
}
close AGP;
close OUT;
'_EOF_'
   # << happy emacs
   chmod +x ../jkStuff/compareSizes2.pl
   perl /cluster/data/danRer4/jkStuff/compareSizes2.pl \
        Zv6.scaffolds.sizes ../Zv6.scaffolds.agp 
   grep different log.txt
   grep not log.txt
   # these are all consistent with the sequence sizes
   # check that the co-ordinates in the agp files are consistent:
   # field 2 is the start position, field 3 is the end and field 8 is the size
   # so check that this is consistent.
   cd /cluster/data/danRer4
   awk '{if ($6 ~ /^Zv6/ && (($3-$2+1) != $8)) print $6;}' Zv6.scaffolds.agp \
       > Zv6.scaffolds.coordCheck 
   # this file is empty so they are ok. do the same for the chunks.agp file
   awk '{if ($6 ~ /^Zv6/ && (($3-$2+1) != $8)) print $6;}' Zv6.chunks.agp \
       > Zv6.chunks.coordCheck
   # this file is empty so ok
   # check that the difference between 7th and 8th fields is the same as the 
   # difference between 11th and 12th fields. 
   awk '{if ($5 != "N" && (($8 - $7) != ($12 - $11))) print $6;}' \
       Zv6.chunks.agp > Zv6.chunks.coordCheck2
   # these are all ok
   rm Zv6.*.coord*

   cat << '_EOF_' > ./jkStuff/checkSizesInAgps.pl
#!/usr/bin/perl -w
use strict;

my ($ch, $sc, %scafsHash);
$sc = $ARGV[0]; # scaffolds agp
$ch = $ARGV[1]; # chunks or contigs agp

open(SCAFS, $sc) || die "Can not open $sc: $!\n";
open(CHUNKS, $ch) || die "Can not open $ch: $!\n";

while (<SCAFS>)
{
my ($l, @f, $name, $e);
$l = $_;
@f = split(/\t/, $l);
if ($f[5] =~ /^Zv/)
   {
   $name = $f[5];
   $e = $f[2];
   $scafsHash{$name} = $e;
   }
}
close SCAFS;

my $scaf = "";
my $prev = "";
my $prevEnd = 0;

while (<CHUNKS>)
{
my ($line, @fi);
$line = $_;
@fi = split(/\t/, $line);

# if it is not a gap line
if ($fi[4] ne "N")
   {
   $scaf = $fi[9];
   if (($scaf ne $prev) && ($prev ne ""))
      {
      checkCoords($prev, $prevEnd);
      }
$prev = $scaf;
$prevEnd = $fi[2];
   }
}
# check last entry in file
checkCoords($prev, $prevEnd);
close CHUNKS;

sub checkCoords {
my ($name, $end) = @_;
if (exists($scafsHash{$prev}))
   {
   if ($scafsHash{$prev} != $prevEnd)
      {
      my $ed = $scafsHash{$prev};
      print "Scaffold $prev is not consistent between agps\n";
      }
   else
      {
      my $ed = $scafsHash{$prev};
      print "Scaffold $prev - ok\n";
      }
   }
}
'_EOF_'
   # << happy emacs
   chmod +x ./jkStuff/checkSizesInAgps.pl
   cd scaffolds
   perl /cluster/data/danRer4/jkStuff/checkSizesInAgps.pl \
        Zv6.scaffolds.agp Zv6.chunks.agp > Zv6.scafsvschunks
   grep "not consistent" Zv6.scafsvschunks
   # no lines were inconsistency was reported
   wc -l Zv6.scafsvschunks
   # 6653 Zv6.scafsvschunks
   grep "Zv6" Zv6.scaffolds.agp | wc -l
   # 6653
   # so all the scaffolds were checked and were ok.
   cd ..
   rm -r scaffolds
 
###########################################################################
# SPLIT AGP FILES BY CHROMOSOME (DONE, 2006-04-13, hartera)
# GENOME FASTA FROM SANGER WAS CREATED USING SCAFFOLDS AGP
   ssh kkstore01
   cd /cluster/data/danRer4
   # There are 2 .agp files: one for scaffolds (supercontigs on danRer1) and
   # then one for chunks (contigs on danRer1) showing how they map on to
   # scaffolds.
  
   # get list of scaffolds from FASTA file and check these are in agp
   grep '>' Zv6_scaffolds.fa | sed -e 's/>//' | sort | uniq > Zv6FaScafs.lst
   # get list of scaffolds from agp - do not print from gap lines
   awk '{if ($7 !~ /contig/) print $6;}' Zv6.scaffolds.agp \
        | sort | uniq > Zv6AgpScafs.lst
   diff Zv6FaScafs.lst Zv6AgpScafs.lst
   # no difference so all scaffolds are in the FASTA file
   # add "chr" prefix for the agp files
   perl -pi -e 's/^([0-9]+)/chr$1/' ./*.agp
   # for chromosomes 1 to 25, create 2 agps for each chrom, one for scaffolds 
   # and one for chunks:
   foreach c (`cat chrom1to25.lst`)
     echo "Processing $c ..."
     mkdir $c
     perl -we "while(<>){if (/^chr$c\t/) {print;}}" \
          ./Zv6.chunks.agp \
          > $c/chr$c.chunks.agp
     perl -we "while(<>){if (/^chr$c\t/) {print;}}" \
          ./Zv6.scaffolds.agp \
          > $c/chr$c.scaffolds.agp
   end
   
###########################################################################
# CREATE AGP FILES FOR chrNA AND chrUn (DONE, 2006-04-13, hartera)
   ssh kkstore01
   # chrNA consists of WGS contigs that could not be related to any
   # FPC contig and the scaffolds and contigs are named Zv5_NAN in the
   # first field of the agp files
   cd /cluster/data/danRer4
   mkdir ./NA
   awk '{if ($1 ~ /Zv6_NA/) print;}' Zv6.chunks.agp \
       > ./NA/NA.chunks.agp
   awk '{if ($1 ~ /Zv6_NA/) print;}' Zv6.scaffolds.agp \
       > ./NA/NA.scaffolds.agp
   # change the first field to "chrUn" then can use agpToFa to process
   perl -pi.bak -e 's/Zv6_NA[0-9]+/chrNA/' ./NA/*.agp
   wc -l ./NA/NA.scaffolds.agp
   # 2898 ./NA/NA.scaffolds.agp
   # check files and remove backup files
   # these are not sorted numerically by scaffold number
   rm ./NA/*.bak
   # then process chrUn - this is made from scaffolds and
   # contigs where the name is Zv6_scaffoldN in the first field of the
   # agp files. These scaffolds and contigs are unmapped to chromosomes
   # in the agp file. chrUn is made up of WGS scaffolds that mapped to
   # FPC contigs, but the chromosome is unknown.
   mkdir ./Un
   awk '{if ($1 ~ /Zv6_scaffold/) print;}' Zv6.chunks.agp \
       > ./Un/Un.chunks.agp
   awk '{if ($1 ~ /Zv6_scaffold/) print;}' Zv6.scaffolds.agp \
       > ./Un/Un.scaffolds.agp
   # change the first field to "chrUn" then can use agpToFa to process
   perl -pi.bak -e 's/Zv6_scaffold[0-9]+/chrUn/' ./Un/*.agp
   wc -l ./Un/Un.scaffolds.agp
   # 68 ./Un/Un.scaffolds.agp
   # check files and remove backup files
   rm ./Un/*.bak
   # get FASTA file of sequences for NA and Un and create agp with
   # Ns between scaffolds
   # from scaffolds agp, get name of scaffolds to be retrieved from the 
   # FASTA file to make the NA and Un chromosomes.
   cd /cluster/data/danRer4
   foreach c (NA Un)
     awk '{print $6;}' $c/$c.scaffolds.agp > $c/chr$c.scaffolds.lst
         $HOME/bin/i386/faSomeRecords /cluster/data/danRer4/Zv6_scaffolds.fa \
         $c/chr$c.scaffolds.lst $c/chr$c.fa
   end
   # check that all scaffolds in the list are in the FASTA file for NA and Un.
   # made a change to scaffoldFaToAgp.c so that the the number of Ns to be
   # inserted between scaffolds can be specified as an option.
   # There are less and smaller random scaffolds than before so use 50,000 Ns
   # between scaffolds as for the human random chromosomes.
   foreach c (NA Un)              
     $HOME/bin/i386/scaffoldFaToAgp -scaffoldGapSize=50000 $c/chr$c.fa
     mv $c/chr$c.fa $c/chr$c.scaffolds.fa
   end
   # change chrUn to chrNA for NA and D to W for NA and Un
   sed -e 's/chrUn/chrNA/' ./NA/chrNA.agp | sed -e 's/D/W/' \
       > ./NA/chrNA.scaffolds.agp
   # the scaffolds agp for chrNA is now sorted numerically by scaffold number
   sed -e 's/D/W/' ./Un/chrUn.agp > ./Un/chrUn.scaffolds.agp
   # edit ./NA/chrNA.scaffolds.agp and ./Un/chrUn.scaffolds.agp and 
   # remove last line as this just adds an extra 500 Ns at the 
   # end of the sequence.
   rm ./NA/chrNA.agp ./Un/chrUn.agp
cat << '_EOF_' > ./jkStuff/createAgpWithGaps.pl
#!/usr/bin/perl
use strict;

# This script takes a chunks agp and inserts Ns between scaffolds for 
# the chunks (contigs) agp file. Could also insert Ns between scaffolds
# for scaffolds agp.

my ($chrom, $numN, $name, $prev, $st, $end, $prevEnd, $id);
my $chrom = $ARGV[0]; # chromosome name
my $numN = $ARGV[1];  # number of Ns to be inserted 
my $type = $ARGV[2]; # contigs or scaffolds

$prev = "";
$st = 1;
$prevEnd = 0;
$id = 0;

while (<STDIN>)
{
my $l = $_;
my @f = split(/\t/, $l);

if ($type eq "contigs")
   {
   $name = $f[9];
   }
else
   {
   $name = $f[5]
   }

my $currSt = $f[1];
my $currEnd = $f[2];
my $size = $currEnd - $currSt;

$id++;
$st = $prevEnd + 1;
$end = $st + $size;

if (($prev ne "") && ($prev ne $name))
   {
   $st = $prevEnd + 1;
   $end = ($st + $numN) - 1;
   print "$chrom\t$st\t$end\t$id\tN\t$numN\tcontig\tno\n";
   $prevEnd = $end;
   $id++;
   }

$st = $prevEnd + 1;
$end = $st + $size;
print "$chrom\t$st\t$end\t$id\t$f[4]\t$f[5]\t$f[6]\t$f[7]\t$f[8]";
if ($type eq "contigs")
   {
   print "\t$f[9]\t$f[10]\t$f[11]";
   }

$prevEnd = $end;
$prev = $name;
}
'_EOF_'
   chmod +x ./jkStuff/createAgpWithGaps.pl
   cd /cluster/data/danRer4/NA
   # for NA, sort the chunks.agp by contig number
   perl -pi.bak -e 's/Zv6_NA//' NA.chunks.agp
   sort -k6,6n NA.chunks.agp > NA.chunks2.agp
   # then put back Zv6_NA
   perl -pi.bak -e 's/([0-9]+\.[0-9]+)/Zv6_NA$1/' NA.chunks2.agp
   mv NA.chunks2.agp NA.chunks.agp
   cd /cluster/data/danRer4
   foreach c (NA Un)
      cd $c
      perl /cluster/data/danRer4/jkStuff/createAgpWithGaps.pl \
           chr${c} 50000 contigs < ${c}.chunks.agp > chr${c}.chunks.agp
      cd ..
   end
   # check co-ordinates
   # field 2 is the start position, field 3 is the end and field 8 is the size
   # so check that this is consistent in scaffolds and chunks agp. 
   # check that the difference between 7th and 8th fields is the same as the 
   # difference between 11th and 12th fields for chunks agp. 
   cd /cluster/data/danRer4
   foreach c (NA Un)
        awk '{if ($6 ~ /^Zv6/ && (($3-$2+1) != $8)) print $6;}' \
            $c/chr${c}.scaffolds.agp > $c/chr${c}.scaffolds.coordCheck 
        awk '{if ($6 ~ /^Zv6/ && (($3-$2+1) != $8)) print $6;}' \
            $c/chr${c}.chunks.agp > $c/chr${c}.chunks.coordCheck 
        awk '{if ($5 != "N" && (($8 - $7) != ($12 - $11))) print $6;}' \
            $c/chr${c}.chunks.agp > $c/chr${c}.chunks.coordCheck2 
   end
   # check the outputs are empty
   wc -l NA/*.coord*
   wc -l Un/*.coord*
   rm NA/*.coord* Un/*.coord*
   # check that the scaffolds and chunks agp files are consistent with
   # each other. 
   foreach c (NA Un)
      perl /cluster/data/danRer4/jkStuff/checkSizesInAgps.pl \
           $c/chr${c}.scaffolds.agp $c/chr${c}.chunks.agp \
           > $c/${c}.scafsvschunks
   end
   foreach c (NA Un)
     grep "not consistent" $c/${c}.scafsvschunks
   end 
   wc -l NA/NA.scafsvschunks 
   wc -l Un/Un.scafsvschunks 
   # no lines were inconsistency was reported
   rm NA/NA.scafsvschunks Un/Un.scafsvschunks
   # clean up
   foreach c (NA Un)
      rm $c/${c}.scaffolds.agp $c/${c}.chunks.agp $c/chr${c}.scaffolds.fa \
         $c/chr${c}.scaffolds.lst $c/*.bak
   end

###########################################################################
# BUILD CHROM-LEVEL SEQUENCE (DONE, 2006-04-13, hartera)
   ssh kkstore01
   cd /cluster/data/danRer4
   # Ignore warnings about chrM files not existing - this chrom has 
   # already been processed - see mitochondrion section above.
   # Sequence is already in upper case so no need to change
   foreach c (`cat chrom.lst`)
     echo "Processing ${c}"
     $HOME/bin/i386/agpToFa -simpleMultiMixed $c/chr$c.scaffolds.agp chr$c \
         $c/chr$c.fa ./Zv6_scaffolds.fa
     echo "${c} - DONE"
   end
   # move scaffolds agp to be chrom agp and clean up
   foreach c (`cat chrom.lst`)
      cd $c
      cp chr${c}.scaffolds.agp chr${c}.agp
      mkdir -p agps
      mv chr${c}.*.agp ./agps/
      cd ..
   end

##########################################################################
# CHECK CHROM AND VIRTUAL CHROM SEQUENCES (DONE, 2006-04-14, hartera)
   # Check that the size of each chromosome .fa file is equal to the last
   # co-ordinate of the correcpsonding agp file.
   ssh hgwdev
   cd /cluster/data/danRer4
   foreach c (`cat chrom.lst`)
     foreach f ( $c/chr$c.agp )
       set agpLen = `tail -1 $f | awk '{print $3;}'`
       set h = $f:r
       set g = $h:r
       echo "Getting size of $g.fa"
       set faLen = `faSize $g.fa | awk '{print $1;}'`
       if ($agpLen == $faLen) then
         echo "   OK: $f length = $g length = $faLen"
       else
         echo "ERROR:  $f length = $agpLen, but $g length = $faLen"
       endif
     end
   end
   # all are the OK so FASTA files are the expected size

###########################################################################
# CREATING DATABASE (DONE, 2006-04-14, hartera)
    # Create the database.
    # next machine
    ssh hgwdev
    echo 'create database danRer4' | hgsql ''
    # if you need to delete that database:  !!! WILL DELETE EVERYTHING !!!
    echo 'drop database danRer4' | hgsql danRer4
    # Use df to make sure there is at least 10 gig free on
    df -h /var/lib/mysql
# Before loading data:
# Filesystem            Size  Used Avail Use% Mounted on
# /dev/sdc1             1.8T  1.5T  173G  90% /var/lib/mysql

###########################################################################
# CREATING GRP TABLE FOR TRACK GROUPING (DONE, 2006-04-14, hartera)
    # next machine
    ssh hgwdev
    #  the following command copies all the data from the table
    #  grp in the database mm8 to the new database danRer4. Use one of the
    #  newest databases to copy from to make sure that the groupings are
    #  up to date.
    echo "create table grp (PRIMARY KEY(NAME)) select * from mm8.grp" \
      | hgsql danRer4
    # if you need to delete that table:   !!! WILL DELETE ALL grp data !!!
    echo 'drop table grp;' | hgsql danRer4

###########################################################################
# MAKE HGCENTRALTEST ENTRY FOR DANRER4 (DONE, 2006-04-14, hartera)
    # Make entry into dbDb and defuaultDb so test browser knows about it.
    ssh hgwdev
    # Add dbDb and defaultDb entries:
    echo 'insert into dbDb (name, description, nibPath, organism,  \
          defaultPos, active, orderKey, genome, scientificName,  \
          htmlPath, hgNearOk, hgPbOk, sourceName)  \
          values("danRer4", "March 2006", \
          "/gbdb/danRer4", "Zebrafish", "chr2:15,906,734-15,926,406", 1, \
          37, "Zebrafish", "Danio rerio", \
          "/gbdb/danRer4/html/description.html", 0,  0, \
          "Sanger Centre, Danio rerio Sequencing Project Zv6");' \
    | hgsql -h genome-testdb hgcentraltest
    # SET AS DEFAULT LATER WHEN READY FOR RELEASE
    # set danRer4 to be the default assembly for Zebrafish
    echo 'update defaultDb set name = "danRer4" \
          where genome = "Zebrafish";' \
          | hgsql -h genome-testdb hgcentraltest
    # Create /gbdb directory for danRer4
    mkdir /gbdb/danRer4

###########################################################################
# BREAK UP SEQUENCE INTO 5MB CHUNKS AT CONTIGS/GAPS FOR CLUSTER RUNS
# (DONE, 2006-04-14, hartera)

     ssh kkstore01
     cd /cluster/data/danRer4
     foreach c (`cat chrom.lst`)
       foreach agp ($c/chr$c.agp)
         if (-e $agp) then
           set fa = $c/chr$c.fa
           echo splitting $agp and $fa
           cp -p $agp $agp.bak
           cp -p $fa $fa.bak
           splitFaIntoContigs $agp $fa . -nSize=5000000
         endif
       end
     end

###########################################################################
# MAKE LIFTALL.LFT (DONE, 2006-04-14, hartera)
     ssh kkstore01
     cd /cluster/data/danRer4
     cat */lift/ordered.lft > jkStuff/liftAll.lft

###########################################################################
# MAKE TRACKDB ENTRY FOR DANRER4 (DONE, 2006-04-14, hartera)
    # Make trackDb table so browser knows what tracks to expect.
    ssh hgwdev
    mkdir -p ~/kent/src/hg/makeDb/trackDb/zebrafish/danRer4
    cd ~/kent/src/hg/makeDb/trackDb/zebrafish
    cvs add danRer4
    cvs commit danRer4
    cd ~/kent/src/hg/makeDb/trackDb
    cvs up -d -P
    # Edit that makefile to add danRer4 in all the right places and do
    make update DBS=danRer4
    make alpha DBS=danRer4
    cvs commit -m "Added danRer4." makefile

###########################################################################
# SIMPLE REPEAT [TRF] TRACK  (DONE, 2006-04-14, hartera)
    # TRF can be run in parallel with RepeatMasker on the file server
    # since it doesn't require masked input sequence.
    # Run this on the kilokluster. Need to mask contig and chromosome 
    # sequences so run trf using contig sequences.
    # First copy over contig sequences to iscratch and then rsync to cluster.
    ssh kkr1u00
    mkdir -p /iscratch/i/danRer4/contigsNoMask
    cd /cluster/data/danRer4
    foreach d (/cluster/data/danRer4/*/chr*_?{,?})
       set ctg = $d:t
       foreach f ($d/${ctg}.fa)
          echo "Copyig $f ..."
          cp $f /iscratch/i/danRer4/contigsNoMask/
       end
    end
    ls /iscratch/i/danRer4/contigsNoMask/*.fa | wc -l
    # 317 sequence files
    # rsync to cluster machines
    foreach R (2 3 4 5 6 7 8)
       rsync -a --progress /iscratch/i/danRer4/ kkr${R}u00:/iscratch/i/danRer4/
    end
     
    ssh kki
    mkdir -p /cluster/data/danRer4/bed/simpleRepeat
    cd /cluster/data/danRer4/bed/simpleRepeat
    mkdir trf
cat << '_EOF_' > runTrf
#!/bin/csh -fe
#
set path1 = $1
set inputFN = $1:t
set outpath = $2
set outputFN = $2:t
mkdir -p /tmp/$outputFN
cp $path1 /tmp/$outputFN
pushd .
cd /tmp/$outputFN
/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $inputFN /dev/null -bedAt=$outputFN -tempDir=/tmp
popd
rm -f $outpath
cp -p /tmp/$outputFN/$outputFN $outpath
rm -fr /tmp/$outputFN/*
rmdir --ignore-fail-on-non-empty /tmp/$outputFN
'_EOF_'
 # << keep emacs coloring happy
    chmod +x runTrf

cat << '_EOF_' > gsub
#LOOP
./runTrf {check in line+ $(path1)}  {check out line trf/$(root1).bed}
#ENDLOOP
'_EOF_'
    # << keep emacs coloring happy

    ls -1S /iscratch/i/danRer4/contigsNoMask/chr*.fa > genome.lst
    gensub2 genome.lst single gsub jobList
    # 317 jobs
    para create jobList
    para try, check, push, check etc...
    para time
# Completed: 317 of 317 jobs
# CPU time in finished jobs:      25083s     418.05m     6.97h    0.29d  0.001 y
# IO & Wait Time:                   933s      15.55m     0.26h    0.01d  0.000 y
# Average job time:                  82s       1.37m     0.02h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            2732s      45.53m     0.76h    0.03d
# Submission to last job:          4604s      76.73m     1.28h    0.05d
    # lift up to chrom level
    liftUp simpleRepeat.bed /cluster/data/danRer4/jkStuff/liftAll.lft warn \
           trf/*.bed

    # Load into the database
    ssh hgwdev
    cd /cluster/data/danRer4/bed/simpleRepeat
    hgLoadBed danRer4 simpleRepeat simpleRepeat.bed \
      -sqlTable=$HOME/kent/src/hg/lib/simpleRepeat.sql
    # Loaded 759659 elements of size 16

###########################################################################
# PROCESS SIMPLE REPEATS INTO MASK (DONE, 2005-06-14, hartera)
   # After the simpleRepeats track has been built, make a filtered version
   # of the trf output: keep trf's with period <= 12:
   ssh kkstore01
   cd /cluster/data/danRer4/bed/simpleRepeat
   mkdir -p trfMask
   foreach f (trf/chr*.bed)
     awk '{if ($5 <= 12) print;}' $f > trfMask/$f:t
   end

   # Lift up filtered trf output to chrom coords as well:
   cd /cluster/data/danRer4
   mkdir bed/simpleRepeat/trfMaskChrom
   
   foreach c (`cat chrom.lst`)
     if (-e $c/lift/ordered.lst) then
       perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
         $c/lift/ordered.lst > $c/lift/oTrf.lst
       liftUp bed/simpleRepeat/trfMaskChrom/chr$c.bed \
         jkStuff/liftAll.lft warn `cat $c/lift/oTrf.lst`
     endif
     if (-e $c/lift/random.lst) then
       perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
          $c/lift/random.lst > $c/lift/rTrf.lst
       liftUp bed/simpleRepeat/trfMaskChrom/chr${c}_random.bed \
         jkStuff/liftAll.lft warn `cat $c/lift/rTrf.lst`
     endif
   end

###########################################################################
# GET ADDITIONAL ZEBRAFISH REPBASE LIBRARY FOR REPEATMASKER AND ADD TO
# DANIO LIBRARY FOR REPEATMASKER (DONE, 2006-04-14, hartera)
# Go to http://www.girinst.org/server/RepBase/RepBase11.02.fasta
# (03-15-2006) and download zebunc.ref.txt containing unclassified zebrafish 
# repeats.
# Need username and password. Copy to /cluster/bluearc/RepeatMasker/Libraries/
   ssh hgwdev
   cd /cluster/bluearc/RepeatMasker/Libraries
   # This is /cluster/bluearc/RepeatMasker060320/Libraries
   # Do a dummy run of RepeatMasker with the -species option. This creates
   # a zebrafish-specific library from the EMBL format RepBase library.
   # Then the zebunc.ref unclassified repeats can be added to this library.
   /cluster/bluearc/RepeatMasker/RepeatMasker -spec danio /dev/null
   # RepeatMasker version development-$Id: RepeatMasker,v 1.13 2006/03/21 
   # This creates a specieslib in Libraries/20060315/danio
   # Format the zebunc.ref library:
   # Sequence is upper case, change to lower case like the specieslib
   cat zebunc.ref.txt | tr '[A-Z]' '[a-z]' > zebunc.ref.format 
   perl -pi.bak -e 's/>dr([0-9]+)/>Dr$1#Unknown/' zebunc.ref.format
   grep '>' zebunc.ref.format | wc -l
   # 958
   cd /cluster/bluearc/RepeatMasker/Libraries/20060315/danio
   grep '>' specieslib | wc -l
   # 219
   mv specieslib danio.lib
   cat danio.lib ../../zebunc.ref.format > specieslib  
   grep '>' specieslib | wc -l
   # 1177
   rm danio.lib
   # make a copy in Libraries directory in case this directory of libraries
   # is removed.
   cp specieslib /cluster/bluearc/RepeatMasker/Libraries/danio.lib
 
###########################################################################
# SPLIT SEQUENCE FOR REPEATMASKER RUN (DONE, 2006-04-14, hartera)
   ssh kkstore01
   cd /cluster/data/danRer4
   
   # break up into 500 kb sized chunks at gaps if possible 
   # for RepeatMasker runs
   foreach c (`cat chrom.lst`)
      foreach d ($c/chr${c}*_?{,?})
        cd $d
        echo "splitting $d"
        set contig = $d:t
        faSplit gap $contig.fa 500000 ${contig}_ -lift=$contig.lft \
            -minGapSize=100
        cd ../..
      end
    end
    # took about 3 minutes. 

###########################################################################
# REPEATMASKER RUN (in progress, 2006-04-14, hartera)
   # When a new library is added for this version of repeatMasker, need to 
   # check in /cluster/bluearc/RepeatMasker/Libraries for a directory made 
   # up of a date e.g. 20060315 here and inside this are species directories
   # for which RepeatMasker has already been run. In this directory it creates
   # a specieslib of the danio repeats. If this exists, this is used for the
   # RepeatMasker run for that species. Check that this contains the 
   # unclassified Zebrafish repeats with IDs beginning with Dr. This library
   # with these repeats should have been created in the section above:
   # Use sequence split into 500 kb chunks.
   ssh kkstore01
   cd /cluster/data/danRer4
   mkdir RMRun
   # Record RM version used:
   ls -l /cluster/bluearc/RepeatMasker
   # lrwxrwxrwx  1 angie protein 18 Mar 20 16:50 /cluster/bluearc/RepeatMasker -> RepeatMasker060320
   # March 20 2006 (open-3-1-5) version of RepeatMasker
   # get RM database version
   grep RELEASE /cluster/bluearc/RepeatMasker/Libraries/RepeatMaskerLib.embl \
        > RMdatabase.version
   # RELEASE 20060315

   cd /cluster/data/danRer4
   cat << '_EOF_' > jkStuff/RMZebrafish
#!/bin/csh -fe

cd $1
pushd .
/bin/mkdir -p /tmp/danRer4/$2
/bin/cp $2 /tmp/danRer4/$2/
cd /tmp/danRer4/$2
/cluster/bluearc/RepeatMasker060320/RepeatMasker -ali -s -species danio $2
popd
/bin/cp /tmp/danRer4/$2/$2.out ./
if (-e /tmp/danRer4/$2/$2.align) /bin/cp /tmp/danRer4/$2/$2.align ./
if (-e /tmp/danRer4/$2/$2.tbl) /bin/cp /tmp/danRer4/$2/$2.tbl ./
if (-e /tmp/danRer4/$2/$2.cat) /bin/cp /tmp/danRer4/$2/$2.cat ./
/bin/rm -fr /tmp/danRer4/$2/*
/bin/rmdir --ignore-fail-on-non-empty /tmp/danRer4/$2
/bin/rmdir --ignore-fail-on-non-empty /tmp/danRer4
'_EOF_'
    chmod +x jkStuff/RMZebrafish
    cp /dev/null RMRun/RMJobs
    foreach c (`cat chrom.lst`)
      foreach d ($c/chr${c}_?{,?})
          set ctg = $d:t
          foreach f ( $d/${ctg}_?{,?}.fa )
            set f = $f:t
            echo /cluster/data/danRer4/jkStuff/RMZebrafish \
                 /cluster/data/danRer4/$d $f \
               '{'check out line+ /cluster/data/danRer4/$d/$f.out'}' \
              >> RMRun/RMJobs
          end
      end
    end
    # Do the run
    ssh pk
    cd /cluster/data/danRer4/RMRun
    para create RMJobs
    # 4382 jobs written to batch
    para try, check, push, check ... etc.
    para time
# 1 job failed with a division by zero error in ProcessRepeats. The sequence
# is chr16_4_10.fa. Reported the error, sequence and zebrafish repeats library
# to Arian Smit and Robert Hubley.
 
###########################################################################
# MAKE DESCRIPTION/SAMPLE POSITION HTML PAGE (in progress, 2006-04-14, hartera)
    ssh hgwdev
    mkdir /cluster/data/danRer4/html
    # make a symbolic link from /gbdb/danRer4/html to /cluster/data/danRer4/html
    ln -s /cluster/data/danRer4/html /gbdb/danRer4/html
    # Add a description page for zebrafish
    cd /cluster/data/danRer4/html
    cp $HOME/kent/src/hg/makeDb/trackDb/zebrafish/danRer3/description.html .
    # Edit this for zebrafish danRer4

    # create a description.html page here
    cd ~/kent/src/hg/makeDb/trackDb/zebrafish/danRer4
    # Add description page here too
    cp /cluster/data/danRer4/html/description.html .
    cvs add description.html
    cvs commit -m "First draft of description page for danRer4." \
        description.html
    cd ~/kent/src/hg/makeDb/trackDb
    make update DBS=danRer4
    make alpha  DBS=danRer4


