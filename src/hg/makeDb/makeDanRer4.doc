#!/bin/csh -f # set emacs mode
exit; # don't actually run this like a script :)

# Danio Rerio (zebrafish) from Sanger, version Zv5 (released 5/20/05)
#  Project website:
#    http://www.sanger.ac.uk/Projects/D_rerio/
#  Assembly notes:
#    http://www.sanger.ac.uk/Projects/D_rerio/
#    ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6_assembl_information.shmtl

###########################################################################
# DOWNLOAD SEQUENCE (DONE, 2006-03-29, hartera)
# CHANGED NAME OF SCAFFOLDS AGP FILE (DONE, 2006-04-13, hartera)
     ssh kkstore01
     mkdir /cluster/store8/danRer4 
     ln -s /cluster/store8/danRer4 /cluster/data
     cd /cluster/data/danRer4
     wget --timestamp \
      ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/README
     wget --timestamp \
      ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6.chunks.agp
     wget --timestamp \
      ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6.scaffold.agp
     wget --timestamp \
      ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6_scaffolds.fa
     wget --timestamp \
    ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv6release/Zv6_scaffolds.stats
     # keep agp file name consistent with Zv5 (hartera, 2006-04-13)
     mv Zv6.scaffold.agp Zv6.scaffolds.agp

###########################################################################
# DOWNLOAD MITOCHONDRION GENOME SEQUENCE (DONE, 2006-03-29, hartera)
# ADDED CHUNKS AGP FILE (DONE, 2006-04-13, hartera)
     ssh kkstore01
     mkdir -p /cluster/data/danRer4/M
     cd /cluster/data/danRer4/M
     # go to http://www.ncbi.nih.gov/ and search the Nucleotide database for
     # "Danio mitochondrion genome".  That shows the gi number:
     # 8576324 for the accession, AC024175
 # Use that number in the entrez linking interface to get fasta:
     wget -O chrM.fa \
      'http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Text&db=Nucleotide&uid=8576324&dopt=FASTA'
     # Edit chrM.fa: make sure the header line says it is the
     # Danio Rerio mitochondrion complete genome, and then replace the
     # header line with just ">chrM".
     perl -pi.bak -e 's/>.+/>chrM/' chrM.fa
     rm *.bak
     # Make a "pseudo-contig" for processing chrM too:
     mkdir ./chrM_1
     sed -e 's/chrM/chrM_1/' ./chrM.fa > ./chrM_1/chrM_1.fa
     mkdir ./lift
     echo "chrM_1/chrM_1.fa.out" > ./lift/oOut.lst
     echo "chrM_1" > ./lift/ordered.lst
     # make sure this is tab delimited:
     echo "0\tM/chrM_1\t16596\tchrM\t16596" > ./lift/ordered.lft
# create a .agp file for chrM as hgGoldGapGl and other
# programs require a .agp file so create chrM.agp
     echo "chrM\t1\t16596\t1\tF\tAC024175.3\t1\t16596\t+" \
          > chrM.agp
     # Create a chrM.chunks.agp (hartera, 2006-04-13)
     mkdir -p /cluster/data/danRer4/M/agps
     cd /cluster/data/danRer4/M/agps
     awk 'BEGIN {OFS="\t"} \
        {print $1, $2, $3, $4, $5, $6, $7, $8, $1, $7, $8}' \
        ../chrM.agp > chrM.chunks.agp
     # make sure that all above *.agp files are tab delimited

###########################################################################
# CREATE LIST OF CHROMOSOMES (DONE, 2006-04-12, hartera)
# Change names of random chroms to chrNA_random and chrUn_random
# (DONE, hartera, 2006-04-21)
     ssh kkstore01
     cd /cluster/data/danRer4
     awk '{if ($1 !~ /Zv6/) print $1;}' Zv6.scaffolds.agp \
         | sort -n | uniq > chrom.lst
     cp chrom.lst chrom1to25.lst
     # add chrM, chrUn and chrNA
     echo "M" >> chrom.lst
     echo "NA" >> chrom.lst
     echo "Un" >> chrom.lst
     # Change names of random chroms to reflect that
     perl -pi.bak -e 's/NA/NA_random/' chrom.lst
     perl -pi.bak -e 's/Un/Un_random/' chrom.lst
     rm *.bak

###########################################################################
# MAKE JKSTUFF AND BED DIRECTORIES (DONE, 2006-04-12, hartera)
    ssh kkstore01
    cd /cluster/data/danRer4
    # This used to hold scripts -- better to keep them inline here 
    # Now it should just hold lift file(s) and
    # temporary scripts made by copy-paste from this file.
    mkdir /cluster/data/danRer4/jkStuff
    # This is where most tracks will be built:
    mkdir /cluster/data/danRer4/bed

###########################################################################
# CHECK AGP FILES AND FASTA SIZE CONSISTENCY (DONE, 2006-04-13, hartera)
# 
    ssh kkstore01
    cd /cluster/data/danRer4
    mkdir -p /cluster/data/danRer4/scaffolds
    cd /cluster/data/danRer4/scaffolds
    faSize detailed=on ../Zv6_scaffolds.fa > Zv6.scaffolds.sizes
    # Check that these sizes correspond to the sizes in the scaffolds agp file
    # use script compareSizes2.pl
    cat << '_EOF_' > ../jkStuff/compareSizes2.pl
#!/usr/bin/perl -w
use strict;

my ($file, $agp);

$file = $ARGV[0];
$agp = $ARGV[1];

open(FILE, $file) || die "Can not open $file: $!\n";
open(AGP, $agp) || die "Can not open $agp: $!\n";
open(OUT, ">log.txt") || die "Can not create log.txt: $!\n";

my ($l, @f, $name, $size, %scafsHash);
while (<FILE>)
{
$l = $_;
@f = split(/\t/, $l);

$name = $f[0]; 
$size = $f[1];
$scafsHash{$name} = $size;
}
close FILE;

while (<AGP>)
{
my ($line, @fi, $scaf, $end);
$line = $_;

if ($line =~ /Zv/)
   {
   @fi = split(/\t/, $line);
   $scaf = $fi[5];
   $end = $fi[7];

   if (exists($scafsHash{$scaf}))
      {
      if ($scafsHash{$scaf} == $end)
         {
         print OUT "$scaf - ok\n";
         }
      else
         {
         print OUT "$scaf - different size to sequence\n";
         }
      }
   else
      {
      print OUT "$scaf - does not exist in list of sizes\n";
      }
   }
}
close AGP;
close OUT;
'_EOF_'
   # << happy emacs
   chmod +x ../jkStuff/compareSizes2.pl
   perl /cluster/data/danRer4/jkStuff/compareSizes2.pl \
        Zv6.scaffolds.sizes ../Zv6.scaffolds.agp 
   grep different log.txt
   grep not log.txt
   # these are all consistent with the sequence sizes
   # check that the co-ordinates in the agp files are consistent:
   # field 2 is the start position, field 3 is the end and field 8 is the size
   # so check that this is consistent.
   cd /cluster/data/danRer4
   awk '{if ($6 ~ /^Zv6/ && (($3-$2+1) != $8)) print $6;}' Zv6.scaffolds.agp \
       > Zv6.scaffolds.coordCheck 
   # this file is empty so they are ok. do the same for the chunks.agp file
   awk '{if ($6 ~ /^Zv6/ && (($3-$2+1) != $8)) print $6;}' Zv6.chunks.agp \
       > Zv6.chunks.coordCheck
   # this file is empty so ok
   # check that the difference between 7th and 8th fields is the same as the 
   # difference between 11th and 12th fields. 
   awk '{if ($5 != "N" && (($8 - $7) != ($12 - $11))) print $6;}' \
       Zv6.chunks.agp > Zv6.chunks.coordCheck2
   # these are all ok
   rm Zv6.*.coord*

   cat << '_EOF_' > ./jkStuff/checkSizesInAgps.pl
#!/usr/bin/perl -w
use strict;

my ($ch, $sc, %scafsHash);
$sc = $ARGV[0]; # scaffolds agp
$ch = $ARGV[1]; # chunks or contigs agp

open(SCAFS, $sc) || die "Can not open $sc: $!\n";
open(CHUNKS, $ch) || die "Can not open $ch: $!\n";

while (<SCAFS>)
{
my ($l, @f, $name, $e);
$l = $_;
@f = split(/\t/, $l);
if ($f[5] =~ /^Zv/)
   {
   $name = $f[5];
   $e = $f[2];
   $scafsHash{$name} = $e;
   }
}
close SCAFS;

my $scaf = "";
my $prev = "";
my $prevEnd = 0;

while (<CHUNKS>)
{
my ($line, @fi);
$line = $_;
@fi = split(/\t/, $line);

# if it is not a gap line
if ($fi[4] ne "N")
   {
   $scaf = $fi[9];
   if (($scaf ne $prev) && ($prev ne ""))
      {
      checkCoords($prev, $prevEnd);
      }
$prev = $scaf;
$prevEnd = $fi[2];
   }
}
# check last entry in file
checkCoords($prev, $prevEnd);
close CHUNKS;

sub checkCoords {
my ($name, $end) = @_;
if (exists($scafsHash{$prev}))
   {
   if ($scafsHash{$prev} != $prevEnd)
      {
      my $ed = $scafsHash{$prev};
      print "Scaffold $prev is not consistent between agps\n";
      }
   else
      {
      my $ed = $scafsHash{$prev};
      print "Scaffold $prev - ok\n";
      }
   }
}
'_EOF_'
   # << happy emacs
   chmod +x ./jkStuff/checkSizesInAgps.pl
   cd scaffolds
   perl /cluster/data/danRer4/jkStuff/checkSizesInAgps.pl \
        Zv6.scaffolds.agp Zv6.chunks.agp > Zv6.scafsvschunks
   grep "not consistent" Zv6.scafsvschunks
   # no lines were inconsistency was reported
   wc -l Zv6.scafsvschunks
   # 6653 Zv6.scafsvschunks
   grep "Zv6" Zv6.scaffolds.agp | wc -l
   # 6653
   # so all the scaffolds were checked and were ok.
   cd ..
   rm -r scaffolds
 
###########################################################################
# SPLIT AGP FILES BY CHROMOSOME (DONE, 2006-04-13, hartera)
# GENOME FASTA FROM SANGER WAS CREATED USING SCAFFOLDS AGP
   ssh kkstore01
   cd /cluster/data/danRer4
   # There are 2 .agp files: one for scaffolds (supercontigs on danRer1) and
   # then one for chunks (contigs on danRer1) showing how they map on to
   # scaffolds.
  
   # get list of scaffolds from FASTA file and check these are in agp
   grep '>' Zv6_scaffolds.fa | sed -e 's/>//' | sort | uniq > Zv6FaScafs.lst
   # get list of scaffolds from agp - do not print from gap lines
   awk '{if ($7 !~ /contig/) print $6;}' Zv6.scaffolds.agp \
        | sort | uniq > Zv6AgpScafs.lst
   diff Zv6FaScafs.lst Zv6AgpScafs.lst
   # no difference so all scaffolds are in the FASTA file
   # add "chr" prefix for the agp files
   perl -pi -e 's/^([0-9]+)/chr$1/' ./*.agp
   # for chromosomes 1 to 25, create 2 agps for each chrom, one for scaffolds 
   # and one for chunks:
   foreach c (`cat chrom1to25.lst`)
     echo "Processing $c ..."
     mkdir $c
     perl -we "while(<>){if (/^chr$c\t/) {print;}}" \
          ./Zv6.chunks.agp \
          > $c/chr$c.chunks.agp
     perl -we "while(<>){if (/^chr$c\t/) {print;}}" \
          ./Zv6.scaffolds.agp \
          > $c/chr$c.scaffolds.agp
   end
   
###########################################################################
# CREATE AGP FILES FOR chrNA AND chrUn (DONE, 2006-04-13, hartera)
# RECREATE AGP FILES WITH chrNA and chrUn RENAMED AS chrNA_random 
# AND chrUn_random (DONE, 2006-04-21, hartera)
# NOTE: IN THIS ASSEMBLY AND IN FUTURE, NAME chrNA AND chrUn AS 
# chrNA_random AND chrUn_random TO REFLECT THAT THEY ARE UNORDERED 
# COLLECTIONS OF SCAFFOLDS.  
   ssh kkstore01
   # chrNA_random consists of WGS contigs that could not be related to any
   # FPC contig and the scaffolds and contigs are named Zv5_NAN in the
   # first field of the agp files where the second N is an number.
   cd /cluster/data/danRer4
   mkdir ./NA_random
   awk '{if ($1 ~ /Zv6_NA/) print;}' Zv6.chunks.agp \
       > ./NA_random/NA_random.chunks.agp
   awk '{if ($1 ~ /Zv6_NA/) print;}' Zv6.scaffolds.agp \
       > ./NA_random/NA_random.scaffolds.agp
   # change the first field to "chrNA_random" then can use agpToFa to process
   perl -pi.bak -e 's/Zv6_NA[0-9]+/chrNA_random/' ./NA_random/*.agp
   wc -l ./NA_random/NA_random.scaffolds.agp
   # 2898 ./NA_random/NA_random.scaffolds.agp
   # check files and remove backup files
   # these are not sorted numerically by scaffold number
   rm ./NA_random/*.bak
   # then process chrUn_random - this is made from scaffolds and
   # contigs where the name is Zv6_scaffoldN in the first field of the
   # agp files. These scaffolds and contigs are unmapped to chromosomes
   # in the agp file. chrUn_random is made up of WGS scaffolds that mapped to
   # FPC contigs, but the chromosome is unknown.
   mkdir ./Un_random
   awk '{if ($1 ~ /Zv6_scaffold/) print;}' Zv6.chunks.agp \
       > ./Un_random/Un_random.chunks.agp
   awk '{if ($1 ~ /Zv6_scaffold/) print;}' Zv6.scaffolds.agp \
       > ./Un_random/Un_random.scaffolds.agp
   # change the first field to "chrUn_random" then can use agpToFa to process
   perl -pi.bak -e 's/Zv6_scaffold[0-9]+/chrUn_random/' ./Un_random/*.agp
   wc -l ./Un_random/Un_random.scaffolds.agp
   # 68 ./Un_random/Un_random.scaffolds.agp
   # check files and remove backup files
   rm ./Un_random/*.bak
   # get FASTA file of sequences for NA_random and Un_random and create agp with
   # Ns between scaffolds
   # from scaffolds agp, get name of scaffolds to be retrieved from the 
   # FASTA file to make the NA_random and Un_random chromosomes.
   cd /cluster/data/danRer4
   foreach c (NA_random Un_random)
     awk '{print $6;}' $c/$c.scaffolds.agp > $c/chr$c.scaffolds.lst
         $HOME/bin/i386/faSomeRecords /cluster/data/danRer4/Zv6_scaffolds.fa \
         $c/chr$c.scaffolds.lst $c/chr$c.fa
   end
   # check that all scaffolds in the list are in the FASTA file for 
   # NA_random and Un_random.
   # made a change to scaffoldFaToAgp.c so that the the number of Ns to be
   # inserted between scaffolds can be specified as an option.
   # There are less and smaller random scaffolds than before so use 50,000 Ns
   # between scaffolds as for the human random chromosomes.
   foreach c (NA_random Un_random)              
     $HOME/bin/i386/scaffoldFaToAgp -scaffoldGapSize=50000 $c/chr$c.fa
     mv $c/chr$c.fa $c/chr$c.scaffolds.fa
   end
   # change chrUn to chrNA_random for NA_random, change chrUn to chrUn_random
   # forUn_random. Change D to W for NA_random and Un_random..
   sed -e 's/chrUn/chrNA_random/' ./NA_random/chrNA_random.agp \
       | sed -e 's/D/W/' > ./NA_random/chrNA_random.scaffolds.agp
   # the scaffolds agp for chrNA_random is now sorted numerically by 
   # scaffold number
   sed -e 's/chrUn/chrUn_random/' ./Un_random/chrUn_random.agp \
       | sed -e 's/D/W/' > ./Un_random/chrUn_random.scaffolds.agp
   # edit ./NA_random/chrNA_random.scaffolds.agp and 
   # ./Un_random/chrUn_random.scaffolds.agp and remove last line as this 
   # just adds an extra 50000 Ns at the 
   # end of the sequence.
   rm ./NA_random/chrNA_random.agp ./Un_random/chrUn_random.agp
cat << '_EOF_' > ./jkStuff/createAgpWithGaps.pl
#!/usr/bin/perl
use strict;

# This script takes a chunks agp and inserts Ns between scaffolds for 
# the chunks (contigs) agp file. Could also insert Ns between scaffolds
# for scaffolds agp.

my ($chrom, $numN, $name, $prev, $st, $end, $prevEnd, $id);
my $chrom = $ARGV[0]; # chromosome name
my $numN = $ARGV[1];  # number of Ns to be inserted 
my $type = $ARGV[2]; # contigs or scaffolds

$prev = "";
$st = 1;
$prevEnd = 0;
$id = 0;

while (<STDIN>)
{
my $l = $_;
my @f = split(/\t/, $l);

if ($type eq "contigs")
   {
   $name = $f[9];
   }
else
   {
   $name = $f[5]
   }

my $currSt = $f[1];
my $currEnd = $f[2];
my $size = $currEnd - $currSt;

$id++;
$st = $prevEnd + 1;
$end = $st + $size;

if (($prev ne "") && ($prev ne $name))
   {
   $st = $prevEnd + 1;
   $end = ($st + $numN) - 1;
   print "$chrom\t$st\t$end\t$id\tN\t$numN\tcontig\tno\n";
   $prevEnd = $end;
   $id++;
   }

$st = $prevEnd + 1;
$end = $st + $size;
print "$chrom\t$st\t$end\t$id\t$f[4]\t$f[5]\t$f[6]\t$f[7]\t$f[8]";
if ($type eq "contigs")
   {
   print "\t$f[9]\t$f[10]\t$f[11]";
   }

$prevEnd = $end;
$prev = $name;
}
'_EOF_'
   chmod +x ./jkStuff/createAgpWithGaps.pl
   cd /cluster/data/danRer4/NA_random
   # for NA_random, sort the chunks.agp by contig number
   perl -pi.bak -e 's/Zv6_NA//' NA_random.chunks.agp
   sort -k6,6n NA_random.chunks.agp > NA_random.chunks2.agp
   # then put back Zv6_NA
   perl -pi.bak -e 's/([0-9]+\.[0-9]+)/Zv6_NA$1/' NA_random.chunks2.agp
   mv NA_random.chunks2.agp NA_random.chunks.agp
   # Un_random.chunks.agp is already sorted by scaffold number
   cd /cluster/data/danRer4
   foreach c (NA_random Un_random)
      cd $c
      perl /cluster/data/danRer4/jkStuff/createAgpWithGaps.pl \
           chr${c} 50000 contigs < ${c}.chunks.agp > chr${c}.chunks.agp
      cd ..
   end
   # check co-ordinates
   # field 2 is the start position, field 3 is the end and field 8 is the size
   # so check that this is consistent in scaffolds and chunks agp. 
   # check that the difference between 7th and 8th fields is the same as the 
   # difference between 11th and 12th fields for chunks agp. 
   cd /cluster/data/danRer4
   foreach c (NA_random Un_random)
        awk '{if ($6 ~ /^Zv6/ && (($3-$2+1) != $8)) print $6;}' \
            $c/chr${c}.scaffolds.agp > $c/chr${c}.scaffolds.coordCheck 
        awk '{if ($6 ~ /^Zv6/ && (($3-$2+1) != $8)) print $6;}' \
            $c/chr${c}.chunks.agp > $c/chr${c}.chunks.coordCheck 
        awk '{if ($5 != "N" && (($8 - $7) != ($12 - $11))) print $6;}' \
            $c/chr${c}.chunks.agp > $c/chr${c}.chunks.coordCheck2 
   end
   # check the outputs are empty
   wc -l NA_random/*.coord*
   wc -l Un_random/*.coord*
   rm NA_random/*.coord* Un_random/*.coord*
   # check that the scaffolds and chunks agp files are consistent with
   # each other. 
cat << '_EOF_' > ./jkStuff/checkSizesInAgps.pl
#!/usr/bin/perl -w
use strict;

my ($ch, $sc, %scafsHash);
$sc = $ARGV[0]; # scaffolds agp
$ch = $ARGV[1]; # chunks or contigs agp

open(SCAFS, $sc) || die "Can not open $sc: $!\n";
open(CHUNKS, $ch) || die "Can not open $ch: $!\n";

while (<SCAFS>)
{
my ($l, @f, $name, $e);
$l = $_;
@f = split(/\t/, $l);
if ($f[5] =~ /^Zv/)
   {
   $name = $f[5];
   $e = $f[2];
   $scafsHash{$name} = $e;
   }
}
close SCAFS;

my $scaf = "";
my $prev = "";
my $prevEnd = 0;

while (<CHUNKS>)
{
my ($line, @fi);
$line = $_;
@fi = split(/\t/, $line);

# if it is not a gap line
if ($fi[4] ne "N")
   {
   $scaf = $fi[9];
   if (($scaf ne $prev) && ($prev ne ""))
      {
      checkCoords($prev, $prevEnd);
      }
$prev = $scaf;
$prevEnd = $fi[2];
   }
}
# check last entry in file
checkCoords($prev, $prevEnd);
close CHUNKS;

sub checkCoords {
my ($name, $end) = @_;
if (exists($scafsHash{$prev}))
   {
   if ($scafsHash{$prev} != $prevEnd)
      {
      my $ed = $scafsHash{$prev};
      print "Scaffold $prev is not consistent between agps\n";
      }
   else
      {
      my $ed = $scafsHash{$prev};
      print "Scaffold $prev - ok\n";
      }
   }
}
'_EOF_'
   # <<< happy emacs   
   chmod +x jkStuff/checkSizesInAgps.pl
   foreach c (NA_random Un_random)
      perl /cluster/data/danRer4/jkStuff/checkSizesInAgps.pl \
           $c/chr${c}.scaffolds.agp $c/chr${c}.chunks.agp \
           > $c/${c}.scafsvschunks
   end
   foreach c (NA_random Un_random)
     grep "not consistent" $c/${c}.scafsvschunks
   end 
   wc -l NA_random/NA_random.scafsvschunks 
   wc -l Un_random/Un_random.scafsvschunks 
   # no lines were inconsistency was reported
   rm NA_random/NA_random.scafsvschunks Un_random/Un_random.scafsvschunks
   # clean up
   foreach c (NA_random Un_random)
      rm $c/${c}.scaffolds.agp $c/${c}.chunks.agp $c/chr${c}.scaffolds.fa \
         $c/chr${c}.scaffolds.lst $c/*.bak
   end
'_EOF_'

###########################################################################
# BUILD CHROM-LEVEL SEQUENCE (DONE, 2006-04-13, hartera)
# REPEAT THIS FOR chrNA_random AND chrUn_random (DONE, 2006-04-21, hartera)
   ssh kkstore01
   cd /cluster/data/danRer4
   # Ignore warnings about chrM files not existing - this chrom has 
   # already been processed - see mitochondrion section above.
   # Sequence is already in upper case so no need to change
   foreach c (`cat chrom.lst`)
     echo "Processing ${c}"
     $HOME/bin/i386/agpToFa -simpleMultiMixed $c/chr$c.scaffolds.agp chr$c \
         $c/chr$c.fa ./Zv6_scaffolds.fa
     echo "${c} - DONE"
   end
   # move scaffolds agp to be chrom agp and clean up
   foreach c (`cat chrom.lst`)
      cd $c
      cp chr${c}.scaffolds.agp chr${c}.agp
      mkdir -p agps
      mv chr${c}.*.agp ./agps/
      cd ..
   end
   # Repeat just for chrNA_random and chrUn_random (2006-04-21, hartera)
   foreach c (NA_random Un_random)
     echo "Processing ${c}"
     $HOME/bin/i386/agpToFa -simpleMultiMixed $c/chr$c.scaffolds.agp chr$c \
         $c/chr$c.fa ./Zv6_scaffolds.fa
     echo "${c} - DONE"
   end
   # move scaffolds agp to be chrom agp and clean up
   foreach c (NA_random Un_random)
      cd $c
      cp chr${c}.scaffolds.agp chr${c}.agp
      mkdir -p agps
      mv chr${c}.*.agp ./agps/
      cd ..
   end

##########################################################################
# CHECK CHROM AND VIRTUAL CHROM SEQUENCES (DONE, 2006-04-14, hartera)
# RE-CHECK THESE AFTER CREATING chrNA_random AND chrUn_random SEQUENCE FILES 
# (DONE, 2006-04-20, hartera)
   # Check that the size of each chromosome .fa file is equal to the last
   # co-ordinate of the corresponding agp file.
   ssh hgwdev
   cd /cluster/data/danRer4
   foreach c (`cat chrom.lst`)
     foreach f ( $c/chr$c.agp )
       set agpLen = `tail -1 $f | awk '{print $3;}'`
       set h = $f:r
       set g = $h:r
       echo "Getting size of $g.fa"
       set faLen = `faSize $g.fa | awk '{print $1;}'`
       if ($agpLen == $faLen) then
         echo "   OK: $f length = $g length = $faLen"
       else
         echo "ERROR:  $f length = $agpLen, but $g length = $faLen"
       endif
     end
   end
   # all are the OK so FASTA files are the expected size

###########################################################################
# CREATING DATABASE (DONE, 2006-04-14, hartera)
    # Create the database.
    # next machine
    ssh hgwdev
    echo 'create database danRer4' | hgsql ''
    # if you need to delete that database:  !!! WILL DELETE EVERYTHING !!!
    echo 'drop database danRer4' | hgsql danRer4
    # Use df to make sure there is at least 10 gig free on
    df -h /var/lib/mysql
# Before loading data:
# Filesystem            Size  Used Avail Use% Mounted on
# /dev/sdc1             1.8T  1.5T  173G  90% /var/lib/mysql

###########################################################################
# CREATING GRP TABLE FOR TRACK GROUPING (DONE, 2006-04-14, hartera)
    # next machine
    ssh hgwdev
    #  the following command copies all the data from the table
    #  grp in the database mm8 to the new database danRer4. Use one of the
    #  newest databases to copy from to make sure that the groupings are
    #  up to date.
    echo "create table grp (PRIMARY KEY(NAME)) select * from mm8.grp" \
      | hgsql danRer4
    # if you need to delete that table:   !!! WILL DELETE ALL grp data !!!
    echo 'drop table grp;' | hgsql danRer4

###########################################################################
# MAKE HGCENTRALTEST ENTRY FOR DANRER4 (DONE, 2006-04-14, hartera)
# CHANGE DATE FORMAT ON HGCENTRALTEST ENTRY (DONE, 2006-04-21, hartera)
    # Make entry into dbDb and defaultDb so test browser knows about it.
    ssh hgwdev
    # Add dbDb and defaultDb entries:
    echo 'insert into dbDb (name, description, nibPath, organism,  \
          defaultPos, active, orderKey, genome, scientificName,  \
          htmlPath, hgNearOk, hgPbOk, sourceName)  \
          values("danRer4", "March 2006", \
          "/gbdb/danRer4", "Zebrafish", "chr2:15,906,734-15,926,406", 1, \
          37, "Zebrafish", "Danio rerio", \
          "/gbdb/danRer4/html/description.html", 0,  0, \
          "Sanger Centre, Danio rerio Sequencing Project Zv6");' \
    | hgsql -h genome-testdb hgcentraltest
    # reformat the date (2006-04-21, hartera)
    echo 'update dbDb set description = "Mar. 2006" where name = "danRer4";' \
         | hgsql -h genome-testdb hgcentraltest

    # Create /gbdb directory for danRer4
    mkdir /gbdb/danRer4
    # SET AS DEFAULT LATER WHEN READY FOR RELEASE
    # set danRer4 to be the default assembly for Zebrafish
    echo 'update defaultDb set name = "danRer4" \
          where genome = "Zebrafish";' \
          | hgsql -h genome-testdb hgcentraltest

###########################################################################
# BREAK UP SEQUENCE INTO 5MB CHUNKS AT CONTIGS/GAPS FOR CLUSTER RUNS
# (DONE, 2006-04-14, hartera)
# RE-DONE JUST FOR chrNA_random AND chrUn_random (DONE, 2006-04-20, hartera)
     ssh kkstore01
     cd /cluster/data/danRer4
     foreach c (`cat chrom.lst`)
       foreach agp ($c/chr$c.agp)
         if (-e $agp) then
           set fa = $c/chr$c.fa
           echo splitting $agp and $fa
           cp -p $agp $agp.bak
           cp -p $fa $fa.bak
           splitFaIntoContigs $agp $fa . -nSize=5000000
         endif
       end
     end
     
     # Repeat just for chrNA_random and chrUn_random (2006-04-21, hartera)
     ssh kkstore01
     cd /cluster/data/danRer4
     foreach c (NA_random Un_random)
       foreach agp ($c/chr$c.agp)
         if (-e $agp) then
           set fa = $c/chr$c.fa
           echo splitting $agp and $fa
           cp -p $agp $agp.bak
           cp -p $fa $fa.bak
           splitFaIntoContigs $agp $fa . -nSize=5000000
         endif
       end
     end

###########################################################################
# MAKE LIFTALL.LFT (DONE, 2006-04-14, hartera)
# REMAKE LIFTALL.LFT WITH chrNA_random AND chrUn_random 
# (DONE, 2006-04-21, hartera)
     ssh kkstore01
     cd /cluster/data/danRer4
     rm jkStuff/liftAll.lft
     foreach c (`cat chrom.lst`)
       cat $c/lift/ordered.lft >> jkStuff/liftAll.lft
     end

###########################################################################
# MAKE TRACKDB ENTRY FOR DANRER4 (DONE, 2006-04-14, hartera)
# Should add this later when adding gold/gap tracks. Angie created a 
# temporary chromInfo table otherwise make update/alpha causes an error
# (2006-04-17)
    # Make trackDb table so browser knows what tracks to expect.
    ssh hgwdev
    mkdir -p ~/kent/src/hg/makeDb/trackDb/zebrafish/danRer4
    cd ~/kent/src/hg/makeDb/trackDb/zebrafish
    cvs add danRer4
    cvs commit danRer4
    cd ~/kent/src/hg/makeDb/trackDb
    cvs up -d -P
    # Edit that makefile to add danRer4 in all the right places and do
    make update DBS=danRer4
    make alpha DBS=danRer4
    cvs commit -m "Added danRer4." makefile

###########################################################################
# MAKE DESCRIPTION/SAMPLE POSITION HTML PAGE (DONE, 2006-04-14, hartera)
    ssh hgwdev
    mkdir /cluster/data/danRer4/html
    # make a symbolic link from /gbdb/danRer4/html to /cluster/data/danRer4/html
    ln -s /cluster/data/danRer4/html /gbdb/danRer4/html
    # Add a description page for zebrafish
    cd /cluster/data/danRer4/html
    cp $HOME/kent/src/hg/makeDb/trackDb/zebrafish/danRer3/description.html .
    # Edit this for zebrafish danRer4

    # create a description.html page here
    cd ~/kent/src/hg/makeDb/trackDb/zebrafish/danRer4
    # Add description page here too
    cp /cluster/data/danRer4/html/description.html .
    cvs add description.html
    cvs commit -m "First draft of description page for danRer4." \
        description.html
    cd ~/kent/src/hg/makeDb/trackDb
    make update DBS=danRer4
    make alpha  DBS=danRer4

###########################################################################
# SIMPLE REPEAT [TRF] TRACK  (DONE, 2006-04-14, hartera)
# RE-RUN FOR chrNA AND chrUn RENAMED AS chrNA_random AND chrUn_random
# AND RELOAD THE TABLE (DONE, 2006-04-21, hartera)
# MADE A NOTE IN THE HISTORY TABLE TO EXPLAIN WHY THE simpleRepeats TABLE
# WAS RELOADED (DONE, 2006-04-22, hartera)
    # TRF can be run in parallel with RepeatMasker on the file server
    # since it doesn't require masked input sequence.
    # Run this on the kilokluster. Need to mask contig and chromosome 
    # sequences so run trf using contig sequences.
    # First copy over contig sequences to iscratch and then rsync to cluster.
    ssh kkr1u00
    rm -r /iscratch/i/danRer4/contigsNoMask
    mkdir -p /iscratch/i/danRer4/contigsNoMask
    cd /cluster/data/danRer4
    foreach d (/cluster/data/danRer4/*/chr*_?{,?})
       set ctg = $d:t
       foreach f ($d/${ctg}.fa)
          echo "Copyig $f ..."
          cp $f /iscratch/i/danRer4/contigsNoMask/
       end
    end
    ls /iscratch/i/danRer4/contigsNoMask/*.fa | wc -l
    # 317 sequence files
    # rsync to cluster machines
    foreach R (2 3 4 5 6 7 8)
       rsync -a --progress /iscratch/i/danRer4/ kkr${R}u00:/iscratch/i/danRer4/
    end
     
    ssh kki
    mkdir -p /cluster/data/danRer4/bed/simpleRepeat
    cd /cluster/data/danRer4/bed/simpleRepeat
    mkdir trf
cat << '_EOF_' > runTrf
#!/bin/csh -fe
#
set path1 = $1
set inputFN = $1:t
set outpath = $2
set outputFN = $2:t
mkdir -p /tmp/$outputFN
cp $path1 /tmp/$outputFN
pushd .
cd /tmp/$outputFN
/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $inputFN /dev/null -bedAt=$outputFN -tempDir=/tmp
popd
rm -f $outpath
cp -p /tmp/$outputFN/$outputFN $outpath
rm -fr /tmp/$outputFN/*
rmdir --ignore-fail-on-non-empty /tmp/$outputFN
'_EOF_'
 # << keep emacs coloring happy
    chmod +x runTrf

cat << '_EOF_' > gsub
#LOOP
./runTrf {check in line+ $(path1)}  {check out line trf/$(root1).bed}
#ENDLOOP
'_EOF_'
    # << keep emacs coloring happy

    ls -1S /iscratch/i/danRer4/contigsNoMask/chr*.fa > genome.lst
    gensub2 genome.lst single gsub jobList
    # 317 jobs
    para create jobList
    para try, check, push, check etc...
    para time
# Completed: 317 of 317 jobs
# CPU time in finished jobs:      25083s     418.05m     6.97h    0.29d  0.001 y
# IO & Wait Time:                   933s      15.55m     0.26h    0.01d  0.000 y
# Average job time:                  82s       1.37m     0.02h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            2732s      45.53m     0.76h    0.03d
# Submission to last job:          4604s      76.73m     1.28h    0.05d

    # Re-do only for chrNA_random and chrUn_random (2006-04-21, hartera)
    ssh kki
    cd /cluster/data/danRer4/bed/simpleRepeat
    rm trf/chrNA*.bed
    rm trf/chrUn*.bed
    rm simpleRepeat.bed
    mkdir -p randomsRun/trf
    cd randomsRun
    cp ../runTrf .
    cp ../gsub .
    ls -1S /iscratch/i/danRer4/contigsNoMask/chr*_random*.fa > genome.lst
    gensub2 genome.lst single gsub jobList
    para create jobList
    # 46 jobs
    para try, check, push, check etc...
    para time
# Completed: 46 of 46 jobs
# CPU time in finished jobs:       1904s      31.73m     0.53h    0.02d  0.000 y
# IO & Wait Time:                   103s       1.72m     0.03h    0.00d  0.000 y
# Average job time:                  44s       0.73m     0.01h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:             241s       4.02m     0.07h    0.00d
# Submission to last job:           269s       4.48m     0.07h    0.00d 
    cp ./trf/*.bed /cluster/data/danRer4/bed/simpleRepeat/trf/
    # lift up to chrom level
    cd /cluster/data/danRer4/bed/simpleRepeat
    rm simpleRepeat.bed
    liftUp simpleRepeat.bed /cluster/data/danRer4/jkStuff/liftAll.lft warn \
           trf/*.bed
    # Reload into the database
    ssh hgwdev
    cd /cluster/data/danRer4/bed/simpleRepeat
    hgsql -e 'drop table simpleRepeat;' danRer4
    hgLoadBed danRer4 simpleRepeat simpleRepeat.bed \
      -sqlTable=$HOME/kent/src/hg/lib/simpleRepeat.sql
    # Loaded 759659 elements of size 16
    # Make a note in the history table to explain why the simpleRepeats
    # table was reloaded (2006-04-22, hartera)
    hgsql -e 'update history set errata = \
      "Dropped table for reloading after changing names of random chroms." \
      where ix = 2;' danRer4

###########################################################################
# PROCESS SIMPLE REPEATS INTO MASK (DONE, 2005-06-14, hartera)
# RE-DO AFTER RENAMING RANDOM CHROMS AS chrNA_random AND chrUn_random
# (DONE, 2006-04-21, hartera)
   # After the simpleRepeats track has been built, make a filtered version
   # of the trf output: keep trf's with period <= 12:
   ssh kkstore01
   cd /cluster/data/danRer4/bed/simpleRepeat
   rm -r trfMask
   mkdir -p trfMask
   foreach f (trf/chr*.bed)
     awk '{if ($5 <= 12) print;}' $f > trfMask/$f:t
   end

   # Lift up filtered trf output to chrom coords as well:
   cd /cluster/data/danRer4
   rm -r ./bed/simpleRepeat/trfMaskChrom
   mkdir bed/simpleRepeat/trfMaskChrom
   
   foreach c (`cat chrom.lst`)
     if (-e $c/lift/ordered.lst) then
       perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
         $c/lift/ordered.lst > $c/lift/oTrf.lst
       liftUp bed/simpleRepeat/trfMaskChrom/chr$c.bed \
         jkStuff/liftAll.lft warn `cat $c/lift/oTrf.lst`
     endif
     if (-e $c/lift/random.lst) then
       perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
          $c/lift/random.lst > $c/lift/rTrf.lst
       liftUp bed/simpleRepeat/trfMaskChrom/chr${c}_random.bed \
         jkStuff/liftAll.lft warn `cat $c/lift/rTrf.lst`
     endif
   end

###########################################################################
# GET ADDITIONAL ZEBRAFISH REPBASE LIBRARY FOR REPEATMASKER AND ADD TO
# DANIO LIBRARY FOR REPEATMASKER (DONE, 2006-04-14, hartera)
# Go to http://www.girinst.org/server/RepBase/RepBase11.02.fasta
# (03-15-2006) and download zebunc.ref.txt containing unclassified zebrafish 
# repeats.
# Need username and password. Copy to /cluster/bluearc/RepeatMasker/Libraries/
   ssh hgwdev
   cd /cluster/bluearc/RepeatMasker/Libraries
   # This is /cluster/bluearc/RepeatMasker060320/Libraries
   # Do a dummy run of RepeatMasker with the -species option. This creates
   # a zebrafish-specific library from the EMBL format RepBase library.
   # Then the zebunc.ref unclassified repeats can be added to this library.
   /cluster/bluearc/RepeatMasker/RepeatMasker -spec danio /dev/null
   # RepeatMasker version development-$Id: RepeatMasker,v 1.13 2006/03/21 
   # This creates a specieslib in Libraries/20060315/danio
   # Format the zebunc.ref library:
   # Sequence is upper case, change to lower case like the specieslib
   cat zebunc.ref.txt | tr '[A-Z]' '[a-z]' > zebunc.ref.format 
   perl -pi.bak -e 's/>dr([0-9]+)/>Dr$1#Unknown/' zebunc.ref.format
   grep '>' zebunc.ref.format | wc -l
   # 958
   cd /cluster/bluearc/RepeatMasker/Libraries/20060315/danio
   grep '>' specieslib | wc -l
   # 219
   mv specieslib danio.lib
   cat danio.lib ../../zebunc.ref.format > specieslib  
   grep '>' specieslib | wc -l
   # 1177
   rm danio.lib
   # make a copy in Libraries directory in case this directory of libraries
   # is removed.
   cp specieslib /cluster/bluearc/RepeatMasker/Libraries/danio.lib
 
###########################################################################
# SPLIT SEQUENCE FOR REPEATMASKER RUN (DONE, 2006-04-14, hartera)
# SPLIT SEQUENCE AGAIN JUST FOR chrNA_random AND chrUn_random AFTER RENAMING
# THESE RANDOM CHROMS (DONE, 2006-04-21, hartera)
   ssh kkstore01
   cd /cluster/data/danRer4
   
   # break up into 500 kb sized chunks at gaps if possible 
   # for RepeatMasker runs
   foreach c (`cat chrom.lst`)
      foreach d ($c/chr${c}*_?{,?})
        cd $d
        echo "splitting $d"
        set contig = $d:t
        faSplit gap $contig.fa 500000 ${contig}_ -lift=$contig.lft \
            -minGapSize=100
        cd ../..
      end
   end
   # took about 3 minutes. 
   # split just for chrNA_random and chrUn_random (2006-04-21, hartera)
   cd /cluster/data/danRer4
   foreach c (NA_random Un_random)
      foreach d ($c/chr${c}*_?{,?})
        cd $d
        echo "splitting $d"
        set contig = $d:t
        faSplit gap $contig.fa 500000 ${contig}_ -lift=$contig.lft \
            -minGapSize=100
        cd ../..
      end
    end

###########################################################################
# REPEATMASKER RUN (DONE, 2006-04-21, hartera)
   # Originally run 2006-04-14. There was one sequence chr16_4_10.fa that 
   # failed with a division by zero error. Sent this as a test case with the 
   # danio library to Robert Hubley who fixed the bug and sent a new
   # version of ProcessRepeats. Checked this into CVS for 
   # /cluster/bluearc/RepeatMasker on 2006-04-19.
   # When a new library is added for this version of RepeatMasker, need to 
   # check in /cluster/bluearc/RepeatMasker/Libraries for a directory made 
   # up of a date e.g. 20060315 here and inside this are species directories
   # for which RepeatMasker has already been run. In this directory it creates
   # a specieslib of the danio repeats. If this exists, this is used for the
   # RepeatMasker run for that species. Check that this contains the 
   # unclassified Zebrafish repeats with IDs beginning with Dr. This library
   # with these repeats should have been created in the section above:
   # Use sequence split into 500 kb chunks.
   ssh kkstore01
   cd /cluster/data/danRer4
   mkdir RMRun
   # Record RM version used:
   ls -l /cluster/bluearc/RepeatMasker
   # lrwxrwxrwx  1 angie protein 18 Mar 20 16:50 /cluster/bluearc/RepeatMasker -> RepeatMasker060320
   # March 20 2006 (open-3-1-5) version of RepeatMasker
   # get RM database version
   grep RELEASE /cluster/bluearc/RepeatMasker/Libraries/RepeatMaskerLib.embl \
        > RMdatabase.version
   # RELEASE 20060315

   cd /cluster/data/danRer4
   cat << '_EOF_' > jkStuff/RMZebrafish
#!/bin/csh -fe

cd $1
pushd .
/bin/mkdir -p /tmp/danRer4/$2
/bin/cp $2 /tmp/danRer4/$2/
cd /tmp/danRer4/$2
/cluster/bluearc/RepeatMasker060320/RepeatMasker -ali -s -species danio $2
popd
/bin/cp /tmp/danRer4/$2/$2.out ./
if (-e /tmp/danRer4/$2/$2.align) /bin/cp /tmp/danRer4/$2/$2.align ./
if (-e /tmp/danRer4/$2/$2.tbl) /bin/cp /tmp/danRer4/$2/$2.tbl ./
if (-e /tmp/danRer4/$2/$2.cat) /bin/cp /tmp/danRer4/$2/$2.cat ./
/bin/rm -fr /tmp/danRer4/$2/*
/bin/rmdir --ignore-fail-on-non-empty /tmp/danRer4/$2
/bin/rmdir --ignore-fail-on-non-empty /tmp/danRer4
'_EOF_'
   # << emacs
   chmod +x jkStuff/RMZebrafish

   # move old files out the way and re-run on 2006-04-19
   cd /cluster/data/danRer4
   mkdir RMOutOld
   foreach d (*/chr*_?{,?})
      set contig = $d:t
      echo $contig
      foreach c ($d/$contig*.fa.*)
         set t=$c:t
         mv $c /cluster/data/danRer4/RMOutOld/$t.bak
      end
   end  
 
   cp /dev/null RMRun/RMJobs
   foreach c (`cat chrom.lst`)
      foreach d ($c/chr${c}_?{,?})
          set ctg = $d:t
          foreach f ( $d/${ctg}_?{,?}.fa )
            set f = $f:t
            echo /cluster/data/danRer4/jkStuff/RMZebrafish \
                 /cluster/data/danRer4/$d $f \
               '{'check out line+ /cluster/data/danRer4/$d/$f.out'}' \
              >> RMRun/RMJobs
          end
      end
   end

   # Do the run again with new version of ProcessRepeats used 
   # for RepeatMasker.
   ssh pk
   cd /cluster/data/danRer4/RMRun
   para create RMJobs
   # 4382 jobs written to batch
   para try, check, push, check ... etc.
   para time
# Completed: 4382 of 4382 jobs
# CPU time in finished jobs:   11745656s  195760.94m  3262.68h  135.95d  0.372 y
# IO & Wait Time:                 18953s     315.88m     5.26h    0.22d  0.001 y
# Average job time:                2685s      44.75m     0.75h    0.03d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            3878s      64.63m     1.08h    0.04d
# Submission to last job:         41887s     698.12m    11.64h    0.48d 
   
   #- Lift up the 500KB chunk .out's to 5MB ("pseudo-contig") level
   ssh kkstore01
   cd /cluster/data/danRer4
   foreach d (*/chr*_?{,?})
     set contig = $d:t
     echo $contig
     liftUp $d/$contig.fa.out $d/$contig.lft warn $d/${contig}_*.fa.out \
        > /dev/null
   end

   #- Lift pseudo-contigs to chromosome level
   foreach c (`cat chrom.lst`)
      echo lifting $c
      cd $c
      if (-e lift/ordered.lft && ! -z lift/ordered.lft) then
        liftUp chr$c.fa.out lift/ordered.lft warn `cat lift/oOut.lst` \
        > /dev/null
      endif
      cd ..
   end
   # Re-run for just chrNA_random and chrUn_random (start on 2006-04-21)
   ssh kkstore01
   mkdir /cluster/data/danRer4/RMRun/randomsRun
   cd /cluster/data/danRer4
   cp /dev/null RMRun/randomsRun/RMJobs
   foreach c (NA_random Un_random)
      foreach d ($c/chr${c}_?{,?})
          set ctg = $d:t
          foreach f ( $d/${ctg}_?{,?}.fa )
            set f = $f:t
            echo /cluster/data/danRer4/jkStuff/RMZebrafish \
                 /cluster/data/danRer4/$d $f \
               '{'check out line+ /cluster/data/danRer4/$d/$f.out'}' \
              >> RMRun/randomsRun/RMJobs
          end
      end
   end

   # Do the run again for chrNA_random and chrUn_random.
   ssh pk
   cd /cluster/data/danRer4/RMRun/randomsRun
   para create RMJobs
   # 468 jobs written to batch
   para try, check, push, check ... etc.
   para time
# Completed: 468 of 468 jobs
# CPU time in finished jobs:     551863s    9197.71m   153.30h    6.39d  0.017 y
# IO & Wait Time:                  2217s      36.96m     0.62h    0.03d  0.000 y
# Average job time:                1184s      19.73m     0.33h    0.01d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            3836s      63.93m     1.07h    0.04d
# Submission to last job:          9086s     151.43m     2.52h    0.11d

   #- Lift up the 500KB chunk .out's to 5MB ("pseudo-contig") level
   ssh kkstore01
   cd /cluster/data/danRer4
   foreach c (NA_random Un_random)
     foreach d (${c}/chr*_?{,?})
       set contig = $d:t
       echo $contig
       liftUp $d/$contig.fa.out $d/$contig.lft warn $d/${contig}_*.fa.out \
              > /dev/null
     end
   end

   #- Lift pseudo-contigs to chromosome level
   foreach c (NA_random Un_random)
      echo lifting $c
      cd $c
      if (-e lift/ordered.lft && ! -z lift/ordered.lft) then
        liftUp chr$c.fa.out lift/ordered.lft warn `cat lift/oOut.lst` \
        > /dev/null
      endif
      cd ..
   end

   # Load tables
   #- Load the .out files into the database with:
   ssh hgwdev
   cd /cluster/data/danRer4
   hgLoadOut danRer4 */chr*.fa.out -verbose=2 >& load.log
# bad rep range [5031, 4990] line 51895 of 14/chr14.fa.out 
# bad rep range [4559, 4558] line 59431 of 16/chr16.fa.out 
# bad rep range [1202, 1201] line 131633 of 16/chr16.fa.out 
# bad rep range [280, 252] line 93608 of 17/chr17.fa.out 
# bad rep range [429, 272] line 43230 of 22/chr22.fa.out 
# bad rep range [262, 261] line 167346 of 3/chr3.fa.out 
# bad rep range [889, 888] line 28495 of 5/chr5.fa.out 
# bad rep range [349, 348] line 113404 of 5/chr5.fa.out 
# bad rep range [1133, 1132] line 200654 of 5/chr5.fa.out 
# bad rep range [965, 920] line 3567 of 8/chr8.fa.out 
# bad rep range [292, 291] line 6354 of NA_random/chrNA_random.fa.out
# note: 11 records dropped due to repStart > repEnd
   # Not too many errors so just ignore, but send examples to Arian Smit
   # and Robert Hubley. 
   # check coverage of repeats masked
   featureBits -chrom=chr1 danRer3 rmsk
   # 25822888 bases of 55500710 (46.527%) in intersection 
   featureBits -chrom=chr1 danRer4 rmsk 
   # 32880041 bases of 70589895 (46.579%) in intersection

###########################################################################
# MASK SEQUENCE WITH REPEATMASKER AND SIMPLE REPEAT/TRF AND BUILD NIB FILES
# (DONE, 2006-04-22, hartera)
    ssh kkstore01
    cd /cluster/data/danRer4
    # Soft-mask (lower-case) the contig and chr .fa's,
    # then make hard-masked versions from the soft-masked.
    set trfCtg=bed/simpleRepeat/trfMask
    set trfChr=bed/simpleRepeat/trfMaskChrom
    # for the chromosomes:
    foreach f (*/chr*.fa)
      echo "repeat- and trf-masking $f"
      maskOutFa -soft $f $f.out $f
      set chr = $f:t:r
      maskOutFa -softAdd $f $trfChr/$chr.bed $f
      echo "hard-masking $f"
      maskOutFa $f hard $f.masked
    end

    # for the contigs:
    foreach c (`cat chrom.lst`)
      echo "repeat- and trf-masking contigs of chr$c"
      foreach d ($c/chr*_?{,?})
        set ctg=$d:t
        set f=$d/$ctg.fa
        maskOutFa -soft $f $f.out $f
        maskOutFa -softAdd $f $trfCtg/$ctg.bed $f
        maskOutFa $f hard $f.masked
      end
    end
    
    # check percent sequence masked
    faSize /cluster/data/danRer4/1/chr1.fa
    # 70589895 bases (904883 N's 69685012 real 36751306 upper 
    # 32933706 lower) in 1 sequences in 1 files

    faSize /cluster/data/danRer3/1/chr1.fa
    # 55805710 bases (1047706 N's 54758004 real 28887275 upper 
    # 25870729 lower) in 1 sequences in 1 files
    # 47% of danRer4 chr1.fa is in lower case so masked
    # Build nib files, using the soft masking in the fa
    mkdir nib
    foreach f (*/chr*.fa)
      faToNib -softMask $f nib/$f:t:r.nib
    end
    ls ./nib/* | wc
    # 28
    
###########################################################################
# STORING O+O SEQUENCE AND ASSEMBLY INFORMATION AND CREATE 2BIT FILE
# (DONE, 2006-04-23, hartera)
    # Make symbolic links from /gbdb/danRer4/nib to the real nibs
    ssh hgwdev
    cd /cluster/data/danRer4
    mkdir -p /gbdb/danRer4/nib
    foreach f (/cluster/data/danRer4/nib/chr*.nib)
      ln -s $f /gbdb/danRer4/nib
    end
    # Load /gbdb/danRer4/nib paths into database and save size info
    # hgNibSeq creates chromInfo table
    hgNibSeq -preMadeNib danRer4 /gbdb/danRer4/nib */chr*.fa
    echo "select chrom,size from chromInfo" | hgsql -N danRer4 > chrom.sizes
    # take a look at chrom.sizes, should be 28 lines
    wc chrom.sizes
    # 28      56     422 chrom.sizes

    # Make one big 2bit file as well, and make a link to it in
    # /gbdb/danRer4 because hgBlat looks there:
    faToTwoBit */chr*.fa danRer4.2bit
    # check the 2bit file
    twoBitInfo danRer4.2bit 2bit.tab
    diff 2bit.tab chrom.sizes
    # should be the same and they are so ok.
    rm 2bit.tab
    # add link to this 2bit file from gbdb danRer4 directory 
    ln -s /cluster/data/danRer4/danRer4.2bit /gbdb/danRer4/

###########################################################################
# MAKE GOLD AND GAP TRACKS (DONE, 2006-04-23, hartera)
    ssh hgwdev
    cd /cluster/data/danRer4
    # the gold and gap tracks are created from the chrN.agp file and this is
    # the scaffolds or supercontigs agp 
    hgGoldGapGl -noGl -chromLst=chrom.lst danRer4 /cluster/data/danRer4 .

    # featureBits danRer4 gold
    # 1626093931 bases of 1626093931 (100.000%) in intersection
    # featureBits danRer3 gold
    # 1630323462 bases of 1630323462 (100.000%) in intersection

    # featureBits danRer4 gap
    # 148566200 bases of 1626093931 (9.136%) in intersection
    # featureBits danRer3 gap
    # 13709500 bases of 1630323462 (0.841%) in intersection
    # there are larger gaps now in chrNA and chrUn so compare just chr1
    # featureBits -chrom=chr1 danRer4 gap
    # 16000 bases of 70573895 (0.023%) in intersection
    # featureBits -chrom=chr1 danRer3 gap
    # 305000 bases of 55500710 (0.550%) in intersection
    # without random or chrUn chroms:
    # featureBits -noRandom danRer4 gap
    # 366200 bases of 1546950119 (0.024%) in intersection
    # featureBits -noRandom danRer3 gap
    # 6240000 bases of 1200146216 (0.520%) in intersection
# Add trackDb.ra entries for gold and gap tracks and also create
# gap.html and gold.html pages.

###########################################################################
# PUT MASKED SEQUENCE OUT ON iSERVERS AND THE SAN FOR CLUSTER RUNS
# (DONE, 2006-04-23, hartera)
    ssh kkr1u00
    # Chrom-level mixed nibs that have been repeat- and trf-masked:
    rm -rf /iscratch/i/danRer4/nib
    mkdir -p /iscratch/i/danRer4/nib
    cp -p /cluster/data/danRer4/nib/chr*.nib /iscratch/i/danRer4/nib
    # Pseudo-contig fa that have been repeat- and trf-masked:
    rm -rf /iscratch/i/danRer4/trfFa
    mkdir /iscratch/i/danRer4/trfFa
    foreach d (/cluster/data/danRer4/*/chr*_?{,?})
      cp -p $d/$d:t.fa /iscratch/i/danRer4/trfFa
    end
    rm -rf /iscratch/i/danRer4/rmsk
    mkdir -p /iscratch/i/danRer4/rmsk
    cp -p /cluster/data/danRer4/*/chr*.fa.out /iscratch/i/danRer4/rmsk
    cp -p /cluster/data/danRer4/danRer4.2bit /iscratch/i/danRer4/
    # rsync files - faster than using iSync
    foreach R (2 3 4 5 6 7 8)
      echo "rsync for kkr${R}u00 ..."
      rsync -a --progress /iscratch/i/danRer4/ kkr${R}u00:/iscratch/i/danRer4/
    end
    # error rsyncing to kkr2u00: 
    # connect to host kkr2u00 port 22: No route to host

    # then add the same sequence files to the san
    ssh kkstore01
    # Chrom-level mixed nibs that have been repeat- and trf-masked:

    mkdir -p /san/sanvol1/scratch/danRer4/nib
    rm -rf /san/sanvol1/scratch/danRer4/nib
    cp -p /cluster/data/danRer4/nib/chr*.nib /san/sanvol1/scratch/danRer4/nib
    # Pseudo-contig fa that have been repeat- and trf-masked:
    rm -rf /san/sanvol1/scratch/danRer4/trfFa
    mkdir /san/sanvol1/scratch/danRer4/trfFa
    foreach d (/cluster/data/danRer4/*/chr*_?{,?})
      cp -p $d/$d:t.fa /san/sanvol1/scratch/danRer4/trfFa
    end
    cp /cluster/data/danRer4/danRer4.2bit /san/sanvol1/scratch/danRer4
    
###########################################################################
# ADD CONTIGS TRACK (DONE, 2006-04-23, hartera)
    # make ctgPos2 (contig name, size, chrom, chromStart, chromEnd) from 
    # chunks (contigs) agp files.
    ssh kkstore01
    mkdir -p /cluster/data/danRer4/bed/ctgPos2
    cd /cluster/data/danRer4/bed/ctgPos2
    # ctgPos2 .sql .as .c and .h files exist - see makeDanRer1.doc
    foreach c (`cat /cluster/data/danRer4/chrom.lst`)
         awk 'BEGIN {OFS="\t"} \
         {if ($5 != "N") print $6, $3-$2+1, $1, $2-1, $3, $5}' \
         /cluster/data/danRer4/$c/agps/chr${c}.chunks.agp >> ctgPos2.tab
    end
    # load the ctgPos2 table
    ssh hgwdev
    cd /cluster/data/danRer4/bed/ctgPos2
    # use hgLoadSqlTab as it gives more error messages than using 
    # "load data local infile ...".
    /cluster/bin/i386/hgLoadSqlTab danRer4 ctgPos2 \
            ~/kent/src/hg/lib/ctgPos2.sql ctgPos2.tab
# create trackDb.ra entry and html page for ctgPos2 track.
# add search for the track and make sure the termRegex will handle
# contigs named "Zv6_scaffoldN.N" where N is an integer and all the 
# contig accessions in the *.chunks.agp files.

###########################################################################
# CREATE gc5Base WIGGLE TRACK (DONE, 2006-04-23, hartera)
    ssh kkstore01
    mkdir -p /cluster/data/danRer4/bed/gc5Base
    cd /cluster/data/danRer4/bed/gc5Base
    nice hgGcPercent -wigOut -doGaps -file=stdout -win=5 danRer4 \
        /cluster/data/danRer4 | wigEncode stdin gc5Base.wig gc5Base.wib
    #       Calculating gcPercent with window size 5
    #       Using twoBit: /cluster/data/danRer4/danRer4.2bit
    #       File stdout created
    #   Converted stdin, upper limit 100.00, lower limit 0.00
    # runs for about 7 minutes 

    # load database with the .wig file and add .wib file to /gbdb/danRer4
    ssh hgwdev
    cd /cluster/data/danRer4/bed/gc5Base
    mkdir /gbdb/danRer4/wib
    ln -s `pwd`/gc5Base.wib /gbdb/danRer4/wib
    time hgLoadWiggle -pathPrefix=/gbdb/danRer4/wib danRer4 gc5Base gc5Base.wig
    # 17 second load time

    #   verify index is correct:
    hgsql danRer4 -e "show index from gc5Base;"
    #   should see good numbers in Cardinality column

###########################################################################
# MAKE 10.OOC, 11.OOC FILES FOR BLAT (DONE, 2005-04-24, hartera)
    # Use -repMatch=512 (based on size -- for human we use 1024, and
    # the zebrafish genome is ~50% of the size of the human genome
    ssh kkr1u00
    mkdir /cluster/data/danRer4/bed/ooc
    cd /cluster/data/danRer4/bed/ooc
    mkdir -p /san/sanvol1/scratch/danRer4
    ls -1 /cluster/data/danRer4/nib/chr*.nib > nib.lst
    blat nib.lst /dev/null /dev/null -tileSize=11 \
      -makeOoc=/san/sanvol1/scratch/danRer4/danRer4_11.ooc -repMatch=512
    # Wrote 50424 overused 11-mers to /cluster/bluearc/danRer4/11.ooc
    # For 10.ooc, repMatch = 4096 for human, so use 2048
    blat nib.lst /dev/null /dev/null -tileSize=10 \
      -makeOoc=/san/sanvol1/scratch/danRer4/danRer4_10.ooc -repMatch=2048
    # Wrote 12231 overused 10-mers to /cluster/bluearc/danRer4/10.ooc 
    # keep copies of ooc files in this directory and copy to iscratch
    cp /san/sanvol1/scratch/danRer4/*.ooc .
    cp -p /san/sanvol1/scratch/danRer4/*.ooc /iscratch/i/danRer4/
    # rsync to iServers
    foreach R (2 3 4 5 6 7 8)
       rsync -a --progress /iscratch/i/danRer4/*.ooc \
             kkr${R}u00:/iscratch/i/danRer4/
    end
      
###########################################################################
# AFFYMETRIX ZEBRAFISH GENOME ARRAY CHIP (DONE, 2006-04-24, hartera)
# NOTE: Jim recommends that, in the future, all AFFY blat alignments should drop
# -mask=lower for blat and drop -minIdentity=95 to -minIdentity=90 as the
# higher minIdentity is causing alignments to be dropped that should not be. 
# e.g.  /cluster/bin/i386/blat -fine -minIdentity=90 -ooc=11.ooc  
# $(path1) $(path2) {check out line+ psl/$(root1)_$(root2).psl}
# pslReps can be used to handle filtering at a later step. Blat's minIdentity 
# seems to be more severe than that for pslReps as it takes insertions and 
# deletions into account. 

# array chip sequences already downloaded for danRer1
    ssh hgwdev
    # need to copy sequences to the bluearc first to transfer to the iServers
    cd /projects/compbio/data/microarray/affyZebrafish
    mkdir -p /cluster/bluearc/affy
    cp -p \
      /projects/compbio/data/microarray/affyZebrafish/Zebrafish_consensus.fa \
      /cluster/bluearc/affy/
    # Set up cluster job to align Zebrafish consensus sequences to danRer3
    ssh kkr1u00
    mkdir -p /cluster/data/danRer4/bed/affyZebrafish.2006-04-24
    ln -s /cluster/data/danRer4/bed/affyZebrafish.2006-04-24 \
          /cluster/data/danRer4/bed/affyZebrafish
    cd /cluster/data/danRer4/bed/affyZebrafish
    mkdir -p /iscratch/i/affy
    cp /cluster/bluearc/affy/Zebrafish_consensus.fa /iscratch/i/affy
    foreach R (2 3 4 5 6 7 8)
       rsync -a --progress /iscratch/i/affy/*.fa \
             kkr${R}u00:/iscratch/i/affy/
    end
    # small cluster run to align sequences
    ssh kki
    cd /cluster/data/danRer4/bed/affyZebrafish
    ls -1 /iscratch/i/affy/Zebrafish_consensus.fa > affy.lst
    ls -1 /iscratch/i/danRer4/trfFa/chr[0-9M]*.fa > genome.lst
    # for output:
    mkdir -p psl
    echo '#LOOP\n/cluster/bin/i386/blat -fine -minIdentity=90 -ooc=/iscratch/i/danRer4/danRer4_11.ooc $(path1) $(path2) {check out line+ psl/$(root1)_$(root2).psl}\n#ENDLOOP' > template.sub

    gensub2 genome.lst affy.lst template.sub para.spec
    para create para.spec
    para try, check, push, check .... etc.
# para time
# CPU time in finished jobs:      13226s     220.44m     3.67h    0.15d  0.000 y
# IO & Wait Time:                   647s      10.78m     0.18h    0.01d  0.000 y
# Time in running jobs:             303s       5.05m     0.08h    0.00d  0.000 y
# Average job time:                  59s       0.99m     0.02h    0.00d
# Longest running job:               62s       1.03m     0.02h    0.00d
# Longest finished job:             101s       1.68m     0.03h    0.00d
# Submission to last job:          1368s      22.80m     0.38h    0.02d
    # do pslSort and liftUp
    ssh kkstore01
    cd /cluster/data/danRer4/bed/affyZebrafish
    # Do sort, best in genome filter, and convert to chromosome coordinates
    # to create affyZebrafish.psl
    pslSort dirs raw.psl tmp psl
    # only use alignments that have at least 95% identity in aligned region.
    # try minCover as now there is less sequence in chrUn and chrNA
    # so less likely that genes are split up.
    grep '>' /cluster/bluearc/affy/Zebrafish_consensus.fa | wc -l
    # 15502
    pslReps -minAli=0.95 -nearTop=0.005 raw.psl contig.psl /dev/null
    # see how many sequences are aligned:
    awk '{print $10;}' contig.psl > contigAligned
    tail +6 contigAligned | sort | uniq -c | sort -nr > contigAligned.count
    wc -l contigAligned.count 
    # 14819 contigAligned.count 
    tail +6 contig.psl | wc -l
    # 21486
    # 96% of sequences are aligned. The sequence with the most alignments 
    # aligns 177 times, then the next is 105, then 86, 85, 69, 69, 54, 54 etc.
    # for danRer3, 14335 were aligned (92% aligned). The sequence with 
    # the most alignments aligned 96 times, then 31, 27, 22, 20, 19 times. 
    # also 854 sequences aligned for danRer4 that did not align for danRer3.
    # 370 were aligned in danRer3 but not for danRer4.
    pslReps -minCover=0.30 -minAli=0.95 -nearTop=0.005 \
            raw.psl contig2.psl /dev/null
    # see how many sequences are aligned:
    awk '{print $10;}' contig2.psl > contig2Aligned
    tail +6 contig2Aligned | sort | uniq -c | sort -nr > contig2Aligned.count
    wc -l contig2Aligned.count 
    # 14528 contig2Aligned.count
    tail +6 contig2.psl | wc -l
    # 18744
    # danRer3 has 21196 total alignments and 14335 sequences aligned. 
    # 94% of sequences are aligned.
    # 785 sequences were aligned for danRer4 using minCover but not for 
    # danRer3 after using pslReps. 592 sequences were aligned for danRer3 
    # but not for danRer4 using minCover after using pslReps.
    # the sequence with the most alignments aligns 105 times, then 85, 69,
    # 54, 50, 47, 44, 37, 26, 31, 29:
# No. of alignments Sequence Name
# 105 Zebrafish:Dr.15955.1.A1_at
# 85 Zebrafish:Dr.20178.1.A1_at
# 69 Zebrafish:Dr.885.1.S1_at
# 54 Zebrafish:Dr.15958.1.S1_at
# 50 Zebrafish:Dr.25427.1.A1_at
# 47 Zebrafish:Dr.16470.1.A1_at
# 44 Zebrafish:Dr.490.1.S1_at
# 37 Zebrafish:Dr.7806.1.A1_at
# 36 Zebrafish:Dr.19.1.A1_at
# 31 Zebrafish:Dr.2825.1.A1_at
# 29 Zebrafish:Dr.19556.1.A1_at
    # aligning with the -mask=lower option doesn't make a difference to the
    # number of alignments and sequences aligned.  
    # there are 291 extra sequences that align when minCover option is
    # not used. Only 7 of these have 22 or more alignments. 
# 86 Zebrafish:Dr.24316.1.S1_at
# 69 Zebrafish:Dr.14452.1.A1_at
# 39 Zebrafish:Dr.12372.1.S1_at
# 26 Zebrafish:Dr.18296.2.S1_a_at
# 23 Zebrafish:Dr.7519.1.A1_at
# 22 Zebrafish:Dr.8680.1.S1_at
# 22 Zebrafish:Dr.22175.1.S1_at
    # clean up 
    rm contig* 
    # use pslReps without the minCover option as it does allow quite a lot
    # more alignments and the number of total alignments/number of sequences
    # aligned is still close to that for danRer3. Using nearTop=0.001 does
    # decrease the number of alignments but also means that some good 
    # alignments are lost.  
    pslReps -minAli=0.95 -nearTop=0.005 raw.psl contig.psl /dev/null
    liftUp affyZebrafish.psl ../../jkStuff/liftAll.lft warn contig.psl
    # shorten names in psl file:
    sed -e 's/Zebrafish://' affyZebrafish.psl > affyZebrafish.psl.tmp
    mv affyZebrafish.psl.tmp affyZebrafish.psl
    pslCheck affyZebrafish.psl
    # co-ordinates are ok. psl is good.
    # load track into database
    ssh hgwdev
    cd /cluster/data/danRer4/bed/affyZebrafish
    hgLoadPsl danRer4 affyZebrafish.psl
    # Add consensus sequences for Zebrafish chip
    # Copy sequences to gbdb if they are not there already
    mkdir -p /gbdb/hgFixed/affyProbes
    ln -s \
       /projects/compbio/data/microarray/affyZebrafish/Zebrafish_consensus.fa \
      /gbdb/hgFixed/affyProbes

    hgLoadSeq -abbr=Zebrafish: danRer4 \
              /gbdb/hgFixed/affyProbes/Zebrafish_consensus.fa
    # Clean up
    rm batch.bak contig.psl raw.psl
# trackDb.ra entry and html are already there in trackDb/zebrafish/
    
###########################################################################
# CREATE ZEBRAFISH AND OTHER SPECIES LINEAGE-SPECIFIC REPEATS DIRECTORY AND 
# ADD CHROM SIZES FOR BLASTZ CLUSTER RUNS (DONE, 2006-04-24, hartera)
    # There are no lineage-specific repeats for zebrafish and other species
    # so use all repeats.
    ssh pk
    mkdir -p /san/sanvol1/scratch/danRer4/linSpecRep.notInOthers
    foreach f (/cluster/data/danRer4/*/chr*.fa.out)
     cp -p $f \
        /san/sanvol1/scratch/danRer4/linSpecRep.notInOthers/$f:t:r:r.out.spec
    end
    cp -p /cluster/data/danRer4/chrom.sizes \
          /san/sanvol1/scratch/danRer4/


###########################################################################
# BLASTZ/CHAIN/NET PREP (DONE 4/25/06 angie)
    ssh kkstore04
    cd /cluster/data/danRer4
    cp -p danRer4.2bit /san/sanvol1/scratch/danRer4/

    # Create a 2bit file for danRer4 with all chroms (1-25 and M) and the
    # scaffolds for NA and Un:
    awk '$1 == $6 {print $1;}' Zv6.scaffolds.agp \
    | faSomeRecords Zv6_scaffolds.fa stdin stdout \
    | faToTwoBit [1-9]/chr*.fa [12][0-9]/chr*.fa M/chrM.fa stdin \
       /san/sanvol1/scratch/danRer4/danRer4ChrUnNAScafs.2bit
    twoBitInfo /san/sanvol1/scratch/danRer4/danRer4ChrUnNAScafs.2bit \
      /san/sanvol1/scratch/danRer4/chromsUnNAScafs.sizes

    # Make a lift file for scaffolds --> {chrUn, chrNA}:
    mkdir /cluster/data/danRer4/liftSupertoChrom
    cd /cluster/data/danRer4/liftSupertoChrom
    /cluster/bin/scripts/agpToLift \
      < ../NA_random/agps/chrNA_random.scaffolds.agp \
      > chrNA_random.lft
    /cluster/bin/scripts/agpToLift \
      < ../Un_random/agps/chrUn_random.scaffolds.agp \
      > chrUn_random.lft
    cat chr*.lft > liftNAandUnScaffoldsToChrom.lft
    cp -p liftNAandUnScaffoldsToChrom.lft /san/sanvol1/scratch/danRer4/

    # Distribute on /iscratch/i too (danRer4.2bit is already there):
    ssh kkr1u00
    cd /iscratch/i/danRer4
    cp -p /san/sanvol1/scratch/danRer4/danRer4ChrUnNAScafs.2bit .
    twoBitInfo danRer4ChrUnNAScafs.2bit chromsUnNAScafs.sizes
    cp -p \
      /cluster/data/danRer4/liftSupertoChrom/liftNAandUnScaffoldsToChrom.lft .
    iSync


###########################################################################
# BLASTZ/CHAIN/NET XENTRO2 (DONE 4/26/06 angie)
    ssh kkstore04
    mkdir /cluster/data/danRer4/bed/blastz.xenTro2.2006-04-25
    cd /cluster/data/danRer4/bed/blastz.xenTro2.2006-04-25
    cat << '_EOF_' > DEF
# zebrafish vs. frog
BLASTZ=/cluster/bin/penn/i386/blastz

# Use same params as used for danRer1-xenTro1 (see makeXenTro1.doc)
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q

# TARGET: Zebrafish danRer4
SEQ1_DIR=/iscratch/i/danRer4/danRer4.2bit
SEQ1_CTGDIR=/iscratch/i/danRer4/danRer4ChrUnNAScafs.2bit
SEQ1_LIFT=/iscratch/i/danRer4/liftNAandUnScaffoldsToChrom.lft
SEQ1_LEN=/cluster/data/danRer4/chrom.sizes
SEQ1_CTGLEN=/iscratch/i/danRer4/chromsUnNAScafs.sizes
SEQ1_CHUNK=50000000
SEQ1_LAP=10000
SEQ1_LIMIT=100

# QUERY: Frog xenTro2 - single chunk big enough to run two of the
#               largest scaffolds in one job
SEQ2_DIR=/scratch/hg/xenTro2/xenTro2.2bit
SEQ2_LEN=/cluster/bluearc/xenTro2/chrom.sizes
SEQ2_CHUNK=20000000
SEQ2_LAP=0
SEQ2_LIMIT=100

BASE=/cluster/data/danRer4/bed/blastz.xenTro2.2006-04-25
'_EOF_'
    # << emacs
    # kkstore04 can't see /iscratch so use an iServer as fileServer:
    doBlastzChainNet.pl -blastzOutRoot=/cluster/bluearc/danRer4XenTro2 \
      -bigClusterHub=kk -fileServer=kkr8u00 -workhorse=kkr8u00 \
      -chainMinScore=5000 -chainLinearGap=loose DEF \
      >& do.log & tail -f do.log
    ln -s blastz.xenTro2.2006-04-25 /cluster/data/danRer4/bed/blastz.xenTro2

###########################################################################
## SWAP MM8 blastz result (DONE - 2006-04-28 - Hiram)
    ssh pk
    cd /cluster/data/mm8/bed/blastzDanRer4.2006-04-26

    time /cluster/bin/scripts/doBlastzChainNet.pl -verbose=2 \
	-bigClusterHub=pk -chainMinScore=5000 -chainLinearGap=loose \
	-swap `pwd`/DEF > swap.out 2>&1 &

    ssh hgwdev
    cd /cluster/data/mm8/bed/blastzDanRer4.2006-04-26
    time nice -n +19 featureBits danRer4 chainMm8Link \
	> fb.danRer4.chainDanRer4Link 2>&1 &
    cat fb.danRer4.chainDanRer4Link
    #	58145856 bases of 1626093931 (3.576%) in intersection
