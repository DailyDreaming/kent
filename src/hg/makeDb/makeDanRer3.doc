#!/bin/csh -f # set emacs mode
exit; # don't actually run this like a script :)
                                                                                
# Danio Rerio (zebrafish) from Sanger, version Zv5 (released 5/20/05)
#  Project website:
#    http://www.sanger.ac.uk/Projects/D_rerio/
#  Assembly notes:
#    http://www.sanger.ac.uk/Projects/D_rerio/Zv5_assembly_information.shtml

# DOWNLOAD SEQUENCE (DONE, 2005-06-06, hartera)
# MOVE DANRER3 DIRECTORY AND CONTENTS TO STORE11 AS STORE3 IS FULL
# (DONE, 2005-07-22, hartera)
     ssh kkstore01
     mkdir /cluster/store9/danRer3
     ln -s /cluster/store9/danRer3 /cluster/data
     cd /cluster/data/danRer3
     wget --timestamp \
       ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv5release/README
     wget --timestamp \
       ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv5release/Zv5.stats
     wget --timestamp \
       ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv5release/Zv5.chunks.agp
     wget --timestamp \
       ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv5release/Zv5.scaffolds.agp     wget --timestamp \
       ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv5release/Zv5.fa
     # 2005-07-22 MOVE danRer3 
     # store9 is 100% full, move danRer3 to store11 which is 10% full
     ssh kkstore02
     cd /cluster/store9
     nohup nice mv danRer3 /cluster/store11 &
     # make link to /cluster/data/danRer3
     ln -s /cluster/store11/danRer3 /cluster/data
     
# DOWNLOAD MITOCHONDRION GENOME SEQUENCE (DONE, 2005-06-13, hartera)
     ssh kkstore01
     mkdir -p /cluster/data/danRer3/M
     cd /cluster/data/danRer3/M
     # go to http://www.ncbi.nih.gov/ and search Nucleotide for
     # "Danio mitochondrion genome".  That shows the gi number:
     # 8576324 for the accession, AC024175
 # Use that number in the entrez linking interface to get fasta:
     wget -O chrM.fa \
      'http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Text&db=Nucleotide&uid=8576324&dopt=FASTA'
     # Edit chrM.fa: make sure the header line says it is the
     # Danio Rerio mitochondrion complete genome, and then replace the
     # header line with just ">chrM".
     perl -pi.bak -e 's/>.+/>chrM/' chrM.fa
     rm *.bak
     # Make a "pseudo-contig" for processing chrM too:
     mkdir ./chrM_1
     sed -e 's/chrM/chrM_1/' ./chrM.fa > ./chrM_1/chrM_1.fa
     mkdir ./lift
     echo "chrM_1/chrM_1.fa.out" > ./lift/oOut.lst
     echo "chrM_1" > ./lift/ordered.lst
     echo "0     M/chrM_1        16596   chrM    16596" > ./lift/ordered.lft
     # make sure this is tab delimited
# create a .agp file for chrM as hgGoldGapGl and other
# programs require a .agp file so create chrM.agp
    cat << '_EOF_' > ./chrM.agp
chrM       1       16596   1       F       AC024175.3      1       16596   +
'_EOF_'
     # Create a chrM.chunks.agp
     mkdir -p /cluster/data/danRer3/M/agps
     cd /cluster/data/danRer3/M/agps
     awk 'BEGIN {OFS="\t"} \
        {print $1, $2, $3, $4, $5, $6, $7, $8, $1, $7, $8}' ../chrM.agp \
         > chrM.chunks.agp
     # make sure that all these above files are tab delimited

# Create list of chromosomes (DONE, 2005-06-08, hartera)
     ssh kkstore01
     cd /cluster/data/danRer3
     awk '{if ($1 !~ /Zv5/) print $1;}' Zv5.scaffolds.agp \
         | sort -n | uniq > chrom.lst
     cp chrom.lst chrom1to25.lst
     # add chrM
     echo "M" >> chrom.lst
     # add chrUn
     echo "Un" >> chrom.lst
     # add NA
     echo "NA" >> chrom.lst

# MAKE JKSTUFF AND BED DIRECTORIES (DONE, 2005-06-09, hartera)
    ssh kkstore01
    cd /cluster/data/danRer3
    # This used to hold scripts -- better to keep them inline here 
    # Now it should just hold lift file(s) and
    # temporary scripts made by copy-paste from this file.
    mkdir /cluster/data/danRer3/jkStuff
    # This is where most tracks will be built:
    mkdir /cluster/data/danRer3/bed

# GET ADDITIONAL ZEBRAFISH REPBASE LIBRARY FOR REPEATMASKER 
# (DONE, 2005-05-10, hartera)
# Go to http://www.girinst.org/server/RepBase/RepBase10.04.fasta
# and download zebunc.ref containing unclassified zebrafish repeats.
# Need username and password. Copy to /cluster/bluearc/RepeatMasker/Libraries/
     ssh hgwdev
     cd /cluster/bluearc/RepeatMasker/Libraries/
     perl -pi.bak -e 's/>(Dr[0-9]+)/>$1#Unknown \@danio [S:]/' zebunc.ref
     # add to RepeatMasker library
     cat zebunc.ref >> RepeatMasker.lib

# CHECK AGP FILES AND FASTA SIZE CONSISTENCY (DONE, 2005-06-10, hartera)

     # The script, createAgpWithGaps.pl (see next section for creating
     # agps and FASTAs for chrNA and chrUn), was used to create a scaffolds 
     # agp file for chrUn to test the program. The agp output was compared to 
     # that from scaffoldFaToAgp and difference was found in the agp file
     # output for scaffoldFaToAgp which used 990568 as the end co-ordinate for
     # Zv5_scaffold1475 instead of 976101 as in the output from the script. So 
     # the co-ordinate numbering is different from there on. The program, 
     # scaffoldFaToAgp is creating the agp file from the FASTA file
     # so perhaps the sequence is a different size than stated in the agp file.
     # Get sequence and find the size:
     ssh kkstore01
     mkdir test
     cd test
     faOneRecord ../Zv5.fa Zv5_scaffold1475 > Zv5_scaffold1475.fa
     faSize Zv5_scaffold1475.fa
     # 990568 bases
     rm Zv5_scaffold1475.fa 
     # reported this inconsistency to Mario Caccamo at Sanger
     # mc2@sanger.ac.uk (2005-06-09) and new scaffolds and chunks agp files
     # were sent on 2005-06-10. There was a chunk (contig) missing from the 
     # chunks agp file and the scaffold therefore had the wrong end 
     # co-ordinate in the agp files.
     # check all sizes of scaffold sequences against those in the agp files
     ssh kkr1u00
     cd /cluster/data/danRer3 
     mkdir -p /iscratch/i/danRer3/scaffolds
     cp Zv5.fa /iscratch/i/danRer3/scaffolds/
     iSync
     
     ssh kk
     mkdir -p /cluster/data/danRer3/scaffolds/run
     cd /cluster/data/danRer3/scaffolds/run
     grep '>' ../Zv5.fa | sed -e 's/>//' > Zv5.scaffolds.lst
cat << '_EOF_' > getSizes.csh
     #!/bin/csh -fe
     set dir=/cluster/bluearc/danRer3/scaffolds
     faOneRecord /iscratch/i/danRer3/scaffolds/Zv5.fa $1 > $dir/$1.fa
     echo $1 >> $dir/$1.size
     faSize $dir/$1.fa >> $dir/$1.size
     rm $dir/$1.fa
'_EOF_'
     # << this line makes emacs coloring happy
     chmod +x getSizes.csh
cat << '_EOF_' > gsub
#LOOP
getSizes.csh $(path1)
#ENDLOOP
'_EOF_'
     # << this line makes emacs coloring happy 
     gensub2 Zv5.scaffolds.lst single gsub jobList
     para create jobList 
     para try,check,push,check etc...
    
     ssh kkstore01
     cd /cluster/bluearc/danRer3/scaffolds
     foreach f (*.size)
        cat $f >> Zv5.scaffolds.sizes
     end	  
     cd /cluster/data/danRer3/scaffolds
     mv /cluster/bluearc/danRer3/scaffolds/Zv5.scaffolds.sizes .
     # Check that these sizes correspond to the sizes in the scaffolds agp file
     # use script compareSizes.pl
     cat << '_EOF_' > compareSizes.pl
#!/usr/bin/perl -w
use strict;

my ($file, $agp);

$file = $ARGV[0];
$agp = $ARGV[1];

open(FILE, $file) || die "Can not open $file: $!\n";
open(AGP, $agp) || die "Can not open $agp: $!\n";
open(OUT, ">log.txt") || die "Can not create log.txt: $!\n";

my ($l, $name, $size, %scafsHash);
while (<FILE>)
{
$l = $_;
if ($l =~ /^(Zv5_(scaffold|NA)[0-9]+)/)
   {
   $name = $1;
   }
elsif ($l =~ /^([0-9]+)\sbases/)
   {
   $size = $1;  
   $scafsHash{$name} = $size;
   }
}
close FILE;

while (<AGP>)
{
my ($line, @fi, $scaf, $end);
$line = $_;

@fi = split(/\t/, $line);
$scaf = $fi[5];
$end = $fi[7];

if (exists($scafsHash{$scaf}))
   {
   if ($scafsHash{$scaf} eq $end)
      {
      print OUT "$scaf - ok\n";
      }
   else
      {
      print OUT "$scaf - different size to sequence\n";
      }
   }
else
   {
   print OUT "$scaf - does not exist in list of sizes\n";
   }
}
close AGP;
close OUT;
'_EOF_'
   # << happy emacs
   chmod +x compareSizes.pl
   perl compareSizes.pl Zv5.scaffolds.sizes ../Zv5.scaffolds.list
   # the only lines where no ID was found in the list of scaffolds with sizes
   # were those lines for gaps.
   grep "different" Zv5_scaffold1475
   # Zv5_scaffold1475 - different size to sequence
   # so only this scaffold is a different size in the agp to the sequence
   # need to check that sizes are consistent between agp files 
   # check also new agp file for scaffolds - newAgps/Zv5.scaffolds.agp
   perl compareSizes.pl Zv5.scaffolds.sizes ../newAgps/Zv5.scaffolds.agp
   # these are all consistent with the sequence sizes
   cd /cluster/data/danRer3/newAgps/
   # print out scaffold names where the co-ordinates are not consistent
   # with sizes given
   awk '{if ($6 ~ /^Zv5/ && (($3-$2+1) != $8)) print $6;}' Zv5.scaffolds.agp \
       > Zv5.scaffolds.coordCheck 
   # this file is empty so they are ok. do the same for the chunks.agp file
   awk '{if ($6 ~ /^Zv5/ && (($3-$2+1) != $8)) print $6;}' Zv5.chunks.agp \ 
       > Zv5.chunks.coordCheck
   # also empty so ok. check that the difference between $7 and $8 is the
   # same as the difference between $11 and $12 fields
   # 8th and 12th fields should be the same
   awk '{if ($6 != 5000 && (($8 - $7) != ($12 - $11))) print $6;}' \
       Zv5.chunks.agp > Zv5.chunks.coordCheck2
   # these are all ok
   rm Zv5.*.coord*
cat << '_EOF_' > checkSizesInAgps.pl
#!/usr/bin/perl -w
use strict;

my ($ch, $sc, %scafsHash);
$sc = $ARGV[0]; # scaffolds agp
$ch = $ARGV[1]; # chunks or contigs agp

open(SCAFS, $sc) || die "Can not open $sc: $!\n";
open(CHUNKS, $ch) || die "Can not open $ch: $!\n";

while (<SCAFS>)
{
my ($l, @f, $name, $e);
$l = $_;
@f = split(/\t/, $l);
if ($f[5] =~ /^Zv5/)
   {
   $name = $f[5];
   $e = $f[2];
   $scafsHash{$name} = $e;
   }
}
close SCAFS;

my $scaf = "";
my $prev = "";
my $prevEnd = 0;

while (<CHUNKS>)
{
my ($line, @fi);
$line = $_;
@fi = split(/\t/, $line);

if ($fi[5] ne "5000")
   {
   $scaf = $fi[9];
   if (($scaf ne $prev) && ($prev ne ""))
      {
      checkCoords($prev, $prevEnd);
      }
$prev = $scaf;
$prevEnd = $fi[2];
   }
}
# check last entry in file
checkCoords($prev, $prevEnd);
close CHUNKS;

sub checkCoords {
my ($name, $end) = @_;
if (exists($scafsHash{$prev}))
   {
   if ($scafsHash{$prev} != $prevEnd)
      {
      my $ed = $scafsHash{$prev};
      print "Scaffold $prev is not consistent between agps\n";
      }
   else
      {
      my $ed = $scafsHash{$prev};
      print "Scaffold $prev - ok\n";
      }
   }
}
'_EOF_'
   # << happy emacs
   chmod +x checkSizesInAgps.pl
   checkSizesInAgps.pl Zv5.scaffolds.agp Zv5.chunks.agp \
         > Zv5.scafsvschunks
   grep "not consistent" Zv5.scafsvschunks
   # no lines were inconsistency was reported
   wc -l Zv5.scafsvschunks
   # 16214 Zv5.scafsvschunks
   grep "Zv5" Zv5.scaffolds.agp | wc -l
   # 16214
   # so all the scaffolds were checked and were ok.
   cd /cluster/data/danRer3
   mv ./newAgps/Zv5.scaffolds.agp .
   mv ./newAgps/Zv5.chunks.agp
   mv ./scaffolds/compareSizes.pl ./jkStuff/
   mv ./newAgps/checkSizesInAgps.pl ./jkStuff/
   rm -r newAgps

# SPLIT AGP FILES BY CHROMOSOME (DONE, 2005-06-13, hartera)
# FASTA WAS CREATED USING SCAFFOLDS AGP
     ssh kkstore01
     cd /cluster/data/danRer3
     # There are 2 .agp files: one for scaffolds (supercontigs on danRer1) and
     # then one for chunks (contigs on danRer1) showing how they map on to
     # scaffolds.

     # get list of scaffolds from FASTA file and check these are in agp
     grep '>' Zv5.fa | sed -e 's/>//' | sort | uniq > Zv5FaScafs.lst
     # get list of scaffolds from agp - do not print from gap lines
     awk '{if ($7 !~ /contig/) print $6;}' Zv5.scaffolds.agp \
        | sort | uniq > Zv5AgpScafs.lst
     diff Zv5FaScafs.lst Zv5AgpScafs.lst
     # no difference so all scaffolds are in the FASTA file
     # add "chr" prefix for the agp files
     perl -pi -e 's/^([0-9]+)/chr$1/' ./*.agp
     # for chromosomes:
     foreach c (`cat chrom1to25.lst`)
       echo "Processing $c ..."
       mkdir $c
       perl -we "while(<>){if (/^chr$c\t/) {print;}}" \
         ./Zv5.chunks.agp \
         > $c/chr$c.chunks.agp
       perl -we "while(<>){if (/^chr$c\t/) {print;}}" \
         ./Zv5.scaffolds.agp \
         > $c/chr$c.scaffolds.agp
     end

# CREATE AGP FILES FOR chrNA AND chrUn (DONE, 2005-06-13, hartera)
     ssh kkstore01
     # chrNA consists of WGS contigs that could not be related to any 
     # FPC contig and the scaffolds and contigs are named Zv5_NAN in the 
     # first field of the agp files
     cd /cluster/data/danRer3
     mkdir ./NA
     awk '{if ($1 ~ /Zv5_NA/) print;}' Zv5.chunks.agp \
         > ./NA/NA.chunks.agp
     awk '{if ($1 ~ /Zv5_NA/) print;}' Zv5.scaffolds.agp \
         > ./NA/NA.scaffolds.agp
     # change the first field to "chrUn" then can use agpToFa to process
     perl -pi.bak -e 's/Zv5_NA[0-9]+/chrNA/' ./NA/*.agp
     # check files and remove backup files
     rm ./NA/*.bak
     # then process chrUn.
     # Re-make chrUn with new agp files - this is made from scaffolds and  
     # contigs where the name is Zv5_scaffoldN in the first field of the 
     # agp files. These scaffolds and contigs are unmapped to chromosomes
     # in the agp file. chrUn is made up of WGS scaffolds that mapped to 
     # FPC contigs, but the chromosome is unknown.
     rm -r Un
     mkdir ./Un
     awk '{if ($1 ~ /Zv5_scaffold/) print;}' Zv5.chunks.agp \
         > ./Un/Un.chunks.agp
     awk '{if ($1 ~ /Zv5_scaffold/) print;}' Zv5.scaffolds.agp \
         > ./Un/Un.scaffolds.agp
     # change the first field to "chrUn" then can use agpToFa to process
     perl -pi.bak -e 's/Zv5_scaffold[0-9]+/chrUn/' ./Un/*.agp
     # check files and remove backup files
     rm ./Un/*.bak

     # get FASTA file of sequences for NA and Un and create agp with 
     # Ns between scaffolds
     # from scaffolds agp, get name of scaffolds to get from FASTA file for NA
     foreach c (NA Un)
       awk '{print $6;}' $c/$c.scaffolds.agp > $c/chr$c.scaffolds.lst
       $HOME/bin/i386/faSomeRecords /cluster/data/danRer3/Zv5.fa \
          $c/chr$c.scaffolds.lst $c/chr$c.fa
     end
     # check that all scaffolds in list are in FASTA file for NA and Un - ok
     # edit scaffoldFaToAgp.c so that it creates agp with 500Ns between 
     # scaffolds as contig gaps for chrNA and compile. chrNA is already large
     # so the number of Ns are reduced to reduce the size.
     foreach c (NA Un)
        $HOME/bin/i386/scaffoldFaToAgp $c/chr$c.fa
        mv $c/chr$c.fa $c/chr$c.scaffolds.fa
     end
     # change chrUn to chrNA for NA and D to W for NA and Un
     sed -e 's/chrUn/chrNA/' ./NA/chrNA.agp | sed -e 's/D/W/' \
         > ./NA/chrNA.scaffolds.agp
     sed -e 's/D/W/' ./Un/chrUn.agp > ./Un/chrUn.scaffolds.agp
     # edit ./NA/chrNA.scaffolds.agp and ./Un/chrUn.scaffolds.agp and 
     # remove last line as this just adds an extra 500 Ns at the 
     # end of the sequence.
     rm ./NA/chrNA.agp ./Un/chrUn.agp

cat << '_EOF_' > /cluster/data/danRer3/jkStuff/createAgpWithGaps.pl
#!/usr/bin/perl
use strict;

# This script takes a chunks agp and inserts Ns between scaffolds for 
# the chunks (contigs) agp file. Could also insert Ns between scaffolds
# for scaffolds agp.

my ($chrom, $numN, $name, $prev, $st, $end, $prevEnd, $id);
my $chrom = $ARGV[0]; # chromosome name
my $numN = $ARGV[1];  # number of Ns to be inserted 
my $type = $ARGV[2]; # contigs or scaffolds

$prev = "";
$st = 1;
$prevEnd = 0;
$id = 0;

while (<STDIN>)
{
my $l = $_;
my @f = split(/\t/, $l);

if ($type eq "contigs")
   {
   $name = $f[9];
   }
else 
   {
   $name = $f[5]
   }

my $currSt = $f[1];
my $currEnd = $f[2];
my $size = $currEnd - $currSt;

$id++;
$st = $prevEnd + 1;
$end = $st + $size;

if (($prev ne "") && ($prev ne $name))
   {
   $st = $prevEnd + 1;
   $end = ($st + $numN) - 1;
   print "$chrom\t$st\t$end\t$id\tN\t$numN\tcontig\tno\n";
   $prevEnd = $end;
   $id++;
   }

$st = $prevEnd + 1;
$end = $st + $size;
print "$chrom\t$st\t$end\t$id\t$f[4]\t$f[5]\t$f[6]\t$f[7]\t$f[8]";
if ($type eq "contigs")
   {
   print "\t$f[9]\t$f[10]\t$f[11]";
   }

$prevEnd = $end;
$prev = $name;
}
'_EOF_'
     chmod +x /cluster/data/danRer3/jkStuff/createAgpWithGaps.pl
     cd /cluster/data/danRer3
     foreach c (NA Un)
        cd $c
        perl ../jkStuff/createAgpWithGaps.pl chr${c} 500 contigs \
             < ${c}.chunks.agp > chr${c}.chunks.agp
        cd ..
     end
     # check co-ordinates
     # clean up
     foreach c (NA Un)
        rm $c/${c}.scaffolds.agp $c/${c}.chunks.agp $c/chr${c}.scaffolds.fa \
           $c/${c}.scaffolds.lst
     end
   
# BUILD CHROM-LEVEL SEQUENCE (DONE, 2005-06-13, hartera)
     ssh kkstore01
     cd /cluster/data/danRer3
     # Sequence is already in upper case so no need to change
     foreach c (`cat chrom.lst`)
       echo "Processing ${c}"
       $HOME/bin/i386/agpToFa -simpleMultiMixed $c/chr$c.scaffolds.agp chr$c \
         $c/chr$c.fa ./Zv5.fa
       echo "${c} - DONE"
     end
     # move scaffolds agp to be chrom agp and clean up
     foreach c (`cat chrom.lst`)
        cd $c
        rm *.bak
        cp chr${c}.scaffolds.agp chr${c}.agp
        mkdir -p agps
        mv chr${c}.*.agp ./agps/
        cd ..
     end

# CHECK CHROM AND VIRTUAL CHROM SEQUENCES (DONE, 2005-06-13, hartera)
     # Check that the size of each chromosome .fa file is equal to the
     # last coord of the .agp:
     ssh hgwdev
     cd /cluster/data/danRer3
     foreach c (`cat chrom.lst`)
       foreach f ( $c/chr$c.agp )
         set agpLen = `tail -1 $f | awk '{print $3;}'`
         set h = $f:r
         set g = $h:r
         echo "Getting size of $g.fa"
         set faLen = `faSize $g.fa | awk '{print $1;}'`
         if ($agpLen == $faLen) then
           echo "   OK: $f length = $g length = $faLen"
         else
           echo "ERROR:  $f length = $agpLen, but $g length = $faLen"
         endif
       end
     end
     # all are the OK so FASTA files are the expected size

# CREATING DATABASE (DONE, 2005-06-13, hartera)
    # Create the database.
    # next machine
    ssh hgwdev
    echo 'create database danRer3' | hgsql ''
    # if you need to delete that database:  !!! WILL DELETE EVERYTHING !!!
    echo 'drop database danRer3' | hgsql danRer3
    # Delete and re-create database as above (hartera, 2004-11-30)
    # Use df to make sure there is at least 10 gig free on
    df -h /var/lib/mysql
# Before loading data:
# Filesystem            Size  Used Avail Use% Mounted on
# /dev/sdc1             1.8T  927G  734G  56% /var/lib/mysql

# CREATING GRP TABLE FOR TRACK GROUPING (DONE, 2005-06-13, hartera)
    # next machine
    ssh hgwdev
    #  the following command copies all the data from the table
    #  grp in the database danRer2 to the new database danRer3
    echo "create table grp (PRIMARY KEY(NAME)) select * from danRer2.grp" \
      | hgsql danRer3
    # if you need to delete that table:   !!! WILL DELETE ALL grp data !!!
    echo 'drop table grp;' | hgsql danRer3

# BREAK UP SEQUENCE INTO 5MB CHUNKS AT CONTIGS/GAPS FOR CLUSTER RUNS
# (DONE, 2004-06-14, hartera)

     ssh kkstore01
     cd /cluster/data/danRer3
     foreach c (`cat chrom.lst`)
       foreach agp ($c/chr$c.agp)
         if (-e $agp) then
           set fa = $c/chr$c.fa
           echo splitting $agp and $fa
           cp -p $agp $agp.bak
           cp -p $fa $fa.bak
           splitFaIntoContigs $agp $fa . -nSize=5000000
         endif
       end
     end

# MAKE LIFTALL.LFT (DONE, 2005-06-14, hartera)
    ssh kkstore01
    cd /cluster/data/danRer3
    cat */lift/ordered.lft > jkStuff/liftAll.lft 

# SIMPLE REPEAT [TRF] TRACK  (DONE, 2005-06-14, hartera)
    # TRF can be run in parallel with RepeatMasker on the file server
    # since it doesn't require masked input sequence.
    # Run this on the kilokluster. Need to mask contig and chromosome 
    # sequences so run trf using contig sequences.
    # First copy over contig sequences to iscratch and then iSync to cluster.
    ssh kkr1u00
    mkdir -p /iscratch/i/danRer3/contigsNoMask
    cd /cluster/data/danRer3
    foreach d (/cluster/data/danRer3/*/chr*_?{,?})
       set ctg = $d:t
       foreach f ($d/${ctg}.fa)
          echo "Copyig $f ..."
          cp $f /iscratch/i/danRer3/contigsNoMask/
       end
    end
    # 288 sequence files
    /cluster/bin/iSync

    ssh kk
    mkdir -p /cluster/data/danRer3/bed/simpleRepeat
    cd /cluster/data/danRer3/bed/simpleRepeat
    mkdir trf
cat << '_EOF_' > runTrf
#!/bin/csh -fe
#
set path1 = $1
set inputFN = $1:t
set outpath = $2
set outputFN = $2:t
mkdir -p /tmp/$outputFN
cp $path1 /tmp/$outputFN
pushd .
cd /tmp/$outputFN
/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $inputFN /dev/null -bedAt=$outputFN -tempDir=/tmp
popd
rm -f $outpath
cp -p /tmp/$outputFN/$outputFN $outpath
rm -fr /tmp/$outputFN/*
rmdir --ignore-fail-on-non-empty /tmp/$outputFN
'_EOF_'
    # << keep emacs coloring happy
    chmod +x runTrf
                                                                                
cat << '_EOF_' > gsub
#LOOP
./runTrf {check in line+ $(path1)}  {check out line trf/$(root1).bed}
#ENDLOOP
'_EOF_'
    # << keep emacs coloring happy
                                                                                
    ls -1S /iscratch/i/danRer3/contigsNoMask/chr*.fa > genome.lst
    gensub2 genome.lst single gsub jobList
    # 288 jobs
    para create jobList
    para try, check, push, check etc...
    para time
# Completed: 288 of 288 jobs
# CPU time in finished jobs:      70742s    1179.03m    19.65h    0.82d  0.002 y
# IO & Wait Time:                  1263s      21.05m     0.35h    0.01d  0.000 y
# Average job time:                 250s       4.17m     0.07h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            6722s     112.03m     1.87h    0.08d
# Submission to last job:         10037s     167.28m     2.79h    0.12d

    # lift up to chrom level
    liftUp simpleRepeat.bed /cluster/data/danRer3/jkStuff/liftAll.lft warn \
           trf/*.bed

    # Load into the database
    ssh hgwdev
    cd /cluster/data/danRer3/bed/simpleRepeat
    hgLoadBed danRer3 simpleRepeat simpleRepeat.bed \
      -sqlTable=$HOME/kent/src/hg/lib/simpleRepeat.sql
    # Loaded 757119 elements of size 16

# PROCESS SIMPLE REPEATS INTO MASK (DONE, 2005-06-14, hartera)
    # After the simpleRepeats track has been built, make a filtered version
    # of the trf output: keep trf's with period <= 12:
    ssh kkstore01
    cd /cluster/data/danRer3/bed/simpleRepeat
    mkdir -p trfMask
    foreach f (trf/chr*.bed)
      awk '{if ($5 <= 12) print;}' $f > trfMask/$f:t
    end

    # Lift up filtered trf output to chrom coords as well:
    cd /cluster/data/danRer3
    mkdir bed/simpleRepeat/trfMaskChrom
    foreach c (`cat chrom.lst`)
      if (-e $c/lift/ordered.lst) then
        perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
          $c/lift/ordered.lst > $c/lift/oTrf.lst
        liftUp bed/simpleRepeat/trfMaskChrom/chr$c.bed \
          jkStuff/liftAll.lft warn `cat $c/lift/oTrf.lst`
      endif
      if (-e $c/lift/random.lst) then
        perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
           $c/lift/random.lst > $c/lift/rTrf.lst
        liftUp bed/simpleRepeat/trfMaskChrom/chr${c}_random.bed \
          jkStuff/liftAll.lft warn `cat $c/lift/rTrf.lst`
      endif
    end

# REPEAT MASKING - Run RepeatMasker on chroms (DONE, 2005-06-15, hartera)
    # When a new library is added for this version of repeatMasker, need to 
    # check in /cluster/bluearc/RepeatMasker/Libraries for a directory made 
    # up of a date e.g. 20050112 here and inside this are species directories
    # for which RepeatMasker has already been run. In this directory it creates
    # a specieslib of the danio repeats. If this exists, this is used for the
    # RepeatMasker run for that species so if new repeats are added to the
    # library, they will not get used unless this is deleted a new specieslib
    # is created using the new library on the first run for danio.
    ssh kkstore01
    rm -r /cluster/bluearc/RepeatMasker/Libraries/20050112/danio/
    cd /cluster/data/danRer3
    #- Split contigs into 500kb chunks, at gaps if possible:
    foreach c (`cat chrom.lst`)
      foreach d ($c/chr${c}*_?{,?})
        cd $d
        echo "splitting $d"
        set contig = $d:t
        ~/bin/i386/faSplit gap $contig.fa 500000 ${contig}_ -lift=$contig.lft \
            -minGapSize=100
        cd ../..
      end
    end

    # For RepeatMasking, use RepeatMasker "open-3.0" with repeat library
    # version RepBase Update 9.11, RM database version 20050112 with the 
    # addition of the zebrafish unclassified repeats (zebunc.ref) - see above
    # section on getting this additional zebrafish RepeatMasker library. 
    #- Make the run directory and job list:
    cd /cluster/data/danRer3
cat << '_EOF_' > jkStuff/RMZebrafish
#!/bin/csh -fe
                                                                                
cd $1
pushd .
/bin/mkdir -p /tmp/danRer3/$2
/bin/cp $2 /tmp/danRer3/$2/
cd /tmp/danRer3/$2
/cluster/bluearc/RepeatMasker/RepeatMasker -ali -s -species danio $2
popd
/bin/cp /tmp/danRer3/$2/$2.out ./
if (-e /tmp/danRer3/$2/$2.align) /bin/cp /tmp/danRer3/$2/$2.align ./
if (-e /tmp/danRer3/$2/$2.tbl) /bin/cp /tmp/danRer3/$2/$2.tbl ./
if (-e /tmp/danRer3/$2/$2.cat) /bin/cp /tmp/danRer3/$2/$2.cat ./
/bin/rm -fr /tmp/danRer3/$2/*
/bin/rmdir --ignore-fail-on-non-empty /tmp/danRer3/$2
/bin/rmdir --ignore-fail-on-non-empty /tmp/danRer3
'_EOF_'
    chmod +x jkStuff/RMZebrafish
    mkdir -p RMRun
    cp /dev/null RMRun/RMJobs
    foreach c (`cat chrom.lst`)
      foreach d ($c/chr${c}_?{,?})
          set ctg = $d:t
          foreach f ( $d/${ctg}_?{,?}.fa )
            set f = $f:t
            echo /cluster/data/danRer3/jkStuff/RMZebrafish \
                 /cluster/data/danRer3/$d $f \
               '{'check out line+ /cluster/data/danRer3/$d/$f.out'}' \
              >> RMRun/RMJobs
          end
      end
    end
    # Do the run
    ssh kk 
    cd /cluster/data/danRer3/RMRun
    para create RMJobs
    para try, para check, para check, para push, para check,...
    para time
# Completed: 4069 of 4069 jobs
# CPU time in finished jobs:   13726314s  228771.90m  3812.87h  158.87d  0.435 y
# IO & Wait Time:                 45762s     762.70m    12.71h    0.53d  0.001 y
# Average job time:                3385s      56.41m     0.94h    0.04d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            4549s      75.82m     1.26h    0.05d
# Submission to last job:         56947s     949.12m    15.82h    0.66d
# This is slow. It should have taken about 5 hours.

    #- Lift up the 500KB chunk .out's to 5MB ("pseudo-contig") level
    ssh kkstore01
    cd /cluster/data/danRer3
    foreach d (*/chr*_?{,?})
      set contig = $d:t
      echo $contig
      liftUp $d/$contig.fa.out $d/$contig.lft warn $d/${contig}_*.fa.out \
        > /dev/null
    end
                                                                                
    #- Lift pseudo-contigs to chromosome level
    foreach c (`cat chrom.lst`)
      echo lifting $c
      cd $c
      if (-e lift/ordered.lft && ! -z lift/ordered.lft) then
        liftUp chr$c.fa.out lift/ordered.lft warn `cat lift/oOut.lst` \
        > /dev/null
      endif
      cd ..
    end

    #- Load the .out files into the database with:
    ssh hgwdev
    cd /cluster/data/danRer3
    hgLoadOut danRer3 */chr*.fa.out -verbose=2
# bad rep range [689, 602] line 105524 of 16/chr16.fa.out 
# bad rep range [147, 146] line 124027 of 16/chr16.fa.out
# bad rep range [280, 258] line 754 of 17/chr17.fa.out 
# bad rep range [280, 258] line 76417 of 17/chr17.fa.out
# bad rep range [314, 311] line 99427 of 19/chr19.fa.out
# bad rep range [367, 366] line 88398 of 23/chr23.fa.out 
# bad rep range [41, 40] line 51509 of 25/chr25.fa.out
# bad rep range [1133, 1132] line 62610 of 9/chr9.fa.out
# bad rep range [6133, 6132] line 122359 of NA/chrNA.fa.out 
# bad rep range [6133, 6132] line 160183 of NA/chrNA.fa.out 
# bad rep range [292, 291] line 252829 of NA/chrNA.fa.out 
# bad rep range [751, 599] line 261276 of NA/chrNA.fa.out 
# bad rep range [360, 359] line 259794 of Un/chrUn.fa.out 
# bad rep range [360, 359] line 259796 of Un/chrUn.fa.out 
# bad rep range [360, 359] line 259798 of Un/chrUn.fa.out 
# bad rep range [1, -56] line 379516 of Un/chrUn.fa.out
# note: 16 records dropped due to repStart > repEnd

# check coverage of repeats masked
# featureBits -chrom=chr1 danRer1 rmsk
# 11589712 bases of 40488791 (28.624%) in intersection
# featureBits -chrom=chr1 danRer2 rmsk
# 26879295 bases of 61678023 (43.580%) in intersection
# featureBits -chrom=chr1 danRer3 rmsk
# 25822888 bases of 55805710 (46.273%) in intersection

# MASK SEQUENCE WITH REPEATMASKER AND SIMPLE REPEAT/TRF AND BUILD NIB FILES
# (DONE, 2005-06-15, hartera)
    ssh kkstore01
    cd /cluster/data/danRer3
    # Soft-mask (lower-case) the contig and chr .fa's,
    # then make hard-masked versions from the soft-masked.
    set trfCtg=bed/simpleRepeat/trfMask
    set trfChr=bed/simpleRepeat/trfMaskChrom
    # for the chromosomes:
    foreach f (*/chr*.fa)
      echo "repeat- and trf-masking $f"
      maskOutFa -soft $f $f.out $f
      set chr = $f:t:r
      maskOutFa -softAdd $f $trfChr/$chr.bed $f
      echo "hard-masking $f"
      maskOutFa $f hard $f.masked
    end
# This warning is extremely rare -- if it indicates a problem, it is only with
# the repeat annotation and does not affect the masking:
# repeat- and trf-masking Un/chrUn.fa
# WARNING: negative rEnd: -56 chrUn:153329594-153329609 MOSAT_DR
    # for the contigs:
    foreach c (`cat chrom.lst`)
      echo "repeat- and trf-masking contigs of chr$c"
      foreach d ($c/chr*_?{,?})
        set ctg=$d:t
        set f=$d/$ctg.fa
        maskOutFa -soft $f $f.out $f
        maskOutFa -softAdd $f $trfCtg/$ctg.bed $f
        maskOutFa $f hard $f.masked
      end
    end
# same warning here too:
# repeat- and trf-masking contigs of chrUn
# WARNING: negative rEnd: -56 chrUn_26:1159145-1159160 MOSAT_DR
    # check percent sequence masked
    faSize /cluster/data/danRer3/1/chr1.fa
    # 55805710 bases (1047706 N's 54758004 real 28887275 upper 25870729 lower)
    # 46% is in lower case so masked
    # for danRer2:
    faSize /cluster/data/danRer2/1/chr1New.fa
    # 62208023 bases (3421437 N's 58786586 real 31874160 upper 26912426 lower)
    # 43% is in lower case so masked
    # Build nib files, using the soft masking in the fa
    mkdir nib
    foreach f (*/chr*.fa)
      faToNib -softMask $f nib/$f:t:r.nib
    end

# STORING O+O SEQUENCE AND ASSEMBLY INFORMATION  (DONE, 2005-06-15, hartera)
# Added link from danRer3.2bit file to the danRer3 gbdb directory
# (2005-06-17, hartera)
    # Make symbolic links from /gbdb/danRer3/nib to the real nibs
    ssh hgwdev
    cd /cluster/data/danRer3
    mkdir -p /gbdb/danRer3/nib
    foreach f (/cluster/data/danRer3/nib/chr*.nib)
      ln -s $f /gbdb/danRer3/nib
    end

# Load /gbdb/danRer3/nib paths into database and save size info
    # hgNibSeq creates chromInfo table
    hgNibSeq -preMadeNib danRer3 /gbdb/danRer3/nib */chr*.fa
    echo "select chrom,size from chromInfo" | hgsql -N danRer3 > chrom.sizes
    # take a look at chrom.sizes, should be 28 lines
    wc chrom.sizes
    # 28      56     409 chrom.sizes
    
    # Make one big 2bit file as well, and make a link to it in
    # /gbdb/danRer2/nib because hgBlat looks there:
    faToTwoBit */chr*.fa danRer3.2bit
    # add link to this 2bit file from gbdb danRer3 directory (2005-06-17)
    ln -s /cluster/data/danRer3/danRer3.2bit /gbdb/danRer3/
    # also make 2 bit files for chrUn and chrNA later on - need masked seq
    # make 2 bit files for chrUn and chrNA scaffolds (2005-06-17)
    ssh kkstore01
    cd /cluster/data/danRer3
    # make scaffolds files
    foreach c (NA Un)
       cd $c
       echo "Processing $c ..."
       mkdir scafSeqs
       awk '{if ($5 != "N") print $6;}' chr${c}.agp > scafSeqs/scaffolds.lst
       cd ..
    end 
    cd /cluster/data/danRer3/NA/scafSeqs
cat << '_EOF_' > getSeqs.csh
     #!/bin/csh -fe
     set dir=/cluster/bluearc/danRer3/scaffolds
     faOneRecord /iscratch/i/danRer3/scaffolds/Zv5.fa $1 > $dir/$1.fa
'_EOF_'
     # << this line makes emacs coloring happy
     chmod +x getSeqs.csh
cat << '_EOF_' > gsub
#LOOP
getSeqs.csh $(path1)
#ENDLOOP
'_EOF_'
     # << this line makes emacs coloring happy 
     ssh kk
     cd /cluster/data/danRer3/NA/scafSeqs
     gensub2 scaffolds.lst single gsub jobList
     para create jobList 
     para try,check,push,check etc...
    
     ssh kkstore01
     cd /cluster/bluearc/danRer3/scaffolds
     foreach f (*.size)
    faToTwoBit ./chrNA/scafSeqs/*.fa danRer3ChrNA.2bit
    faToTwoBit ./chrUn/scafSeqs *.fa danRer3ChrUn.2bit

# MAKE GOLD AND GAP TRACKS (DONE, 2005-06-15, hartera)
# Add trackDb entry and html page for gold and gap tracks (2005-06-16, hartera)
    ssh hgwdev
    cd /cluster/data/danRer3
    # the gold and gap tracks are created from the chrN.agp file and this is
    # the scaffolds or supercontigs agp 
    hgGoldGapGl -noGl -chromLst=chrom.lst danRer3 /cluster/data/danRer3 .
    # featureBits danRer3 gold
    # 1630323462 bases of 1630323462 (100.000%) in intersection
    # featureBits danRer2 gold
    # 1560497282 bases of 1560497282 (100.000%) in intersection
    # featureBits danRer1 gold
    # 1459132082 bases of 1459132082 (100.000%) in intersection

    # featureBits danRer3 gap
    # 13709500 bases of 1630323462 (0.841%) in intersection
    # featureBits danRer2 gap
    # 28776000 bases of 1560497282 (1.844%) in intersection
    # featureBits danRer1 gap
    # 64174000 bases of 1459132082 (4.398%) in intersection
# Add trackDb.ra entries for gold and gap tracks and also create
# gap.html and gold.html pages.

# MAKE TRACKDB ENTRY FOR DANRER3 (DONE, 2005-06-16, hartera)
    ssh hgwdev
    # Make trackDb table so browser knows what tracks to expect:
    mkdir -p ~/kent/src/hg/makeDb/trackDb/zebrafish/danRer3
    cd ~/kent/src/hg/makeDb/trackDb/zebrafish
    cvs add danRer3
    cvs commit danRer3
    cd ~/kent/src/hg/makeDb/trackDb
    cvs up -d -P
    # Edit that makefile to add danRer3 in all the right places and do
    make update
    make alpha
    cvs commit -m "Added danRer3." makefile
    
# MAKE DESCRIPTION/SAMPLE POSITION HTML PAGE (DONE, 2005-06-16, hartera)
    ssh hgwdev
    mkdir /cluster/data/danRer3/html
   # make a symbolic link from /gbdb/danRer3/html to /cluster/data/danRer3/html
    ln -s /cluster/data/danRer3/html /gbdb/danRer3/html
    # Add a description page for zebrafish
    cd /cluster/data/danRer3/html
    cp $HOME/kent/src/hg/makeDb/trackDb/zebrafish/danRer2/description.html .
    # Edit this for zebrafish danRer3
                                                                                
    # create a description.html page here
    cd ~/kent/src/hg/makeDb/trackDb/zebrafish/danRer3
    # Add description page here too
    cp /cluster/data/danRer3/html/description.html .
    cvs add description.html
    cvs commit -m "First draft of description page for danRer3." \
        description.html
    cd ~/kent/src/hg/makeDb/trackDb
    make update
    make alpha

# MAKE HGCENTRALTEST ENTRY FOR DANRER3 (DONE, 2004-06-16, hartera)
    # Make trackDb table so browser knows what tracks to expect:
    ssh hgwdev
    # Add dbDb and defaultDb entries:
    echo 'insert into dbDb (name, description, nibPath, organism,  \
          defaultPos, active, orderKey, genome, scientificName,  \
          htmlPath, hgNearOk, hgPbOk, sourceName)  \
          values("danRer3", "May 2005", \
          "/gbdb/danRer3", "Zebrafish", "chr2:15,906,734-15,926,406", 1, \
          37, "Zebrafish", "Danio rerio", \
          "/gbdb/danRer3/html/description.html", 0,  0, \
          "Sanger Centre, Danio rerio Sequencing Project Zv5");' \
    | hgsql -h genome-testdb hgcentraltest
    # set danRer3 to be the default assembly for Zebrafish
    echo 'update defaultDb set name = "danRer3" \
          where genome = "Zebrafish";' \
          | hgsql -h genome-testdb hgcentraltest

# PUT MASKED SEQUENCE OUT FOR CLUSTER RUNS AND ON BLUEARC
# (DONE, 2005-06-16, hartera)
    ssh kkr1u00
    # Chrom-level mixed nibs that have been repeat- and trf-masked:
    rm -rf /iscratch/i/danRer3/nib
    mkdir -p /iscratch/i/danRer3/nib
    cp -p /cluster/data/danRer3/nib/chr*.nib /iscratch/i/danRer3/nib
    # Pseudo-contig fa that have been repeat- and trf-masked:
    rm -rf /iscratch/i/danRer3/trfFa
    mkdir /iscratch/i/danRer3/trfFa
    foreach d (/cluster/data/danRer3/*/chr*_?{,?})
      cp -p $d/$d:t.fa /iscratch/i/danRer3/trfFa
    end
    rm -rf /iscratch/i/danRer3/rmsk
    mkdir -p /iscratch/i/danRer3/rmsk
    cp -p /cluster/data/danRer3/*/chr*.fa.out /iscratch/i/danRer3/rmsk
    cp -p /cluster/data/danRer3/danRer3.2bit /iscratch/i/danRer3/
    /cluster/bin/iSync
    # add to the bluearc
    ssh kkstore01
    mkdir -p /cluster/bluearc/danRer3/nib
    cp -p /cluster/data/danRer3/nib/chr*.nib /cluster/bluearc/danRer3/nib
    mkdir -p /cluster/bluearc/danRer3/trfFa
    foreach d (/cluster/data/danRer3/*/chr*_?{,?})
      cp -p $d/$d:t.fa /cluster/bluearc/danRer3/trfFa
    end
    cp /cluster/data/danRer3/danRer3.2bit /cluster/bluearc/danRer3/

# ADD CONTIGS TRACK (DONE, 2005-06-16, hartera)
# make ctgPos2 (contig name, size, chrom, chromStart, chromEnd) from 
# chunks (contigs) agp files.
    ssh kkstore01
    mkdir -p /cluster/data/danRer3/bed/ctgPos2
    cd /cluster/data/danRer3/bed/ctgPos2
    # ctgPos2 .sql .as .c and .h files exist - see makeDanRer1.doc
    foreach c (`cat /cluster/data/danRer3/chrom.lst`)
         awk 'BEGIN {OFS="\t"} \
         {if ($5 != "N") print $6, $3-$2+1, $1, $2-1, $3, $5}' \
         /cluster/data/danRer3/$c/agps/chr${c}.chunks.agp >> ctgPos2.tab
    end
                                                                                
    ssh hgwdev
    cd /cluster/data/danRer3/bed/ctgPos2
    hgsql danRer3 < ~/kent/src/hg/lib/ctgPos2.sql
    echo "load data local infile 'ctgPos2.tab' into table ctgPos2" \
         | hgsql danRer3
# create trackDb.ra entry and html page for ctgPos2 track.

# CREATE gc5Base WIGGLE TRACK (DONE, 2005-06-16, hartera)
# FIX LINK FOR WIB FILES TO POINT TO danRer3 ON store11 (2005-07-25, hartera)
    ssh kkstore01
    mkdir -p /cluster/data/danRer3/bed/gc5Base
    cd /cluster/data/danRer3/bed/gc5Base
    # The number of bases that hgGcPercent claimed it measured is calculated,
    # which is not necessarily always 5 if it ran into gaps, and then the
    # division by 10.0 scales down the numbers from hgGcPercent to the range
    # [0-100].  wigEncode now replaces wigAsciiToBinary and the previous
    # processing step between these two programs. The result file is *.wig.
    # Each value represents the measurement over five bases beginning with
    # <position>. wigEncode also calculates the zoomed set of data.
    # Uses the 2bit file in /cluster/data/danRer3 as sequence input.
                                                                                
    nice hgGcPercent -wigOut -doGaps -file=stdout -win=5 danRer3 \
        /cluster/data/danRer3 | \
        wigEncode stdin gc5Base.wig gc5Base.wib
    # load the .wig file back on hgwdev:
    ssh hgwdev
    cd /cluster/data/danRer3/bed/gc5Base
    hgLoadWiggle -pathPrefix=/gbdb/danRer3/wib/gc5Base \
                 danRer3 gc5Base gc5Base.wig
    # and symlink the .wib file into /gbdb
    # fix link as danRer3 is now in store 11 (2005-07-25, hartera)
    rm -r /gbdb/danRer3/wib/gc5Base
    mkdir -p /gbdb/danRer3/wib/gc5Base
    ln -s `pwd`/gc5Base.wib /gbdb/danRer3/wib/gc5Base

# MAKE 10.OOC, 11.OOC FILE FOR BLAT (DONE, 2005-06-17, hartera)
    # Use -repMatch=512 (based on size -- for human we use 1024, and
    # the zebrafish genome is ~50% of the size of the human genome
    ssh kkr1u00
    mkdir /cluster/data/danRer3/bed/ooc
    cd /cluster/data/danRer3/bed/ooc
    mkdir -p /cluster/bluearc/danRer3
    ls -1 /cluster/data/danRer3/nib/chr*.nib > nib.lst
    blat nib.lst /dev/null /dev/null -tileSize=11 \
      -makeOoc=/cluster/bluearc/danRer3/danRer3_11.ooc -repMatch=512
    # Wrote 50575 overused 11-mers to /cluster/bluearc/danRer3/11.ooc
    # For 10.ooc, repMatch = 4096 for human, so use 2048
    blat nib.lst /dev/null /dev/null -tileSize=10 \
      -makeOoc=/cluster/bluearc/danRer3/danRer3_10.ooc -repMatch=2048
    # Wrote 12574 overused 10-mers to /cluster/bluearc/danRer3/10.ooc 
    # keep copies of ooc files in this directory and copy to iscratch
    cp /cluster/bluearc/danRer3/*.ooc .
    cp -p /cluster/bluearc/danRer3/*.ooc /iscratch/i/danRer3/
    /cluster/bin/iSync

# MAKE HGCENTRALTEST BLATSERVERS ENTRY FOR danRer3 (DONE, 2005-07-20, kuhn)
    # hgcentraltest is now on hgwdev                                            
    ssh hgwdev
   # DNA port is "0", trans prot port is "1"
 echo 'insert into blatServers values("danRer3", "blat2", "17778", "1", "0");    insert into blatServers values("danRer3", "blat2", "17779", "0", "1");' \
    | hgsql hgcentraltest
    # this enables blat and isPcr, isPcr is enabled by loading blat server
    # with tilesize=5 (ask for this when request blat servers from 
    # cluster admin).
    # if you need to delete those entries
    echo 'delete from blatServers where db="danRer3";' \
    | hgsql hgcentraltest
    # to check the entries:
    echo 'select * from blatServers where db="danRer3";' \
    | hgsql hgcentraltest

# AFFYMETRIX ZEBRAFISH GENOME ARRAY CHIP (DONE, 2005-07-22, hartera)
# REMAKE THIS TRACK USING chrUn AND chrNA SCAFFOLDS (DONE, 2005-08-19, hartera)
    # sequences already downloaded for danRer1
    ssh hgwdev
    cd /projects/compbio/data/microarray/affyZebrafish
    mkdir /cluster/bluearc/affy
    cp /projects/compbio/data/microarray/affyZebrafish/Zebrafish_consensus.fa \       /cluster/bluearc/affy/
    # Set up cluster job to align Zebrafish consensus sequences to danRer2
    ssh kkr1u00
    mkdir -p /cluster/data/danRer3/bed/affyZebrafish.2005-08-19
    ln -s /cluster/data/danRer3/bed/affyZebrafish.2005-08-19 \
          /cluster/data/danRer3/bed/affyZebrafish
    cd /cluster/data/danRer3/bed/affyZebrafish
    mkdir -p /iscratch/i/affy
    cp /cluster/bluearc/affy/Zebrafish_consensus.fa /iscratch/i/affy
    /cluster/bin/iSync

    # the kilokluster is down, so run on the pitakluster
    ssh pk
    cd /cluster/data/danRer3/bed/affyZebrafish
    ls -1 /cluster/bluearc/affy/Zebrafish_consensus.fa > affy.lst
    ls -1 /cluster/bluearc/danRer3/trfFa/chr[0-9M]*.fa > genome.lst
    # for output:
    mkdir -p /san/sanvol1/danRer3/affy/pslChrom
    echo '#LOOP\n/cluster/bin/i386/blat -fine -mask=lower -minIdentity=95 -ooc=/cluster/bluearc/danRer3/danRer3_11.ooc $(path1) $(path2) {check out line+ /san/sanvol1/danRer3/affy/pslChrom/$(root1)_$(root2).psl}\n#ENDLOOP' > template.sub

    gensub2 genome.lst affy.lst template.sub para.spec
    para create para.spec
    para try, check, push ... etc.
# para time
# Completed: 208 of 208 jobs
# CPU time in finished jobs:       1355s      22.59m     0.38h    0.02d  0.000 y
# IO & Wait Time:                  9988s     166.46m     2.77h    0.12d  0.000 y
# Average job time:                  55s       0.91m     0.02h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:              74s       1.23m     0.02h    0.00d
# Submission to last job:           217s       3.62m     0.06h    0.00d

    # then run the 2bit file of scaffolds
    ssh pk 
    cd /cluster/data/danRer3/bed/affyZebrafish
    mkdir scaffoldsNAandUnRun
    cd scaffoldsNAandUnRun
    ls -1 /cluster/bluearc/affy/Zebrafish_consensus.fa > affy.lst
    foreach f (/cluster/bluearc/scratch/danRer3/scaffoldsSoftMask/*.fa)
       ls -1 $f >> scafs.lst
    end
    mkdir -p /san/sanvol1/danRer3/affy/pslScaffoldsNAandUn
    echo '#LOOP\n/cluster/bin/i386/blat -fine -mask=lower -minIdentity=95 -ooc=/cluster/bluearc/danRer3/danRer3_11.ooc $(path1) $(path2) {check out line+ /san/sanvol1/danRer3/affy/pslScaffoldsNAandUn/$(root1)_$(root2).psl}\n#ENDLOOP' > template2.sub

    gensub2 scafs.lst affy.lst template2.sub para.spec
    para create para.spec
    para try, check, push ... etc.
# para time
# Completed: 14941 of 14941 jobs
# CPU time in finished jobs:      27574s     459.57m     7.66h    0.32d  0.001 y
# IO & Wait Time:                 47642s     794.03m    13.23h    0.55d  0.002 y
# Average job time:                   5s       0.08m     0.00h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:              35s       0.58m     0.01h    0.00d
# Submission to last job:           339s       5.65m     0.09h    0.00d

    
    # need to do pslSort and lift up for each separate run
    cd /cluster/data/danRer3/bed/affyZebrafish
    cd /san/sanvol1/danRer3/affy/pslScaffoldsNAandUn
    # Do sort, best in genome filter, and convert to chromosome coordinates
    # to create affyZebrafish.psl
    # only use alignments that have at least
    # 95% identity in aligned region.
    # do not use minCover since a lot of sequence is in Un, NA and Finished
    # so genes may be split up so good to see all alignments
    # first do the chr1-25 and chrM alignments
    pslSort dirs raw.psl tmp pslChrom
    pslReps -minAli=0.95 -nearTop=0.005 raw.psl contig.psl /dev/null
    # Processed 27408 alignments
    pslSort dirs rawNAandUn.psl tmp pslScaffoldsNAandUn
    pslReps -minAli=0.95 -nearTop=0.005 rawNAandUn.psl scafNAandUn.psl /dev/null
    # Processed 9888 alignments
    # lift up chrom contigs to chrom level
    liftUp affyZfishChroms.psl \
        /cluster/data/danRer3/jkStuff/liftAll.lft warn contig.psl
    liftUp affyZfishScafsNAandUn.psl \
      /cluster/data/danRer3/liftSupertoChrom/liftNAandUnScaffoldsToChrom.lft \
      warn scafNAandUn.psl
    # sort and merge these files
    mkdir psl
    cp affyZfish* ./psl/
    pslSort dirs affyZebrafish.psl tmp1 psl
    
    # rsync these psl files 
    rsync -a --progress /san/sanvol1/danRer3/affy/*.psl \
         /cluster/data/danRer3/bed/affyZebrafish/
    ssh kkstore02
    cd /cluster/data/danRer3/bed/affyZebrafish
    # shorten names in psl file
    sed -e 's/Zebrafish://' affyZebrafish.psl > affyZebrafish.psl.tmp
    mv affyZebrafish.psl.tmp affyZebrafish.psl
    pslCheck affyZebrafish.psl
    # psl is good
    # load track into database
    ssh hgwdev
    cd /cluster/data/danRer3/bed/affyZebrafish
    hgLoadPsl danRer3 affyZebrafish.psl
    # Add consensus sequences for Zebrafish chip
    # Copy sequences to gbdb if they are not there already
    mkdir -p /gbdb/hgFixed/affyProbes
    ln -s \
       /projects/compbio/data/microarray/affyZebrafish/Zebrafish_consensus.fa \
      /gbdb/hgFixed/affyProbes
                                                                                
    hgLoadSeq -abbr=Zebrafish: danRer3 \
              /gbdb/hgFixed/affyProbes/Zebrafish_consensus.fa
    # Clean up
    rm batch.bak contig.psl raw.psl
    # moved affyZebrafish.html description and trackDb.ra track entry and
    # search for Affy Zebrafish track to
    # ~/kent/src/hg/makeDb/trackDb/zebrafish since it is common to all 
    # danRer assemblies. 

# LIFT FILES FROM SCAFFOLDS TO chrUn AND chrNA (DONE, 2005-07-27, hartera)
    ssh kkstore02
    mkdir -p /cluster/data/danRer3/liftSupertoChrom
    cd /cluster/data/danRer3/liftSupertoChrom
    # lift files are already created when scaffoldFaToAgp was run for chrUn.fa
    # and chrNA.fa. These need to be edited as the last 500 Ns were removed 
    # from the agp file making the sequence 184125739 bp and not 184126239 bp
    # for chrUn, for chrNA, it is 253521007 bp instead of 253521507 bp and need 
    # to change chrUn to chrNA
    cp /cluster/data/danRer3/Un/tmp/chrUn.lft .
    cp /cluster/data/danRer3/NA/tmp/chrNA.lft .
    # edit to remove last lines of each file first
    # then use perl to change co-ordinates
    perl -pi.bak -e 's/184126239/184125739/' chrUn.lft
    perl -pi.bak -e 's/253521507/253521007/' chrNA.lft
    perl -pi.bak -e 's/chrUn/chrNA/' chrNA.lft
    cat *.lft >> liftNAandUnScaffoldsToChrom.lft
    # clean up 
    rm *.bak

# ENSEMBL GENES (DONE, 2005-07-29, hartera)
    ssh hgwdev  
    mkdir -p /cluster/data/danRer3/bed/ensembl
    cd /cluster/data/danRer3/bed/ensembl
    # Get the ensembl protein data from
    # http://www.ensembl.org/Multi/martview
    # Follow this sequence through the pages: (NOTE: this interface has changed
    # a little since danRer2)
    # Page 1) Select the Ensembl dataset (32 here) and the 
    # Danio_rerio choice (ZFISH5 here). Hit next.
    # Page 2) Then hit next.
    # Page 3) Choose the "Structures" Attribute Page from the pulldown menu
    # at the top. Choose GTF as the output. Choose gzip compression.  
    # hit export. Save as ensemblGene.gtf.gz

    # the Ensembl gene predictions are mapped to chromosomes except for 
    # chrNA and chrUn. Use lift files for scaffolds to these chroms.
    # get chrUn and chrNA Ensembl records 
    ssh kkstore02
    cd /cluster/data/danRer3/bed/ensembl
    gunzip ensemblGene.gtf.gz
    awk '$1 ~ /^Zv5_NA[0-9]+/ || $1 ~ /^Zv5_scaffold[0-9]+/' ensemblGene.gtf \
                    > ensemblGenechrUns.gtf
    # get records for all other chroms
    awk '$1 ~ /^[0-9]+/' ensemblGene.gtf > ensemblGenechroms.gtf
    wc -l *.gtf
    # 513421 ensemblGenechroms.gtf
    # 125319 ensemblGenechrUns.gtf
    # 638740 ensemblGene.gtf
    # total lines of files made equal to original file so ok
    liftUp -type=.gtf ensemblGenechrUns.lifted \
     /cluster/data/danRer3/liftSupertoChrom/liftNAandUnScaffoldsToChrom.lft \ 
     warn ensemblGenechrUns.gtf
     # Got 29880 lifts in 
     # /cluster/data/danRer3/liftSupertoChrom/liftNAandUnScaffoldsToChrom.lft
     sed -e "s/^/chr/" ensemblGenechroms.gtf > ensGene.gtf
     cat ensemblGenechrUns.lifted >> ensGene.gtf
     # check file sizes -ok and some of the lifted co-ordinates
     # there were some erroneous lines with "1;" or "2;" - 8 lines total
     # Notified Ensembl and they fixed it so downloaded file again 
     # and reloaded into database
     # Also remove the suffix that denotes the transcript version number. 
     # This is not in the ensGtp or ensPep tables.
     perl -pi.bak -e 's/\.[0-9]+//'g ensGene.gtf
 
     # load into database
     ssh hgwdev
     cd /cluster/data/danRer3/bed/ensembl
     hgsql -e 'drop table ensGene;' danRer3
     /cluster/bin/i386/ldHgGene danRer3 ensGene ensGene.gtf
     # Read 32143 transcripts in 638732 lines in 1 files
     # 32143 groups 27 seqs 1 sources 4 feature types
     # 32143 gene predictions

     # ensGtp associates geneId/transcriptId/proteinId for hgPepPred and
     # hgKnownToSuper.  Use ensMart to create it as above, except:
     # Page 3) Choose the "Features" box. In "Ensembl Attributes", check
     # Ensembl Gene ID, Ensembl Transcript ID, Ensembl Peptide ID.
     # Choose Text, tab-separated as the output format and gzip compression.  
     # Result name: ensGtp.
     gunzip ensGtp.tsv.gz
     # edit to remove first header line
     hgsql danRer3 < ~/kent/src/hg/lib/ensGtp.sql
     # remove header line from ensGtp.txt
     echo "load data local infile 'ensGtp.tsv' into table ensGtp" \
         | hgsql -N danRer3

         # Get the ensembl peptide sequences from
    # http://www.ensembl.org/Multi/martview
    # Choose Danio Rerio as the organism
    # Follow this sequence through the pages:
    # Page 1) Choose the Ensembl Genes choice. Hit next.
    # Page 2) Then hit next.
    # Page 3) Choose "Sequences" from the Attributes pulldown menu at the top.
    # Page 4) Choose Peptide as type of sequence to export and select 
    # Ensembl Gene ID from Gene Attributes and 
    # Ensembl Transcript ID and Ensembl Peptide Stable ID from 
    # Transcript Attributes as the output,
    # choose text/fasta and gzip compression,
    # name the file ensemblPep.fa.gz and then hit export.
    gunzip ensemblPep.fa.gz
    hgPepPred danRer3 ensembl ensemblPep.fa
    # added code to hgc.c so that the link to the Ensembl Protein
    # is also displayed on the description page.


FOR MGC GENES:
 - wait one day for nightly build to align and load them into the db
   - rebuild trackDb

# SPLIT UP ZEBRAFISH MASKED SEQUENCE FROM chrUn and chrNA INTO SCAFFOLDS
# (DONE, 2005-08-04, hartera)
# ADD SOFT-MASKED SCAFFOLDS TO ISERVERS FOR CLUSTER RUNS 
# (DONE, 2005-08-15, hartera) AND TO BLUEARC (DONE, 2005-08-19)
    ssh kkstore02
    cd /cluster/data/danRer3
    # for chrUn and chrNA, get masked sequence for soft and hard-masked 
    foreach c (Un NA)
      cd $c
      mkdir scaffoldsSoftMask scaffoldsHardMask
      awk 'BEGIN {FS="\t"}{if ($5 != "N") \
       print "faFrag -mixed chr'${c}'.fa",$2-1, $3, $6".fa";}' chr${c}.agp \
       >> ./scaffoldsSoftMask/faFragSoftMask.csh
      awk 'BEGIN {FS="\t"}{if ($5 != "N") \
        print "faFrag -mixed chr'${c}'.fa.masked",$2-1, $3, $6".fa.masked";}' \
        chr${c}.agp >> ./scaffoldsHardMask/faFragHardMask.csh
      cd ..
    end 

    # change permissions run scripts to get sequences
    foreach d (Un NA)
       chmod +x $d/scaffoldsSoftMask/faFragSoftMask.csh
       chmod +x $d/scaffoldsHardMask/faFragHardMask.csh
    end

    cat << '_EOF_' > jkStuff/getMaskedScaffolds.csh
#!/bin/csh
foreach c (Un NA)
   set dir=/cluster/data/danRer3
   echo "Processing $c"
   cd $dir/$c/scaffoldsSoftMask
   cp ../chr${c}.fa .
   echo "Getting soft-masked sequences ..." 
   nice faFragSoftMask.csh >& faFrag.log
   echo "Getting hard-masked sequences ..." 
   cd $dir/$c/scaffoldsHardMask
   cp ../chr${c}.fa.masked .
   nice faFragHardMask.csh >& faFrag.log
end 
'_EOF_'
   chmod +x jkStuff/getMaskedScaffolds.csh
   nice ./jkStuff/getMaskedScaffolds.csh &
   # check a few sequences that they are correct
   # add name of scaffold to sequence fasta and cat together
   foreach c (Un NA)
      set dir = /cluster/data/danRer3
      foreach d (scaffoldsSoftMask scaffoldsHardMask)
         cd $dir/$c/$d
         foreach f (Zv5*)
           if ($d == "scaffoldsHardMask") then
              set b=$f:r
              set g=$b:r
              set sc=scaffoldMasked${c}.fa
           else
              set g=$f:r
              set sc=scaffold${c}.fa
           endif 
           perl -pi.bak -e "s/>chr[0-9A-Za-z\-\:]+/>$g/" $f
           cat $f >> $sc
           rm *.bak
         end
         cp scaffold* $dir/$c/
      end
   end
   # check sizes of final FASTA file with all sequences. check a few
   # sequence files to see that they are correct - ok 
   # Add soft-masked scaffolds to the iservers for cluster runs 
   # (2005-08-15, hartera)
   ssh kkr1u00
   mkdir -p /iscratch/i/danRer3/scaffoldsSoftMask
   cd /cluster/data/danRer3
   foreach c (NA Un)
      foreach f (/cluster/data/danRer3/$c/scaffoldsSoftMask/Zv5_*.fa)
      cp -p $f /iscratch/i/danRer3/scaffoldsSoftMask
      end
   end
   /cluster/bin/iSync
   # Add soft-masked scaffolds to the bluearc for cluster runs 
   # (2005-08-19, hartera)
   ssh kkr1u00
   cd /cluster/data/danRer3/
   mkdir -p /cluster/bluearc/scratch/danRer3/scaffoldsSoftMask
   foreach c (NA Un)
      foreach f (/cluster/data/danRer3/$c/scaffoldsSoftMask/Zv5_*.fa)
         rsync -a --progress $f \
         /cluster/bluearc/scratch/danRer3/scaffoldsSoftMask/
      end 
   end 

# MAKE DOWNLOADABLE SEQUENCE FILES (DONE, 2005-08-05, hartera)
    ssh kkstore02
    cd /cluster/data/danRer3
    #- Build the .zip files
    cat << '_EOF_' > jkStuff/gzipAll.csh
rm -rf gzip
mkdir gzip
# chrom AGP's
tar cvzf gzip/chromAgp.tar.gz [0-9A-Z]*/chr*.agp
# chrom RepeatMasker out files
tar cvzf gzip/chromOut.tar.gz */chr*.fa.out
# soft masked chrom fasta
tar cvzf gzip/chromFa.tar.gz */chr*.fa
# soft masked chrNA and chrUn scaffolds
tar cvzf gzip/scaffoldUnsFa.tar.gz NA/scaffoldNA.fa \
    Un/scaffoldUn.fa
# hard masked chrom fasta
tar cvzf gzip/chromFaMasked.tar.gz */chr*.fa.masked
# hard masked chrNA and chrUn scaffolds
tar cvzf gzip/scaffoldUnsFaMasked.tar.gz \
    NA/scaffoldMaskedNA.fa \
    Un/scaffoldMaskedUn.fa
# chrom TRF output files
cd bed/simpleRepeat
tar cvzf ../../gzip/chromTrf.tar.gz trfMaskChrom/chr*.bed
cd ../..

# get GenBank native mRNAs
cd /cluster/data/genbank
./bin/i386/gbGetSeqs -db=danRer3 -native GenBank mrna \
        /cluster/data/danRer3/gzip/mrna.fa
# get GenBank xeno mRNAs
./bin/i386/gbGetSeqs -db=danRer3 -xeno GenBank mrna \
        /cluster/data/danRer3/gzip/xenoMrna.fa
# get native RefSeq mRNAs
./bin/i386/gbGetSeqs -db=danRer3 -native refseq mrna \
/cluster/data/danRer3/gzip/refMrna.fa
# get native GenBank ESTs
./bin/i386/gbGetSeqs -db=danRer3 -native GenBank est \
/cluster/data/danRer3/gzip/est.fa
                                                                                
cd /cluster/data/danRer3/gzip
# gzip GenBank native and xeno mRNAs, native ESTs and RefSeq mRNAs
gzip mrna.fa
gzip xenoMrna.fa
gzip refMrna.fa
gzip est.fa
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x ./jkStuff/gzipAll.csh
    csh ./jkStuff/gzipAll.csh |& tee ./jkStuff/gzipAll.log
    #- Look at zipAll.log to make sure all file lists look reasonable.
    # Make upstream files and Copy the .zip files to
    # hgwdev:/usr/local/apache/...
    ssh hgwdev
    cd /cluster/data/danRer3/gzip
    # make upstream files for zebrafish RefSeq
    featureBits danRer3 refGene:upstream:1000 -fa=upstream1000.fa
    gzip upstream1000.fa
    featureBits danRer3 refGene:upstream:2000 -fa=upstream2000.fa
    gzip upstream2000.fa
    set gp = /usr/local/apache/htdocs/goldenPath/danRer3
    mkdir -p $gp/bigZips
    cp -p *.gz $gp/bigZips
    mkdir -p $gp/chromosomes
    foreach f (../*/chr*.fa)
       cp $f $gp/chromosomes
    end
    foreach c (NA Un)
       cd /cluster/data/danRer3/$c
       cp scaffold${c}.fa.gz $gp/chromosomes
    end
    cd $gp/bigZips
    md5sum *.gz > md5sum.txt
    cd $gp/chromosomes
    # gzip the chromosome and scaffold FASTAs individually
    foreach f (*.fa)
      gzip $f
    end
    md5sum *.gz > md5sum.txt
    # Take a look at bigZips/* and chromosomes/*
    # copy README.txt's from danRer2 and update

# MAKE NIB FILES AND 2BIT FILE FOR SOFT MASKED chrUn AND chrNA SCAFFOLDS
# (DONE, 2005-08-06, hartera)
# ADD chrUn AND chrNA SCAFFOLDS 2BIT FILE TO BLUEARC (DONE, 2005-08-19, hartera)

    ssh kkstore02
    cd /cluster/data/danRer3
    mkdir scaffoldsNAandUnNib
    # Build nib files, using the soft masking in the fa
    foreach c (NA Un)
       echo "Processing $c"
       foreach f ($c/scaffoldsSoftMask/Zv5*.fa)
         faToNib -softMask $f scaffoldsNAandUnNib/$f:t:r.nib
       end
    end
    # check correct number of nib files in directory: 14941
    # there are 14676 chrNA scaffolds and 265 chrUn scaffolds
    # copy chromosome 1-25 and chrNA and chrUn scaffolds nibs to a directory
    # on iscratch and iSync for use in cluster runs
    ssh kkr1u00
    mkdir -p /iscratch/i/danRer3/chromandScafNib
    cp -p /cluster/data/danRer3/nib/chr[0-9]*.nib \
       /iscratch/i/danRer3/chromandScafNib
    foreach f (/cluster/data/danRer3/scaffoldsNAandUnNib/Zv5*.nib)
       cp -p $f /iscratch/i/danRer3/chromandScafNib
    end
    ssh kkstore02
    # make a 2 bit file of all the scaffolds for chrNA and chrUn
    # for blastz cluster runs
    cd /cluster/data/danRer3/
    cat NA/scaffoldNA.fa Un/scaffoldUn.fa > danRer3NAandUnScaffolds.fa
    grep '>' danRer3NAandUnScaffolds.fa | wc -l
    # 14941
    faToTwoBit danRer3NAandUnScaffolds.fa danRer3NAandUnScaf.2bit
    ssh kkr1u00
    mkdir -p /iscratch/i/danRer3/NAandUnScafs
    cp /cluster/data/danRer3/danRer3NAandUnScaf.2bit \
       /iscratch/i/danRer3/NAandUnScafs
    /cluster/bin/iSync
    
    # get sizes of scaffolds for the .len file used by blastz
    ssh kolossus
    mkdir -p /panasas/store/danRer3/NAandUnScafSizes
    cd /cluster/data/danRer3
cat << '_EOF_' > jkStuff/getNAandUnScafSizes.csh
#!/bin/csh -fe
foreach c (NA Un)
  set sizeDir=/panasas/store/danRer3/NAandUnScafSizes
  cd /cluster/data/danRer3/$c/scaffoldsSoftMask
  foreach f (Zv5*.fa)
     set g=$f:r
     faSize detailed=on $f >> $sizeDir/NAandUnScafs.sizes
  end
end
'_EOF_'
    chmod +x jkStuff/getNAandUnScafSizes.csh
    nice jkStuff/getNAandUnScafSizes.csh >& size.log &
    # took about 1 minute
    wc -l /panasas/store/danRer3/NAandUnScafSizes/NAandUnScafs.sizes
    # 14941 /panasas/store/danRer3/NAandUnScafSizes/NAandUnScafs.sizes
    # so correct number of scaffolds
    cp /panasas/store/danRer3/NAandUnScafSizes/NAandUnScafs.sizes \
       /cluster/data/danRer3
    # add 2 bit to bluearc for cluster runs (2005-08-19, hartera)
    ssh kkr1u00
    mkdir -p /cluster/bluearc/scratch/danRer3
    cp /cluster/data/danRer3/danRer3NAandUnScaf.2bit \
       /cluster/bluearc/scratch/danRer3/

# BLASTZ SWAP FOR MOUSE (mm6) (DONE, 2005-08-10, hartera)
# CREATE CHAIN AND NET TRACKS, AXTNET, MAFNET AND ALIGNMENT DOWNLOADS
# REMAKE AXTNET AND COPY TO DOWNLOADS. REMAKE MAFNET (DONE, 2005-08-17, hartera)
    ssh kkr1u00
    # blastz requires lineage-specific repeats
    # Treat all repeats as lineage-specific
    # if not done already, get lineage-specific repeats
    mkdir -p /iscratch/i/mm6/linSpecRep.notInZebrafish
    foreach f (/panasas/store/mm6/rmsk/chr*.fa.out)
      cp -p $f /iscratch/i/mm6/linSpecRep.notInZebrafish/$f:t:r:r.out.spec
    end

    mkdir -p /iscratch/i/danRer3/linSpecRep.notInMouse
    foreach f (/iscratch/i/danRer3/rmsk/chr*.fa.out)
      cp -p $f /iscratch/i/danRer3/linSpecRep.notInMouse/$f:t:r:r.out.spec
    end
    /cluster/bin/iSync
  
    # do swap of mm6 vs. danRer3 chain and net alignments to 
    # create danRer3 vs. mm6. see makeMm6.doc for details.
    ssh kk
    cd /cluster/data/mm6/bed/blastz.danRer3
    mkdir -p /panasas/store/danRer3vsmm6Out
    nohup nice /cluster/bin/scripts/doBlastzChainNet.pl `pwd`/DEF \
        -stop download -blastzOutRoot /panasas/store/danRer3vsmm6Out \
        -swap -chainMinScore=5000 >& doSwap.log &
    # Start: Aug 10 16:30
    # Finish: Aug 10 16:54
    # Blastz parameters are as for mm6 vs. danRer3 danRer3 - see makeMm6.doc
# BLASTZ_H=2000
# BLASTZ_Y=3400
# BLASTZ_L=6000
# BLASTZ_K=2200
# BLASTZ_Q=/cluster/data/blastz/HoxD55.q
# BLASTZ_ABRIDGE_REPEATS=1
  # do cleanup step and specify a different file server as can not 
  # access panasas from kkstore02.
  nice /cluster/bin/scripts/doBlastzChainNet.pl `pwd`/DEF \
        -continue cleanup -fileServer eieio \
        -blastzOutRoot /panasas/store/danRer3vsmm6Out \
        -swap -chainMinScore=5000 >& doSwapCleanUp.log &
  # make html files and trackDb.ra entry for chain and net tracks.
  # check README.txt for downloads.
# featureBits -chrom=chr1 danRer3 refGene:cds chainMm6Link -enrichment
# refGene:cds 0.688%, chainMm6Link 8.193%, both 0.543%, cover 78.94%, 
# enrich 9.64x
# featureBits -chrom=chr1 danRer2 refGene:cds chainMm5Link -enrichment
# refGene:cds 0.642%, chainMm5Link 4.499%, both 0.492%, cover 76.60%, 
# enrich 17.02x
# featureBits -chrom=chr2 danRer3 refGene:cds chainMm6Link -enrichment 
# refGene:cds 0.705%, chainMm6Link 8.219%, both 0.557%, cover 79.04%, 
# enrich 9.62x
# featureBits -chrom=chr2 danRer2 refGene:cds chainMm5Link -enrichment 
# refGene:cds 0.739%, chainMm5Link 4.539%, both 0.579%, cover 78.37%, 
# enrich 17.26x
# looks good, although enrichment is lower than for danRer2 and mm5, there are 
# more chains in the score <10000 range for danRer3 than for danRer2 but 
# this does not make up for all the extra chains in danRer3 over danRer2. 
# Maybe there are more high scoring alignments to the chrUn and chrNA chains 
# due to the scaffolds being used for the alignments.
# danRer3 has a extra sequence compared to danRer2. danRer3 chr2 is 48.2 Mb
# and for danRer2, chr2 is 52 Mb so in this case the chrom is smaller.
# featureBits -chrom=chrNA danRer3 refGene:cds chainMm6Link -enrichment
# refGene:cds 0.449%, chainMm6Link 10.952%, both 0.350%, cover 77.94%, 
# enrich 7.12x
# featureBits -chrom=chrNA danRer2 refGene:cds chainMm5Link -enrichment
# refGene:cds 0.499%, chainMm5Link 4.176%, both 0.372%, cover 74.60%, 
# enrich 17.86x

   # netToAxt was processing nets incorrectly so remake these with 
   # new version of netToAxt and transfer to downloads dir. 
   ssh kkstore02
   cd /cluster/data/danRer3/bed/blastz.mm6.swap
   rm -r axtNet
   # Make axtNet for download: one .axt per danRer3 seq.
   # remake noClass.net
   # Make nets("noClass", i.e. without rmsk/class stats which are added later):
   cd axtChain
chainPreNet danRer3.mm6.all.chain.gz /cluster/data/mm6/bed/blastz.danRer3/S2.len /cluster/data/mm6/bed/blastz.danRer3/S1.len stdout \
| chainNet stdin -minSpace=1 /cluster/data/mm6/bed/blastz.danRer3/S2.len /cluster/data/mm6/bed/blastz.danRer3/S1.len stdout /dev/null \
| netSyntenic stdin noClass.net

   # create net for each chrom again
   netSplit noClass.net net
   # also split up chains again
   mkdir chain
   zcat danRer3.mm6.all.chain.gz | chainSplit chain stdin
   ssh hgwdev
   cd /cluster/data/danRer3/bed/blastz.mm6.swap
   mkdir axtNet
   foreach f (axtChain/net/*.net)
     netToAxt $f axtChain/chain/$f:t:r.chain \
    /cluster/bluearc/danRer3/nib /panasas/store/mm6/nib stdout \
     | axtSort stdin stdout \
     | gzip -c > axtNet/$f:t:r.danRer3.mm6.net.axt.gz
   end

   # cleanup 
   ssh kkstore02 
   cd /cluster/data/danRer3/bed/blastz.mm6.swap/axtChain
   rm noClass.net
   rm -r net
   rm -r chain
   # remake mafNet from the new axtNet
   cd /cluster/data/danRer3/bed/blastz.mm6.swap
   rm -r mafNet
   # Make mafNet for multiz: one .maf per danRer3 seq.
   mkdir mafNet
   foreach f (axtNet/*.danRer3.mm6.net.axt.gz)
     axtToMaf -tPrefix=danRer3. -qPrefix=mm6. $f \      
    /cluster/data/mm6/bed/blastz.danRer3/S2.len /cluster/data/mm6/bed/blastz.danRer3/S1.len \
     stdout \
     | gzip -c > mafNet/$f:t:r:r:r:r:r.maf.gz
   end

   # copy the new axtNet files to downloads and replace old ones
   ssh hgwdev
   rm -r /usr/local/apache/htdocs/goldenPath/danRer3/vsMm6/axtNet
   cd /usr/local/apache/htdocs/goldenPath/danRer3/vsMm6
   mkdir -p /usr/local/apache/htdocs/goldenPath/danRer3/vsMm6/axtNet
   ln -s /cluster/data/danRer3/bed/blastz.mm6.swap/axtNet/*.axt.gz axtNet/
   # remake md5sum.txt 
   rm md5sum.txt
   md5sum *.gz */*.gz > md5sum.txt

# BLASTZ FOR FUGU (fr1) (DONE, 2005-08-18, hartera)
# CREATE CHAIN AND NET TRACKS, AXTNET, MAFNET AND ALIGNMENT DOWNLOADS
  ssh kk
  mkdir /cluster/data/danRer3/bed/blastz.fr1.2005-08-13
  cd /cluster/data/danRer3/bed
  ln -s blastz.fr1.2005-08-13 blastz.fr1
# use parameters for fr1 in makeDanRer2.doc. Using scaffolds makes this run
# slower so it is best to have the scaffolds in the query. Use HoxD55.q 
# matrix as Fugu is quite distant from zebrafish. Blastz uses 
# lineage-specfic repeats but there are none for these two species.
cat << '_EOF_' > DEF
# zebrafish (danRer3) vs. Fugu (fr1)
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
BLASTZ_Q=/cluster/data/blastz/HoxD55.q
BLASTZ_ABRIDGE_REPEATS=0

# TARGET - zebrafish (danRer3)
# soft-masked chroms, 1-25 and M
SEQ1_DIR=/iscratch/i/danRer3/chromNib
SEQ1_RMSK=
# lineage-specific repeats
# we don't have that information for these species
SEQ1_SMSK=
SEQ1_FLAG=
SEQ1_IN_CONTIGS=0
# 10 MB chunk for target
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY - Fugu (fr1)
# soft-masked scaffolds in 2bit format
SEQ2_DIR=/iscratch/i/fr1/UnScaffolds/fr1UnScaffolds.2bit
SEQ2_RMSK=
SEQ2_SMSK=
SEQ2_FLAG=
SEQ2_IN_CONTIGS=0
# 10 Mbase for query
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/cluster/data/danRer3/bed/blastz.fr1

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len

#DEBUG=1
'_EOF_'
    # << this line keeps emacs coloring happy
    chmod +x DEF
    cp /cluster/data/danRer3/chrom.sizes ./S1.len
    # make S2.len for fr1 scaffolds
    twoBitInfo /cluster/data/fr1/fr1UnScaffolds.2bit ./S2.len
    wc -l *.len
    # 28 S1.len
    # 20379 S2.len
    # make output directory
    mkdir -p /cluster/bluearc/danRer3vsfr1Out
    # do blastz and create chains for fr1 scaffolds on danRer3 chr1-25 and chrM 
    # chickenHumanTuned.gap scoring matrix is now used by default 
    # by axtChain.
    nohup nice /cluster/bin/scripts/doBlastzChainNet.pl `pwd`/DEF \
       -blastzOutRoot /cluster/bluearc/danRer3vsfr1Out -chainMinScore=5000 \
       -stop chainMerge >& do.log &
    # Start: Aug 13 10:48 
    # Finish: Aug 13 13:35
    # then run the danRer3 NA and Un scaffolds against fugu scaffolds 
    mkdir NAandUnScaffolds
    cd NAandUnScaffolds
cat << '_EOF_' > DEF
# zebrafish (danRer3) vs. Fugu (fr1)
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
BLASTZ_Q=/cluster/data/blastz/HoxD55.q
BLASTZ_ABRIDGE_REPEATS=0

# TARGET - zebrafish (danRer3)
# soft-masked scaffolds for chrNA and chrUn in 2 bit format
SEQ1_DIR=/iscratch/i/danRer3/NAandUnScafs/danRer3NAandUnScaf.2bit
SEQ1_RMSK=
# lineage-specific repeats
# we don't have that information for these species
SEQ1_SMSK=
SEQ1_FLAG=
SEQ1_IN_CONTIGS=0
# 10 MB chunk for target
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY - Fugu (fr1)
# soft-masked scaffolds in 2bit format
SEQ2_DIR=/iscratch/i/fr1/UnScaffolds/fr1UnScaffolds.2bit
SEQ2_RMSK=
SEQ2_SMSK=
SEQ2_FLAG=
SEQ2_IN_CONTIGS=0
# 10 Mbase for query
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/cluster/data/danRer3/bed/blastz.fr1/NAandUnScaffolds

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len

#DEBUG=1
'_EOF_'
    # << this line keeps emacs coloring happy
    chmod +x DEF
    twoBitInfo /cluster/data/danRer3/danRer3NAandUnScaf.2bit ./S1.len
    # make S2.len for fr1 scaffolds
    twoBitInfo /cluster/data/fr1/fr1UnScaffolds.2bit ./S2.len
    wc -l *.len
    # 14941 S1.len
    # 20379 S2.len
    # make output directory
    mkdir -p /cluster/bluearc/danRer3vsfr1Out/NAandUnScaffolds
    # do blastz and create chains for fr1 scaffolds on danRer3 
    # chrNA and chrUn scaffolds
    nohup nice /cluster/bin/scripts/doBlastzChainNet.pl `pwd`/DEF \
       -blastzOutRoot /cluster/bluearc/danRer3vsfr1Out/NAandUnScaffolds \
       -chainMinScore=5000 -stop chainMerge >& do.log & 
    # Start: Aug 13 14:05
    # Finish: Aug 14 20:58
    # The log file says it is finished. chainSplit was not run as SEQ1 has
    # is not < 100 sequences. Need to do liftUp before running chainSplit.
    cd /cluster/data/danRer3/bed/blastz.fr1/NAandUnScaffolds/axtChain/run
    # Lifting up chains:
    # need to lift these chains up to chrom level for Fugu for chrom run and 
    # for danRer3 and Fugu for the NA and Un scaffolds run.
    # first for Fugu in the danRer3 chrom run
    ssh kkstore02
    cd /cluster/data/danRer3/bed/blastz.fr1/axtChain
    mkdir liftedChain
    foreach f (chain/*.chain)
       set c=$f:t:r
       echo $c
       liftUp -chainQ liftedChain/${c}.lifted.chain \
             /cluster/data/fr1/Un/lift/ordered.lft warn $f
    end
    # lift up for danRer3 scaffolds run.
    ssh kkstore02
    cd /cluster/data/danRer3/bed/blastz.fr1/NAandUnScaffolds/axtChain
    # first lift Fugu fr1 query, there is no split chains here as there
    # were not < 100 sequences for the target.
 zcat danRer3.fr1.all.chain.gz | liftUp -chainQ danRer3.fr1.liftedQall.chain \
          /cluster/data/fr1/Un/lift/ordered.lft warn stdin
    # then liftUp target coords for danRer3 
    liftUp danRer3.fr1.liftedQandTall.chain \
      /cluster/data/danRer3/liftSupertoChrom/liftNAandUnScaffoldsToChrom.lft \
      warn danRer3.fr1.liftedQall.chain
    # gzip lifted danRer3.fr1 chain file
    gzip danRer3.fr1.liftedQandTall.chain
    # merge the chains from the danRer3 chrom run and the danRer3
    # NA and Un scaffolds run. chains are sorted by score and IDs are uniqued.
    cd /cluster/data/danRer3/bed/blastz.fr1/axtChain
    mv danRer3.fr1.all.chain.gz danRer3.fr1.chroms.chain.gz
    set blastz=/cluster/data/danRer3/bed/blastz.fr1
    # copy over lifted chains for danRer3 scaffolds vs fr1 
    cp $blastz/NAandUnScaffolds/axtChain/danRer3.fr1.liftedQandTall.chain.gz \ 
       ./liftedChain
    gunzip ./liftedChain/*.gz
    nice chainMergeSort liftedChain/*.chain \
         | nice gzip -c > danRer3.fr1.all.chain.gz
    # then split up into chains again
    mv chain chromChain
    mkdir chain
    nice zcat danRer3.fr1.all.chain.gz | chainSplit chain stdin
    # then pick up the doBlastzChainNet.pl script at the net step.
    ssh kkstore02
    cd /cluster/data/danRer3/bed/blastz.fr1
    cp DEF DEF.chroms
    # edit DEF file to include the all nib files for danRer3 and the 
    # nib file for the chrUn of Fugu fr1. Since all the coords have now
    # been lifted to chrom level then these are now needed.
    # SEQ1_DIR=/iscratch/i/danRer3/nib
    # SEQ2_DIR=/cluster/bluearc/fugu/fr1/chromNib
    # use kkr1u00 for computationally intensive steps as kolossus is down.
    # need to create new S2.len for whole chrUn for Fugu
    mv S2.len S2.scaffolds.len
    cp /cluster/data/fr1/chrom.sizes S2.len 
    nohup nice /cluster/bin/scripts/doBlastzChainNet.pl `pwd`/DEF \
       -blastzOutRoot /cluster/bluearc/danRer3vsfr1Out -chainMinScore=5000 \
       -workhorse kkr1u00 -continue net >& doNet.log &
    # crashed at cleanup step when trying to access kkstore02 
    # The authenticity of host 'kkstore02 (128.114.50.155)' can't be
    # established.  Re-run from this step.
    nohup nice /cluster/bin/scripts/doBlastzChainNet.pl `pwd`/DEF \
       -blastzOutRoot /cluster/bluearc/danRer3vsfr1Out -chainMinScore=5000 \
       -workhorse kkr1u00 -continue cleanup >& doNet2.log &
    # netToAxt was processing nets incorrectly so remake these with 
    # new version of netToAxt. 
    # and transfer to downloads dir.
    ssh kkstore02
    cd /cluster/data/danRer3/bed/blastz.fr1
    rm -r axtNet
    # Make axtNet for download: one .axt per danRer3 seq.
    # remake noClass.net
    # Make nets("noClass", i.e. without rmsk/class stats which are added later):
    cd axtChain
    chainPreNet danRer3.fr1.all.chain.gz \
/cluster/data/danRer3/bed/blastz.fr1/S1.len /cluster/data/danRer3/bed/blastz.fr1/S2.len stdout \
| chainNet stdin -minSpace=1 /cluster/data/danRer3/bed/blastz.fr1/S1.len \
/cluster/data/danRer3/bed/blastz.fr1/S2.len stdout /dev/null \
| netSyntenic stdin noClass.net
    # create net for each chrom again
    netSplit noClass.net net
    # also split up chains again
    mkdir chain
    zcat danRer3.fr1.all.chain.gz | chainSplit chain stdin
    ssh hgwdev
    cd /cluster/data/danRer3/bed/blastz.fr1
    mkdir axtNet
    foreach f (axtChain/net/*.net)
       netToAxt $f axtChain/chain/$f:t:r.chain \
       /cluster/bluearc/danRer3/nib /cluster/bluearc/fugu/fr1/chromNib stdout \
       | axtSort stdin stdout \
       | gzip -c > axtNet/$f:t:r.danRer3.fr1.net.axt.gz
    end
    # cleanup 
    ssh kkstore02 
    cd /cluster/data/danRer3/bed/blastz.fr1/axtChain
    rm noClass.net
    rm -r net
    rm -r chain
    # remake mafNet from the new axtNet
    cd /cluster/data/danRer3/bed/blastz.fr1
    rm -r mafNet
    mkdir mafNet
    foreach f (axtNet/*.danRer3.fr1.net.axt.gz)
      axtToMaf -tPrefix=danRer3. -qPrefix=fr1. $f \
     /cluster/data/danRer3/bed/blastz.fr1/S1.len /cluster/data/danRer3/bed/blastz.fr1/S2.len \
     stdout \
     | gzip -c > mafNet/$f:t:r:r:r:r:r.maf.gz
    end

    # copy the new axtNet files to downloads and replace old ones
    ssh hgwdev
    rm -r /usr/local/apache/htdocs/goldenPath/danRer3/vsFr1/axtNet
    cd /usr/local/apache/htdocs/goldenPath/danRer3/vsFr1
    mkdir -p /usr/local/apache/htdocs/goldenPath/danRer3/vsFr1/axtNet
    ln -s /cluster/data/danRer3/bed/blastz.fr1/axtNet/*.axt.gz axtNet/
    # remake md5sum.txt 
    rm md5sum.txt
    md5sum *.gz */*.gz > md5sum.txt

    # Check README in downloads section and add a note about how the 
    # unordered chroms were split up into scaffolds.
    # Add trackDb entry for chain and net tracks to 
    # trackDb/zebrafish/danRer3/trackDb.ra 
    # Do swap to get danRer3 chains on Fugu, fr1 - see makeFr1.doc
# featureBits -chrom=chr2 danRer3 refGene:cds chainFr1Link -enrichment
# refGene:cds 0.705%, chainFr1Link 8.960%, both 0.645%, cover 91.53%, 
# enrich 10.22x
# featureBits -chrom=chr2 danRer2 refGene:cds chainFr1Link -enrichment
# refGene:cds 0.739%, chainFr1Link 4.537%, both 0.620%, cover 83.90%, 
# enrich 18.49x
# featureBits -chrom=chrNA danRer3 refGene:cds chainFr1Link -enrichment
# refGene:cds 0.449%, chainFr1Link 7.129%, both 0.399%, cover 88.78%, 
# enrich 12.45x
# featureBits -chrom=chrNA danRer2 refGene:cds chainFr1Link -enrichment
# refGene:cds 0.499%, chainFr1Link 3.901%, both 0.409%, cover 81.90%, 
# enrich 20.99x

# BLASTZ TETRAODON (tetNig1) (in progress, 2005-08-19, hartera)
    # Tetraodon is quite distant from zebrafish, more distant than human/chicken
    # so use the HoxD55.q matrix in for blastz.
    # blastz requires lineage-specific repeats but there are none
    # available between these two fish species 
    ssh kk
    mkdir -p /cluster/data/danRer3/bed/blastz.tetNig1.2005-08-19
    ln -s /cluster/data/danRer3/bed/blastz.tetNig1.2004-08-19 \
          /cluster/data/danRer3/bed/blastz.tetNig1
    cd /cluster/data/danRer3/bed/blastz.tetNig1
# use tetraodon sequence in contigs for dynamic masking - see below
# for dynamic masking: M=50
# NEED TO DO danRer3 chroms vs tetNig1 chroms (non-random) with chroms
# in contigs. lift up after blastz for chroms. carry on to chainMerge
# danRer3 chroms vs tetNig1 randoms and through to chainMerge then lift up
# danRer3 scaffolds vs tetNig1 chroms (non-random) to after blastz, lift up
# for tetNig1, then go to chainMerge, lift up for danRer3.
# danRer3 scaffolds vs tetNi1 randoms as scaffolds through to chainMerge step
# lift up both danRer3 and tetNig1
# merge together all chains from above.

cat << '_EOF_' > DEF
# zebrafish (danRer2) vs. tetraodon (tetNig1)
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin

# use BLASTZ_M=50 in blastz-run1
ALIGN=blastz-run1
BLASTZ=blastz
BLASTZ_H=2500
BLASTZ_Q=HoxD55.q

#BLASTZ_ABRIDGE_REPEATS=1 if SMSK is specified
BLASTZ_ABRIDGE_REPEATS=0

# TARGET - zebrafish (danRer3) chr1-25 and chrM
SEQ1_DIR=/iscratch/i/danRer3/chromNib
SEQ1_RMSK=
# lineage-specific repeats
# we don't have that information for these species
SEQ1_SMSK=
SEQ1_FLAG=
SEQ1_IN_CONTIGS=0
# 0.5 Mb chunk for target
SEQ1_CHUNK=500000
SEQ1_LAP=500
# QUERY - Tetraodon (tetNig1)
# soft-masked chrom nib
SEQ2_DIR=/iscratch/i/tetNig1/contigs
SEQ2_RMSK=
SEQ2_SMSK=
SEQ2_FLAG=
SEQ2_IN_CONTIGS=1
SEQ2_CHUNK=
SEQ2_LAP=

BASE=/cluster/data/danRer3/bed/blastz.tetNig1

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len

#DEBUG=1
'_EOF_'
    # << this line keeps emacs coloring happy
    chmod +x DEF

# BACENDS TRACK (in progress, 2005-06-16, hartera)
    # start again and download files from Sanger ftp site
    ssh kkstore02
    mkdir -p /cluster/data/danRer3/bed/bacends
    cd /cluster/data/danRer3/bed/bacends
    # get contents of ftp directory
    wget --timestamp \
 ftp://ftp.ensembl.org/pub/traces/danio_rerio/fasta/
    # from index.html, grep lines withe cloneEnd 
    grep "cloneEnd" index.html > cloneEnds
    awk 'BEGIN {FS="\""} {print "wget --timestamp",$2;}' cloneEnds \
        > getCloneEnds.csh
    chmod +x getCloneEnds.csh 
    cat getCloneEnds.csh
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/mpgeb-zfish-cloneEnd-1025270298.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/mpgeb-zfish-cloneEnd-1025273988.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/mpgeb-zfish-cloneEnd-1025278580.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/mpgeb-zfish-cloneEnd-1035416745.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/mpgeb-zfish-cloneEnd-1035417824.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/mpgeb-zfish-cloneEnd-1040215846.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/mpgeb-zfish-cloneEnd-1048006071.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/sanger-zfish-CHORI-73-cloneEnd-1114727127.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/sanger-zfish-CHORI-73-cloneEnd-1115222417.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/sanger-zfish-CHORI-73-cloneEnd-1115226483.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/sanger-zfish-CHORI-73-cloneEnd-1115230498.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/sanger-zfish-CHORI-73-cloneEnd-1115234585.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/sanger-zfish-CHORI-73-cloneEnd-1115238038.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/sanger-zfish-CHORI-73-cloneEnd-1115240957.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/sanger-zfish-cloneEnd-1039514906.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/sanger-zfish-cloneEnd-1039603426.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/sanger-zfish-cloneEnd-1039604741.fasta.gz
wget --timestamp ftp://ftp.ensembl.org:21/pub/traces/danio_rerio/fasta/sanger-zfish-cloneEnd-1040231265.fasta.gz
    nice getCloneEnds.csh >& bac.log &
    # PID 17949 Start: Aug 13 12:10
    # check log to see everything downloaded ok and then remove
    rm bac.log index.html
    # unzip files
    gunzip *.gz
    # cat together CHORI73 FASTA files
    cat sanger-zfish-CHORI*.fasta > CHORI73_bacends.fa
    grep '>' CHORI73_bacends.fa > CHORI73.names
    perl -pi.bak -e 's/>//' CHORI73.names
    sort CHORI73.names | uniq > CHORI73.names.sort
    wc -l CHORI73.names.sort
    # 265235 CHORI73.names.sort
    cat sanger-zfish-cloneEnd*.fasta > zfish_bacends.fa
    cat mpgeb-zfish-cloneEnd*.fasta > zfishmpgeb_bacends.fa
    grep '>' zfish_bacends.fa | wc -l
    # 164302
    grep '>' zfishmpgeb_bacends.fa | wc -l
    # 264633
    # compared this list of sequence names for zf_bacends.fa and got more
    # sequences in the zf_bacends.fa - checked and some are in the trace
    # repository and some are not. 
    # for CHORI_73 there are 394 extra sequences in the downloaded file
    # and over 7000 in the original sent. Just use original as the 
    # sequences will probably be filtered out if there are bad alignments
    # get list of sequences for which there are more than 2 
    grep '>' DH_bacends.fa > DH.names
    perl -pi.bak -e 's/>//' DH.names 
    perl -pi.bak -e 's/(CHORI73_[0-9a-z]+)\.[a-z0-9]+.+/$1/' DH.names
    sort DH.names | uniq -c | sort -nr > DH.names.counts 
    awk '{if ($1 > 2) print $2;}' DH.names.counts > DH.names.morethan2
    # translate to upper case and remove leading zeros
    cat DH.names.morethan2 | tr '[a-z]' '[A-Z]' > DH.names.morethan2.upper
    # remove leading 0
    perl -pi.bak -e 's/(CHORI73_[0-9]+[A-Z])0([0-9]+)/$1$2/' \
        DH.names.morethan2.upper
    sort DH.names.morethan2.upper | uniq > DH.names.morethan2.upper.sort
    wc -l *.sort
    # 265235 CHORI73.names.sort
    # 6020 DH.names.morethan2.upper.sort
    comm -12 CHORI73.names.sort DH.names.morethan2.upper.sort | wc
    # 5299
    # so 721 are not in this list so they are probably not in the repository
    # but align these anyway.
    # for those that are then use the versions in CHORI73.names
    comm -12 CHORI73.names.sort DH.names.morethan2.upper.sort \
        > CHORI73.names.touse
    comm -13 CHORI73.names.sort DH.names.morethan2.upper.sort \
        > DHmorethan2.DHonly
    awk '{if ($1 <= 2) print $2;}' DH.names.counts > DH.names.2orless
    # this is list of sequences to get from DH_bacends.fa
    # need to back translate the list in DHmorethan2.DHonly
    cat DHmorethan2.DHonly | tr '[A-Z]' '[a-z]' > DHtmp
    sed -e 's/chori/CHORI/' DHtmp > DHmorethan2.DHonly.format
    # need to put leading zeros back and "." at the end to help
    # pattern matching with grep.
cat << '_EOF_' > addZeros.pl
#/usr/bin/perl -w
use strict;

my ($file);
$file = $ARGV[0];

open (FILE, $file) || die "Can not open $file: $!\n";

while (<FILE>)
{
chomp;
my ($l,$id);
$l = $_;
if ($l =~ /^CHORI73_[0-9]+[a-z][0-9]{2,}/)
   {
   print "$l\\.\n";
   }
elsif($l =~ /^(CHORI73_[0-9]+[a-z])([0-9]{1})/)
  {
  $id = $1 . "0" . $2 . "\\.";
  print "$id\n";
  }
}
close FILE;
'_EOF_'
    chmod +x addZeros.pl
    perl addZeros.pl DHmorethan2.DHonly.format > DHmorethan2.DHonly.format2
    wc -l DHmorethan2.DHonly*
    # 721 DHmorethan2.DHonly
    # 721 DHmorethan2.DHonly.format
    # 721 DHmorethan2.DHonly.format2
    # need to get full sequence names
    grep '>' DH_bacends.fa > DHBacs.fullnames
    perl -pi.bak -e 's/>//' DHBacs.fullnames
    perl -pi.bak -e 's/(CHORI73_[0-9a-z]+\.[a-z0-9A-Z]+) bases.+/$1/' \
         DHBacs.fullnames
    grep -f DHmorethan2.DHonly.format2 DHBacs.fullnames \
            > DHmorethan2.DHonly.fullnames
    wc -l DHmorethan2.DHonly.fullnames
    # 2352 DHmorethan2.DHonly.fullnames
    sort DHmorethan2.DHonly.fullnames > DHmorethan2.DHonly.fullnames.sort
    # do for those with less than 2 sequences to get the full names
   # awk '{print $1"\\.";}' DH.names.2orless > DH.names.2orless2
  #   grep -f DH.names.2orless2 DHBacs.fullnames > DH.fullnames.2orless
    # PID 1625 Start: 22:40
    # grep is slow for this so write script
    perl getFullNames.pl DHBacs.fullnames DH.names.2orless \
         > DH.fullnames.2orless
   
    # do the same for CHORI73.names.touse to get full names
    awk '{print $1"SP6"}' CHORI73.names.touse > CHORI73.namesSP6.touse
    awk '{print $1"T7"}' CHORI73.names.touse > CHORI73.namesT7.touse
    cat CHORI73.namesSP6.touse CHORI73.namesT7.touse \
        > CHORI73.namesSP6andT7.touse
    wc -l CHORI73.names*
    # 265235 CHORI73.names.sort
    # 10598 CHORI73.namesSP6andT7.touse
    # 5299 CHORI73.namesSP6.touse
    # 5299 CHORI73.namesT7.touse
    # 5299 CHORI73.names.touse

    grep '>' CHORI73_bacends.fa > CHORI73.fullnames
    perl -pi.bak -e 's/>//' CHORI73.fullnames
    grep -f CHORI73.namesSP6andT7.touse CHORI73.fullnames \
         > CHORI73.fullnames.touse
    # so get all the sequence records
    ssh kkstore02
    cd /cluster/data/danRer3/bed/bacends
    mkdir bacSeqs
    # get all sequences from DH_bacends.fa that have 2 or less for the clone.
    # This might include cases where there are duplicate reads for one end
    # only but these will go into the singles track anyway.
    faSomeRecords DH_bacends.fa DH.fullnames.2orless ./bacSeqs/DHBacs.2orless.fa
    # get all sequences with more than 2 sequences for that clone but
    # with no sequence in the new downloaded BAC ends sequence file that 
    # has only one sequence for each BAC end.
    faSomeRecords DH_bacends.fa DHmorethan2.DHonly.fullnames.sort \
         ./bacSeqs/DHBacs.2ormore.orig.fa
    # get all sequences for BAC ends where there are more than 2 read for 
    # ends for one clone so there are replicate reads for at least one end.
    # use the sequence in the downloaded CHORI73 set of clone ends for these.
    faSomeRecords CHORI73_bacends.fa CHORI73.fullnames.touse \
         ./bacSeqs/CHORI73.fromDH.morethan2.fa
    cd bacSeqs
    # translate to upper case and remove leading zeros
    cat DHBacs.2orless.fa | tr '[a-z]' '[A-Z]' > DHBacs.2orless.format.fa
    cat DHBacs.2ormore.orig.fa | tr '[a-z]' '[A-Z]' \
        > DHBacs.2ormore.orig.format.fa
    # remove leading 0 and just use name as FASTA header
    # need to leave in a or w as in p1kaSP6 or q1kaT7 or p1kSP6w or q1kT7w
    # these will distinguish replicate reads from the same sequence and will
    # be removed later when the best alignment is selected.
    perl -pi.bak -e \
    's/(CHORI73_[0-9]+[A-Z])0([0-9]+)\.[0-9A-Z]+(AT7|T7|ASP6|SP6|T7W|SP6W) BASES.+/$1$2$3/' \
        DHBacs*format.fa
    cat CHORI73.*.fa DHBacs*.format.fa > CHORI73BACends.fa
    grep '>' CHORI73BACends.fa | wc -l
    # 295722
    # then combine these with the zf_bacends.fa from Sanger which contain
    # the rest of the BAC end sequences.
    cat ../zf_bacends.fa CHORI73BACends.fa > Zv5BACends.fa
    grep '>' Zv5BACends.fa | wc -l
    # 720571
    faSize Zv5BACends.fa 
    # 674252474 bases (10674972 N's 663577502 real 663577502 upper 0 lower) in 
    # 720571 sequences in 1 files Total size: mean 935.7 sd 239.8 
    # min 26 (CHORI73_189M4SP6) max 5403 (zC259G13.zb) median 882
    # N count: mean 14.8 sd 72.4
    # U count: mean 920.9 sd 239.6
    # L count: mean 0.0 sd 0.0
# UP TO HERE
    # blat run of BAC ends on the pitakluster, pk, (as the kilokluster 
    # is down) against the danRer3 genome sequence
    ssh pk
    cd /cluster/data/danRer3/bed/bacends/bacSeqs
    mkdir -p /cluster/bluearc/danRer3/Zv5bacends
    # split up sequence for cluster runs
    faSplit sequence Zv5BACends.fa 20 \
            /cluster/bluearc/danRer3/Zv5bacends/bacends
    mkdir /cluster/data/danRer3/bed/bacends/run
    cd /cluster/data/danRer3/bed/bacends/run
    ls -1S /cluster/bluearc/danRer3/Zv5bacends/*.fa > bacends.lst
    # do blat just for chr1-25 and chrM
    ls -1S /cluster/bluearc/danRer3/trfFa/chr[0-9M]*.fa > chroms.lst
    # make directory for output, do not have output going to /cluster/data dir
    # as it is very large.
    mkdir -p /san/sanvol1/danRer3/bacends/pslChrom
# use Blat parameters as for mm5 and hg17
cat << '_EOF_' > template
#LOOP
/cluster/bin/i386/blat $(path1) $(path2) -ooc=/cluster/bluearc/danRer3/danRer3_11.ooc {check out line+ /san/sanvol1/danRer3/bacends/pslChrom/$(root1)_$(root2).psl}
#ENDLOOP
'_EOF_'
   # << this line keeps emacs coloring happy
    gensub2 chroms.lst bacends.lst template jobList
    para create jobList
    para try, check, push, check, ...
# para time
# Completed: 4160 of 4160 jobs
# 
# CPU time in finished jobs:    2720110s   45335.16m   755.59h   31.48d  0.086 y
# IO & Wait Time:                 33091s     551.52m     9.19h    0.38d  0.001 y
# Average job time:                 662s      11.03m     0.18h    0.01d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            2847s      47.45m     0.79h    0.03d
# Submission to last job:          7415s     123.58m     2.06h    0.09d
    # then run the soft masked scaffolds for chrUn and chrNA 
    # against the BAC ends
    # NEED TO RE-RUN DUE TO POWER OUTAGE
    ssh kkr1u00
    mkdir -p /cluster/data/danRer3/bed/bacends/scaffoldsNAandUnRun
    cd /cluster/data/danRer3/bed/bacends/scaffoldsNAandUnRun 
    ls -1S /cluster/bluearc/danRer3/Zv5bacends/*.fa > bacends.lst
    # do blat just for NA and Un scaffolds
    foreach f (/cluster/bluearc/scratch/danRer3/scaffoldsSoftMask/Zv5_*.fa)
       echo $f >> scafs.lst
    end
    ssh pk
    # make directory for output, do not have output going to /cluster/data dir
    # as it is very large.
    mkdir -p /san/sanvol1/danRer3/bacends/pslNAandUnScafs
    cd /cluster/data/danRer3/bed/bacends/scaffoldsNAandUnRun 
# use Blat parameters as for mm5 and hg17
cat << '_EOF_' > template
#LOOP
/cluster/bin/i386/blat $(path1) $(path2) -ooc=/cluster/bluearc/danRer3/danRer3_11.ooc {check out line+ /san/sanvol1/danRer3/bacends/pslNAandUnScafs/$(root1)_$(root2).psl}
#ENDLOOP
'_EOF_'
   # << this line keeps emacs coloring happy
    gensub2 scafs.lst bacends.lst template jobList
    para create jobList
    para try, check, push, check, ...
# para time

    ssh kkstore01
    # BAC ends sequence files provided by Mario Caccamo at Sanger
    # mc2@sanger.ac.uk
    mkdir -p /cluster/data/danRer3/bed/bacends
    cd /cluster/data/danRer3/bed/bacends

    wget --timestamp ftp://ftp.sanger.ac.uk/pub/mc2/zf_bacends.fa.gz
    wget --timestamp ftp://ftp.sanger.ac.uk/pub/mc2/DH_bacends.fa.gz
    wget --timestamp ftp://ftp.sanger.ac.uk/pub/mc2/bacend_placement.txt.gz
    gunzip *.gz  
    # DH_bacends.fa are from the new library from a doubled haploid zebrafish
    # zf_bacends.fa are from the existing libraries used in danRer2 and danRer1
    # Several reads are present for some of the BAC ends and these have
    # names like p1kaSP6 or q1kaT7 for duplicated reads and p1kSP6w or q1kT7w
    # for multiple reads. In the trace repository, the most recent sequence
    # is stored and the 'a' or 'w' is dropped from the name.
    # for the DH_bacends.fa from the CHORI73 library, the names are 
    # experiment file name                  trace_name
    # ========================              ================
    # CHORI73_139g06.p1kSP6                 CHORI73_139G6SP6
    # CHORI73_165b21.q1kT7                  CHORI73_165B21T7
    # The trace name is that stored in the trace archive with leading zeros
    # dropped and ".p1k" or ".q1k" and lower case changed to upper. 
    ssh kkstore02
    cd /cluster/data/danRer3/bed/bacends
    # check list of prefixes in zf_bacends.fa
    grep '>' zf_bacends.fa > zf.names
    perl -pi.bak -e 's/>//' zf.names
    perl -pi.bak -e 's/^([A-Za-z]+)[0-9]+.+/$1/' zf.names
    sort -u zf.names
    # bZ
    # zC
    # zK
    # zKp
    # in DH_bacends.fa, all are CHORI73_
    # For DH_bacends.fa, need to clean up, change names to Trace archive
    # format as above. Then choose most recent sequence, those that are bad
    # with lots of Ns should be removed at the alignment stage as they will 
    # not pass the Blat or pslReps criteria. 
   #  cat zf_bacends.fa DH_bacends.fa >> Zv5Bacends.fa
  #  faSize Zv5Bacends.fa
    # 680121953 bases (11160014 N's 668961939 real 668961939 upper 0 lower) 
    # in 729101 sequences in 1 files
    # Total size: mean 932.8 sd 242.6 min 26 (CHORI73_189m04.p1kSP6) 
    # max 5717 (CHORI73_255a17.q1kT7) median 882
    # N count: mean 15.3 sd 75.7
    # U count: mean 917.5 sd 242.2
    # L count: mean 0.0 sd 0.0
    wc -l *.fa
    # 6412741 DH_bacends.fa
    # 14700258 Zv5Bacends.fa
    # 8287517 zf_bacends.fa
    grep '>' DH_bacends.fa | wc -l
    # 304252
    grep '>' zf_bacends.fa | wc -l
    # 424849
    # for DH_bacends.fa there are replicate reads. If duplicate plates 
    # have been made (i.e. read names like ..p1kaSP6 or ..q1kaT7) or plates 
    # have been sequenced multiple times (i.e. read names like ..p1kSP6w or 
    # ..q1kT7w), the Sanger trace repository has the most recent read and 
    # dropped the 'a' or 'w' from the trace name.
    # some are not in the repository. They had bad quality reads with a lot 
    # of Ns or runs of the same base. These should be dropped in the 
    # alignment filtering. 
     
    ssh kkr1u00
    cd /cluster/data/danRer3/bed/bacends
    mkdir -p /iscratch/i/danRer3/bacends
    # split up sequence for cluster runs
    faSplit sequence Zv5Bacends.fa 20 /iscratch/i/danRer3/bacends/bacends
    # iSync bacends to kilokluster
    /cluster/bin/iSync
     # FROM OLD RUN BELOW, KEEP FOR NOW FOR PARAMS
     ssh kkstore02
     cd /cluster/data/danRer3/bed/bacends
     nice pslSort dirs raw.psl temp psl
     # PID 28757
     # use pslReps parameters used for mm6
     pslReps -nearTop=0.01 -minCover=0.7 -minAli=0.8 -noIntrons raw.psl \
             bacEnds.psl /dev/null
     # those for hg17
     pslReps -nearTop=0.02 -minCover=0.60 -minAli=0.85 -noIntrons \
                raw.psl  bacEnds2.psl /dev/null
     # see how many align in each case
     awk '{print $10;}' bacEnds.psl | sort | uniq -c \
         | sort -nr > bacEnds.qNames.sort
     awk '{print $10;}' bacEnds2.psl | sort | uniq -c \
         | sort -nr > bacEnds2.qNames.sort
     wc -l bacEnds*qNames.sort
     # 549086 bacEnds2.qNames.sort
     # 519773 bacEnds.qNames.sort
     grep '>' Zv5Bacends.fa | wc -l
     # 729101
     # so 71% of sequences aligned in bacEnds.psl
     # and 75% of sequences aligned in bacEnds2.psl
     # use textHistogram to look at number of alignments
     # bacEnds.psl has 374002 with only 1 alignment
     # bacEnds2.psl has 362364 with only 1 alignment
     # bacEnds.psl - most alignments for 1 sequence is 515, 
     # for bacEnds2.psl - most alignments for 1 sequence is 1272
     # when these are split up into bacEndPairs, bacEndPairsBad and 
     # bacEndSingles, the number of alignments per sequence is reduced
     # so use bacEnds2.psl
     mv bacEnds2.psl bacEnds.psl
     liftUp bacEnds.lifted.psl /cluster/data/danRer3/jkStuff/liftAll.lft \
           warn bacEnds.psl
     wc -l bacEnds.lifted.psl
     # 6150918 bacEnds.lifted.psl
     # clean up
     rm -r temp
     rm *.sort *.hist
     # run pslCheck
     pslCheck bacEnds.lifted.psl >& pslCheck.log
     # distribution of errors by chromosomes:
     # 1 chr15
     # 1 chr17
     # 2 chr18
     # 1 chr19
     # 8 chr20
     # 1 chr5
     # 1 chr8
     # 1 chrNA
     # 621 chrUn
     # total = 637
     # 637/549086 => 0.12%
     
     # Process BAC end alignments
     ssh kkstore02
     mkdir -p /cluster/data/danRer3/bed/bacends/pairs
     mkdir -p /cluster/data/danRer3/bed/bacends/bacends.1
     # Downloaded BAC ends accessions from SRS
     # Go to http://srs.sanger.ac.uk
     # Go to "Select Databanks" tab and check DBGSS
     # Go to "Query Form" tab
     # Select Organism as field and enter "Danio*" as search term
     # Select AllText as field and enter "*Sanger*" as search term
     # Select AllText as filed and enter "T7|SP6" as search term
     # Select a view
     # Download as BACEndAccs.txt to bacend.1 directory 
     cd /cluster/data/danRer3/bed/bacends/bacends.1
     cp /cluster/data/danRer2/bed/ZonLab/bacends/bacends.1/getBacEndInfo.pl .
     # get lists of SP6 and T7 accessions and merge lists
     awk 'BEGIN {FS="\t"}{OFS="\t"} {if ($7 ~ /SP6/) print $3"SP6",$4}' \
         BACEndAccs.txt > BACEndSP6.accs
     awk 'BEGIN {FS="\t"}{OFS="\t"} {if ($7 ~ /T7/) print $3"T7",$4}' \
         BACEndAccs.txt > BACEndT7.accs
     cat BACEndSP6.accs BACEndT7.accs > BACEnd_accessions.txt
     grep '>' ../bacSeqs/Zv5BACends.fa | sed -e 's/>//' > allBacEnds.names
     # get BAC clone accessions:
     # go to http://srs.sanger.ac.uk
     # 1) select EMBL as database under "Select Databanks tab"
     # 2) select Query Form tab and select "Organism Name": Danio*
     # and then "Features:FtQualifier": clone giving the query
     # Query "([embl-Organism:Danio*] & ([embl-FtQualifier:clone*] > parent )) "     # found 825073 entries
     # 3) select the Custom Views tab
     # 4) Select EMBL and EMBL Features, click "Create New View", then select 
     # Accession from EMBL and FtQualifier and FtDescription from EMBL Features
     # and click on "Save New View".
     # 5) Select Results tab and select results, click on "Save"
     # 6) Select Text as Output and select All as Number of entries to 
     # download, select to Save As ASCII text/table and select the custom
     # view from the "Save with view:" pulldown. Click on "Save" and save
     # as BACClones.accs.txt
     # get a list of BAC clones and accessions from this in the format of
     # <column 1> <column 2>: <external name> <accession>
     # create script to parse out clone ID and the accession:
cat << '_EOF_' > getAccsandIds.pl
#!/usr/bin/perl -w
use strict;

my @clonePrefixes = ("CH211-", "ch211-", "DKEY-", "DKEYP-", "RP71-", "BUSM1-", "CH73-", "CHORI-");
my %cloneHash = qw {  
   CH211-  zC
   DKEY-   zK
   DKEYP-  zKp 
   RP71-   bZ
   BUSM1-  dZ
   CH73-   CHORI73_
};

while (<STDIN>)
{
my ($l, @f, $acc, $id, $i, $j, $found, $upperId, $intId, $upperPref, $intPref);
chomp;
$l = $_;
@f = split(/\s+/, $l);
$acc = $f[1];
$id = "";
# set flag for finding an ID
$found = "FALSE"; 
for ($i = 0; $i <= $#f && ($found eq "FALSE"); $i++)
   {
   for ($j = 0; $j <= $#clonePrefixes && ($found eq "FALSE"); $j++)
      {
      if ($f[$i] =~ /^$clonePrefixes[$j]/i)
        {
        $found = "TRUE";
        $id = $f[$i];
        $upperId = $id;
        $upperPref = $clonePrefixes[$j];
        # translate ID to upper case
        $upperId =~ tr/a-z/A-Z/;
        $intId = "";
        if (exists($cloneHash{$upperPref}))
           {
           $intPref = $cloneHash{$upperPref};
           $intId = $upperId;
           # translate external to internal ID
           $intId =~ s/$upperPref/$intPref/;
           }
        print "$intId\t$acc\t$upperId\n";
        }
      }
   }
}
'_EOF_'
     chmod +x getAccsandIds.pl
     perl getAccsandIds.pl < BACClones.accs.txt > BACClonesIdsandAccs.txt
     # Start: Aug 18 12:45 Finish: Aug 18 13:21
     # edit getBacEndInfo.pl to be getBacEndInfov2.pl
     # CHORI73_ is a new prefix, this is for the internal name of 
     # BAC clones from the CHORI73 doubled haploid library.
     cd /cluster/data/danRer3/bed/bacends/pairs
     set bacDir = /cluster/data/danRer3/bed/bacends/bacends.1
     # try different parameters
      ~/bin/i386/pslPairs -tInsert=10000 -minId=0.91 -noBin -min=2000 \
     -max=650000 -slopval=10000 -hardMax=800000 -slop -short -long -orphan \
     -mismatch -verbose ../bacEnds.lifted.psl $bacDir/bacEndPairs.txt \
     all_bacends bacEnds


     
# VEGA
    # get transcripts in transcripts_coords from e-mail from Mario Caccamo
    # at Sanger 06/16/05.
    # also README for Vega
    ssh kkstore01
    mkdir -p /cluster/data/danRer3/bed/vegaGene
    cd /cluster/data/danRer3/bed/vegaGene 

# AUTO UPDATE GENBANK MRNA AND EST AND MGC GENES RUN (DONE, 2005-08-22, markd)
    # align with revised genbank process
    cd ~kent/src/makeDb/genbank
    cvs update -d etc
    # edit etc/genbank.conf to add danRer3, had to run on pk, due to kk
    # being down.  Set temporary locations for server files

# danRer3 (zebrafish)
# Lift file partitions unplaced sequence pseudo-chroms (disabled)
danRer3.serverGenome = /cluster/data/danRer3/danRer3.2bit
##danRer3.clusterGenome = /iscratch/i/danRer3/danRer3.2bit
##danRer3.ooc = /iscratch/i/danRer3/danRer3_11.ooc
danRer3.clusterGenome = /san/sanvol1/scratch/danRer3/danRer3.2bit
danRer3.ooc = /san/sanvol1/scratch/danRer3/danRer3_11.ooc
##danRer3.align.unplacedChroms = chrNA chrUn
##danRer3.lift = /cluster/data/danRer3/liftSupertoChrom/liftNAandUnScaffoldsToChrom.lft
danRer3.lift = no
danRer3.downloadDir = danRer3
danRer3.mgcTables.default = full
danRer3.mgcTables.mgc = all

    # update /cluster/data/genbank/
    make etc-update

    ssh kkstore02
    cd /cluster/data/genbank
    nice bin/gbAlignStep -initial danRer3 &

    # load database when finished
    ssh hgwdev
    cd /cluster/data/genbank
    nice ./bin/gbDbLoadStep -drop -initialLoad  danRer3&


    # enable daily alignment and update of hgwdev
    cd ~kent/src/makeDb/genbank
    cvs update -d etc
    # add danRer3 to:
        etc/align.dbs
        etc/hgwdev.dbs 
    cvs commit
    make etc-update
