#!/bin/csh -f # set emacs mode
exit; # don't actually run this like a script :)
                                                                                
# Danio Rerio (zebrafish) from Sanger, version Zv5 (released 5/20/05)
#  Project website:
#    http://www.sanger.ac.uk/Projects/D_rerio/
#  Assembly notes:
#    http://www.sanger.ac.uk/Projects/D_rerio/Zv5_assembly_information.shtml

# DOWNLOAD SEQUENCE (DONE, 2005-06-06, hartera)
     ssh kkstore01
     mkdir /cluster/store9/danRer3
     ln -s /cluster/store9/danRer3 /cluster/data
     cd /cluster/data/danRer3
     wget --timestamp \
       ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv5release/README
     wget --timestamp \
       ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv5release/stats
     wget --timestamp \
       ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv5release/Zv5.chunks.agp
     wget --timestamp \
       ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv5release/Zv5.scaffolds.agp     wget --timestamp \
       ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv5release/Zv5.fa

# DOWNLOAD MITOCHONDRION GENOME SEQUENCE (DONE, 2005-06-13, hartera)
     ssh kkstore01
     mkdir -p /cluster/data/danRer3/M
     cd /cluster/data/danRer3/M
     # go to http://www.ncbi.nih.gov/ and search Nucleotide for
     # "Danio mitochondrion genome".  That shows the gi number:
     # 8576324 for the accession, AC024175
 # Use that number in the entrez linking interface to get fasta:
     wget -O chrM.fa \
      'http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Text&db=Nucleotide&uid=8576324&dopt=FASTA'
     # Edit chrM.fa: make sure the header line says it is the
     # Danio Rerio mitochondrion complete genome, and then replace the
     # header line with just ">chrM".
     perl -pi.bak -e 's/>.+/>chrM/' chrM.fa
     rm *.bak
     # Make a "pseudo-contig" for processing chrM too:
     mkdir ./chrM_1
     sed -e 's/chrM/chrM_1/' ./chrM.fa > ./chrM_1/chrM_1.fa
     mkdir ./lift
     echo "chrM_1/chrM_1.fa.out" > ./lift/oOut.lst
     echo "chrM_1" > ./lift/ordered.lst
     echo "0     M/chrM_1        16596   chrM    16596" > ./lift/ordered.lft
     # make sure this is tab delimited
# create a .agp file for chrM as hgGoldGapGl and other
# programs require a .agp file so create chrM.agp
    cat << '_EOF_' > ./chrM.agp
chrM       1       16596   1       F       AC024175.3      1       16596   +
'_EOF_'
     # Create a chrM.chunks.agp
     mkdir -p /cluster/data/danRer3/M/agps
     cd /cluster/data/danRer3/M/agps
     awk 'BEGIN {OFS="\t"} \
        {print $1, $2, $3, $4, $5, $6, $7, $8, $1, $7, $8}' ../chrM.agp \
         > chrM.chunks.agp
     # make sure that all these above files are tab delimited

# Create list of chromosomes (DONE, 2005-06-08, hartera)
     ssh kkstore01
     cd /cluster/data/danRer3
     awk '{if ($1 !~ /Zv5/) print $1;}' Zv5.scaffolds.agp \
         | sort -n | uniq > chrom.lst
     cp chrom.lst chrom1to25.lst
     # add chrM
     echo "M" >> chrom.lst
     # add chrUn
     echo "Un" >> chrom.lst
     # add NA
     echo "NA" >> chrom.lst

# MAKE JKSTUFF AND BED DIRECTORIES (DONE, 2005-06-09, hartera)
    ssh kkstore01
    cd /cluster/data/danRer3
    # This used to hold scripts -- better to keep them inline here 
    # Now it should just hold lift file(s) and
    # temporary scripts made by copy-paste from this file.
    mkdir /cluster/data/danRer3/jkStuff
    # This is where most tracks will be built:
    mkdir /cluster/data/danRer3/bed

# GET ADDITIONAL ZEBRAFISH REPBASE LIBRARY FOR REPEATMASKER 
# (DONE, 2005-05-10, hartera)
# Go to http://www.girinst.org/server/RepBase/RepBase10.04.fasta
# and download zebunc.ref containing unclassified zebrafish repeats.
# Need username and password. Copy to /cluster/bluearc/RepeatMasker/Libraries/
     ssh hgwdev
     cd /cluster/bluearc/RepeatMasker/Libraries/
     perl -pi.bak -e 's/>(Dr[0-9]+)/>$1#Unknown \@danio [S:]/' zebunc.ref
     # add to RepeatMasker library
     cat zebunc.ref >> RepeatMasker.lib

# CHECK AGP FILES AND FASTA SIZE CONSISTENCY (DONE, 2005-06-10, hartera)

     # The script, createAgpWithGaps.pl (see next section for creating
     # agps and FASTAs for chrNA and chrUn), was used to create a scaffolds 
     # agp file for chrUn to test the program. The agp output was compared to 
     # that from scaffoldFaToAgp and difference was found in the agp file
     # output for scaffoldFaToAgp which used 990568 as the end co-ordinate for
     # Zv5_scaffold1475 instead of 976101 as in the output from the script. So 
     # the co-ordinate numbering is different from there on. The program, 
     # scaffoldFaToAgp is creating the agp file from the FASTA file
     # so perhaps the sequence is a different size than stated in the agp file.
     # Get sequence and find the size:
     ssh kkstore01
     mkdir test
     cd test
     faOneRecord ../Zv5.fa Zv5_scaffold1475 > Zv5_scaffold1475.fa
     faSize Zv5_scaffold1475.fa
     # 990568 bases
     rm Zv5_scaffold1475.fa 
     # reported this inconsistency to Mario Caccamo at Sanger
     # mc2@sanger.ac.uk (2005-06-09) and new scaffolds and chunks agp files
     # were sent on 2005-06-10. There was a chunk (contig) missing from the 
     # chunks agp file and the scaffold therefore had the wrong end 
     # co-ordinate in the agp files.
     # check all sizes of scaffold sequences against those in the agp files
     ssh kkr1u00
     cd /cluster/data/danRer3 
     mkdir -p /iscratch/i/danRer3/scaffolds
     cp Zv5.fa /iscratch/i/danRer3/scaffolds/
     iSync
     
     ssh kk
     mkdir -p /cluster/data/danRer3/scaffolds/run
     cd /cluster/data/danRer3/scaffolds/run
     grep '>' ../Zv5.fa | sed -e 's/>//' > Zv5.scaffolds.lst
cat << '_EOF_' > getSizes.csh
     #!/bin/csh -fe
     set dir=/cluster/bluearc/danRer3/scaffolds
     faOneRecord /iscratch/i/danRer3/scaffolds/Zv5.fa $1 > $dir/$1.fa
     echo $1 >> $dir/$1.size
     faSize $dir/$1.fa >> $dir/$1.size
     rm $dir/$1.fa
'_EOF_'
     # << this line makes emacs coloring happy
     chmod +x getSizes.csh
cat << '_EOF_' > gsub
#LOOP
getSizes.csh {check in exists $(path1)}
#ENDLOOP
'_EOF_'
     # << this line makes emacs coloring happy 
     gensub2 Zv5.scaffolds.lst single gsub jobList
     para create jobList 
     para try,check,push,check etc...
    
     ssh kkstore01
     cd /cluster/bluearc/danRer3/scaffolds
     foreach f (*.size)
        cat $f >> Zv5.scaffolds.sizes
     end	  
     cd /cluster/data/danRer3/scaffolds
     mv /cluster/bluearc/danRer3/scaffolds/Zv5.scaffolds.sizes .
     # Check that these sizes correspond to the sizes in the scaffolds agp file
     # use script compareSizes.pl
     cat << '_EOF_' > compareSizes.pl
#!/usr/bin/perl -w
use strict;

my ($file, $agp);

$file = $ARGV[0];
$agp = $ARGV[1];

open(FILE, $file) || die "Can not open $file: $!\n";
open(AGP, $agp) || die "Can not open $agp: $!\n";
open(OUT, ">log.txt") || die "Can not create log.txt: $!\n";

my ($l, $name, $size, %scafsHash);
while (<FILE>)
{
$l = $_;
if ($l =~ /^(Zv5_(scaffold|NA)[0-9]+)/)
   {
   $name = $1;
   }
elsif ($l =~ /^([0-9]+)\sbases/)
   {
   $size = $1;  
   $scafsHash{$name} = $size;
   }
}
close FILE;

while (<AGP>)
{
my ($line, @fi, $scaf, $end);
$line = $_;

@fi = split(/\t/, $line);
$scaf = $fi[5];
$end = $fi[7];

if (exists($scafsHash{$scaf}))
   {
   if ($scafsHash{$scaf} eq $end)
      {
      print OUT "$scaf - ok\n";
      }
   else
      {
      print OUT "$scaf - different size to sequence\n";
      }
   }
else
   {
   print OUT "$scaf - does not exist in list of sizes\n";
   }
}
close AGP;
close OUT;
'_EOF_'
   # << happy emacs
   chmod +x compareSizes.pl
   perl compareSizes.pl Zv5.scaffolds.sizes ../Zv5.scaffolds.list
   # the only lines where no ID was found in the list of scaffolds with sizes
   # were those lines for gaps.
   grep "different" Zv5_scaffold1475
   # Zv5_scaffold1475 - different size to sequence
   # so only this scaffold is a different size in the agp to the sequence
   # need to check that sizes are consistent between agp files 
   # check also new agp file for scaffolds - newAgps/Zv5.scaffolds.agp
   perl compareSizes.pl Zv5.scaffolds.sizes ../newAgps/Zv5.scaffolds.agp
   # these are all consistent with the sequence sizes
   cd /cluster/data/danRer3/newAgps/
   # print out scaffold names where the co-ordinates are not consistent
   # with sizes given
   awk '{if ($6 ~ /^Zv5/ && (($3-$2+1) != $8)) print $6;}' Zv5.scaffolds.agp \
       > Zv5.scaffolds.coordCheck 
   # this file is empty so they are ok. do the same for the chunks.agp file
   awk '{if ($6 ~ /^Zv5/ && (($3-$2+1) != $8)) print $6;}' Zv5.chunks.agp \ 
       > Zv5.chunks.coordCheck
   # also empty so ok. check that the difference between $7 and $8 is the
   # same as the difference between $11 and $12 fields
   # 8th and 12th fields should be the same
   awk '{if ($6 != 5000 && (($8 - $7) != ($12 - $11))) print $6;}' \
       Zv5.chunks.agp > Zv5.chunks.coordCheck2
   # these are all ok
   rm Zv5.*.coord*
cat << '_EOF_' > checkSizesInAgps.pl
#!/usr/bin/perl -w
use strict;

my ($ch, $sc, %scafsHash);
$sc = $ARGV[0]; # scaffolds agp
$ch = $ARGV[1]; # chunks or contigs agp

open(SCAFS, $sc) || die "Can not open $sc: $!\n";
open(CHUNKS, $ch) || die "Can not open $ch: $!\n";

while (<SCAFS>)
{
my ($l, @f, $name, $e);
$l = $_;
@f = split(/\t/, $l);
if ($f[5] =~ /^Zv5/)
   {
   $name = $f[5];
   $e = $f[2];
   $scafsHash{$name} = $e;
   }
}
close SCAFS;

my $scaf = "";
my $prev = "";
my $prevEnd = 0;

while (<CHUNKS>)
{
my ($line, @fi);
$line = $_;
@fi = split(/\t/, $line);

if ($fi[5] ne "5000")
   {
   $scaf = $fi[9];
   if (($scaf ne $prev) && ($prev ne ""))
      {
      checkCoords($prev, $prevEnd);
      }
$prev = $scaf;
$prevEnd = $fi[2];
   }
}
# check last entry in file
checkCoords($prev, $prevEnd);
close CHUNKS;

sub checkCoords {
my ($name, $end) = @_;
if (exists($scafsHash{$prev}))
   {
   if ($scafsHash{$prev} != $prevEnd)
      {
      my $ed = $scafsHash{$prev};
      print "Scaffold $prev is not consistent between agps\n";
      }
   else
      {
      my $ed = $scafsHash{$prev};
      print "Scaffold $prev - ok\n";
      }
   }
}
'_EOF_'
   # << happy emacs
   chmod +x checkSizesInAgps.pl
   checkSizesInAgps.pl Zv5.scaffolds.agp Zv5.chunks.agp \
         > Zv5.scafsvschunks
   grep "not consistent" Zv5.scafsvschunks
   # no lines were inconsistency was reported
   wc -l Zv5.scafsvschunks
   # 16214 Zv5.scafsvschunks
   grep "Zv5" Zv5.scaffolds.agp | wc -l
   # 16214
   # so all the scaffolds were checked and were ok.
   cd /cluster/data/danRer3
   mv ./newAgps/Zv5.scaffolds.agp .
   mv ./newAgps/Zv5.chunks.agp
   mv ./scaffolds/compareSizes.pl ./jkStuff/
   mv ./newAgps/checkSizesInAgps.pl ./jkStuff/
   rm -r newAgps

# SPLIT AGP FILES BY CHROMOSOME (DONE, 2005-06-13, hartera)
# FASTA WAS CREATED USING SCAFFOLDS AGP
     ssh kkstore01
     cd /cluster/data/danRer3
     # There are 2 .agp files: one for scaffolds (supercontigs on danRer1) and
     # then one for chunks (contigs on danRer1) showing how they map on to
     # scaffolds.

     # get list of scaffolds from FASTA file and check these are in agp
     grep '>' Zv5.fa | sed -e 's/>//' | sort | uniq > Zv5FaScafs.lst
     # get list of scaffolds from agp - do not print from gap lines
     awk '{if ($7 !~ /contig/) print $6;}' Zv5.scaffolds.agp \
        | sort | uniq > Zv5AgpScafs.lst
     diff Zv5FaScafs.lst Zv5AgpScafs.lst
     # no difference so all scaffolds are in the FASTA file
     # add "chr" prefix for the agp files
     perl -pi -e 's/^([0-9]+)/chr$1/' ./*.agp
     # for chromosomes:
     foreach c (`cat chrom1to25.lst`)
       echo "Processing $c ..."
       mkdir $c
       perl -we "while(<>){if (/^chr$c\t/) {print;}}" \
         ./Zv5.chunks.agp \
         > $c/chr$c.chunks.agp
       perl -we "while(<>){if (/^chr$c\t/) {print;}}" \
         ./Zv5.scaffolds.agp \
         > $c/chr$c.scaffolds.agp
     end

# CREATE AGP FILES FOR chrNA AND chrUn (DONE, 2005-06-13, hartera)
     ssh kkstore01
     # chrNA consists of WGS contigs that could not be related to any 
     # FPC contig and the scaffolds and contigs are named Zv5_NAN in the 
     # first field of the agp files
     cd /cluster/data/danRer3
     mkdir ./NA
     awk '{if ($1 ~ /Zv5_NA/) print;}' Zv5.chunks.agp \
         > ./NA/NA.chunks.agp
     awk '{if ($1 ~ /Zv5_NA/) print;}' Zv5.scaffolds.agp \
         > ./NA/NA.scaffolds.agp
     # change the first field to "chrUn" then can use agpToFa to process
     perl -pi.bak -e 's/Zv5_NA[0-9]+/chrNA/' ./NA/*.agp
     # check files and remove backup files
     rm ./NA/*.bak
     # then process chrUn.
     # Re-make chrUn with new agp files - this is made from scaffolds and  
     # contigs where the name is Zv5_scaffoldN in the first field of the 
     # agp files. These scaffolds and contigs are unmapped to chromosomes
     # in the agp file. chrUn is made up of WGS scaffolds that mapped to 
     # FPC contigs, but the chromosome is unknown.
     rm -r Un
     mkdir ./Un
     awk '{if ($1 ~ /Zv5_scaffold/) print;}' Zv5.chunks.agp \
         > ./Un/Un.chunks.agp
     awk '{if ($1 ~ /Zv5_scaffold/) print;}' Zv5.scaffolds.agp \
         > ./Un/Un.scaffolds.agp
     # change the first field to "chrUn" then can use agpToFa to process
     perl -pi.bak -e 's/Zv5_scaffold[0-9]+/chrUn/' ./Un/*.agp
     # check files and remove backup files
     rm ./Un/*.bak

     # get FASTA file of sequences for NA and Un and create agp with 
     # Ns between scaffolds
     # from scaffolds agp, get name of scaffolds to get from FASTA file for NA
     foreach c (NA Un)
       awk '{print $6;}' $c/$c.scaffolds.agp > $c/chr$c.scaffolds.lst
       $HOME/bin/i386/faSomeRecords /cluster/data/danRer3/Zv5.fa \
          $c/chr$c.scaffolds.lst $c/chr$c.fa
     end
     # check that all scaffolds in list are in FASTA file for NA and Un - ok
     # edit scaffoldFaToAgp.c so that it creates agp with 500Ns between 
     # scaffolds as contig gaps for chrNA and compile. chrNA is already large
     # so the number of Ns are reduced to reduce the size.
     foreach c (NA Un)
        $HOME/bin/i386/scaffoldFaToAgp $c/chr$c.fa
        mv $c/chr$c.fa $c/chr$c.scaffolds.fa
     end
     # change chrUn to chrNA for NA and D to W for NA and Un
     sed -e 's/chrUn/chrNA/' ./NA/chrNA.agp | sed -e 's/D/W/' \
         > ./NA/chrNA.scaffolds.agp
     sed -e 's/D/W/' ./Un/chrUn.agp > ./Un/chrUn.scaffolds.agp
     # edit ./NA/chrNA.scaffolds.agp and ./Un/chrUn.scaffolds.agp and 
     # remove last line as this just adds an extra 500 Ns at the 
     # end of the sequence.
     rm ./NA/chrNA.agp ./Un/chrUn.agp

cat << '_EOF_' > /cluster/data/danRer3/jkStuff/createAgpWithGaps.pl
#!/usr/bin/perl
use strict;

# This script takes a chunks agp and inserts Ns between scaffolds for 
# the chunks (contigs) agp file. Could also insert Ns between scaffolds
# for scaffolds agp.

my ($chrom, $numN, $name, $prev, $st, $end, $prevEnd, $id);
my $chrom = $ARGV[0]; # chromosome name
my $numN = $ARGV[1];  # number of Ns to be inserted 
my $type = $ARGV[2]; # contigs or scaffolds

$prev = "";
$st = 1;
$prevEnd = 0;
$id = 0;

while (<STDIN>)
{
my $l = $_;
my @f = split(/\t/, $l);

if ($type eq "contigs")
   {
   $name = $f[9];
   }
else 
   {
   $name = $f[5]
   }

my $currSt = $f[1];
my $currEnd = $f[2];
my $size = $currEnd - $currSt;

$id++;
$st = $prevEnd + 1;
$end = $st + $size;

if (($prev ne "") && ($prev ne $name))
   {
   $st = $prevEnd + 1;
   $end = ($st + $numN) - 1;
   print "$chrom\t$st\t$end\t$id\tN\t$numN\tcontig\tno\n";
   $prevEnd = $end;
   $id++;
   }

$st = $prevEnd + 1;
$end = $st + $size;
print "$chrom\t$st\t$end\t$id\t$f[4]\t$f[5]\t$f[6]\t$f[7]\t$f[8]";
if ($type eq "contigs")
   {
   print "\t$f[9]\t$f[10]\t$f[11]";
   }

$prevEnd = $end;
$prev = $name;
}
'_EOF_'
     chmod +x /cluster/data/danRer3/jkStuff/createAgpWithGaps.pl
     cd /cluster/data/danRer3
     foreach c (NA Un)
        cd $c
        perl ../jkStuff/createAgpWithGaps.pl chr${c} 500 contigs \
             < ${c}.chunks.agp > chr${c}.chunks.agp
        cd ..
     end
     # check co-ordinates
     # clean up
     foreach c (NA Un)
        rm $c/${c}.scaffolds.agp $c/${c}.chunks.agp $c/chr${c}.scaffolds.fa \
           $c/${c}.scaffolds.lst
     end
   
# BUILD CHROM-LEVEL SEQUENCE (DONE, 2005-06-13, hartera)
     ssh kkstore01
     cd /cluster/data/danRer3
     # Sequence is already in upper case so no need to change
     foreach c (`cat chrom.lst`)
       echo "Processing ${c}"
       $HOME/bin/i386/agpToFa -simpleMultiMixed $c/chr$c.scaffolds.agp chr$c \
         $c/chr$c.fa ./Zv5.fa
       echo "${c} - DONE"
     end
     # move scaffolds agp to be chrom agp and clean up
     foreach c (`cat chrom.lst`)
        cd $c
        rm *.bak
        cp chr${c}.scaffolds.agp chr${c}.agp
        mkdir -p agps
        mv chr${c}.*.agp ./agps/
        cd ..
     end

# CHECK CHROM AND VIRTUAL CHROM SEQUENCES (DONE, 2005-06-13, hartera)
     # Check that the size of each chromosome .fa file is equal to the
     # last coord of the .agp:
     ssh hgwdev
     cd /cluster/data/danRer3
     foreach c (`cat chrom.lst`)
       foreach f ( $c/chr$c.agp )
         set agpLen = `tail -1 $f | awk '{print $3;}'`
         set h = $f:r
         set g = $h:r
         echo "Getting size of $g.fa"
         set faLen = `faSize $g.fa | awk '{print $1;}'`
         if ($agpLen == $faLen) then
           echo "   OK: $f length = $g length = $faLen"
         else
           echo "ERROR:  $f length = $agpLen, but $g length = $faLen"
         endif
       end
     end
     # all are the OK so FASTA files are the expected size

# CREATING DATABASE (DONE, 2005-06-13, hartera)
    # Create the database.
    # next machine
    ssh hgwdev
    echo 'create database danRer3' | hgsql ''
    # if you need to delete that database:  !!! WILL DELETE EVERYTHING !!!
    echo 'drop database danRer2' | hgsql danRer2
    # Delete and re-create database as above (hartera, 2004-11-30)
    # Use df to make sure there is at least 10 gig free on
    df -h /var/lib/mysql
# Before loading data:
# Filesystem            Size  Used Avail Use% Mounted on
# /dev/sdc1             1.8T  927G  734G  56% /var/lib/mysql

# CREATING GRP TABLE FOR TRACK GROUPING (DONE, 2005-06-13, hartera)
    # next machine
    ssh hgwdev
    #  the following command copies all the data from the table
    #  grp in the database danRer1 to the new database danRer2
    echo "create table grp (PRIMARY KEY(NAME)) select * from danRer2.grp" \
      | hgsql danRer3
    # if you need to delete that table:   !!! WILL DELETE ALL grp data !!!
    echo 'drop table grp;' | hgsql danRer3

# BREAK UP SEQUENCE INTO 5MB CHUNKS AT CONTIGS/GAPS FOR CLUSTER RUNS
# (DONE, 2004-06-14, hartera)

     ssh kkstore01
     cd /cluster/data/danRer3
     foreach c (`cat chrom.lst`)
       foreach agp ($c/chr$c.agp)
         if (-e $agp) then
           set fa = $c/chr$c.fa
           echo splitting $agp and $fa
           cp -p $agp $agp.bak
           cp -p $fa $fa.bak
           splitFaIntoContigs $agp $fa . -nSize=5000000
         endif
       end
     end

# MAKE LIFTALL.LFT (DONE, 2005-06-14, hartera)
    ssh kkstore01
    cd /cluster/data/danRer3
    cat */lift/ordered.lft > jkStuff/liftAll.lft 

# SIMPLE REPEAT [TRF] TRACK  (DONE, 2005-06-14, hartera)
    # TRF can be run in parallel with RepeatMasker on the file server
    # since it doesn't require masked input sequence.
    # Run this on the kilokluster. Need to mask contig and chromosome 
    # sequences so run trf using contig sequences.
    # First copy over contig sequences to iscratch and then iSync to cluster.
    ssh kkr1u00
    mkdir -p /iscratch/i/danRer3/contigsNoMask
    cd /cluster/data/danRer3
    foreach d (/cluster/data/danRer3/*/chr*_?{,?})
       set ctg = $d:t
       foreach f ($d/${ctg}.fa)
          echo "Copyig $f ..."
          cp $f /iscratch/i/danRer3/contigsNoMask/
       end
    end
    # 288 sequence files
    /cluster/bin/iSync

    ssh kk
    mkdir -p /cluster/data/danRer3/bed/simpleRepeat
    cd /cluster/data/danRer3/bed/simpleRepeat
    mkdir trf
cat << '_EOF_' > runTrf
#!/bin/csh -fe
#
set path1 = $1
set inputFN = $1:t
set outpath = $2
set outputFN = $2:t
mkdir -p /tmp/$outputFN
cp $path1 /tmp/$outputFN
pushd .
cd /tmp/$outputFN
/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $inputFN /dev/null -bedAt=$outputFN -tempDir=/tmp
popd
rm -f $outpath
cp -p /tmp/$outputFN/$outputFN $outpath
rm -fr /tmp/$outputFN/*
rmdir --ignore-fail-on-non-empty /tmp/$outputFN
'_EOF_'
    # << keep emacs coloring happy
    chmod +x runTrf
                                                                                
cat << '_EOF_' > gsub
#LOOP
./runTrf {check in line+ $(path1)}  {check out line trf/$(root1).bed}
#ENDLOOP
'_EOF_'
    # << keep emacs coloring happy
                                                                                
    ls -1S /iscratch/i/danRer3/contigsNoMask/chr*.fa > genome.lst
    gensub2 genome.lst single gsub jobList
    # 288 jobs
    para create jobList
    para try, check, push, check etc...
    para time
# Completed: 288 of 288 jobs
# CPU time in finished jobs:      70742s    1179.03m    19.65h    0.82d  0.002 y
# IO & Wait Time:                  1263s      21.05m     0.35h    0.01d  0.000 y
# Average job time:                 250s       4.17m     0.07h    0.00d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            6722s     112.03m     1.87h    0.08d
# Submission to last job:         10037s     167.28m     2.79h    0.12d

    # lift up to chrom level
    liftUp simpleRepeat.bed /cluster/data/danRer3/jkStuff/liftAll.lft warn \
           trf/*.bed

    # Load into the database
    ssh hgwdev
    cd /cluster/data/danRer3/bed/simpleRepeat
    hgLoadBed danRer3 simpleRepeat simpleRepeat.bed \
      -sqlTable=$HOME/kent/src/hg/lib/simpleRepeat.sql
    # Loaded 757119 elements of size 16

# PROCESS SIMPLE REPEATS INTO MASK (DONE, 2005-06-14, hartera)
    # After the simpleRepeats track has been built, make a filtered version
    # of the trf output: keep trf's with period <= 12:
    ssh kkstore01
    cd /cluster/data/danRer3/bed/simpleRepeat
    mkdir -p trfMask
    foreach f (trf/chr*.bed)
      awk '{if ($5 <= 12) print;}' $f > trfMask/$f:t
    end

    # Lift up filtered trf output to chrom coords as well:
    cd /cluster/data/danRer3
    mkdir bed/simpleRepeat/trfMaskChrom
    foreach c (`cat chrom.lst`)
      if (-e $c/lift/ordered.lst) then
        perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
          $c/lift/ordered.lst > $c/lift/oTrf.lst
        liftUp bed/simpleRepeat/trfMaskChrom/chr$c.bed \
          jkStuff/liftAll.lft warn `cat $c/lift/oTrf.lst`
      endif
      if (-e $c/lift/random.lst) then
        perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
           $c/lift/random.lst > $c/lift/rTrf.lst
        liftUp bed/simpleRepeat/trfMaskChrom/chr${c}_random.bed \
          jkStuff/liftAll.lft warn `cat $c/lift/rTrf.lst`
      endif
    end

# REPEAT MASKING - Run RepeatMasker on chroms (DONE, 2005-06-16, hartera)
    # When a new library is added for this version of repeatMasker, need to 
    # check in /cluster/bluearc/RepeatMasker/Libraries for a directory made 
    # up of a date e.g. 20050112 here and inside this are species directories
    # for which RepeatMasker has already been run. In this directory it creates
    # a specieslib of the danio repeats. If this exists, this is used for the
    # RepeatMasker run for that species so if new repeats are added to the
    # library, they will not get used unless this is deleted a new specieslib
    # is created using the new library on the first run for danio.
    ssh kkstore01
    rm -r /cluster/bluearc/RepeatMasker/Libraries/20050112/danio/
    cd /cluster/data/danRer3
    #- Split contigs into 500kb chunks, at gaps if possible:
    foreach c (`cat chrom.lst`)
      foreach d ($c/chr${c}*_?{,?})
        cd $d
        echo "splitting $d"
        set contig = $d:t
        ~/bin/i386/faSplit gap $contig.fa 500000 ${contig}_ -lift=$contig.lft \
            -minGapSize=100
        cd ../..
      end
    end

    # For RepeatMasking, use RepeatMasker "open-3.0" with repeat library
    # version RepBase Update 9.11, RM database version 20050112 with the 
    # addition of the zebrafish unclassified repeats (zebunc.ref) - see above
    # section on getting this additional zebrafish RepeatMasker library. 
    #- Make the run directory and job list:
    cd /cluster/data/danRer3
cat << '_EOF_' > jkStuff/RMZebrafish
#!/bin/csh -fe
                                                                                
cd $1
pushd .
/bin/mkdir -p /tmp/danRer3/$2
/bin/cp $2 /tmp/danRer3/$2/
cd /tmp/danRer3/$2
/cluster/bluearc/RepeatMasker/RepeatMasker -ali -s -species danio $2
popd
/bin/cp /tmp/danRer3/$2/$2.out ./
if (-e /tmp/danRer3/$2/$2.align) /bin/cp /tmp/danRer3/$2/$2.align ./
if (-e /tmp/danRer3/$2/$2.tbl) /bin/cp /tmp/danRer3/$2/$2.tbl ./
if (-e /tmp/danRer3/$2/$2.cat) /bin/cp /tmp/danRer3/$2/$2.cat ./
/bin/rm -fr /tmp/danRer3/$2/*
/bin/rmdir --ignore-fail-on-non-empty /tmp/danRer3/$2
/bin/rmdir --ignore-fail-on-non-empty /tmp/danRer3
'_EOF_'
    chmod +x jkStuff/RMZebrafish
    mkdir -p RMRun
    cp /dev/null RMRun/RMJobs
    foreach c (`cat chrom.lst`)
      foreach d ($c/chr${c}_?{,?})
          set ctg = $d:t
          foreach f ( $d/${ctg}_?{,?}.fa )
            set f = $f:t
            echo /cluster/data/danRer3/jkStuff/RMZebrafish \
                 /cluster/data/danRer3/$d $f \
               '{'check out line+ /cluster/data/danRer3/$d/$f.out'}' \
              >> RMRun/RMJobs
          end
      end
    end
    # Do the run
    ssh kk 
    cd /cluster/data/danRer3/RMRun
    para create RMJobs
    para try, para check, para check, para push, para check,...
    para time
# Completed: 4069 of 4069 jobs
# CPU time in finished jobs:   13726314s  228771.90m  3812.87h  158.87d  0.435 y
# IO & Wait Time:                 45762s     762.70m    12.71h    0.53d  0.001 y
# Average job time:                3385s      56.41m     0.94h    0.04d
# Longest running job:                0s       0.00m     0.00h    0.00d
# Longest finished job:            4549s      75.82m     1.26h    0.05d
# Submission to last job:         56947s     949.12m    15.82h    0.66d
# This is slow. It should have taken about 5 hours.

    #- Lift up the 500KB chunk .out's to 5MB ("pseudo-contig") level
    ssh kkstore01
    cd /cluster/data/danRer3
    foreach d (*/chr*_?{,?})
      set contig = $d:t
      echo $contig
      liftUp $d/$contig.fa.out $d/$contig.lft warn $d/${contig}_*.fa.out \
        > /dev/null
    end
                                                                                
    #- Lift pseudo-contigs to chromosome level
    foreach c (`cat chrom.lst`)
      echo lifting $c
      cd $c
      if (-e lift/ordered.lft && ! -z lift/ordered.lft) then
        liftUp chr$c.fa.out lift/ordered.lft warn `cat lift/oOut.lst` \
        > /dev/null
      endif
      cd ..
    end

    #- Load the .out files into the database with:
    ssh hgwdev
    cd /cluster/data/danRer3
    hgLoadOut danRer3 */chr*.fa.out -verbose=2
# bad rep range [689, 602] line 105524 of 16/chr16.fa.out 
# bad rep range [147, 146] line 124027 of 16/chr16.fa.out
# bad rep range [280, 258] line 754 of 17/chr17.fa.out 
# bad rep range [280, 258] line 76417 of 17/chr17.fa.out
# bad rep range [314, 311] line 99427 of 19/chr19.fa.out
# bad rep range [367, 366] line 88398 of 23/chr23.fa.out 
# bad rep range [41, 40] line 51509 of 25/chr25.fa.out
# bad rep range [1133, 1132] line 62610 of 9/chr9.fa.out
# bad rep range [6133, 6132] line 122359 of NA/chrNA.fa.out 
# bad rep range [6133, 6132] line 160183 of NA/chrNA.fa.out 
# bad rep range [292, 291] line 252829 of NA/chrNA.fa.out 
# bad rep range [751, 599] line 261276 of NA/chrNA.fa.out 
# bad rep range [360, 359] line 259794 of Un/chrUn.fa.out 
# bad rep range [360, 359] line 259796 of Un/chrUn.fa.out 
# bad rep range [360, 359] line 259798 of Un/chrUn.fa.out 
# bad rep range [1, -56] line 379516 of Un/chrUn.fa.out
# note: 16 records dropped due to repStart > repEnd

# check coverage of repeats masked
# featureBits -chrom=chr1 danRer1 rmsk
# 11589712 bases of 40488791 (28.624%) in intersection
# featureBits -chrom=chr1 danRer2 rmsk
# 26879295 bases of 61678023 (43.580%) in intersection
# featureBits -chrom=chr1 danRer3 rmsk
# 25822888 bases of 55805710 (46.273%) in intersection

# MASK SEQUENCE WITH REPEATMASKER AND SIMPLE REPEAT/TRF AND BUILD NIB FILES
# (DONE, 2005-06-16, hartera)
    ssh kkstore01
    cd /cluster/data/danRer3
    # Soft-mask (lower-case) the contig and chr .fa's,
    # then make hard-masked versions from the soft-masked.
    set trfCtg=bed/simpleRepeat/trfMask
    set trfChr=bed/simpleRepeat/trfMaskChrom
    # for the chromosomes:
    foreach f (*/chr*.fa)
      echo "repeat- and trf-masking $f"
      maskOutFa -soft $f $f.out $f
      set chr = $f:t:r
      maskOutFa -softAdd $f $trfChr/$chr.bed $f
      echo "hard-masking $f"
      maskOutFa $f hard $f.masked
    end
# This warning is extremely rare -- if it indicates a problem, it is only with
# the repeat annotation and does not affect the masking:
# repeat- and trf-masking Un/chrUn.fa
# WARNING: negative rEnd: -56 chrUn:153329594-153329609 MOSAT_DR
    # for the contigs:
    foreach c (`cat chrom.lst`)
      echo "repeat- and trf-masking contigs of chr$c"
      foreach d ($c/chr*_?{,?})
        set ctg=$d:t
        set f=$d/$ctg.fa
        maskOutFa -soft $f $f.out $f
        maskOutFa -softAdd $f $trfCtg/$ctg.bed $f
        maskOutFa $f hard $f.masked
      end
    end
# same warning here too:
# repeat- and trf-masking contigs of chrUn
# WARNING: negative rEnd: -56 chrUn_26:1159145-1159160 MOSAT_DR
    # check percent sequence masked
    faSize /cluster/data/danRer3/1/chr1.fa
    # 55805710 bases (1047706 N's 54758004 real 28887275 upper 25870729 lower)
    # 46% is in lower case so masked
    # for danRer2:
    faSize /cluster/data/danRer2/1/chr1New.fa
    # 62208023 bases (3421437 N's 58786586 real 31874160 upper 26912426 lower)
    # 43% is in lower case so masked
    # Build nib files, using the soft masking in the fa
    mkdir nib
    foreach f (*/chr*.fa)
      faToNib -softMask $f nib/$f:t:r.nib
    end

# STORING O+O SEQUENCE AND ASSEMBLY INFORMATION  (DONE, 2005-06-16, hartera)
    # Make symbolic links from /gbdb/danRer3/nib to the real nibs
    ssh hgwdev
    cd /cluster/data/danRer3
    mkdir -p /gbdb/danRer3/nib
    foreach f (/cluster/data/danRer3/nib/chr*.nib)
      ln -s $f /gbdb/danRer3/nib
    end

# Load /gbdb/danRer3/nib paths into database and save size info
    # hgNibSeq creates chromInfo table
    hgNibSeq -preMadeNib danRer3 /gbdb/danRer3/nib */chr*.fa
    echo "select chrom,size from chromInfo" | hgsql -N danRer2 > chrom.sizes
    # take a look at chrom.sizes, should be 28 lines
    wc chrom.sizes
    # 28      56     409 chrom.sizes
    
    # Make one big 2bit file as well, and make a link to it in
    # /gbdb/danRer2/nib because hgBlat looks there:
    faToTwoBit */chr*.fa danRer3.2bit
    ln -s /cluster/data/danRer3/danRer3.2bit /gbdb/danRer3/
    # also make 2 bit files for chrUn and chrNA later on - need masked seq

