#!/bin/csh -f # set emacs mode
exit; # don't actually run this like a script :)

# This file describes how we made the browser database on 
# NCBI build 34 (July 18, 2003 freeze)

# HOW TO BUILD A ASSEMBLY FROM NCBI FILES
# ---------------------------------------

# 0) Make gs.17 directory, gs.17/build34 directory, and gs.17/ffa directory.
    mkdir /cluster/store4/gs.17
    mkdir /cluster/store4/gs.17/build34
    mkdir /cluster/store4/gs.17/agp
    mkdir /cluster/store4/gs.17/ffa

#    Make a symbolic link from /cluster/store1 to this location
	
    cd /cluster/store1
    ln -s /cluster/store4/gs.17 ./gs.17

#    Make a symbolic link from your home directory to the build dir:

    ln -s /cluster/store4/gs.17/build34 ~/oo

# NCBI download site:

    ftp ftp.ncbi.nih.gov
    # Login: hgpguest
    # Password: tElo8=Hp (private, do not share!)
    cd build_34    
 
# Download all finished agp's and fa's into gs.17/agp

    mget chr*.agp
    mget chr*.fa.gz
    gunzip *.gz

# Download contig agp's into gs.17/build34

    get ref_placed.agp   # used to be in reference.agp
    get ref_unplaced.agp # used to be in reference.agp
    get DR51.agp
    get DR52.agp
    get PAR.agp          # new for this build - PAR regions added to chrY
    cat ref_placed.agp ref_unplaced.agp DR51.agp DR52.agp > ncbi_build34.agp

# Download contig fa's into gs.17/ffa

    get ref_placed.fa.gz   # used to be in reference.fa
    get ref_unplaced.fa.gz # used to be in reference.fa
    get DR51.fa.gz
    get DR52.fa.gz
    get PAR.fa.gz          # new for this build - PAR regions added to chrY
    cat ref_placed.fa ref_unplaced.fa DR51.fa DR52.fa > ncbi_build34.fa
    *** get sequence.inf ***

# Download assembly related files into gs.17/build34

    get seq_contig.md
    *** get contig_overlaps.agp ***
    
# Sanity check
    /cluster/bin/i386/checkYbr build33/ncbi_build34.agp ffa/ncbi_build34.fa \
      build34/seq_contig.md

# Convert fa files into UCSC style fa files and place in "contigs" directory
# inside the gs.17/build34 directory 

    cd build34
    mkdir contigs
    /cluster/bin/i386/faNcbiToUcsc -split -ntLast ../ffa/ncbi_build34.fa \
      contigs
   
# Copy over chrM contig from previous version
    cd ~/oo
    cp -r gs.16/build33/M .

# Determine the chromosome sizes from agps

    /cluster/bin/scripts/getChromSizes ../agp

# Create lift files (this will create chromosome directory structure) and inserts file
  
    /cluster/bin/scripts/createNcbiLifts -s chrom_sizes seq_contig.md .

# Create contig agp files (will create contig directory structure)
	
    /cluster/bin/scripts/createNcbiCtgAgp seq_contig.md ncbi_build34.agp .

# CREATE STS/FISH/BACENDS/CYTOBANDS DIRECTORY STRUCTURE AND SETUP

# Create directory structure to hold information for these tracks
        cd /projects/hg2/booch/psl/


# Change Makefile parameters for OOVERS, GSVERS, PREVGS, PREVOO
        make new

# Update all Makefiles with latest OOVERS and GSVERS, DATABASE, and locations of .fa files

# Makefile in:
#     /gs.16/build33/
#     /gs.16/build33/bacends
#     /gs.16/build33/cytobands
#     /gs.16/build33/cytoPlots
#     /gs.16/build33/fish
#     /gs.16/build33/fosends
#     /gs.16/build33/g2g
#     /gs.16/build33/geneticPlots
#     /gs.16/build33/primers
#     /gs.16/build33/recombrate
#     /gs.16/build33/sts
#     /gs.16/build33/stsPlots

# Create accession_info file *****
	make accession_info.rdb

# UPDATE STS INFORMATION
# Download and unpack updated information from dbSTS:

    	cd /projects/hg2/booch/psl/update
	wget ftp://ftp.ncbi.nih.gov/repository/dbSTS/dbSTS.sts
	wget ftp://ftp.ncbi.nih.gov/repository/dbSTS/dbSTS.aliases
	#wget ftp://ftp.ncbi.nih.gov/repository/dbSTS/
	wget ftp://ftp.ncbi.nih.gov/blast/db/FASTA/sts.Z
	mv sts.Z dbSTS.FASTA.dailydump.Z
	gunzip dbSTS.FASTA.dailydump.Z

# Edit Makefile to latest sts.X version from PREV, and update STS files
        make update

# Make new directory for this info and move files there
        mkdir /cluster/store1/sts.8
        cp all.STS.fa /cluster/store1/sts.8
        cp all.primers /cluster/store1/sts.8
        cp all.primers.fa /cluster/store1/sts.8

# Copy new files to cluster
        ssh kkstore
        cd /cluster/store1/sts.8
        cp /cluster/store1/sts.8 /*.* /scratch/hg/STS

# Ask for propagation from sysadmin

# Load the sequences (change sts.# to match correct location)
	ssh hgwdev
	mkdir /gbdb/hg16/sts.8
	cd /gbdb/hg16/sts.8
	ln -s /cluster/store1/sts.8/all.STS.fa ./all.STS.fa
	ln -s /cluster/store1/sts.8/all.primers.fa ./all.primers.fa
	*cd /cluster/store2/tmp
	*hgLoadRna addSeq hg16 /gbdb/hg16/sts.8/all.STS.fa
	*hgLoadRna addSeq hg16 /gbdb/hg16/sts.8/all.primers.fa

# UPDATE BACEND SEQUENCES

# **** Sequences were determined to not have changed since bacends.4 *****
# **** No new sequences downloaded ***** 

# UPDATE FISH CLONES INFORMATION

# Download the latest info from NCBI
        # point browser at http://www.ncbi.nlm.nih.gov/genome/cyto/cytobac.cgi?CHR=all&VERBOSE=ctg
        # change "Show details on sequence-tag" to "yes"
        # change "Download or Display" to "Download table for UNIX"
        # press Submit - save as /projects/hg2/booch/psl/fish/hbrc/hbrc.20030723.table

# Format file just downloaded.  
        cd /projects/hg2/booch/psl/fish/

# Edit Makefile to point at file just downloaded (variables HBRC, HBRCFORMAT)
        make HBRC

# (Manually added 21 results from FHCRC)

# Copy it to the new freeze location
        cp /projects/hg2/booch/psl/fish/all.fish.format /projects/hg2/booch/psl/gs.17/build34/fish/

# REPEAT MASKING (WORKING - 2003-07-25 - Hiram)
    # Split contigs, run RepeatMasker, lift results
    # Notes: 
    # * Using new RepeatMasker in /cluster/bluearc/RepeatMasker030619
    # * Contigs (*/NT_*/NT_*.fa) are split into 500kb chunks to make 
    #   RepeatMasker runs manageable on the cluster ==> results need lifting.
    # * For the NCBI assembly we repeat mask on the sensitive mode setting
    #   (RepeatMasker -s)

    #- Split contigs into 500kb chunks:
    ssh eieio
    cd ~/hg16
    mkdir contigOut
    cd contigOut
    mkdir split
    foreach d (../contigs/NT_* )
      set contig = $d:t
	echo "splitting $contig"
      faSplit size ../contigs/$contig 500000 split/${contig}_ -lift=split/$contig.lft \
        -maxN=500000
    end

    #- Make the run directory and job list:
    cd ~/hg16
    mkdir jkStuff
    #  According to RepeatMasker help file, no arguments are required to
    #	specify species because its default is set for primate (human)
    #  This run script saves the .tbl file to be sent to Arian.  He uses
    # those for his analysis.  Sometimes he needs the .cat and .align files for
    # checking problems.  They are pretty big, do not want to save them
    # if not absolutely necessary.

    cat << '_EOF_' > jkStuff/RMHuman
#!/bin/csh -fe

cd $1
pushd .
/bin/mkdir -p /tmp/hg16/$2
/bin/cp $2 /tmp/hg16/$2/
cd /tmp/hg16/$2
/cluster/bluearc/RepeatMasker030619/RepeatMasker -ali -s $2
popd
/bin/cp /tmp/hg16/$2/$2.out ./
# if (-e /tmp/hg16/$2/$2.align) /bin/cp /tmp/hg16/$2/$2.align ./
if (-e /tmp/hg16/$2/$2.tbl) /bin/cp /tmp/hg16/$2/$2.tbl ./
# if (-e /tmp/hg16/$2/$2.cat) /bin/cp /tmp/hg16/$2/$2.cat ./
/bin/rm -fr /tmp/hg16/$2/*
/bin/rmdir --ignore-fail-on-non-empty /tmp/hg16/$2
/bin/rmdir --ignore-fail-on-non-empty /tmp/hg16
'_EOF_'
    chmod +x jkStuff/RMHuman
    mkdir RMRun
    rm -f RMRun/RMJobs
    touch RMRun/RMJobs
   foreach f ( /cluster/store4/gs.17/build34/contigOut/split/NT_*.fa )
        set f = $f:t
        echo /cluster/store4/gs.17/build34/jkStuff/RMHuman \
   		/cluster/store4/gs.17/build34/contigOut/split $f \
            '{'check out line+ /cluster/store4/gs.17/build34/contigOut/split/$f.out'}' \
          >> RMRun/RMJobs
    end

    # We have 5994 jobs in RMJobs:
    wc RMRun/RMJobs
#	5994   41958 1140350 RMRun/RMJobs

    #- Do the run
    ssh kk
    cd ~/hg16/RMRun
    para create RMJobs
    para try, para check, para check, para push, para check,...
    #- While that is running, you can run TRF (simpleRepeat) on the small
    # cluster.  See SIMPLE REPEAT section below


# SIMPLE REPEAT [TRF] TRACK (WORKING - 2003-07-25 - Hiram)
    # Distribute contigs to /iscratch/i
    ssh kkr1u00
    rm -rf /iscratch/i/gs.17/build34/contigs
    mkdir -p /iscratch/i/gs.17/build34/contigs
    cd ~/hg16
    cp -p contigs/*.fa /iscratch/i/gs.17/build34/contigs
    # Make sure the total size looks like what you'd expect:
    du ./contigs /iscratch/i/gs.17/build34/contigs
    # 2839768 ./contigs
    # 2839768 /iscratch/i/gs.17/build34/contigs
    ~kent/bin/iSync

    # Create cluster parasol job like so:
    mkdir -p ~/hg16/bed/simpleRepeat
    cd ~/hg16/bed/simpleRepeat
    mkdir trf
    cat << '_EOF_' > runTrf
#!/bin/csh -fe
#
set path1 = $1
set inputFN = $1:t
set outpath = $2
set outputFN = $2:t
mkdir -p /tmp/$outputFN
cp $path1 /tmp/$outputFN
pushd .
cd /tmp/$outputFN
/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $inputFN /dev/null -bedAt=$outputFN -tempDir=/tmp
popd
rm -f $outpath
cp -p /tmp/$outputFN/$outputFN $outpath
rm -fr /tmp/$outputFN/*
rmdir --ignore-fail-on-non-empty /tmp/$outputFN
'_EOF_'
    chmod +x runTrf

    cat << '_EOF_' > gsub
#LOOP
./runTrf {check in line+ $(path1)}  {check out line trf/$(root1).bed}
#ENDLOOP
'_EOF_'

    ls -1S /iscratch/i/gs.17/build34/contigs/*.fa > genome.lst
    gensub2 genome.lst single gsub spec
    para create spec
    para try
    para check
    para push
    para check
    # When cluster run is done
    liftUp simpleRepeat.bed ~/hg16/jkStuff/liftAll.lft warn trf/*.bed

    # Load into the database:
    ssh hgwdev
    cd ~/hg16/bed/simpleRepeat
    ~matt/bin/i386/hgLoadBed hg16 simpleRepeat simpleRepeat.bed \
      -sqlTable=$HOME/src/hg/lib/simpleRepeat.sql

