# CREATE EMPTY DATABASE AND TABLES.
hgsql -e "create database visiGeneNew" mysql
hgsql visiGeneNew < ~/kent/src/hg/visiGene/visiGene.sql
makeTableDescriptions visiGeneNew ~/kent/src/hg/visiGene/visiGene.as

# LOAD PAUL GRAY/MAHONEY LAB DATA.
# Transferred images from Paul Gray's Mac to mine and converted
# his spreadsheet to a tab-separated file, cloning.tab.
cd ~/kent/src/hg/visiGene/vgLoadMahoney
vgLoadMahoney /gbdb/visiGene mm5 cloning.tab clonePcr.bed outDir
cd outDir
visiGeneLoad whole.ra whole.tab /dev/null -database=visiGeneNew
visiGeneLoad slices.ra slices.tab /dev/null -database=visiGeneNew

# LOAD JACKSON LABS DATA.
# First ask Galt to create a local copy of the Jackson labs
# database.  I'm not sure how he did it.
cd ~/kent/src/hg/visiGene/vgLoadJax
vgLoadJax /gbdb/visiGene jackson visiGene
./loadNew

# Update the privateUser fields where we don't have permissions by entering
# this at the mysql prompt.
update submissionSet,journal set submissionSet.privateUser=-1 
   where (journal.name like 'Nat %' or journal.name = 'Nature')  
   and submissionSet.journal = journal.id and submissionSet.name like 'jax%'

# LOAD NIBB IMAGES
# Do this after creating the nibbImageProbe.fa file as described
# in makeXenTro1.doc, and after creating the nibbImageProbes table
# in hg17 as describe in makeHg17.doc.  The image files are
# loaded in /cluster/store11/visiGene/offline/nibbFrog.
ssh kolossus
cd /cluster/store11/visiGene/offline
nibbParseImageDir nibbFrog nibFrog.tab bad.tab
nibbPrepImages nibbFrog nibFrog.tab \
	/cluster/store11/visiGene/gbdb/200/inSitu/XenopusLaevis/nibb \
	/cluster/store11/visiGene/gbdb/full/inSitu/XenopusLaevis/nibb
# Note the nibbPrepImages step is a 2 day process, next time may
# want to run it on the kki cluster.  It does need to be run on a 64
# bit machine because of bugs in the 32 bit image magick convert program.

ssh hgwdev
cd ~/kent/src/hg/visiGene/vgLoadNibb
hgMapToGene hg17 nibbImageProbes knownGene knownToNibbImage

# Now go into the gene sorter on hg17, configure it to just show
# the name, genbank, and NIBB Xenopus columns.  Filter on * in the
# NIBB Xenopus column (which will get rid of rows with no data in that
# column).  Save the text output to names.raw.  Then get rid of names
# that are no more than genbank accessions as so:
awk '$1 != $2 {printf("%s\t%s\n", $1, $3);}' names.raw > names.txt

# Now create the .tab and .ra files as so:
vgLoadNibb /cluster/store11/visiGene/offline/nibbFrog \
	/cluster/store11/visiGene/offline/nibbFrog.tab \
	/cluster/data/xenTro1/bed/nibbPics/nibbImageProbes.fa \
	names.tab stage.tab outDir
visiGeneLoad outDir/nibb.ra outDir/nibb.tab /dev/null -database=visiGeneNew


# LOAD GENSAT IMAGES
# This was done with the assistance of Mike Dicuccio at NCBI, 
# dicuccio@ncbi.nlm.nih.gov.  If updating probably it's best to
# get in touch with him and make sure that the ftp site is up to
# date.   

# Download data from NCBI into /cluster/store11/visiGene/offline/gensat
cd /cluster/store11/visiGene/offline
mkdir gensat
cd gensat
mkdir RawData
cd RawData
wget --timeStamping ftp://ftp.ncbi.nih.gov/pub/gensat/RawData/GENSAT-20051120.xml.gz
wget --timeStamping ftp://ftp.ncbi.nih.gov/pub/gensat/RawData/NCBI_Gensat-20051120.dtd

# At this point if the dtd has changed you may need to remake 
# kent/src/hg/visiGene/gensat/lib/gs.c with autoXml.  Once
# this is done then do the download with gensatImageDownload.
# It'll take about 3 days. The results will be in the Institutions dir.
cd /cluster/store11/visiGene/offline/gensat
zcat RawData/GENSAT-20051120.xml.gz | gensatImageDownload . download.log

# Create parasol directory and a list of the jpg files.
ssh kki
cd /cluster/store11/visiGene/offline/gensat
mkdir prepImageRun
find Institutions -name '*.jpg' -print | sed 's/Institutions\//' > prepImageRun/jpg.lst
cd prepImageRun

# Create parasol batch
cat << '_EOF_' > gsub
#LOOP
vgPrepImage /cluster/store11/visiGene/offline/gensat/Institutions /cluster/store11/
visiGene/gbdb/200/inSitu/Mouse/gensat /cluster/store11/visiGene/gbdb/full/inSitu/Mo
use/gensat $(path1)
#ENDLOOP
'_EOF_'
# << this line makes emacs coloring happy
gensub2 jpg.lst single gsub spec
para make spec

# Note the above procedure would take about 3 days.  I ended up copying the
# data over to /san/sanvol1, and doing it on the pita cluster.  The job
# there just took two hours, with just 100 cpus available.  It took
# an hour to copy the data over, and eight hours to copy it back though,
# and some tweaking.

# MAKE FULL TEXT INDEX
cd /cluster/store11/visiGene/gbdb
vgGetText visiGene.text mm7 hg17
ixIxx visiGene.text visiGene.ix visiGene.ixx


# (Galt 2006-02)
# RSYNC'd from /cluster/store11/visiGene to /san/sanvol1/visiGene
# and moved the /gbdb/visiGene symlink to point to the new location.
# I also had to manually run a script to find symlinks pointing from full/ over to 
# /cluster/store11/offline and remake them to point correctly to /san/sanvol1/visiGene/offline.

# Allen Brain Atlas jp2 image prep (Galt 2006-02-12)
# Create parasol directory and a list of the jpg files.
ssh pk
cd /san/sanvol1/visiGene/offline/allenBrain
mkdir prepImageRun
find imageDisk -name '*.jp2' -print | sed 's/imageDisk\///' > prepImageRun/jpg.lst
cd prepImageRun
# Create parasol batch
cat << '_EOF_' > gsub
#LOOP
vgPrepImage /san/sanvol1/visiGene/offline/allenBrain/imageDisk /san/sanvol1/visiGene/gbdb/200/inSitu/Mouse/allenBrain /san/sanvol1/visiGene/gbdb/full/inSitu/Mouse/allenBrain $(path1)
#ENDLOOP
'_EOF_'
# << this line makes emacs coloring happy
gensub2 jpg.lst single gsub spec
para make spec -maxNode=50

[pk:/san/sanvol1/visiGene/offline/allenBrain/prepImageRun> /parasol/bin/para time
11748 jobs in batch
4291 jobs (including everybody's) in Parasol queue.
Checking finished jobs
Completed: 11748 of 11748 jobs
CPU time in finished jobs:     474919s    7915.32m   131.92h    5.50d  0.015 y
IO & Wait Time:               5029116s   83818.60m  1396.98h   58.21d  0.159 y
Average job time:                 469s       7.81m     0.13h    0.01d
Longest running job:                0s       0.00m     0.00h    0.00d
Longest finished job:           41811s     696.85m    11.61h    0.48d
Submission to last job:        172301s    2871.68m    47.86h    1.99d


# -maxNode=50 was needed. 
# Note that because it opens up to 40 output files at the same time, it overwhelms NFS
# when a lot of nodes are running, it can bring down the SAN.  Because I was nearly
# done when it came back up, I just re-pushed with -maxNode=50 to keep it under control.
# However in the future, something like this should be done to keep the file access local
# as much as possible.
# Here is the proposed new way:
# -----------------------
cat << '_EOF_' > gsub
#LOOP
./vgPrep.csh $(path1) $(root1) $(file1)
#ENDLOOP
'_EOF_'
# << this line makes emacs coloring happy

cat << '_EOF_' > vgPrep.csh
#!/bin/tcsh
mkdir -p /san/sanvol1/visiGene/gbdb/200/inSitu/Mouse/allenBrain/$1
mkdir -p /san/sanvol1/visiGene/gbdb/full/inSitu/Mouse/allenBrain/$1
cp /san/sanvol1/visiGene/offline/allenBrain/imageDisk/$1 /scratch/tmp/$3
vgPrepImage /san/sanvol1/visiGene/offline/allenBrain/imageDisk /scratch/tmp/vg200$2 /scratch/tmp/vgfull$2 $1
set err = $status
if (! $err ) then
    cp -r /scratch/tmp/vg200$2/* /san/sanvol1/visiGene/gbdb/200/inSitu/Mouse/allenBrain
    cp -r /scratch/tmp/vgfull$2/* /san/sanvol1/visiGene/gbdb/full/inSitu/Mouse/allenBrain
endif
rm -f  /scratch/tmp/$3
rm -fr /scratch/tmp/vg200$2
rm -fr /scratch/tmp/vgfull$2
if ( $err ) then
    exit 1
endif
'_EOF_'
# << this line makes emacs coloring happy

# -----------------------

# ADDED TWO ADDITIONAL ZOOM-OUT LEVELS 5 AND 6:
# Ran /san/sanvol1/offline/level56Run/ cluster jog on a list of all files dumped
#  from the visiGene.imageFile table so that we made new zoom out levels 5 and 6
#  for all pictures.  Since it was a special one-time deal, I just used ImageMagick.
# vgPrepImage.c has been modified to do the 2 new zoomout levels so that they
#  will be built automatically in future.

# Ran several checks to make sure no files were missing, fixed any errors.
# Found embedded space in some nibb filenames, found a couple of gensat 
# images which had previously failed to download and redownloaded them ok.
# Found a few missing things and 0 bytes jpgs and re-ran them.  
# It should be pretty clean right now.

# LOAD ALLEN BRAIN DATA
vgLoadAllen \
 /san/sanvol1/visiGene/gbdb/full/inSitu/Mouse/allenBrain \
 /san/sanvol1/visiGene/offline/allenBrain/probesAndData/allen20051021.tab \
 /cluster/data/mm7/bed/allenBrain/allProbes.fa \
 /cluster/data/mm7/bed/allenBrain/allProbes.tab \
 output
#backed-up data in case of trouble:
mkdir /san/sanvol1/visiGene/dump/visiGene.20060220
hgsqldump visiGene -T /san/sanvol1/visiGene/dump/visiGene.20060220
#load into visiGene db
visiGeneLoad -database=visiGene output/aba.ra output/aba.tab /dev/null

# RE-MAKE knownToVisiGene tables (see respective makedocs for hg17,mm7)
#knownToVisiGene hg17 -fromProbePsl=vgImageProbes
#knownToVisiGene mm7

# Manually added several researchers names to the contributor and submissionContributor tables
# at the request of Susan Sunkin as well as updating the text for contributor, copyright, acknowledgements.
# I manually also updated aba.ra and vgLoadAllen.c to reflect her changes.  The manual mods
# to contributor which work great in the visiGene search are not currently automatically
# supported, and would thus be lost if we ever nuke it and start fresh.
# At some point, we will probably add an additional field to the .ra structure
# and have visiGeneLoad support it.

# RE-MAKE FULL TEXT INDEX
cd hg/visiGene/vgGetText
make alpha
# basically does this, but puts it in cgi-bin/visiGeneData/:
#vgGetText visiGene.text mm7 hg17
#ixIxx visiGene.text visiGene.ix visiGene.ixx
# (hgVisiGene cgi v128 now knows about this new location)

