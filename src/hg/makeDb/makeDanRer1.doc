#!/bin/csh -f # set emacs mode
exit; # don't actually run this like a script :)

# Danio Rerio (zebrafish) from Sanger, version Zv3 (released 11/27/03)
#   Project website:
#    http://www.sanger.ac.uk/Projects/D_rerio/
#  Assembly notes:
#    http://www.sanger.ac.uk/Projects/D_rerio/Zv3_assembly_information.shtml

# DOWNLOAD SEQUENCE (DONE, 2004-05-17, kate)

    ssh kksilo
    mkdir /cluster/store7/danRer1
    ln -s /cluster/store7/danRer1 /cluster/data
    cd /cluster/data/danRer1
    wget ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv3release/README
    wget ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv3release/Zv3.contigs.agp
    wget ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv3release/Zv3.supercontigs.agp
    wget ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv3release/Zv3.supercontigs.fa 
    wget ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv3release/Zv3.supercontigs.fa.tag

# DOWNLOAD MITOCHONDRION GENOME SEQUENCE (DONE, 2004-05-24, hartera)
# CREATE ChrM.agp (DONE, 2004-05-27, hartera)
# Add "chr" prefix to chrM.agp (DONE, 2004-07-06, hartera)
    mkdir /cluster/data/danRer1/M
    cd /cluster/data/danRer1/M
    # go to http://www.ncbi.nih.gov/ and search Nucleotide for
    # "Danio mitochondrion genome".  That shows the gi number:
    # 8576324 for the accession, AC024175
 # Use that number in the entrez linking interface to get fasta:
    wget -O chrM.fa \
      'http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Text&db=Nucleotide&uid=8576324&dopt=FASTA'
    # Edit chrM.fa: make sure the header line says it is the
    # Danio Rerio mitochondrion complete genome, and then replace the
    # header line with just ">chrM".

    # Make a "pseudo-contig" for processing chrM too:
    mkdir M/chrM_1
    sed -e 's/chrM/chrM_1/' M/chrM.fa > M/chrM_1/chrM_1.fa
    mkdir M/lift
    echo "chrM_1/chrM_1.fa.out" > M/lift/oOut.lst
    echo "chrM_1" > M/lift/ordered.lst
    echo "0	M/chrM_1	16596	chrM	16596" > M/lift/ordered.lft

# create a .agp file for chrM as hgGoldGapGl and other
# programs require a .agp file so create chrM.agp
    cat << '_EOF_' > M/chrM.agp
M       1       16596	1       F	AC024175.3      1       16596	+
'_EOF_'

    # Add "chr" prefix to M in chrM.agp (2004-07-06)
    perl -pi.bak -e 's/M/chrM/' ./M/chrM.agp
    # check file then remove backup
    rm ./M/chrM.agp.bak

# SPLIT AGP FILES BY CHROMOSOME (DONE, 2004-05-26, hartera)
    ssh kksilo
    cd /cluster/data/danRer1
    # There are 2 .agp files: one for supercontigs and then one for contigs
    # showing how they map on to supercontigs.

    # split up the agp into one per chrom.
    foreach c ( 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
                                 21 22 23 24 25 )
      mkdir $c
      perl -we "while(<>){if (/^$c\t/) {print;}}" \
        ./Zv3.contigs.agp \
        > $c/chr$c.contigs.agp
      perl -we "while(<>){if (/^$c\t/) {print;}}" \
        ./Zv3.supercontigs.agp \
        > $c/chr$c.supercontigs.agp

     end 
     # From agp files, get supercontigs and contigs for
     # ctg* as chrUn, NA* as chrNA and Finished* as chrFinished
     foreach t ( ctg NA Finished )
        if ($t == "ctg") then 
           set c = "Un"
        else 
           set c = $t
        endif
        mkdir $c
        perl -we "while(<>){if (/^$t/) {print;} }" \
                   ./Zv3.contigs.agp \
        >> $c/chr$c.contigs.agp
        perl -we "while(<>){if (/^$t/) {print;} }" \
                 ./Zv3.supercontigs.agp \
        >> $c/chr$c.supercontigs.agp

     end 

# BUILD CHROM-LEVEL SEQUENCE (DONE, 2004-05-27, hartera)
     ssh kksilo
     cd /cluster/data/danRer1
     # Sequence is already in upper case so no need to change

     foreach c ( 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
                                 21 22 23 24 25)
       echo "Processing ${c}"
       $HOME/bin/i386/agpToFa -simpleMultiMixed $c/chr$c.supercontigs.agp $c \
         $c/chr$c.fa ./Zv3.supercontigs.fa
       echo "${c} - DONE"
     end

     # Need to change the number for each chromosome in the .agp and .fa 
     # files to read "chrN" - should have done this before processing 
     # original sequence and .agp files

     foreach c ( 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
                                 21 22 23 24 25)
       echo "Processing ${c}"
         perl -pi -e 's/^>([0-9]+)/>chr$1/' $c/*.fa
         perl -pi -e 's/^([0-9]+)/chr$1/' $c/*.agp
       echo "${c} - DONE"
     end
     
# CREATE ChrUn, chrNA AND chrFinished FASTA, AGP, GAP AND LIFT FILES FROM 
# ctg*, NA* AND Finished* SUPERCONTIGS 
# (DONE, 2005-05-27, hartera)
# ADD CORRECT FRAGMENT TYPE TO .agp FILES (DONE, 2004-07-07, hartera)
     ssh kksilo
     cd /cluster/data/danRer1
     foreach c ( Finished NA Un )
       awk '{print $1;}' $c/chr$c.supercontigs.agp > $c/chr$c.supercontigs.lst
       $HOME/bin/i386/faSomeRecords /cluster/data/danRer1/Zv3.supercontigs.fa \
          $c/chr$c.supercontigs.lst $c/chr$c.fa
     end 
  
     # check FASTA files then generate AGP and lift files 
     # from the chromosome fastas 
    
     foreach c ( Finished NA Un )
        $HOME/bin/i386/scaffoldFaToAgp $c/chr$c.fa
        mv $c/chr$c.fa $c/chr$c.supercontigs.fa
        perl -pi -e "s/chrUn/chr$c/" $c/chr$c.*
        $HOME/bin/i386/agpToFa -simpleMultiMixed $c/chr$c.agp \
                      chr$c $c/chr$c.fa ./Zv3.supercontigs.fa 
     end
     # chrFinished
     # scaffold gap size is 1000, total scaffolds: 209
     # chrom size is 40167097
     # chrNA
     # scaffold gap size is 1000, total scaffolds: 54798
     # chrom size is 390413307
     # chrUn
     # scaffold gap size is 1000, total scaffolds: 1842
     # chrom size is 367113659
     # Add correct fragment type to .agp files (2004-07-07)
     # chrUn is "W", chrFinished is "F" and chrNA is "W" for all supercontigs
     ssh kksilo
     cd /cluster/data/danRer1
     foreach c (Un NA Finished)
        if ($c == "Un" || $c == "NA") then
           set f = "W"
        else
           set f = "F"
        endif
        perl -pi.bak -e "s/D/$f/;" $c/chr${c}.agp
     end
     # check .agp files and then remove backup files
     foreach c (Un NA Finished)
        rm ./$c/chr${c}.agp.bak
     end

# CHECK CHROM AND VIRTUAL CHROM SEQUENCES (DONE, 2004-05-27, hartera)
     # Check that the size of each chromosome .fa file is equal to the 
     # last coord of the .agp:
     ssh hgwdev
     cd /cluster/data/danRer1
     foreach c ( Finished NA Un )
       foreach f ( $c/chr$c.agp )
         set agpLen = `tail -1 $f | awk '{print $3;}'`
         set g = $f:r
         set faLen = `faSize $g.fa | awk '{print $1;}'`
         if ($agpLen == $faLen) then
           echo "   OK: $f length = $g length = $faLen"
         else
           echo "ERROR:  $f length = $agpLen, but $g length = $faLen"          
         endif
       end
     end

     foreach c ( 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
                                 21 22 23 24 25)
       foreach f ( $c/chr$c.supercontigs.agp )
         set agpLen = `tail -1 $f | awk '{print $3;}'`
         set h = $f:r
         set g = $h:r
         echo "Getting size of $g.fa"
         set faLen = `faSize $g.fa | awk '{print $1;}'`
         if ($agpLen == $faLen) then
           echo "   OK: $f length = $g length = $faLen"
         else
           echo "ERROR:  $f length = $agpLen, but $g length = $faLen"          
         endif
       end
     end
     # All are OK so all Fasta files are the correct size

# BREAK UP SEQUENCE INTO 5MB CHUNKS AT CONTIGS/GAPS FOR CLUSTER RUNS
# (DONE, 2005-05-27, hartera)
    
     ssh kksilo
     cd /cluster/data/danRer1
     foreach c ( 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
                                  21 22 23 24 25 Finished NA Un )
       foreach agp ($c/chr$c.{,contigs.}agp)
         if (-e $agp) then
           set fa = $c/chr$c.fa
           echo splitting $agp and $fa
           cp -p $agp $agp.bak
           cp -p $fa $fa.bak
           splitFaIntoContigs $agp $fa . -nSize=5000000
         endif
       end
     end

# Create list of chromsosomes (DONE, 2004-05-27, hartera)
    ssh hgwdev
    cd /cluster/data/danRer1
    foreach f (*/*.agp)
      set chr = `echo $f:h | sed -e 's/^chr//'`
      echo $chr >> chrom
    end
    sort -n chrom | uniq > chrom.lst
    rm chrom

# MAKE JKSTUFF AND BED DIRECTORIES (DONE, 2004-05-27, hartera)
    # This used to hold scripts -- better to keep them inline here 
    # Now it should just hold lift file(s) and
    # temporary scripts made by copy-paste from this file.
    mkdir /cluster/data/danRer1/jkStuff
    # This is where most tracks will be built:
    mkdir /cluster/data/danRer1/bed

# CREATING DATABASE (DONE, 2004-05-27 - hartera)   
    # Create the database.
    # next machine                                           
    ssh hgwdev
    echo 'create database danRer1' | hgsql ''
    # if you need to delete that database:  !!! WILL DELETE EVERYTHING !!!
    echo 'drop database danRer1' | hgsql danRer1
    # Use df to ake sure there is at least 5 gig free on
    df -h /var/lib/mysql
# Before loading data:
# Filesystem            Size  Used Avail Use% Mounted on
# /dev/sdc1             1.8T  303G  1.4T  19% /var/lib/mysql

# CREATING GRP TABLE FOR TRACK GROUPING (DONE, 2004-05-27, hartera)
    # next machine
    ssh hgwdev
    #  the following command copies all the data from the table
    #  grp in the database galGal2 to our new database ce2
    echo "create table grp (PRIMARY KEY(NAME)) select * from galGal2.grp" \
      | hgsql danRer1
    # if you need to delete that table:   !!! WILL DELETE ALL grp data !!!
    echo 'drop table grp;' | hgsql danRer1

# REPEAT MASKING - Run RepeatMasker on chroms (DONE, 2004-05-28, hartera)    
#- Split contigs into 500kb chunks, at gaps if possible:
    ssh kksilo
    cd /cluster/data/danRer1
    foreach c (`cat chrom.lst`)
      foreach d ($c/chr${c}*_?{,?})
        cd $d
        echo "splitting $d"
        set contig = $d:t
        ~/bin/i386/faSplit gap $contig.fa 500000 ${contig}_ -lift=$contig.lft \
            -minGapSize=100
        cd ../..
      end
    end

#- Make the run directory and job list:
    cd /cluster/data/danRer1
    cat << '_EOF_' > jkStuff/RMZebrafish
#!/bin/csh -fe
                                                                                
cd $1
pushd .
/bin/mkdir -p /tmp/danRer1/$2
/bin/cp $2 /tmp/danRer1/$2/
cd /tmp/danRer1/$2
/cluster/bluearc/RepeatMasker/RepeatMasker -ali -s -spec danio $2
popd
/bin/cp /tmp/danRer1/$2/$2.out ./
if (-e /tmp/danRer1/$2/$2.align) /bin/cp /tmp/danRer1/$2/$2.align ./
if (-e /tmp/danRer1/$2/$2.tbl) /bin/cp /tmp/danRer1/$2/$2.tbl ./
if (-e /tmp/danRer1/$2/$2.cat) /bin/cp /tmp/danRer1/$2/$2.cat ./
/bin/rm -fr /tmp/danRer1/$2/*
/bin/rmdir --ignore-fail-on-non-empty /tmp/danRer1/$2
/bin/rmdir --ignore-fail-on-non-empty /tmp/danRer1
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x jkStuff/RMZebrafish
    mkdir RMRun
    cp /dev/null RMRun/RMJobs
    foreach c (`cat chrom.lst`)
      foreach d ($c/chr${c}_?{,?})
          set ctg = $d:t
          foreach f ( $d/${ctg}_?{,?}.fa )
            set f = $f:t
            echo /cluster/data/danRer1/jkStuff/RMZebrafish \
                 /cluster/data/danRer1/$d $f \
               '{'check out line+ /cluster/data/danRer1/$d/$f.out'}' \
              >> RMRun/RMJobs
          end
      end
    end

    #- Do the run
    ssh kk
    cd /cluster/data/danRer1/RMRun
    para create RMJobs
    para try, para check, para check, para push, para check,...
# para time
# Completed: 3623 of 3623 jobs
# CPU time in finished jobs:    6514173s  108569.55m  1809.49h   75.40d  0.207 y
# IO & Wait Time:                 34466s     574.43m     9.57h    0.40d  0.001 y
# Average job time:                1808s      30.13m     0.50h    0.02d
# Longest job:                     2539s      42.32m     0.71h    0.03d
# Submission to last job:         23551s     392.52m     6.54h    0.27d

    #- Lift up the 500KB chunk .out's to 5MB ("pseudo-contig") level
    ssh kksilo
    cd /cluster/data/danRer1
    foreach d (*/chr*_?{,?})
      set contig = $d:t
      echo $contig
      liftUp $d/$contig.fa.out $d/$contig.lft warn $d/${contig}_*.fa.out \
        > /dev/null
    end

    #- Lift pseudo-contigs to chromosome level
    foreach c (`cat chrom.lst`)
      echo lifting $c
      cd $c
      if (-e lift/ordered.lft && ! -z lift/ordered.lft) then
        liftUp chr$c.fa.out lift/ordered.lft warn `cat lift/oOut.lst` \
        > /dev/null
      endif
      cd ..
    end

    #- Load the .out files into the database with:
    ssh hgwdev
    cd /cluster/data/danRer1
    hgLoadOut danRer1 */chr*.fa.out

# MAKE LIFTALL.LFT (DONE, 2004-05-28, hartera)
    ssh kksilo
    cd /cluster/data/danRer1
    cat */lift/ordered.lft > jkStuff/liftAll.lft

# SIMPLE REPEAT [TRF] TRACK  (DONE, 2004-06-07, hartera)
    # TRF runs pretty quickly now... it takes a few hours total runtime, 
    # so instead of binrsyncing and para-running, just do this on the
    # local fileserver
    ssh kksilo
    mkdir -p /cluster/data/danRer1/bed/simpleRepeat
    cd /cluster/data/danRer1/bed/simpleRepeat
    mkdir trf
    cp /dev/null jobs.csh
    foreach d (/cluster/data/danRer1/*/chr*_?{,?})
      set ctg = $d:t
      foreach f ($d/${ctg}.fa)
        set fout = $f:t:r.bed
        echo $fout
        echo "/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $f /dev/null -bedAt=trf/$fout -tempDir=/tmp" \
        >> jobs.csh
      end
    end

    chmod a+x jobs.csh
    csh -ef jobs.csh >&! jobs.log &
    # check on this with
    tail -f jobs.log
    wc -l jobs.csh
    ls -1 trf | wc -l
    endsInLf trf/*
# trf crashes with chr10_5 so remove from job list for now and run the rest 
# of the jobs.  use jobs1.csh
# contacted authors of trf - they are going to send a new binary. In the
# meantime, they ran chr10_5.fa through the fixed version of trf
# Need to change filenames from chr10_5.fa.* to chr10_5.tf.*
    mkdir /cluster/data/danRer1/bed/simpleRepeat/test
    cd /cluster/data/danRer1/bed/simpleRepeat/test
    mkdir trf

    # Download and unzip results.zip file in this directory
cat << '_EOF_' > changefileName.pl
#!/usr/bin/perl -w
use strict;
                                                                                
while (<STDIN>) {
    chomp;
    my $a = $_;
    if ($a =~ /^chr10_5/) {
       print "cp $a ";
       $a =~ s/chr10_5.fa/chr10_5.tf/;
       print "$a\n";
    }
}
'_EOF_'

    # change filenames and copy to /tmp/ directory
    ls > fileList
    perl changeFilename.pl < fileList > newFiles
    cp chr10_5.tf* /tmp/

    # Edit ~/kent/src/hg/trfBig/trfBig.c to remove calls to trfSysCall 
    # so trf is not used but output files are post processed
    # make and run trfBig
    /cluster/home/hartera/bin/i386/trfBig -trf=/cluster/bin/i386/trf \
            /cluster/data/danRer1/10/chr10_5/chr10_5.fa /dev/null \
            -bedAt=trf/chr10_5.bed -tempDir=/tmp

    # copy bed file output to directory of bed files for the other sequences
    cp /cluster/data/danRer1/bed/simpleRepeat/test/trf/chr10_5.bed \
       /cluster/data/danRer1/bed/simpleRepeat/trf

    liftUp simpleRepeat.bed /cluster/data/danRer1/jkStuff/liftAll.lft warn \
      trf/*.bed

    # Load into the database:
    ssh hgwdev
    hgLoadBed danRer1 simpleRepeat \
      /cluster/data/danRer1/bed/simpleRepeat/simpleRepeat.bed \
      -sqlTable=$HOME/src/hg/lib/simpleRepeat.sql

# PROCESS SIMPLE REPEATS INTO MASK (DONE, 2004-06-07, hartera)
    # After the simpleRepeats track has been built, make a filtered version
    # of the trf output: keep trf's with period <= 12:
    ssh kksilo
    cd /cluster/data/danRer1/bed/simpleRepeat
    mkdir -p trfMask
    foreach f (trf/chr*.bed)
      awk '{if ($5 <= 12) print;}' $f > trfMask/$f:t
    end
    # Lift up filtered trf output to chrom coords as well:
    cd /cluster/data/danRer1
    mkdir bed/simpleRepeat/trfMaskChrom
    foreach c (`cat chrom.lst`)
      if (-e $c/lift/ordered.lst) then
        perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
          $c/lift/ordered.lst > $c/lift/oTrf.lst
        liftUp bed/simpleRepeat/trfMaskChrom/chr$c.bed \
          jkStuff/liftAll.lft warn `cat $c/lift/oTrf.lst`
      endif
      if (-e $c/lift/random.lst) then
        perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
           $c/lift/random.lst > $c/lift/rTrf.lst
        liftUp bed/simpleRepeat/trfMaskChrom/chr${c}_random.bed \
          jkStuff/liftAll.lft warn `cat $c/lift/rTrf.lst`
      endif
    end

# MASK SEQUENCE WITH REPEATMASKER AND SIMPLE REPEAT/TRF 
# (DONE, 2004-06-07, hartera)
    ssh kksilo
    cd /cluster/data/danRer1
    # Soft-mask (lower-case) the contig and chr .fa's,
    # then make hard-masked versions from the soft-masked.
    set trfCtg=bed/simpleRepeat/trfMask
    set trfChr=bed/simpleRepeat/trfMaskChrom
    foreach f (*/chr*.fa)
      echo "repeat- and trf-masking $f"
      maskOutFa -soft $f $f.out $f
      set chr = $f:t:r
      maskOutFa -softAdd $f $trfChr/$chr.bed $f
      echo "hard-masking $f"
      maskOutFa $f hard $f.masked
    end
# This warning is extremely rare -- if it indicates a problem, it's only
# with the repeat annotation and doesn't affect the masking.
# WARNING: negative rEnd: -54 chr5:3971603-3971634 (TCTG)n
# WARNING: negative rEnd: -31 chrNA:145096456-145096484 (TTTG)n
# WARNING: negative rEnd: -29 chrNA:206929524-206929579 (TCCA)n

    foreach c (`cat chrom.lst`)
      echo "repeat- and trf-masking contigs of chr$c"
      foreach d ($c/chr*_?{,?})
        set ctg=$d:t
        set f=$d/$ctg.fa
        maskOutFa -soft $f $f.out $f
        maskOutFa -softAdd $f $trfCtg/$ctg.bed $f
        maskOutFa $f hard $f.masked
      end
    end
# same warning here too. 
# WARNING: negative rEnd: -31 chrNA_29:4620077-4620105 (TTTG)n
# WARNING: negative rEnd: -29 chrNA_42:1194625-1194680 (TCCA)n
# WARNING: negative rEnd: -54 chr5_1:3971603-3971634 (TCTG)n
# sent these to Arian 
    # Build nib files, using the soft masking in the fa
    mkdir nib
    foreach f (*/chr*.fa)
      faToNib -softMask $f nib/$f:t:r.nib
    end

# STORING O+O SEQUENCE AND ASSEMBLY INFORMATION  (DONE, 2004-06-08 - hartera)
    # Make symbolic links from /gbdb/galGal2/nib to the real nibs.
    ssh hgwdev
    cd /cluster/data/danRer1
    mkdir -p /gbdb/danRer1/nib
    foreach f (/cluster/data/danRer1/nib/chr*.nib)
      ln -s $f /gbdb/danRer1/nib
    end
    # Load /gbdb/ce2/nib paths into database and save size info
    # hgNibSeq creates chromInfo table
    hgNibSeq -preMadeNib danRer1 /gbdb/danRer1/nib */chr*.fa
    echo "select chrom,size from chromInfo" | hgsql -N danRer1 > chrom.sizes
    # take a look at chrom.sizes, should be 29 lines
    wc chrom.sizes
    
    # Make one big 2bit file as well, and make a link to it in
    # /gbdb/danRer1/nib because hgBlat looks there:
    faToTwoBit */chr*.fa danRer1.2bit
    ln -s /cluster/data/danRer1/danRer1.2bit /gbdb/danRer1i/nib/

# MAKE GOLD AND GAP TRACKS (DONE, 2004-06-08, hartera)
# REMAKE GOLD AND GAP TRACKS (DONE, 2004-07-07, hartera)
    ssh hgwdev
    cd /cluster/data/danRer1
    # the gold and gap tracks are created from the chrN.agp file and this is
    # the contigs agp so need to use chrN.supercontigs.agp to create
    # an assembly scaffolds track as for panTro1
   #mv chrN.agp to a "contigs" dir and rename chrN.supercontigs.agp to chrN.agp
    foreach c (1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
               21 22 23 24 25)
       mkdir ./$c/contigs
       cp ./$c/chr${c}.agp ./$c/contigs/
       mv ./$c/chr${c}.contigs.agp ./$c/contigs/
       mv ./$c/chr${c}.supercontigs.agp ./$c/chr${c}.agp
    end
                                                                                
    # check track and delete the chrN_supercontigs gap and gold tables
    foreach c (1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
               21 22 23 24 25)
       hgsql -e "drop table chr${c}_supercontigs_gold;" danRer1
       hgsql -e "drop table chr${c}_supercontigs_gap;" danRer1
    end
    hgGoldGapGl -noGl -chromLst=chrom.lst danRer1 /cluster/data/danRer1 .

# MAKE HGCENTRALTEST ENTRY AND TRACKDB TABLE FOR DANRER1 
# (DONE, 2004-06-08, hartera)

    # Make trackDb table so browser knows what tracks to expect:
    ssh hgwdev
    mkdir -p ~/kent/src/hg/makeDb/trackDb/zebrafish/danRer1

    cd $HOME/kent/src/hg/makeDb/trackDb
    cvs up -d -P
    # Edit that makefile to add danRer1 in all the right places and do
    make update
    make alpha
    cvs commit -m "Added danRer1." makefile

    # Add dbDb and defaultDb entries:
    echo 'insert into dbDb (name, description, nibPath, organism,  \
          defaultPos, active, orderKey, genome, scientificName,  \
          htmlPath, hgNearOk, hgPbOk, sourceName)  \
          values("danRer1", "Nov. 2003", \
          "/gbdb/danRer1/nib", "Zebrafish", "chr1:1000-4000", 1, \
          41, "Zebrafish", "Danio Rerio", \
          "/gbdb/danRer1/html/description.html", 0, 0, \
          "Sanger Centre, Danio rerio Sequencing Project Zv3");' \
    | hgsql -h genome-testdb hgcentraltest
    echo 'insert into defaultDb (genome, name) \
          values ("Zebrafish", "danRer1");' \ 
          | hgsql -h genome-testdb hgcentraltest
# Move Zebrafish so it is between Chicken and Fugu in the organism list
    echo 'update dbDb set orderKey = 38 where name = "danRer1";' \
         | hgsql -h genome-testdb hgcentraltest

# MAKE DESCRIPTION/SAMPLE POSITION HTML PAGE (DONE, 2004-06-30, hartera)
    ssh hgwdev
    mkdir /cluster/data/danRer1/html
    cd /cluster/data/danRer1/html
    # make a symbolic link from /gbdb/danRer1/html to /cluster/data/danRer1/html    ln -s /cluster/data/danRer1/html /gbdb/danRer1/html
    # Add a description page for zebrafish
    cd /cluster/data/danRer1/html
    cp /cluster/data/fr1/html/*.html .
    # Edit this for zebrafish
    
    # create a description.html page here
    mkdir -p ~/kent/src/hg/makeDb/trackDb/zebrafish/danRer1
    cd ~/kent/src/hg/makeDb/trackDb/
    cvs add zebrafish
    cvs commit zebrafish
    cd zebrafish
    cvs add danRer1
    cvs commit danRer1
    # Add description page here too
    cp /cluster/data/danRer1/html/description.html \
       $HOME/kent/src/hg/makeDb/trackDb/zebrafish/danRer1/
    chmod a+r \
          $HOME/kent/src/hg/makeDb/trackDb/zebrafish/danRer1/description.html
    cd $HOME/kent/src/hg/makeDb/trackDb/zebrafish/danRer1/
    # Check it in and copy (ideally using "make alpha" in trackDb) to
    # /gbdb/danRer1/html
    cvs add description.html
    cvs commit description.html

# PUT MASKED SEQUENCE OUT FOR CLUSTER RUNS (DONE, 2004-06-09, hartera)
    ssh kkr1u00
    # Chrom-level mixed nibs that have been repeat- and trf-masked:
    rm -rf /iscratch/i/danRer1/nib
    mkdir -p /iscratch/i/danRer1/nib
    cp -p /cluster/data/danRer1/nib/chr*.nib /iscratch/i/danRer1/nib
    # Pseudo-contig fa that have been repeat- and trf-masked:
    rm -rf /iscratch/i/danRer1/trfFa
    mkdir /iscratch/i/danRer1/trfFa
    foreach d (/cluster/data/danRer1/*/chr*_?{,?})
      cp $d/$d:t.fa /iscratch/i/danRer1/trfFa
    end
    rm -rf /iscratch/i/danRer1/rmsk
    mkdir -p /iscratch/i/danRer1/rmsk
    cp -p /cluster/data/danRer1/*/chr*.fa.out /iscratch/i/danRer1/rmsk
    cp -p /cluster/data/danRer1/danRer1.2bit /iscratch/i/danRer1/
    iSync

# CREATE gc5Base wiggle TRACK (DONE, 2004-06-09, hartera)
    ssh kki
    mkdir /cluster/data/danRer1/bed/gc5Base
    cd /cluster/data/danRer1/bed/gc5Base
    #   in the script below, the 'grep -w GC' selects the lines of
    #   output from hgGcPercent that are real data and not just some
    #   information from hgGcPercent.  The awk computes the number
    #   of bases that hgGcPercent claimed it measured, which is not
    #   necessarily always 5 if it ran into gaps, and then the division
    #   by 10.0 scales down the numbers from hgGcPercent to the range
    #   [0-100].  Two columns come out of the awk print statement:
    #   <position> and <value> which are fed into wigAsciiToBinary through
    #   the pipe.  It is set at a dataSpan of 5 because each value
    #   represents the measurement over five bases beginning with
    #   <position>.  The result files end up in ./wigData5.
    #   A new script is used (from makeHg17.doc) which gets around the
    #   problem that wigAsciiToBinary was calculating chromEnd to be 
    #   beyond the real chromosome end 

    mkdir wigData5 dataLimits5
    cat << '_EOF_' > kkRun.sh
#!/bin/sh
NIB=$1
                                                                                
chr=${NIB/.nib/}
chrom=${chr#chr}
                                                                                
hgGcPercent -chr=${chr} -doGaps -file=stdout -win=5 danRer1 \
        /iscratch/i/danRer1/nib | \
    grep -w GC | \
    awk '{if (($3-$2) >= 5) {printf "%d\t%.1f\n", $2+1, $5/10.0} }' | \
    wigAsciiToBinary -dataSpan=5 -chrom=${chr} \
        -wibFile=wigData5/gc5Base_${chrom} \
            -name=${chrom} stdin 2> dataLimits5/${chr}
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x kkRun.sh

    ls /iscratch/i/danRer1/nib  > nibList
    cat << '_EOF_' > gsub
#LOOP
./kkRun.sh $(path1)
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    gensub2 nibList single gsub jobList
    para create jobList
    para try, check, push ... etc
# para time
# Completed: 29 of 29 jobs
# CPU time in finished jobs:       2450s      40.84m     0.68h    0.03d  0.000 y
# IO & Wait Time:                     0s       0.00m     0.00h    0.00d  0.000 y
# Average job time:                  80s       1.33m     0.02h    0.00d
# Longest job:                      506s       8.43m     0.14h    0.01d
# Submission to last job:           650s      10.83m     0.18h    0.01d

    # load the .wig files back on hgwdev:
    ssh hgwdev
    cd /cluster/data/danRer1/bed/gc5Base
    hgLoadWiggle -pathPrefix=/gbdb/danRer1/wib/gc5Base \
                 danRer1 gc5Base wigData5/*.wig
    # and symlink the .wib files into /gbdb
    mkdir /gbdb/danRer1/wib/gc5Base
    ln -s `pwd`/wigData5/*.wib /gbdb/danRer1/wib/gc5Base

    # And then the zoomed data view
    ssh kki
    cd /cluster/data/danRer1/bed/gc5Base
    mkdir wigData5_1K dataLimits5_1K

    cat << '_EOF_' > kkRunZoom.sh
#!/bin/sh
NIB=$1
                                                                                
chr=${NIB/.nib/}
chrom=${chr#chr}
                                                                                
hgGcPercent -chr=${chr} -doGaps -file=stdout -win=5 danRer1 \
        /iscratch/i/danRer1/nib | \
    grep -w GC | \
    awk '{if (($3-$2) >= 5) {printf "%d\t%.1f\n", $2+1, $5/10.0} }' | \
    wigZoom -dataSpan=1000 stdin | wigAsciiToBinary -dataSpan=1000 \
        -chrom=${chr} -wibFile=wigData5_1K/gc5Base_${chrom}_1K \
            -name=${chrom} stdin 2> dataLimits5_1K/${chr}
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x kkRunZoom.sh

    cat << '_EOF_' > gsubZoom
#LOOP
./kkRunZoom.sh $(path1)
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
    gensub2 nibList single gsubZoom jobListZoom
    para create jobListZoom
    para try, check, push, ... etc.
# para time
# Completed: 29 of 29 jobs
# CPU time in finished jobs:       2433s      40.55m     0.68h    0.03d  0.000 y
# IO & Wait Time:                     0s       0.00m     0.00h    0.00d  0.000 y
# Average job time:                  81s       1.36m     0.02h    0.00d
# Longest job:                      522s       8.70m     0.14h    0.01d
# Submission to last job:          2503s      41.72m     0.70h    0.03d
                                                                                
    #   Then load these .wig files into the same database as above
    ssh hgwdev
    cd /cluster/data/danRer1/bed/gc5Base
    hgLoadWiggle -pathPrefix=/gbdb/danRer1/wib/gc5Base \
        -oldTable danRer1 gc5Base wigData5_1K/*.wig
    # and symlink these .wib files into /gbdb
    mkdir -p /gbdb/danRer1/wib/gc5Base
    ln -s `pwd`/wigData5_1K/*.wib /gbdb/danRer1/wib/gc5Base

# BLASTZ FOR HG17 (DONE, 2004-06-16, hartera)
    ssh kkr1u00
    # blastz requires lineage-specific repeats
    # Treat all repeats as lineage-specific.
                                                                                
    mkdir /iscratch/i/danRer1/linSpecRep.notInHuman
    foreach f (/iscratch/i/danRer1/rmsk/chr*.fa.out)
      cp -p $f /iscratch/i/danRer1/linSpecRep.notInHuman/$f:t:r:r.out.spec
    end
                                                                                
    mkdir /iscratch/i/gs.18/build35/linSpecRep.notInZebrafish
    foreach f (/iscratch/i/gs.18/build35/rmsk/chr*.fa.out)
 cp -p $f /iscratch/i/gs.18/build35/linSpecRep.notInZebrafish/$f:t:r:r.out.spec
    end
    iSync
                                                                                
    ssh kk
    mkdir -p /cluster/data/danRer1/bed/blastz.hg17.2004-06-08
    ln -s /cluster/data/danRer1/bed/blastz.hg17.2004-06-08 \
          /cluster/data/danRer1/bed/blastz.hg17
    cd /cluster/data/danRer1/bed/blastz.hg17
    # Set L=6000 and abridge repeats - these are the same parameters used
    # for hg16 and Fugu.
                                                                                
    cat << '_EOF_' > DEF
# zebrafish vs human (hg17)
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin
                                                                                
ALIGN=blastz-run
BLASTZ=blastz
                                                                                
# Reuse parameters from hg16-fr1.
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q
BLASTZ_ABRIDGE_REPEATS=1
                                                                                
# TARGET: Zebrafish
SEQ1_DIR=/iscratch/i/danRer1/nib/
SEQ1_RMSK=
SEQ1_FLAG=
SEQ1_SMSK=/iscratch/i/danRer1/linSpecRep.notInHuman
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000
                                                                                
# QUERY: Human (hg17)
SEQ2_DIR=/iscratch/i/gs.18/build35/bothMaskedNibs
SEQ2_RMSK=
SEQ2_FLAG=
SEQ2_SMSK=/iscratch/i/gs.18/build35/linSpecRep.notInZebrafish
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=0
                                                                                
BASE=/cluster/data/danRer1/bed/blastz.hg17
                                                                                
DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len
'_EOF_'
    # << this line keeps emacs coloring happy
    # Save the DEF file in the current standard place
    cp DEF ~angie/hummus/DEF.danRer1-hg17.2004-06-08
    # Need shell scripts from mm4 to do cluster runs
    mv /cluster/data/mm4/jkStuff/BlastZ*.sh /cluster/data/danRer1/jkStuff/
    # edit BlastZ_run0.sh
    # replace line 22: /cluster/home/angie/schwartzbin/ with /cluster/bin/penn/
    # this is the directory for the latest version of blastz-run
                                                                                
    # prepare first cluster run
    ssh kk
    cd /cluster/data/danRer1/bed/blastz.hg17
    bash # if a csh/tcsh user
    . ./DEF
    /cluster/data/danRer1/jkStuff/BlastZ_run0.sh
    cd run.0
    # check batch looks ok then
    para try, check, push, check, ....
# para time
# Completed: 57970 of 57970 jobs
# CPU time in finished jobs:   20265671s  337761.18m  5629.35h  234.56d  0.643 y# IO & Wait Time:                570125s    9502.09m   158.37h    6.60d  0.018 y# Average job time:                 359s       5.99m     0.10h    0.00d
# Longest job:                    10482s     174.70m     2.91h    0.12d
# Submission to last job:         73680s    1228.00m    20.47h    0.85d
    # Took about 17 hours to run. Output is 6.3 Gigabytes - rather large.
    # Try processing and check in browser before adjusting parameters
    #   Second cluster run to convert the .out's to .lav's
    cd /cluster/data/danRer1/bed/blastz.hg17
    bash # if a csh/tcsh user
    . ./DEF
    /cluster/data/danRer1/jkStuff/BlastZ_run1.sh
    cd run.1
    para try, check, push, etc ...
# para time
# Completed: 170 of 170 jobs
# CPU time in finished jobs:      16047s     267.45m     4.46h    0.19d  0.001 y# IO & Wait Time:                146162s    2436.03m    40.60h    1.69d  0.005 y# Average job time:                 954s      15.90m     0.27h    0.01d
# Longest job:                     2179s      36.32m     0.61h    0.03d
# Submission to last job:          2451s      40.85m     0.68h    0.03d
                                                                                
    #   Third cluster run to convert lav's to axt's
    ssh kk
    cd /cluster/data/danRer1/bed/blastz.hg17
    mkdir axtChrom
    # a new run directory
    mkdir run.2
    cd run.2
cat << '_EOF_' > do.csh
#!/bin/csh
cd $1
cat `ls -1 *.lav | sort -g` \
| lavToAxt stdin /iscratch/i/danRer1/nib \
/iscratch/i/gs.18/build35/bothMaskedNibs stdout \
| axtSort stdin $2
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x do.csh
    cat << '_EOF_' > gsub
#LOOP
./do.csh {check in exists $(path1)} {check out line+ /cluster/data/danRer1/bed/blastz.hg17/axtChrom/$(root1).axt}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
    \ls -1Sd ../lav/chr* > chrom.list
    gensub2 chrom.list single gsub jobList
    wc -l jobList
    head jobList
    para create jobList
    para try, check, push, check,...
# para time
# Completed: 27 of 29 jobs
# Crashed: 2 jobs
# CPU time in finished jobs:       2063s      34.38m     0.57h    0.02d  0.000 y# IO & Wait Time:                 19245s     320.75m     5.35h    0.22d  0.001 y# Average job time:                 789s      13.15m     0.22h    0.01d
# Longest job:                     9103s     151.72m     2.53h    0.11d
# Submission to last job:         14332s     238.87m     3.98h    0.17d
                                                                                
# chrNA and chrUn are the crashed jobs. These are very large and ran out of
# memory. Re-run these separately on kolossus
                                                                                
     ssh kk
     # put nibs on bluearc
     mkdir -p /cluster/bluearc/danRer1/nib
     mkdir -p /cluster/bluearc/hg17/bothMaskedNibs
     cp /iscratch/i/danRer1/nib/* /cluster/bluearc/danRer1/nib
     cp /iscratch/i/gs.18/build35/bothMaskedNibs/* \
        /cluster/bluearc/hg17/bothMaskedNibs
                                                                                
     cat << '_EOF_' > do2.csh
#!/bin/csh
cd $1
cat `ls -1 *.lav | sort -g` \
| lavToAxt stdin /cluster/bluearc/danRer1/nib \
/cluster/bluearc/hg17/bothMaskedNibs stdout \
| axtSort stdin $2
'_EOF_'
                                                                                
    ssh kolossus
    cd /cluster/data/danRer1/bed/blastz.hg17/run.2
    ./do2.csh ../lav/chrNA \
              /cluster/data/danRer1/bed/blastz.hg17/axtChrom/chrNA.axt
    ./do2.csh ../lav/chrUn \
             /cluster/data/danRer1/bed/blastz.hg17/axtChrom/chrUn.axt
                                                                                
    # translate sorted axt files into psl
    ssh kolossus
    cd /cluster/data/danRer1/bed/blastz.hg17
    mkdir -p pslChrom
    set tbl = "blastzHg17"
    foreach f (axtChrom/chr*.axt)
      set c=$f:t:r
      echo "Processing chr $c"
      /cluster/bin/i386/axtToPsl $f S1.len S2.len pslChrom/${c}_${tbl}.psl
    end
                                                                                
    # Load database tables
    ssh hgwdev
    cd /cluster/data/danRer1/bed/blastz.hg17/pslChrom
                                                                                
    foreach f (./*.psl)
      /cluster/bin/i386/hgLoadPsl -noTNameIx danRer1 $f
      echo "$f Done"
    end
# To tune Blastz parameters: these parameters above where used but it was 
# thought that there were probably too many pileups. So then the
# blastz was repeated with L=10000 (as for hg16-galGal2) and with an
# intermediate value of L=8000. L is the threshold for gapped alignments.
# Looking at alignments in the browser it was found that L=10000 and L=8000
# were too stringent and alignments to coding regions were being lost.
# L=6000 (as above) seemed to be the best choice and then low scoring 
# alignments could be filtered at the chaining step instead to reduce pileups.
# e.g. using the browser with the non-zebrafish mRNAs track turned on
# chr1:24,547-28,249 region lost an exon with all but blastz with L=6000
# chr2:1,124,174-1,135,282 is worse with an entire gene being lost except
# with the Blastz with L=6000.

# RESCORE HG17 BLASTZ (DONE, 2004-06-21, hartera)
# USE HG17 BLASTZ WITH L=6000 SO AS NOT TO MISS
    # Low scores can occur with repeats abridged and using the
    # HoxD55.q matrix. PSU's restore_rpts program rescored alignments
    # with the default matrix instead of the BLASTZ_Q matrix.
    # Rescore them here so the chainer sees the higher scores:
                                                                                
    ssh kolossus
    cd /cluster/data/danRer1/bed/blastz.hg17
    mkdir axtChrom.rescore
    foreach f (axtChrom/chr*.axt)
        axtRescore -scoreScheme=/cluster/data/blastz/HoxD55.q \
        $f axtChrom.rescore/$f:t
    end
    mv axtChrom axtChrom.orig
    mv axtChrom.rescore axtChrom
                                                                                
#   psl files and blastz tables will be the same regardless of score so
#   no need to reload
                                                                                
# CHAIN HG17 BLASTZ (DONE, 2004-06-24, hartera)
    # Re do chains with rescored blastz Hg17
    # Run axtChain on little cluster
    ssh kki
    cd /cluster/data/danRer1/bed/blastz.hg17
    mv axtChain axtChain.orig
    mkdir -p axtChain/run1
    cd axtChain/run1
    mkdir out chain
    # create 2 input lists as need to process chrNA and chrUn separately
    ls -1S /cluster/data/danRer1/bed/blastz.hg17/axtChrom/*.axt \
        > input.lst
    grep "chrNA" input.lst > inputNAandUn.lst
    grep "chrUn" input.lst >> inputNAandUn.lst
    # remove chrNA and chrUn from input.lst
                                                                                
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out line+ out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
                                                                                
    # Reuse gap penalties from hg16 vs chicken run.
    cat << '_EOF_' > ../../chickenHumanTuned.gap
tablesize^V     11
smallSize^V     111
position^V      1^V     2^V     3^V     11^V    111^V   2111^V  12111^V 32111^V
72111^V 152111^V        252111
qGap^V  325^V   360^V   400^V   450^V   600^V   1100^V  3600^V  7600^V  15600^V
31600^V 56600
bothGap^V       625^V   660^V   700^V   750^V   900^V   1400^V  4000^V  8000^V
16000^V 32000^V 57000
'_EOF_'
    # << this line makes emacs coloring happy
                                                                                
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out line+ out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
cat << '_EOF_' > doChain
#!/bin/csh
axtFilter $1 \
| axtChain -scoreScheme=/cluster/data/blastz/HoxD55.q \
                      -linearGap=../../chickenHumanTuned.gap \
                      -minScore=10000 stdin \
    /iscratch/i/danRer1/nib \
    /iscratch/i/gs.18/build35/bothMaskedNibs $2 >& $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doChain
    gensub2 input single gsub jobList
    para create jobList
    para try, check, push, check...
# para time
# Completed: 27 of 27 jobs
# CPU time in finished jobs:       4572s      76.21m     1.27h    0.05d  0.000 y# IO & Wait Time:                  1252s      20.86m     0.35h    0.01d  0.000 y# Average job time:                 216s       3.60m     0.06h    0.00d
# Longest job:                     1092s      18.20m     0.30h    0.01d
# Submission to last job:          3242s      54.03m     0.90h    0.04d
                                                                                
    cat << '_EOF_' > gsub2
#LOOP
doChain2 {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out line+ out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
cat << '_EOF_' > doChain2
#!/bin/csh
axtFilter $1 \
| axtChain -scoreScheme=/cluster/data/blastz/HoxD55.q \
                      -linearGap=../../chickenHumanTuned.gap \
                      -minScore=15000 stdin \
    /iscratch/i/danRer1/nib \
    /iscratch/i/gs.18/build35/bothMaskedNibs $2 >& $3
'_EOF_'
    # << this line makes emacs coloring happy
    gensub2 inputNAandUn.lst single gsub2 jobList2
    para create jobList2
    para try, check, push, check...
# para time
# crashes (out of memory) so try again on kolossus
    ssh kolossus
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChain/run1
    # use sequences on cluearc for kolossus
    cat << '_EOF_' > doChain2
#!/bin/csh
axtFilter $1 \
| axtChain -scoreScheme=/cluster/data/blastz/HoxD55.q \
                      -linearGap=../../chickenHumanTuned.gap \
                      -minScore=15000 stdin \
    /cluster/bluearc/danRer1/nib \
    /cluster/bluearc/hg17/bothMaskedNibs $2 >& $3
'_EOF_'
    chmod +x doChain2
                                                                                
    doChain2 /cluster/data/danRer1/bed/blastz.hg17/axtChrom/chrNA.axt \
               chain/chrNA.chain out/chrNA.out
    doChain2 /cluster/data/danRer1/bed/blastz.hg17/axtChrom/chrUn.axt \
               chain/chrUn.chain out/chrUn.out
                                                                                
    # now on the cluster server, sort chains
    ssh kksilo
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChain
    chainMergeSort run1/chain/*.chain > all.chain
    chainSplit chain all.chain
# take a look at score distr's,try also with larger bin size.
    foreach f (chain/*.chain)
      grep chain $f | awk '{print $2;}' | sort -nr > /tmp/score.$f:t:r
      echo $f:t:r >> hist.out
      textHistogram -binSize=10000 /tmp/score.$f:t:r >> hist.out
      echo ""
    end
                                                                                
    # Load chains into database
    # next machine
    ssh hgwdev
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChain/chain
    foreach i (*.chain)
        set c = $i:r
        hgLoadChain danRer1 ${c}_chainHg17 $i
        echo done $c
    end
# load in original chains as chainHg17NoFilt - minScore = 5000 on all chroms
# featureBits -chrom=chr1 danRer1 chainHg17
# 12359874 bases of 40488791 (30.527%) in intersection
# featureBits -chrom=chr1 danRer1 refGene:cds chainHg17
# 147375 bases of 40488791 (0.364%) in intersection
# featureBits -chrom=chr1 danRer1 chainHg17NoFilt
# 13518074 bases of 40488791 (33.387%) in intersection
# featureBits -chrom=chr1 danRer1 refGene:cds chainHg17NoFilt
#  149120 bases of 40488791 (0.368%) in intersection
                                                                                
# featureBits -chrom=chrNA danRer1 chainHg17
# 79678867 bases of 335615307 (23.741%) in intersection
# featureBits -chrom=chrNA danRer1 refGene:cds chainHg17
# 556727 bases of 335615307 (0.166%) in intersection
# featureBits -chrom=chrNA danRer1 chainHg17NoFilt  
# 100579306 bases of 335615307 (29.969%) in intersection
# featureBits -chrom=chrNA danRer1 refGene:cds chainHg17NoFilt
# 622781 bases of 335615307 (0.186%) in intersection
# featureBits -chrom=chrNA danRer1 refGene:cds
# 741426 bases of 335615307 (0.221%) in intersection

# Using minScore=10000 for all chroms but 15000 for chrUn and chrNA
# reduces low scoring chains but without compromising CDS region coverage
# too much.

# NET HG17 BLASTZ (DONE, 2004-07-07, hartera)
                                                                                
    ssh kksilo
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChain
    mkdir preNet
    cd chain
    foreach i (*.chain)
       echo preNetting $i
       /cluster/bin/i386/chainPreNet $i ../../S1.len ../../S2.len \
                                     ../preNet/$i
    end
                                                                                
    cd ..
    mkdir n1
    cd preNet
    foreach i (*.chain)
      set n = $i:r.net
      echo primary netting $i
      /cluster/bin/i386/chainNet $i -minSpace=1 ../../S1.len ../../S2.len \
                                 ../n1/$n /dev/null
    end
    cd ..
    cat n1/*.net | /cluster/bin/i386/netSyntenic stdin noClass.net
    #  memory usage 111640576, utime 625 s/100, stime 120
                                                                                
# Add classification info using db tables:
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChain
    # netClass looks for ancient repeats in one of the databases
    # hg17 has this table - hand-curated by Arian but this is for
    # human-rodent comparisons so do not use here, use -noAr option
    mkdir -p /cluster/bluearc/danRer1/linSpecRep.notInHuman
    mkdir -p /cluster/bluearc/hg17/linSpecRep.notInZebrafish
    cp /iscratch/i/gs.18/build35/linSpecRep.notInZebrafish/* \
       /cluster/bluearc/hg17/linSpecRep.notInZebrafish
    cp /iscratch/i/danRer1/linSpecRep.notInHuman/* \
       /cluster/bluearc/danRer1/linSpecRep.notInHuman
    ssh hgwdev
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChain
                                                                                
    time netClass noClass.net danRer1 hg17 humanhg17.net \
         -tNewR=/cluster/bluearc/danRer1/linSpecRep.notInHuman \
         -qNewR=/cluster/bluearc/hg17/linSpecRep.notInZebrafish -noAr
    # 85.530u 52.020s 5:15.86 43.5%   0+0k 0+0io 2072pf+0w
    ssh hgwdev
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChain
    netFilter -minGap=10 humanhg17.net |  hgLoadNet danRer1 netHg17 stdin


# MAKE 10.OOC, 11.OOC FILE FOR BLAT (DONE, 2004-06-09, hartera)
    # Use -repMatch=460 (based on size -- for human we use 1024, and
    # the zebrafish genome is ~45% of the size of the human genome
    ssh kkr1u00
    mkdir /cluster/data/danRer1/bed/ooc
    cd /cluster/data/danRer1/bed/ooc
    mkdir -p /cluster/bluearc/danRer1
    ls -1 /cluster/data/danRer1/nib/chr*.nib > nib.lst
    blat nib.lst /dev/null /dev/null -tileSize=11 \
      -makeOoc=/cluster/bluearc/danRer1/11.ooc -repMatch=460
    # Wrote 44155 overused 11-mers to /cluster/bluearc/danRer1/11.ooc
    # For 10.ooc, repMatch = 4096 for human, so use 1840
        blat nib.lst /dev/null /dev/null -tileSize=10 \
      -makeOoc=/cluster/bluearc/danRer1/10.ooc -repMatch=1840
    # Wrote 10767 overused 10-mers to /cluster/bluearc/danRer1/10.ooc
    # keep copies of ooc files in this directory and copy to iscratch
    cp /cluster/bluearc/danRer1/*.ooc .
    cp -p /cluster/bluearc/danRer1/*.ooc /iscratch/i/danRer1/
    iSync

# AUTO UPDATE GENBANK MRNA AND EST RUN  (in progress, 2004-06-09, hartera)
    ssh eieio
    cd /cluster/data/genbank
    # This is a new organism, edit the etc/genbank.conf file and add:
# danRer1 (zebrafish)
danRer1.genome = /iscratch/i/danRer1/nib/chr*.nib
danRer1.lift = /cluster/data/danRer1/jkStuff/liftAll.lft
danRer1.downloadDir = danRer1
     # NEED TO DO THIS STILL, WAIT FOR JIM TO CONFIRM, ALSO DO WE NEED 
     # MAXINTRON FLAG FOR BLAT. LET MARK KNOW WHEN READY TO DO THIS.
    cvs commit -m "Added danRer1" etc/genbank.conf
    make

    # Edit src/align/gbBlat to add /iscratch/i/danRer1/11.ooc
    # Add line:  DANRER_OOC=/iscratch/i/danRer1/11.ooc
    cvs diff src/align/gbBlat
    make
    cvs commit -m "Added 11.ooc for danRer1." src/align/gbBlat

# ENSEMBL GENES (in progress, 2004-06-08, hartera)

   mkdir /cluster/data/danRer1/bed/ensembl
   cd /cluster/data/danRer1/bed/ensembl
    # Get the ensembl protein data from
    # http://www.ensembl.org/Danio_rerio/martview
    # Follow this sequence through the pages:
    # Page 1) Make sure that the Danio_rerio choice is selected. Hit next.
    # Page 2) Uncheck the "Limit to" box in the region choice. Then hit next.
    # Page 3) Choose the "Structures" box.
    # Page 4) Choose GTF as the ouput.  choose gzip compression.  hit export.
    # Save as ensemblGene.gtf.gz


