#!/bin/csh -f # set emacs mode
exit; # don't actually run this like a script :)

# Danio Rerio (zebrafish) from Sanger, version Zv3 (released 11/27/03)
#   Project website:
#    http://www.sanger.ac.uk/Projects/D_rerio/
#  Assembly notes:
#    http://www.sanger.ac.uk/Projects/D_rerio/Zv3_assembly_information.shtml

# DOWNLOAD SEQUENCE (DONE, 2004-05-17, kate)

    ssh kksilo
    mkdir /cluster/store7/danRer1
    ln -s /cluster/store7/danRer1 /cluster/data
    cd /cluster/data/danRer1
    wget ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv3release/README
    wget ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv3release/Zv3.contigs.agp
    wget ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv3release/Zv3.supercontigs.agp
    wget ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv3release/Zv3.supercontigs.fa 
    wget ftp://ftp.ensembl.org/pub/assembly/zebrafish/Zv3release/Zv3.supercontigs.fa.tag

# DOWNLOAD MITOCHONDRION GENOME SEQUENCE (DONE, 2004-05-24, hartera)
# CREATE ChrM.agp (DONE, 2004-05-27, hartera)
# Add "chr" prefix to chrM.agp (DONE, 2004-07-06, hartera)
    mkdir /cluster/data/danRer1/M
    cd /cluster/data/danRer1/M
    # go to http://www.ncbi.nih.gov/ and search Nucleotide for
    # "Danio mitochondrion genome".  That shows the gi number:
    # 8576324 for the accession, AC024175
 # Use that number in the entrez linking interface to get fasta:
    wget -O chrM.fa \
      'http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Text&db=Nucleotide&uid=8576324&dopt=FASTA'
    # Edit chrM.fa: make sure the header line says it is the
    # Danio Rerio mitochondrion complete genome, and then replace the
    # header line with just ">chrM".

    # Make a "pseudo-contig" for processing chrM too:
    mkdir M/chrM_1
    sed -e 's/chrM/chrM_1/' M/chrM.fa > M/chrM_1/chrM_1.fa
    mkdir M/lift
    echo "chrM_1/chrM_1.fa.out" > M/lift/oOut.lst
    echo "chrM_1" > M/lift/ordered.lst
    echo "0	M/chrM_1	16596	chrM	16596" > M/lift/ordered.lft

# create a .agp file for chrM as hgGoldGapGl and other
# programs require a .agp file so create chrM.agp
    cat << '_EOF_' > M/chrM.agp
M       1       16596	1       F	AC024175.3      1       16596	+
'_EOF_'

    # Add "chr" prefix to M in chrM.agp (2004-07-06)
    perl -pi.bak -e 's/M/chrM/' ./M/chrM.agp
    # check file then remove backup
    rm ./M/chrM.agp.bak

# SPLIT AGP FILES BY CHROMOSOME (DONE, 2004-05-26, hartera)
    ssh kksilo
    cd /cluster/data/danRer1
    # There are 2 .agp files: one for supercontigs and then one for contigs
    # showing how they map on to supercontigs.

    # split up the agp into one per chrom.
    foreach c ( 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
                                 21 22 23 24 25 )
      mkdir $c
      perl -we "while(<>){if (/^$c\t/) {print;}}" \
        ./Zv3.contigs.agp \
        > $c/chr$c.contigs.agp
      perl -we "while(<>){if (/^$c\t/) {print;}}" \
        ./Zv3.supercontigs.agp \
        > $c/chr$c.supercontigs.agp

     end 
     # From agp files, get supercontigs and contigs for
     # ctg* as chrUn, NA* as chrNA and Finished* as chrUn
     foreach t ( ctg NA Finished )
        if ($t == "ctg") then 
           set c = "Un"
        else 
           set c = $t
        endif
        mkdir $c
        perl -we "while(<>){if (/^$t/) {print;} }" \
                   ./Zv3.contigs.agp \
        >> $c/chr$c.contigs.agp
        perl -we "while(<>){if (/^$t/) {print;} }" \
                 ./Zv3.supercontigs.agp \
        >> $c/chr$c.supercontigs.agp

     end 

# BUILD CHROM-LEVEL SEQUENCE (DONE, 2004-05-27, hartera)
     ssh kksilo
     cd /cluster/data/danRer1
     # Sequence is already in upper case so no need to change

     foreach c ( 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
                                 21 22 23 24 25)
       echo "Processing ${c}"
       $HOME/bin/i386/agpToFa -simpleMultiMixed $c/chr$c.supercontigs.agp $c \
         $c/chr$c.fa ./Zv3.supercontigs.fa
       echo "${c} - DONE"
     end

     # Need to change the number for each chromosome in the .agp and .fa 
     # files to read "chrN" - should have done this before processing 
     # original sequence and .agp files

     foreach c ( 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
                                 21 22 23 24 25)
       echo "Processing ${c}"
         perl -pi -e 's/^>([0-9]+)/>chr$1/' $c/*.fa
         perl -pi -e 's/^([0-9]+)/chr$1/' $c/*.agp
       echo "${c} - DONE"
     end
     
# CREATE ChrUn, chrNA AND chrFinished FASTA, AGP, GAP AND LIFT FILES FROM 
# ctg*, NA* AND Finished* SUPERCONTIGS 
# (DONE, 2005-05-27, hartera)
# ADD CORRECT FRAGMENT TYPE TO .agp FILES (DONE, 2004-07-07, hartera)
     ssh kksilo
     cd /cluster/data/danRer1
     foreach c ( Finished NA Un )
       awk '{print $1;}' $c/chr$c.supercontigs.agp > $c/chr$c.supercontigs.lst
       $HOME/bin/i386/faSomeRecords /cluster/data/danRer1/Zv3.supercontigs.fa \
          $c/chr$c.supercontigs.lst $c/chr$c.fa
     end 
  
     # check FASTA files then generate AGP and lift files 
     # from the chromosome fastas 
    
     foreach c ( Finished NA Un )
        $HOME/bin/i386/scaffoldFaToAgp $c/chr$c.fa
        mv $c/chr$c.fa $c/chr$c.supercontigs.fa
        perl -pi -e "s/chrUn/chr$c/" $c/chr$c.*
        $HOME/bin/i386/agpToFa -simpleMultiMixed $c/chr$c.agp \
                      chr$c $c/chr$c.fa ./Zv3.supercontigs.fa 
     end
     # chrFinished
     # scaffold gap size is 1000, total scaffolds: 209
     # chrom size is 40167097
     # chrNA
     # scaffold gap size is 1000, total scaffolds: 54798
     # chrom size is 390413307
     # chrUn
     # scaffold gap size is 1000, total scaffolds: 1842
     # chrom size is 367113659
     # Add correct fragment type to .agp files (2004-07-07)
     # chrUn is "W", chrFinished is "F" and chrNA is "W" for all supercontigs
     ssh kksilo
     cd /cluster/data/danRer1
     foreach c (Un NA Finished)
        if ($c == "Un" || $c == "NA") then
           set f = "W"
        else
           set f = "F"
        endif
        perl -pi.bak -e "s/D/$f/;" $c/chr${c}.agp
     end
     # check .agp files and then remove backup files
     foreach c (Un NA Finished)
        rm ./$c/chr${c}.agp.bak
     end

# CHECK CHROM AND VIRTUAL CHROM SEQUENCES (DONE, 2004-05-27, hartera)
     # Check that the size of each chromosome .fa file is equal to the 
     # last coord of the .agp:
     ssh hgwdev
     cd /cluster/data/danRer1
     foreach c ( Finished NA Un )
       foreach f ( $c/chr$c.agp )
         set agpLen = `tail -1 $f | awk '{print $3;}'`
         set g = $f:r
         set faLen = `faSize $g.fa | awk '{print $1;}'`
         if ($agpLen == $faLen) then
           echo "   OK: $f length = $g length = $faLen"
         else
           echo "ERROR:  $f length = $agpLen, but $g length = $faLen"          
         endif
       end
     end

     foreach c ( 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
                                 21 22 23 24 25)
       foreach f ( $c/chr$c.supercontigs.agp )
         set agpLen = `tail -1 $f | awk '{print $3;}'`
         set h = $f:r
         set g = $h:r
         echo "Getting size of $g.fa"
         set faLen = `faSize $g.fa | awk '{print $1;}'`
         if ($agpLen == $faLen) then
           echo "   OK: $f length = $g length = $faLen"
         else
           echo "ERROR:  $f length = $agpLen, but $g length = $faLen"          
         endif
       end
     end
     # All are OK so all Fasta files are the correct size

# BREAK UP SEQUENCE INTO 5MB CHUNKS AT CONTIGS/GAPS FOR CLUSTER RUNS
# (DONE, 2005-05-27, hartera)
    
     ssh kksilo
     cd /cluster/data/danRer1
     foreach c ( 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
                                  21 22 23 24 25 Finished NA Un )
       foreach agp ($c/chr$c.{,contigs.}agp)
         if (-e $agp) then
           set fa = $c/chr$c.fa
           echo splitting $agp and $fa
           cp -p $agp $agp.bak
           cp -p $fa $fa.bak
           splitFaIntoContigs $agp $fa . -nSize=5000000
         endif
       end
     end

# Create list of chromsosomes (DONE, 2004-05-27, hartera)
    ssh hgwdev
    cd /cluster/data/danRer1
    foreach f (*/*.agp)
      set chr = `echo $f:h | sed -e 's/^chr//'`
      echo $chr >> chrom
    end
    sort -n chrom | uniq > chrom.lst
    rm chrom

# MAKE JKSTUFF AND BED DIRECTORIES (DONE, 2004-05-27, hartera)
    # This used to hold scripts -- better to keep them inline here 
    # Now it should just hold lift file(s) and
    # temporary scripts made by copy-paste from this file.
    mkdir /cluster/data/danRer1/jkStuff
    # This is where most tracks will be built:
    mkdir /cluster/data/danRer1/bed

# CREATING DATABASE (DONE, 2004-05-27 - hartera)   
    # Create the database.
    # next machine                                           
    ssh hgwdev
    echo 'create database danRer1' | hgsql ''
    # if you need to delete that database:  !!! WILL DELETE EVERYTHING !!!
    echo 'drop database danRer1' | hgsql danRer1
    # Use df to ake sure there is at least 5 gig free on
    df -h /var/lib/mysql
# Before loading data:
# Filesystem            Size  Used Avail Use% Mounted on
# /dev/sdc1             1.8T  303G  1.4T  19% /var/lib/mysql

# CREATING GRP TABLE FOR TRACK GROUPING (DONE, 2004-05-27, hartera)
    # next machine
    ssh hgwdev
    #  the following command copies all the data from the table
    #  grp in the database galGal2 to our new database ce2
    echo "create table grp (PRIMARY KEY(NAME)) select * from galGal2.grp" \
      | hgsql danRer1
    # if you need to delete that table:   !!! WILL DELETE ALL grp data !!!
    echo 'drop table grp;' | hgsql danRer1

# REPEAT MASKING - Run RepeatMasker on chroms (DONE, 2004-05-28, hartera)    
#- Split contigs into 500kb chunks, at gaps if possible:
    ssh kksilo
    cd /cluster/data/danRer1
    foreach c (`cat chrom.lst`)
      foreach d ($c/chr${c}*_?{,?})
        cd $d
        echo "splitting $d"
        set contig = $d:t
        ~/bin/i386/faSplit gap $contig.fa 500000 ${contig}_ -lift=$contig.lft \
            -minGapSize=100
        cd ../..
      end
    end

#- Make the run directory and job list:
    cd /cluster/data/danRer1
    cat << '_EOF_' > jkStuff/RMZebrafish
#!/bin/csh -fe
                                                                                
cd $1
pushd .
/bin/mkdir -p /tmp/danRer1/$2
/bin/cp $2 /tmp/danRer1/$2/
cd /tmp/danRer1/$2
/cluster/bluearc/RepeatMasker/RepeatMasker -ali -s -spec danio $2
popd
/bin/cp /tmp/danRer1/$2/$2.out ./
if (-e /tmp/danRer1/$2/$2.align) /bin/cp /tmp/danRer1/$2/$2.align ./
if (-e /tmp/danRer1/$2/$2.tbl) /bin/cp /tmp/danRer1/$2/$2.tbl ./
if (-e /tmp/danRer1/$2/$2.cat) /bin/cp /tmp/danRer1/$2/$2.cat ./
/bin/rm -fr /tmp/danRer1/$2/*
/bin/rmdir --ignore-fail-on-non-empty /tmp/danRer1/$2
/bin/rmdir --ignore-fail-on-non-empty /tmp/danRer1
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x jkStuff/RMZebrafish
    mkdir RMRun
    cp /dev/null RMRun/RMJobs
    foreach c (`cat chrom.lst`)
      foreach d ($c/chr${c}_?{,?})
          set ctg = $d:t
          foreach f ( $d/${ctg}_?{,?}.fa )
            set f = $f:t
            echo /cluster/data/danRer1/jkStuff/RMZebrafish \
                 /cluster/data/danRer1/$d $f \
               '{'check out line+ /cluster/data/danRer1/$d/$f.out'}' \
              >> RMRun/RMJobs
          end
      end
    end

    #- Do the run
    ssh kk
    cd /cluster/data/danRer1/RMRun
    para create RMJobs
    para try, para check, para check, para push, para check,...
# para time
# Completed: 3623 of 3623 jobs
# CPU time in finished jobs:    6514173s  108569.55m  1809.49h   75.40d  0.207 y
# IO & Wait Time:                 34466s     574.43m     9.57h    0.40d  0.001 y
# Average job time:                1808s      30.13m     0.50h    0.02d
# Longest job:                     2539s      42.32m     0.71h    0.03d
# Submission to last job:         23551s     392.52m     6.54h    0.27d

    #- Lift up the 500KB chunk .out's to 5MB ("pseudo-contig") level
    ssh kksilo
    cd /cluster/data/danRer1
    foreach d (*/chr*_?{,?})
      set contig = $d:t
      echo $contig
      liftUp $d/$contig.fa.out $d/$contig.lft warn $d/${contig}_*.fa.out \
        > /dev/null
    end

    #- Lift pseudo-contigs to chromosome level
    foreach c (`cat chrom.lst`)
      echo lifting $c
      cd $c
      if (-e lift/ordered.lft && ! -z lift/ordered.lft) then
        liftUp chr$c.fa.out lift/ordered.lft warn `cat lift/oOut.lst` \
        > /dev/null
      endif
      cd ..
    end

    #- Load the .out files into the database with:
    ssh hgwdev
    cd /cluster/data/danRer1
    hgLoadOut danRer1 */chr*.fa.out

# MAKE LIFTALL.LFT (DONE, 2004-05-28, hartera)
    ssh kksilo
    cd /cluster/data/danRer1
    cat */lift/ordered.lft > jkStuff/liftAll.lft

# SIMPLE REPEAT [TRF] TRACK  (DONE, 2004-06-07, hartera)
    # TRF runs pretty quickly now... it takes a few hours total runtime, 
    # so instead of binrsyncing and para-running, just do this on the
    # local fileserver
    ssh kksilo
    mkdir -p /cluster/data/danRer1/bed/simpleRepeat
    cd /cluster/data/danRer1/bed/simpleRepeat
    mkdir trf
    cp /dev/null jobs.csh
    foreach d (/cluster/data/danRer1/*/chr*_?{,?})
      set ctg = $d:t
      foreach f ($d/${ctg}.fa)
        set fout = $f:t:r.bed
        echo $fout
        echo "/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $f /dev/null -bedAt=trf/$fout -tempDir=/tmp" \
        >> jobs.csh
      end
    end

    chmod a+x jobs.csh
    csh -ef jobs.csh >&! jobs.log &
    # check on this with
    tail -f jobs.log
    wc -l jobs.csh
    ls -1 trf | wc -l
    endsInLf trf/*
# trf crashes with chr10_5 so remove from job list for now and run the rest 
# of the jobs.  use jobs1.csh
# contacted authors of trf - they are going to send a new binary. In the
# meantime, they ran chr10_5.fa through the fixed version of trf
# Need to change filenames from chr10_5.fa.* to chr10_5.tf.*
    mkdir /cluster/data/danRer1/bed/simpleRepeat/test
    cd /cluster/data/danRer1/bed/simpleRepeat/test
    mkdir trf

    # Download and unzip results.zip file in this directory
cat << '_EOF_' > changefileName.pl
#!/usr/bin/perl -w
use strict;
                                                                                
while (<STDIN>) {
    chomp;
    my $a = $_;
    if ($a =~ /^chr10_5/) {
       print "cp $a ";
       $a =~ s/chr10_5.fa/chr10_5.tf/;
       print "$a\n";
    }
}
'_EOF_'

    # change filenames and copy to /tmp/ directory
    ls > fileList
    perl changeFilename.pl < fileList > newFiles
    cp chr10_5.tf* /tmp/

    # Edit ~/kent/src/hg/trfBig/trfBig.c to remove calls to trfSysCall 
    # so trf is not used but output files are post processed
    # make and run trfBig
    /cluster/home/hartera/bin/i386/trfBig -trf=/cluster/bin/i386/trf \
            /cluster/data/danRer1/10/chr10_5/chr10_5.fa /dev/null \
            -bedAt=trf/chr10_5.bed -tempDir=/tmp

    # copy bed file output to directory of bed files for the other sequences
    cp /cluster/data/danRer1/bed/simpleRepeat/test/trf/chr10_5.bed \
       /cluster/data/danRer1/bed/simpleRepeat/trf

    liftUp simpleRepeat.bed /cluster/data/danRer1/jkStuff/liftAll.lft warn \
      trf/*.bed

    # Load into the database:
    ssh hgwdev
    hgLoadBed danRer1 simpleRepeat \
      /cluster/data/danRer1/bed/simpleRepeat/simpleRepeat.bed \
      -sqlTable=$HOME/src/hg/lib/simpleRepeat.sql

# PROCESS SIMPLE REPEATS INTO MASK (DONE, 2004-06-07, hartera)
    # After the simpleRepeats track has been built, make a filtered version
    # of the trf output: keep trf's with period <= 12:
    ssh kksilo
    cd /cluster/data/danRer1/bed/simpleRepeat
    mkdir -p trfMask
    foreach f (trf/chr*.bed)
      awk '{if ($5 <= 12) print;}' $f > trfMask/$f:t
    end
    # Lift up filtered trf output to chrom coords as well:
    cd /cluster/data/danRer1
    mkdir bed/simpleRepeat/trfMaskChrom
    foreach c (`cat chrom.lst`)
      if (-e $c/lift/ordered.lst) then
        perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
          $c/lift/ordered.lst > $c/lift/oTrf.lst
        liftUp bed/simpleRepeat/trfMaskChrom/chr$c.bed \
          jkStuff/liftAll.lft warn `cat $c/lift/oTrf.lst`
      endif
      if (-e $c/lift/random.lst) then
        perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
           $c/lift/random.lst > $c/lift/rTrf.lst
        liftUp bed/simpleRepeat/trfMaskChrom/chr${c}_random.bed \
          jkStuff/liftAll.lft warn `cat $c/lift/rTrf.lst`
      endif
    end

# MASK SEQUENCE WITH REPEATMASKER AND SIMPLE REPEAT/TRF 
# (DONE, 2004-06-07, hartera)
    ssh kksilo
    cd /cluster/data/danRer1
    # Soft-mask (lower-case) the contig and chr .fa's,
    # then make hard-masked versions from the soft-masked.
    set trfCtg=bed/simpleRepeat/trfMask
    set trfChr=bed/simpleRepeat/trfMaskChrom
    foreach f (*/chr*.fa)
      echo "repeat- and trf-masking $f"
      maskOutFa -soft $f $f.out $f
      set chr = $f:t:r
      maskOutFa -softAdd $f $trfChr/$chr.bed $f
      echo "hard-masking $f"
      maskOutFa $f hard $f.masked
    end
# This warning is extremely rare -- if it indicates a problem, it's only
# with the repeat annotation and doesn't affect the masking.
# WARNING: negative rEnd: -54 chr5:3971603-3971634 (TCTG)n
# WARNING: negative rEnd: -31 chrNA:145096456-145096484 (TTTG)n
# WARNING: negative rEnd: -29 chrNA:206929524-206929579 (TCCA)n

    foreach c (`cat chrom.lst`)
      echo "repeat- and trf-masking contigs of chr$c"
      foreach d ($c/chr*_?{,?})
        set ctg=$d:t
        set f=$d/$ctg.fa
        maskOutFa -soft $f $f.out $f
        maskOutFa -softAdd $f $trfCtg/$ctg.bed $f
        maskOutFa $f hard $f.masked
      end
    end
# same warning here too. 
# WARNING: negative rEnd: -31 chrNA_29:4620077-4620105 (TTTG)n
# WARNING: negative rEnd: -29 chrNA_42:1194625-1194680 (TCCA)n
# WARNING: negative rEnd: -54 chr5_1:3971603-3971634 (TCTG)n
# sent these to Arian 
    # Build nib files, using the soft masking in the fa
    mkdir nib
    foreach f (*/chr*.fa)
      faToNib -softMask $f nib/$f:t:r.nib
    end

# STORING O+O SEQUENCE AND ASSEMBLY INFORMATION  (DONE, 2004-06-08 - hartera)
    # Make symbolic links from /gbdb/galGal2/nib to the real nibs.
    ssh hgwdev
    cd /cluster/data/danRer1
    mkdir -p /gbdb/danRer1/nib
    foreach f (/cluster/data/danRer1/nib/chr*.nib)
      ln -s $f /gbdb/danRer1/nib
    end
    # Load /gbdb/ce2/nib paths into database and save size info
    # hgNibSeq creates chromInfo table
    hgNibSeq -preMadeNib danRer1 /gbdb/danRer1/nib */chr*.fa
    echo "select chrom,size from chromInfo" | hgsql -N danRer1 > chrom.sizes
    # take a look at chrom.sizes, should be 29 lines
    wc chrom.sizes
    
    # Make one big 2bit file as well, and make a link to it in
    # /gbdb/danRer1/nib because hgBlat looks there:
    faToTwoBit */chr*.fa danRer1.2bit
    ln -s /cluster/data/danRer1/danRer1.2bit /gbdb/danRer1/nib/

# MAKE GOLD AND GAP TRACKS (DONE, 2004-06-08, hartera)
# REMAKE GOLD AND GAP TRACKS (DONE, 2004-07-07, hartera)
    ssh hgwdev
    cd /cluster/data/danRer1
    # the gold and gap tracks are created from the chrN.agp file and this is
    # the contigs agp so need to use chrN.supercontigs.agp to create
    # an assembly scaffolds track as for panTro1
   #mv chrN.agp to a "contigs" dir and rename chrN.supercontigs.agp to chrN.agp
    foreach c (1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
               21 22 23 24 25)
       mkdir ./$c/contigs
       cp ./$c/chr${c}.agp ./$c/contigs/
       mv ./$c/chr${c}.contigs.agp ./$c/contigs/
       mv ./$c/chr${c}.supercontigs.agp ./$c/chr${c}.agp
    end
                                                                                
    # check track and delete the chrN_supercontigs gap and gold tables
    foreach c (1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
               21 22 23 24 25)
       hgsql -e "drop table chr${c}_supercontigs_gold;" danRer1
       hgsql -e "drop table chr${c}_supercontigs_gap;" danRer1
    end
    hgGoldGapGl -noGl -chromLst=chrom.lst danRer1 /cluster/data/danRer1 .

# MAKE HGCENTRALTEST ENTRY AND TRACKDB TABLE FOR DANRER1 
# (DONE, 2004-06-08, hartera)
# CHANGE DEFAULT POSITION FOR BROWSER (DONE, 2004-07-16, hartera)

    # Make trackDb table so browser knows what tracks to expect:
    ssh hgwdev
    mkdir -p ~/kent/src/hg/makeDb/trackDb/zebrafish/danRer1

    cd $HOME/kent/src/hg/makeDb/trackDb
    cvs up -d -P
    # Edit that makefile to add danRer1 in all the right places and do
    make update
    make alpha
    cvs commit -m "Added danRer1." makefile

    # Add dbDb and defaultDb entries:
    echo 'insert into dbDb (name, description, nibPath, organism,  \
          defaultPos, active, orderKey, genome, scientificName,  \
          htmlPath, hgNearOk, hgPbOk, sourceName)  \
          values("danRer1", "Nov. 2003", \
          "/gbdb/danRer1/nib", "Zebrafish", "chr1:1000-4000", 1, \
          41, "Zebrafish", "Danio Rerio", \
          "/gbdb/danRer1/html/description.html", 0, 0, \
          "Sanger Centre, Danio rerio Sequencing Project Zv3");' \
    | hgsql -h genome-testdb hgcentraltest
    echo 'insert into defaultDb (genome, name) \
          values ("Zebrafish", "danRer1");' \ 
          | hgsql -h genome-testdb hgcentraltest
# Move Zebrafish so it is between Chicken and Fugu in the organism list
    echo 'update dbDb set orderKey = 38 where name = "danRer1";' \
         | hgsql -h genome-testdb hgcentraltest
# Change default position to show "tiggy-winkle hedgehog" gene 
# involved in signaling in development
# (2004-07-16, hartera)

    echo 'update dbDb set defaultPos = "chr2:16,330,443-16,335,196" \
         where name = "danRer1";' | hgsql -h genome-testdb hgcentraltest

# MAKE DESCRIPTION/SAMPLE POSITION HTML PAGE (DONE, 2004-06-30, hartera)
    ssh hgwdev
    mkdir /cluster/data/danRer1/html
    cd /cluster/data/danRer1/html
    # make a symbolic link from /gbdb/danRer1/html to /cluster/data/danRer1/html    ln -s /cluster/data/danRer1/html /gbdb/danRer1/html
    # Add a description page for zebrafish
    cd /cluster/data/danRer1/html
    cp /cluster/data/fr1/html/*.html .
    # Edit this for zebrafish
    
    # create a description.html page here
    mkdir -p ~/kent/src/hg/makeDb/trackDb/zebrafish/danRer1
    cd ~/kent/src/hg/makeDb/trackDb/
    cvs add zebrafish
    cvs commit zebrafish
    cd zebrafish
    cvs add danRer1
    cvs commit danRer1
    # Add description page here too
    cp /cluster/data/danRer1/html/description.html \
       $HOME/kent/src/hg/makeDb/trackDb/zebrafish/danRer1/
    chmod a+r \
          $HOME/kent/src/hg/makeDb/trackDb/zebrafish/danRer1/description.html
    cd $HOME/kent/src/hg/makeDb/trackDb/zebrafish/danRer1/
    # Check it in and copy (ideally using "make alpha" in trackDb) to
    # /gbdb/danRer1/html
    cvs add description.html
    cvs commit description.html

# PUT MASKED SEQUENCE OUT FOR CLUSTER RUNS (DONE, 2004-06-09, hartera)
    ssh kkr1u00
    # Chrom-level mixed nibs that have been repeat- and trf-masked:
    rm -rf /iscratch/i/danRer1/nib
    mkdir -p /iscratch/i/danRer1/nib
    cp -p /cluster/data/danRer1/nib/chr*.nib /iscratch/i/danRer1/nib
    # Pseudo-contig fa that have been repeat- and trf-masked:
    rm -rf /iscratch/i/danRer1/trfFa
    mkdir /iscratch/i/danRer1/trfFa
    foreach d (/cluster/data/danRer1/*/chr*_?{,?})
      cp $d/$d:t.fa /iscratch/i/danRer1/trfFa
    end
    rm -rf /iscratch/i/danRer1/rmsk
    mkdir -p /iscratch/i/danRer1/rmsk
    cp -p /cluster/data/danRer1/*/chr*.fa.out /iscratch/i/danRer1/rmsk
    cp -p /cluster/data/danRer1/danRer1.2bit /iscratch/i/danRer1/
    iSync

# CREATE gc5Base wiggle TRACK (DONE, 2004-06-09, hartera)
    ssh kki
    mkdir /cluster/data/danRer1/bed/gc5Base
    cd /cluster/data/danRer1/bed/gc5Base
    #   in the script below, the 'grep -w GC' selects the lines of
    #   output from hgGcPercent that are real data and not just some
    #   information from hgGcPercent.  The awk computes the number
    #   of bases that hgGcPercent claimed it measured, which is not
    #   necessarily always 5 if it ran into gaps, and then the division
    #   by 10.0 scales down the numbers from hgGcPercent to the range
    #   [0-100].  Two columns come out of the awk print statement:
    #   <position> and <value> which are fed into wigAsciiToBinary through
    #   the pipe.  It is set at a dataSpan of 5 because each value
    #   represents the measurement over five bases beginning with
    #   <position>.  The result files end up in ./wigData5.
    #   A new script is used (from makeHg17.doc) which gets around the
    #   problem that wigAsciiToBinary was calculating chromEnd to be 
    #   beyond the real chromosome end 

    mkdir wigData5 dataLimits5
    cat << '_EOF_' > kkRun.sh
#!/bin/sh
NIB=$1
                                                                                
chr=${NIB/.nib/}
chrom=${chr#chr}
                                                                                
hgGcPercent -chr=${chr} -doGaps -file=stdout -win=5 danRer1 \
        /iscratch/i/danRer1/nib | \
    grep -w GC | \
    awk '{if (($3-$2) >= 5) {printf "%d\t%.1f\n", $2+1, $5/10.0} }' | \
    wigAsciiToBinary -dataSpan=5 -chrom=${chr} \
        -wibFile=wigData5/gc5Base_${chrom} \
            -name=${chrom} stdin 2> dataLimits5/${chr}
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x kkRun.sh

    ls /iscratch/i/danRer1/nib  > nibList
    cat << '_EOF_' > gsub
#LOOP
./kkRun.sh $(path1)
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    gensub2 nibList single gsub jobList
    para create jobList
    para try, check, push ... etc
# para time
# Completed: 29 of 29 jobs
# CPU time in finished jobs:       2450s      40.84m     0.68h    0.03d  0.000 y
# IO & Wait Time:                     0s       0.00m     0.00h    0.00d  0.000 y
# Average job time:                  80s       1.33m     0.02h    0.00d
# Longest job:                      506s       8.43m     0.14h    0.01d
# Submission to last job:           650s      10.83m     0.18h    0.01d

    # load the .wig files back on hgwdev:
    ssh hgwdev
    cd /cluster/data/danRer1/bed/gc5Base
    hgLoadWiggle -pathPrefix=/gbdb/danRer1/wib/gc5Base \
                 danRer1 gc5Base wigData5/*.wig
    # and symlink the .wib files into /gbdb
    mkdir /gbdb/danRer1/wib/gc5Base
    ln -s `pwd`/wigData5/*.wib /gbdb/danRer1/wib/gc5Base

    # And then the zoomed data view
    ssh kki
    cd /cluster/data/danRer1/bed/gc5Base
    mkdir wigData5_1K dataLimits5_1K

    cat << '_EOF_' > kkRunZoom.sh
#!/bin/sh
NIB=$1
                                                                                
chr=${NIB/.nib/}
chrom=${chr#chr}
                                                                                
hgGcPercent -chr=${chr} -doGaps -file=stdout -win=5 danRer1 \
        /iscratch/i/danRer1/nib | \
    grep -w GC | \
    awk '{if (($3-$2) >= 5) {printf "%d\t%.1f\n", $2+1, $5/10.0} }' | \
    wigZoom -dataSpan=1000 stdin | wigAsciiToBinary -dataSpan=1000 \
        -chrom=${chr} -wibFile=wigData5_1K/gc5Base_${chrom}_1K \
            -name=${chrom} stdin 2> dataLimits5_1K/${chr}
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x kkRunZoom.sh

    cat << '_EOF_' > gsubZoom
#LOOP
./kkRunZoom.sh $(path1)
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
    gensub2 nibList single gsubZoom jobListZoom
    para create jobListZoom
    para try, check, push, ... etc.
# para time
# Completed: 29 of 29 jobs
# CPU time in finished jobs:       2433s      40.55m     0.68h    0.03d  0.000 y
# IO & Wait Time:                     0s       0.00m     0.00h    0.00d  0.000 y
# Average job time:                  81s       1.36m     0.02h    0.00d
# Longest job:                      522s       8.70m     0.14h    0.01d
# Submission to last job:          2503s      41.72m     0.70h    0.03d
                                                                                
    #   Then load these .wig files into the same database as above
    ssh hgwdev
    cd /cluster/data/danRer1/bed/gc5Base
    hgLoadWiggle -pathPrefix=/gbdb/danRer1/wib/gc5Base \
        -oldTable danRer1 gc5Base wigData5_1K/*.wig
    # and symlink these .wib files into /gbdb
    mkdir -p /gbdb/danRer1/wib/gc5Base
    ln -s `pwd`/wigData5_1K/*.wib /gbdb/danRer1/wib/gc5Base

# BLASTZ FOR HG17 (DONE, 2004-06-16, hartera)
    ssh kkr1u00
    # blastz requires lineage-specific repeats
    # Treat all repeats as lineage-specific.
                                                                                
    mkdir /iscratch/i/danRer1/linSpecRep.notInHuman
    foreach f (/iscratch/i/danRer1/rmsk/chr*.fa.out)
      cp -p $f /iscratch/i/danRer1/linSpecRep.notInHuman/$f:t:r:r.out.spec
    end
                                                                                
    mkdir /iscratch/i/gs.18/build35/linSpecRep.notInZebrafish
    foreach f (/iscratch/i/gs.18/build35/rmsk/chr*.fa.out)
 cp -p $f /iscratch/i/gs.18/build35/linSpecRep.notInZebrafish/$f:t:r:r.out.spec
    end
    iSync
                                                                                
    ssh kk
    mkdir -p /cluster/data/danRer1/bed/blastz.hg17.2004-06-08
    ln -s /cluster/data/danRer1/bed/blastz.hg17.2004-06-08 \
          /cluster/data/danRer1/bed/blastz.hg17
    cd /cluster/data/danRer1/bed/blastz.hg17
    # Set L=6000 and abridge repeats - these are the same parameters used
    # for hg16 and Fugu.
                                                                                
    cat << '_EOF_' > DEF
# zebrafish vs human (hg17)
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin
                                                                                
ALIGN=blastz-run
BLASTZ=blastz
                                                                                
# Reuse parameters from hg16-fr1.
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q
BLASTZ_ABRIDGE_REPEATS=1
                                                                                
# TARGET: Zebrafish
SEQ1_DIR=/iscratch/i/danRer1/nib/
SEQ1_RMSK=
SEQ1_FLAG=
SEQ1_SMSK=/iscratch/i/danRer1/linSpecRep.notInHuman
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000
                                                                                
# QUERY: Human (hg17) 
SEQ2_DIR=/iscratch/i/gs.18/build35/bothMaskedNibs
SEQ2_RMSK=
SEQ2_FLAG=
SEQ2_SMSK=/iscratch/i/gs.18/build35/linSpecRep.notInZebrafish
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=0
                                                                                
BASE=/cluster/data/danRer1/bed/blastz.hg17
                                                                                
DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len
'_EOF_'
    # << this line keeps emacs coloring happy
    # Save the DEF file in the current standard place
    cp DEF ~angie/hummus/DEF.danRer1-hg17.2004-06-08
    # Need shell scripts from mm4 to do cluster runs
    mv /cluster/data/mm4/jkStuff/BlastZ*.sh /cluster/data/danRer1/jkStuff/
    # edit BlastZ_run0.sh
    # replace line 22: /cluster/home/angie/schwartzbin/ with /cluster/bin/penn/
    # this is the directory for the latest version of blastz-run
                                                                                
    # prepare first cluster run
    ssh kk
    cd /cluster/data/danRer1/bed/blastz.hg17
    bash # if a csh/tcsh user
    . ./DEF
    /cluster/data/danRer1/jkStuff/BlastZ_run0.sh
    cd run.0
    # check batch looks ok then
    para try, check, push, check, ....
# para time
# Completed: 57970 of 57970 jobs
# CPU time in finished jobs:   20265671s  337761.18m  5629.35h  234.56d  0.643 y# IO & Wait Time:                570125s    9502.09m   158.37h    6.60d  0.018 y# Average job time:                 359s       5.99m     0.10h    0.00d
# Longest job:                    10482s     174.70m     2.91h    0.12d
# Submission to last job:         73680s    1228.00m    20.47h    0.85d
    # Took about 17 hours to run. Output is 6.3 Gigabytes - rather large.
    # Try processing and check in browser before adjusting parameters
    #   Second cluster run to convert the .out's to .lav's
    cd /cluster/data/danRer1/bed/blastz.hg17
    bash # if a csh/tcsh user
    . ./DEF
    /cluster/data/danRer1/jkStuff/BlastZ_run1.sh
    cd run.1
    para try, check, push, etc ...
# para time
# Completed: 170 of 170 jobs
# CPU time in finished jobs:      16047s     267.45m     4.46h    0.19d  0.001 y# IO & Wait Time:                146162s    2436.03m    40.60h    1.69d  0.005 y# Average job time:                 954s      15.90m     0.27h    0.01d
# Longest job:                     2179s      36.32m     0.61h    0.03d
# Submission to last job:          2451s      40.85m     0.68h    0.03d
                                                                                
    #   Third cluster run to convert lav's to axt's
    ssh kk
    cd /cluster/data/danRer1/bed/blastz.hg17
    mkdir axtChrom
    # a new run directory
    mkdir run.2
    cd run.2
cat << '_EOF_' > do.csh
#!/bin/csh
cd $1
cat `ls -1 *.lav | sort -g` \
| lavToAxt stdin /iscratch/i/danRer1/nib \
/iscratch/i/gs.18/build35/bothMaskedNibs stdout \
| axtSort stdin $2
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x do.csh
    cat << '_EOF_' > gsub
#LOOP
./do.csh {check in exists $(path1)} {check out line+ /cluster/data/danRer1/bed/blastz.hg17/axtChrom/$(root1).axt}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
    \ls -1Sd ../lav/chr* > chrom.list
    gensub2 chrom.list single gsub jobList
    wc -l jobList
    head jobList
    para create jobList
    para try, check, push, check,...
# para time
# Completed: 27 of 29 jobs
# Crashed: 2 jobs
# CPU time in finished jobs:       2063s      34.38m     0.57h    0.02d  0.000 y# IO & Wait Time:                 19245s     320.75m     5.35h    0.22d  0.001 y# Average job time:                 789s      13.15m     0.22h    0.01d
# Longest job:                     9103s     151.72m     2.53h    0.11d
# Submission to last job:         14332s     238.87m     3.98h    0.17d
                                                                                
# chrNA and chrUn are the crashed jobs. These are very large and ran out of
# memory. Re-run these separately on kolossus
                                                                                
     ssh kk
     # put nibs on bluearc
     mkdir -p /cluster/bluearc/danRer1/nib
     mkdir -p /cluster/bluearc/hg17/bothMaskedNibs
     cp /iscratch/i/danRer1/nib/* /cluster/bluearc/danRer1/nib
     cp /iscratch/i/gs.18/build35/bothMaskedNibs/* \
        /cluster/bluearc/hg17/bothMaskedNibs
                                                                                
     cat << '_EOF_' > do2.csh
#!/bin/csh
cd $1
cat `ls -1 *.lav | sort -g` \
| lavToAxt stdin /cluster/bluearc/danRer1/nib \
/cluster/bluearc/hg17/bothMaskedNibs stdout \
| axtSort stdin $2
'_EOF_'
                                                                                
    ssh kolossus
    cd /cluster/data/danRer1/bed/blastz.hg17/run.2
    ./do2.csh ../lav/chrNA \
              /cluster/data/danRer1/bed/blastz.hg17/axtChrom/chrNA.axt
    ./do2.csh ../lav/chrUn \
             /cluster/data/danRer1/bed/blastz.hg17/axtChrom/chrUn.axt
                                                                                
    # translate sorted axt files into psl
    ssh kolossus
    cd /cluster/data/danRer1/bed/blastz.hg17
    mkdir -p pslChrom
    set tbl = "blastzHg17"
    foreach f (axtChrom/chr*.axt)
      set c=$f:t:r
      echo "Processing chr $c"
      /cluster/bin/i386/axtToPsl $f S1.len S2.len pslChrom/${c}_${tbl}.psl
    end
                                                                                
    # Load database tables
    ssh hgwdev
    cd /cluster/data/danRer1/bed/blastz.hg17/pslChrom
                                                                                
    foreach f (./*.psl)
      /cluster/bin/i386/hgLoadPsl -noTNameIx danRer1 $f
      echo "$f Done"
    end
# To tune Blastz parameters: these parameters above where used but it was 
# thought that there were probably too many pileups. So then the
# blastz was repeated with L=10000 (as for hg16-galGal2) and with an
# intermediate value of L=8000. L is the threshold for gapped alignments.
# Looking at alignments in the browser it was found that L=10000 and L=8000
# were too stringent and alignments to coding regions were being lost.
# L=6000 (as above) seemed to be the best choice and then low scoring 
# alignments could be filtered at the chaining step instead to reduce pileups.
# e.g. using the browser with the non-zebrafish mRNAs track turned on
# chr1:24,547-28,249 region lost an exon with all but blastz with L=6000
# chr2:1,124,174-1,135,282 is worse with an entire gene being lost except
# with the Blastz with L=6000.

# RESCORE HG17 BLASTZ (DONE, 2004-06-21, hartera)
# USE HG17 BLASTZ WITH L=6000 SO AS NOT TO MISS TOO MUCH CDS ALIGING
    # Low scores can occur with repeats abridged and using the
    # HoxD55.q matrix. PSU's restore_rpts program rescored alignments
    # with the default matrix instead of the BLASTZ_Q matrix.
    # Rescore them here so the chainer sees the higher scores:
                                                                                
    ssh kolossus
    cd /cluster/data/danRer1/bed/blastz.hg17
    mkdir axtChrom.rescore
    foreach f (axtChrom/chr*.axt)
        axtRescore -scoreScheme=/cluster/data/blastz/HoxD55.q \
        $f axtChrom.rescore/$f:t
    end
    mv axtChrom axtChrom.orig
    mv axtChrom.rescore axtChrom
                                                                                
#   psl files and blastz tables will be the same regardless of score so
#   no need to reload
                                                                                
# CHAIN HG17 BLASTZ (DONE, 2004-06-24, hartera)
    # Re do chains with rescored blastz Hg17
    # Run axtChain on little cluster
    ssh kki
    cd /cluster/data/danRer1/bed/blastz.hg17
    mv axtChain axtChain.orig
    mkdir -p axtChain/run1
    cd axtChain/run1
    mkdir out chain
    # create 2 input lists as need to process chrNA and chrUn separately
    ls -1S /cluster/data/danRer1/bed/blastz.hg17/axtChrom/*.axt \
        > input.lst
    grep "chrNA" input.lst > inputNAandUn.lst
    grep "chrUn" input.lst >> inputNAandUn.lst
    # remove chrNA and chrUn from input.lst
                                                                                
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out line+ out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
                                                                                
    # Reuse gap penalties from hg16 vs chicken run.
    cat << '_EOF_' > ../../chickenHumanTuned.gap
tablesize^V     11
smallSize^V     111
position^V      1^V     2^V     3^V     11^V    111^V   2111^V  12111^V 32111^V
72111^V 152111^V        252111
qGap^V  325^V   360^V   400^V   450^V   600^V   1100^V  3600^V  7600^V  15600^V
31600^V 56600
bothGap^V       625^V   660^V   700^V   750^V   900^V   1400^V  4000^V  8000^V
16000^V 32000^V 57000
'_EOF_'
    # << this line makes emacs coloring happy
                                                                                
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out line+ out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
cat << '_EOF_' > doChain
#!/bin/csh
axtFilter $1 \
| axtChain -scoreScheme=/cluster/data/blastz/HoxD55.q \
                      -linearGap=../../chickenHumanTuned.gap \
                      -minScore=10000 stdin \
    /iscratch/i/danRer1/nib \
    /iscratch/i/gs.18/build35/bothMaskedNibs $2 >& $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doChain
    gensub2 input single gsub jobList
    para create jobList
    para try, check, push, check...
# para time
# Completed: 27 of 27 jobs
# CPU time in finished jobs:       4572s      76.21m     1.27h    0.05d  0.000 y# IO & Wait Time:                  1252s      20.86m     0.35h    0.01d  0.000 y# Average job time:                 216s       3.60m     0.06h    0.00d
# Longest job:                     1092s      18.20m     0.30h    0.01d
# Submission to last job:          3242s      54.03m     0.90h    0.04d
                                                                                
    cat << '_EOF_' > gsub2
#LOOP
doChain2 {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out line+ out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
cat << '_EOF_' > doChain2
#!/bin/csh
axtFilter $1 \
| axtChain -scoreScheme=/cluster/data/blastz/HoxD55.q \
                      -linearGap=../../chickenHumanTuned.gap \
                      -minScore=15000 stdin \
    /iscratch/i/danRer1/nib \
    /iscratch/i/gs.18/build35/bothMaskedNibs $2 >& $3
'_EOF_'
    # << this line makes emacs coloring happy
    gensub2 inputNAandUn.lst single gsub2 jobList2
    para create jobList2
    para try, check, push, check...
# para time
# crashes (out of memory) so try again on kolossus
    ssh kolossus
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChain/run1
    # use sequences on bluearc for kolossus
    cat << '_EOF_' > doChain2
#!/bin/csh
axtFilter $1 \
| axtChain -scoreScheme=/cluster/data/blastz/HoxD55.q \
                      -linearGap=../../chickenHumanTuned.gap \
                      -minScore=15000 stdin \
    /cluster/bluearc/danRer1/nib \
    /cluster/bluearc/hg17/bothMaskedNibs $2 >& $3
'_EOF_'
    chmod +x doChain2
                                                                                
    doChain2 /cluster/data/danRer1/bed/blastz.hg17/axtChrom/chrNA.axt \
               chain/chrNA.chain out/chrNA.out
    doChain2 /cluster/data/danRer1/bed/blastz.hg17/axtChrom/chrUn.axt \
               chain/chrUn.chain out/chrUn.out
                                                                                
    # now on the cluster server, sort chains
    ssh kksilo
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChain
    chainMergeSort run1/chain/*.chain > all.chain
    chainSplit chain all.chain
# take a look at score distr's,try also with larger bin size.
    foreach f (chain/*.chain)
      grep chain $f | awk '{print $2;}' | sort -nr > /tmp/score.$f:t:r
      echo $f:t:r >> hist.out
      textHistogram -binSize=10000 /tmp/score.$f:t:r >> hist.out
      echo ""
    end
                                                                                
    # Load chains into database
    # next machine
    ssh hgwdev
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChain/chain
    foreach i (*.chain)
        set c = $i:r
        hgLoadChain danRer1 ${c}_chainHg17 $i
        echo done $c
    end
# load in original chains as chainHg17NoFilt - minScore = 5000 on all chroms
# featureBits -chrom=chr1 danRer1 chainHg17
# 12359874 bases of 40488791 (30.527%) in intersection
# featureBits -chrom=chr1 danRer1 refGene:cds chainHg17
# 147375 bases of 40488791 (0.364%) in intersection
# featureBits -chrom=chr1 danRer1 chainHg17NoFilt
# 13518074 bases of 40488791 (33.387%) in intersection
# featureBits -chrom=chr1 danRer1 refGene:cds chainHg17NoFilt
#  149120 bases of 40488791 (0.368%) in intersection
                                                                                
# featureBits -chrom=chrNA danRer1 chainHg17
# 79678867 bases of 335615307 (23.741%) in intersection
# featureBits -chrom=chrNA danRer1 refGene:cds chainHg17
# 556727 bases of 335615307 (0.166%) in intersection
# featureBits -chrom=chrNA danRer1 chainHg17NoFilt  
# 100579306 bases of 335615307 (29.969%) in intersection
# featureBits -chrom=chrNA danRer1 refGene:cds chainHg17NoFilt
# 622781 bases of 335615307 (0.186%) in intersection
# featureBits -chrom=chrNA danRer1 refGene:cds
# 741426 bases of 335615307 (0.221%) in intersection

# Using minScore=10000 for all chroms but 15000 for chrUn and chrNA
# reduces low scoring chains but without compromising CDS region coverage
# too much.

# NET HG17 BLASTZ (DONE, 2004-07-07, hartera)
                                                                                
    ssh kksilo
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChain
    mkdir preNet
    cd chain
    foreach i (*.chain)
       echo preNetting $i
       /cluster/bin/i386/chainPreNet $i ../../S1.len ../../S2.len \
                                     ../preNet/$i
    end
                                                                                
    cd ..
    mkdir n1
    cd preNet
    foreach i (*.chain)
      set n = $i:r.net
      echo primary netting $i
      /cluster/bin/i386/chainNet $i -minSpace=1 ../../S1.len ../../S2.len \
                                 ../n1/$n /dev/null
    end
    cd ..
    cat n1/*.net | /cluster/bin/i386/netSyntenic stdin noClass.net
    #  memory usage 111640576, utime 625 s/100, stime 120
                                                                                
# Add classification info using db tables:
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChain
    # netClass looks for ancient repeats in one of the databases
    # hg17 has this table - hand-curated by Arian but this is for
    # human-rodent comparisons so do not use here, use -noAr option
    mkdir -p /cluster/bluearc/danRer1/linSpecRep.notInHuman
    mkdir -p /cluster/bluearc/hg17/linSpecRep.notInZebrafish
    cp /iscratch/i/gs.18/build35/linSpecRep.notInZebrafish/* \
       /cluster/bluearc/hg17/linSpecRep.notInZebrafish
    cp /iscratch/i/danRer1/linSpecRep.notInHuman/* \
       /cluster/bluearc/danRer1/linSpecRep.notInHuman
    ssh hgwdev
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChain
                                                                                
    time netClass noClass.net danRer1 hg17 humanhg17.net \
         -tNewR=/cluster/bluearc/danRer1/linSpecRep.notInHuman \
         -qNewR=/cluster/bluearc/hg17/linSpecRep.notInZebrafish -noAr
    # 85.530u 52.020s 5:15.86 43.5%   0+0k 0+0io 2072pf+0w
    ssh hgwdev
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChain
    netFilter -minGap=10 humanhg17.net |  hgLoadNet danRer1 netHg17 stdin

# MAKE 10.OOC, 11.OOC FILE FOR BLAT (DONE, 2004-06-09, hartera)
    # Use -repMatch=460 (based on size -- for human we use 1024, and
    # the zebrafish genome is ~45% of the size of the human genome
    ssh kkr1u00
    mkdir /cluster/data/danRer1/bed/ooc
    cd /cluster/data/danRer1/bed/ooc
    mkdir -p /cluster/bluearc/danRer1
    ls -1 /cluster/data/danRer1/nib/chr*.nib > nib.lst
    blat nib.lst /dev/null /dev/null -tileSize=11 \
      -makeOoc=/cluster/bluearc/danRer1/11.ooc -repMatch=460
    # Wrote 44155 overused 11-mers to /cluster/bluearc/danRer1/11.ooc
    # For 10.ooc, repMatch = 4096 for human, so use 1840
        blat nib.lst /dev/null /dev/null -tileSize=10 \
      -makeOoc=/cluster/bluearc/danRer1/10.ooc -repMatch=1840
    # Wrote 10767 overused 10-mers to /cluster/bluearc/danRer1/10.ooc
    # keep copies of ooc files in this directory and copy to iscratch
    cp /cluster/bluearc/danRer1/*.ooc .
    cp -p /cluster/bluearc/danRer1/*.ooc /iscratch/i/danRer1/
    iSync

# AUTO UPDATE GENBANK MRNA AND EST RUN  (DONE, 2004-07-01, hartera)
    ssh eieio
    cd /cluster/data/genbank
    # This is a new organism, edit the etc/genbank.conf file and add:
# danRer1 (zebrafish)
danRer1.genome = /iscratch/i/danRer1/nib/chr*.nib
danRer1.lift = /cluster/data/danRer1/jkStuff/liftAll.lft
danRer1.downloadDir = danRer1
    # Default includes native genbank mRNAs and ESTs,
    # genbank xeno mRNAs but no xenoESTs, native RefSeq mRNAs but
    # not xeno RefSeq
    cvs commit -m "Added danRer1" etc/genbank.conf
    # Edit src/align/gbBlat to add /iscratch/i/danRer1/11.ooc
    # Add line:  DANRER_OOC=/iscratch/i/danRer1/11.ooc
    cvs diff src/align/gbBlat
    make
    cvs commit -m "Added 11.ooc for danRer1." src/align/gbBlat
    # Edit src/lib/gbGenome.c to add new genome
    # ADD: static char *danRerNames[] = {"Danio rerio", NULL};
    # ADD: {"danRer", danRerNames, NULL},
    # to static struct dbToSpecies dbToSpeciesMap[]
    cvs commit -m "Added zebrafish, danRer1." lib/gbGenome.c
    # Install to /cluster/data/genbank:
    make install-server
                                                                                
    ssh eieio
    cd /cluster/data/genbank
    # This is an -initial run, RefSeq mRNA only:
    nice bin/gbAlignStep -srcDb=refseq -type=mrna -verbose=1 -initial danRer1
    # Load results for RefSeq mRNAs:
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 -drop -initialLoad danRer1
    # To start next run, results need to be moved out the way
    cd /cluster/data/genbank/work
    mv initial.danRer1 initial.danRer1.refseq.mrna
                                                                                
    # -initial for GenBank mRNAs
    ssh eieio
    cd /cluster/data/genbank
    nice bin/gbAlignStep -srcDb=genbank -type=mrna -verbose=1 -initial danRer1
                                                                                
    # Load results for GenBank mRNAs
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 danRer1
    cd /cluster/data/genbank/work
    mv initial.danRer1 initial.danRer1.genbank.mrna
                                                                                
    # -initial for ESTs
    ssh eieio
    cd /cluster/data/genbank
    nice bin/gbAlignStep -srcDb=genbank -type=est -verbose=1 -initial danRer1
                                                                                
    # Load results for GenBank ESTs
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 danRer1
    cd /cluster/data/genbank/work
    mv initial.danRer1 initial.danRer1.genbank.est

# ENSEMBL GENES (DONE, 2004-06-22, hartera)

   mkdir /cluster/data/danRer1/bed/ensembl
   cd /cluster/data/danRer1/bed/ensembl
    # Get the ensembl protein data from
    # http://www.ensembl.org/Danio_rerio/martview
    # Follow this sequence through the pages:
    # Page 1) Make sure that the Danio_rerio choice is selected. Hit next.
    # Page 2) Uncheck the "Limit to" box in the region choice. Then hit next.
    # Page 3) Choose the "Structures" box.
    # Page 4) Choose GTF as the ouput.  choose gzip compression.  hit export.
    # Save as ensemblGene.gtf.gz
# Need to get lift files for each chrom from supercontig to chrom level
    ssh kksilo
    cd /cluster/data/danRer1/bed/ensembl/liftSupertoChrom
    foreach c (NA Un Finished)
       set b = "/cluster/data/danRer1"
       awk '{print $6;}' $b/$c/chr$c.supercontigs.agp > chr$c.supercontigs.lst
       $HOME/bin/i386/faSomeRecords $b/Zv3.supercontigs.fa \
          ./chr$c.supercontigs.lst ./chr$c.fa
    echo $c " done"
    end

     # check FASTA files then generate AGP and lift files 
     # from the chromosome fastas 
     foreach c (NA Un Finished)
        $HOME/bin/i386/scaffoldFaToAgp ./chr$c.fa
        sed -e "s/chrUn/${c}/;" chr$c.agp > chr$c.superToChrom.agp
        sed -e "s/chrUn/${c}/;" chr$c.lft > chr$c.superToChrom.lft
     end
     cat *.superToChrom.lft > liftUnSuperToChrom.lft

     # get chrUn, Finished and NA records
     cd /cluster/data/danRer1/bed/ensembl
     awk '$1 ~ /NA/ || $1 ~ /ctg/ || $1 ~ /Finished/' ensemblGene.gtf \
                    > ensemblGenechrUns.gtf
     awk '$1 !~ /NA/ && $1 !~ /ctg/ && $1 !~ /Finished/' ensemblGene.gtf \
                    > ensemblGenechroms.gtf
     liftUp -type=.gtf ensemblGenechrUns.lifted \
         ./liftSupertoChrom/liftUnSuperToChrom.lft warn ensemblGenechrUns.gtf
     # Got 113698 lifts in ./liftSupertoChrom/liftUnSuperToChrom.lft

     # Process chroms and virtual chroms. Add "chr" to front of each line
     cp ensemblGenechroms.gtf all.gtf
     cat ensemblGenechrUns.lifted >> all.gtf

     sed -e "s/^/chr/" all.gtf > ensGene.gtf
     # load into database
     ssh hgwdev
     /cluster/bin/i386/ldHgGene danRer1 ensGene \
            /cluster/data/danRer1/bed/ensembl/ensGene.gtf
# Read 30783 transcripts in 554794 lines in 1 files
#   30783 groups 28 seqs 1 sources 4 feature types
# 30783 gene predictions

    # ensGtp associates geneId/transcriptId/proteinId for hgPepPred and 
    # hgKnownToSuper.  Use ensMart to create it as above, except:
    # Page 3) Choose the "Features" box. In "Ensembl Attributes", check 
    # Ensembl Gene ID, Ensembl Transcript ID, Ensembl Peptide ID.  
    # Choose Text, tab-separated as the output format.  Result name ensGtp.
    # Save file as ensGtp.txt.gz
    gunzip ensGtp.txt.gz
    hgsql danRer1 < ~/kent/src/hg/lib/ensGtp.sql
    # remove header line from ensGtp.txt
    echo "load data local infile 'ensGtp.txt' into table ensGtp" \
         | hgsql -N danRer1

    # Get the ensembl peptide sequences from
    # http://www.ensembl.org/Danio_rerio/martview
    # Choose Danio Rerio as the organism
    # Follow this sequence through the pages:
    # Page 1) Choose the Ensembl Genes choice. Hit next.
    # Page 2) Uncheck the "Limit to" box in the region choice. Then hit next.
    # Page 3) Choose the "Sequences" tab.
    # Page 4) Choose Transcripts/Proteins and peptide Only as the output,
    # choose text/fasta and gzip compression,
    # name the file ensGeneDanRer1.pep.gz and then hit export.

    gunzip ensGeneDanRer1.pep.gz
    # delete * at end of some proteins
    cat ensGeneDanRer1.pep | sed 's/\*$//' > ensembl.pep
    ~matt/bin/fixPep.pl ensembl.pep fixPep_ensembl.pep
     hgPepPred danRer1 generic ensPep fixPep_ensembl.pep

# BLASTZ FUGU (Fr1) (DONE, 2004-06-13, hartera)
    ssh kkr1u00
    # blastz requires lineage-specific repeats but there are none
    # available between these two fish species to make empty files
    mkdir -p /iscratch/i/danRer1/linSpecRep.notInDanRer
    mkdir -p /iscratch/i/fugu/linSpecRep.notInFugu
    cd /cluster/data/danRer1
      # for zebrafish
    foreach c (`cat chrom.lst`)
      cp /dev/null /iscratch/i/danRer1/linSpecRep.notInDanRer/chr${c}.out.spec
    end
      # for Fugu
    cp /dev/null /iscratch/i/fugu/linSpecRep.notInFugu/chrUn.out.spec
    iSync

    ssh kk
    mkdir -p /cluster/data/danRer1/bed/blastz.fr1.2004-06-13
    ln -s /cluster/data/danRer1/bed/blastz.fr1.2004-06-13 \
          /cluster/data/danRer1/bed/blastz.fr1
    cd /cluster/data/danRer1/bed/blastz.fr1

    cat << '_EOF_' > DEF
# zebrafish (danRer1) vs. Fugu (fr1)
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
#BLASTZ_ABRIDGE_REPEATS=1 if SMSK is specified
BLASTZ_ABRIDGE_REPEATS=0

# TARGET - zebrafish (danRer1)
SEQ1_DIR=/iscratch/i/danRer1/nib
SEQ1_RMSK=
# lineage-specific repeats
# we don't have that information for these species so these files are empty
SEQ1_SMSK=/iscratch/i/danRer1/linSpecRep.notInDanRer
SEQ1_FLAG=
SEQ1_IN_CONTIGS=0
# 10 MB chunk for target
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY - Fugu (fr1)
# soft-masked chrom nib
SEQ2_DIR=/cluster/bluearc/fugu/fr1/chromNib
SEQ2_RMSK=
SEQ2_SMSK=/iscratch/i/fugu/linSpecRep.notInFugu
SEQ2_FLAG=
SEQ2_IN_CONTIGS=0
# 10 Mbase for query
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/cluster/data/danRer1/bed/blastz.fr1

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len

#DEBUG=1
'_EOF_'
    # << this line keeps emacs coloring happy

    # save the DEF file in the current standard place
    chmod +x DEF
    cp DEF ~angie/hummus/DEF.danRer1-fr1.2004-06-13

    # setup cluster run
    # source the DEF file
    bash
    . ./DEF
    /cluster/data/danRer1/jkStuff/BlastZ_run0.sh
    cd run.0
    # check batch looks ok then
    para try, check, push, check, ....
# Completed: 5950 of 5950 jobs
# CPU time in finished jobs:    1590351s   26505.86m   441.76h   18.41d  0.050 y
# IO & Wait Time:                 27786s     463.09m     7.72h    0.32d  0.001 y
# Average job time:                 272s       4.53m     0.08h    0.00d
# Longest job:                      601s      10.02m     0.17h    0.01d
# Submission to last job:          4298s      71.63m     1.19h    0.05d

    cd /cluster/data/danRer1/bed/blastz.fr1
    bash # if a csh/tcsh user
    . ./DEF
    /cluster/data/danRer1/jkStuff/BlastZ_run1.sh
    cd run.1
    para try, check, push, etc ...
# para time
# Completed: 170 of 170 jobs
# CPU time in finished jobs:       1722s      28.69m     0.48h    0.02d  0.000 y
# IO & Wait Time:                 16055s     267.59m     4.46h    0.19d  0.001 y
# Average job time:                 105s       1.74m     0.03h    0.00d
# Longest job:                      152s       2.53m     0.04h    0.00d
# Submission to last job:           310s       5.17m     0.09h    0.00d

    #   Third cluster run to convert lav's to axt's
    ssh kk
    cd /cluster/data/danRer1/bed/blastz.fr1
    mkdir axtChrom
    # a new run directory
    mkdir run.2
    cd run.2
cat << '_EOF_' > do.csh
#!/bin/csh
cd $1
cat `ls -1 *.lav | sort -g` \
| lavToAxt stdin /iscratch/i/danRer1/nib \
/cluster/bluearc/fugu/fr1/chromNib stdout \
| axtSort stdin $2
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x do.csh
    cat << '_EOF_' > gsub
#LOOP
./do.csh {check in exists $(path1)} {check out line+ /cluster/data/danRer1/bed/blastz.fr1/axtChrom/$(root1).axt}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
    \ls -1Sd ../lav/chr* > chrom.list
    gensub2 chrom.list single gsub jobList
    wc -l jobList
    head jobList
    para create jobList
    para try, check, push, check,...
# para time
# Completed: 29 of 29 jobs
# CPU time in finished jobs:        600s      10.00m     0.17h    0.01d  0.000 y
# IO & Wait Time:                  2782s      46.37m     0.77h    0.03d  0.000 y
# Average job time:                 117s       1.94m     0.03h    0.00d
# Longest job:                      401s       6.68m     0.11h    0.00d
# Submission to last job:           966s      16.10m     0.27h    0.01d

    # translate sorted axt files into psl
    ssh kolossus
    cd /cluster/data/danRer1/bed/blastz.fr1
    mkdir -p pslChrom
    set tbl = "blastzFr1"
    foreach f (axtChrom/chr*.axt)
      set c=$f:t:r
      echo "Processing chr $c"
      /cluster/bin/i386/axtToPsl $f S1.len S2.len pslChrom/${c}_${tbl}.psl
    end

    # Load database tables
    ssh hgwdev
    cd /cluster/data/danRer1/bed/blastz.fr1/pslChrom

    foreach f (./*.psl)
      /cluster/bin/i386/hgLoadPsl danRer1 $f
      echo "$f Done"
    end

# CHAIN FUGU (Fr1) BLASTZ (DONE, 2004-06-24, hartera)
    # Create chains using -minScore=5000 for axtChain for chroms and
    # -minScore = 10000 for the messy chroms - chrUn and chrNA.
    # Run axtChain on little cluster
    ssh kki
    cd /cluster/data/danRer1/bed/blastz.fr1
    mkdir -p axtChain2/run1
    cd axtChain/run1
    mkdir out chain
    # create 2 input lists as need to process chrNA and chrUn separately
    ls -1S /cluster/data/danRer1/bed/blastz.fr1/axtChrom/*.axt \
        > input.lst
    grep "chrNA" input.lst > inputNAandUn.lst
    grep "chrUn" input.lst >> inputNAandUn.lst
    # remove chrNA and chrUn from input.lst
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out line+ out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    cat << '_EOF_' > doChain
#!/bin/csh
axtChain -minScore=5000 $1 \
    /iscratch/i/danRer1/nib \
    /cluster/bluearc/fugu/fr1/chromNib $2 >& $3
'_EOF_'
    # << this line makes emacs coloring happy

    chmod a+x doChain
    gensub2 input.lst single gsub jobList
    para create jobList
    para try, check, push, check...
# para time
# Completed: 27 of 27 jobs
# CPU time in finished jobs:        356s       5.94m     0.10h    0.00d  0.000 y
# IO & Wait Time:                   128s       2.13m     0.04h    0.00d  0.000 y# Average job time:                  18s       0.30m     0.00h    0.00d
# Longest job:                       28s       0.47m     0.01h    0.00d
# Submission to last job:           830s      13.83m     0.23h    0.01d

    cat << '_EOF_' > gsub2
#LOOP
doChain2 {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out line+ out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    cat << '_EOF_' > doChain2
#!/bin/csh
axtChain -minScore=10000 $1 \
    /iscratch/i/danRer1/nib \
    /cluster/bluearc/fugu/fr1/chromNib $2 >& $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x doChain2
    gensub2 inputNAandUn.lst single gsub2 jobList2
    para create jobList2
    para try, check, ...
# para time
# CPU time in finished jobs:        175s       2.92m     0.05h    0.00d  0.000 y
# IO & Wait Time:                    64s       1.06m     0.02h    0.00d  0.000 y
# Average job time:                 120s       1.99m     0.03h    0.00d
# Longest job:                      120s       2.00m     0.03h    0.00d
# Submission to last job:           120s       2.00m     0.03h    0.00d

    # now on the cluster server, sort chains
    ssh kksilo
    cd /cluster/data/danRer1/bed/blastz.fr1/axtChain2
    chainMergeSort run1/chain/*.chain > all.chain
    chainSplit chain all.chain

    # Load chains into database
    # next machine
    ssh hgwdev
    cd /cluster/data/danRer1/bed/blastz.fr1/
    # replace original axtChain with default minScore (=1000)
    mv axtChain axtChain.orig
    mv axtChain2 axtChain
    # remove old chain tables from database for Filt and Filt2

    cd /cluster/data/danRer1/bed/blastz.fr1/axtChain/chain
    foreach i (*.chain)
        set c = $i:r
        hgLoadChain danRer1 ${c}_chainFr1 $i
        echo done $c
    end

# chain tables for fr1:
# chainFr1: default minScore = 1000
# chainFr1Filt1: minScore = 10000 for all chroms, except 15000 for Un and NA
# chainFr1Filt2: minScore = 5000 for all chroms, except 10000 for Un and NA
# (chainFr1Filt2 is now chainFr1)
# coverage is calculated as
#     size(refGene:CDS & Fr1) / size(refGene:CDS)
# enrichment is
#     (size(refGene:CDS & Fr1) / size(Fr1) ) / size(refGene:CDS)                # Chains:
# featureBits -chrom=chr1 danRer1 chainFr1Link
# 2170390 bases of 40488791 (5.360%) in intersection
# featureBits -chrom=chr1 danRer1 chainFr1FiltLink
# 1505825 bases of 40488791 (3.719%) in intersection
# featureBits -chrom=chr1 danRer1 chainFr1Filt2Link
# 1897128 bases of 40488791 (4.686%) in intersection

# featureBits -chrom=chr1 danRer1 refGene:cds chainFr1Link
# 149280 bases of 40488791 (0.369%) in intersection
# featureBits -chrom=chr1 danRer1 refGene:cds chainFr1FiltLink
# 135943 bases of 40488791 (0.336%) in intersection
# featureBits -chrom=chr1 danRer1 refGene:cds chainFr1Filt2Link
# 145585 bases of 40488791 (0.360%) in intersection

# featureBits -chrom=chr1 danRer1 refGene:cds
# 165468 bases of 40488791 (0.409%) in intersection
# featureBits danRer1 refGene:cds
# 4852787 bases of 1459132082 (0.333%) in intersection
#                Coverage Enrichment
# chainFr1Link:      90.2%   17.0X
# chainFr1FiltLink:  82.2%   22.1X
# chainFr1Filt2Link: 88.0%   18.8X

# NET FUGU (Fr1) BLASTZ (DONE, 2004-06-25, hartera)
    ssh kksilo
    cd /cluster/data/danRer1/bed/blastz.fr1/axtChain
    mkdir preNet
    cd chain
    foreach i (*.chain)
       echo preNetting $i
       /cluster/bin/i386/chainPreNet $i ../../S1.len ../../S2.len \
                                     ../preNet/$i
    end

    cd ..
    mkdir n1
    cd preNet
    foreach i (*.chain)
      set n = $i:r.net
      echo primary netting $i
      /cluster/bin/i386/chainNet $i -minSpace=1 ../../S1.len ../../S2.len \
                                 ../n1/$n /dev/null
    end

    cd ..
    cat n1/*.net | /cluster/bin/i386/netSyntenic stdin noClass.net
    # memory usage 97132544, utime 737 s/100, stime 115
    # Add classification info using db tables:
    ssh hgwdev
    cd /cluster/data/danRer1/bed/blastz.fr1/axtChain
    # use -noAr option - don't look for ancient repeats
    netClass -noAr noClass.net danRer1 fr1 fr1.net

    # Load the nets into database
    ssh hgwdev
    cd /cluster/data/danRer1/bed/blastz.fr1/axtChain
    netFilter -minGap=10 fr1.net | hgLoadNet danRer1 netFr1 stdin

# EXTRACT AXT'S AND MAF'S FROM FUGU NET (DONE, 2004-06-25, hartera)
    ssh eieio
    cd /cluster/data/danRer1/bed/blastz.fr1/axtChain
    netSplit fr1.net fr1Net
    mkdir -p ../axtNet
# make sorted axts from net 
cat > axtNet.csh << 'EOF'
    foreach f (fr1Net/chr*.net)
        set c = $f:t:r
        echo "axtNet on $c"
        netToAxt fr1Net/$c.net chain/$c.chain \
        /cluster/data/danRer1/nib /cluster/data/fr1/nib stdout \
        | axtSort stdin ../axtNet/$c.axt
    echo "Complete: $c.net -> $c.axt"
    end
'EOF'
    csh axtNet.csh >&! axtNet.log &
    tail -100f axtNet.log

    ssh eieio
    cd /cluster/data/danRer1/bed/blastz.fr1
    cd axtNet
    mkdir ../mafNet

cat > makeMaf.csh << 'EOF'
    foreach f (chr*.axt)
      set maf = $f:t:r.fr1.maf
      echo translating $f to $maf
      axtToMaf $f \
            /cluster/data/danRer1/chrom.sizes /cluster/data/fr1/chrom.sizes \
            ../mafNet/$maf -tPrefix=danRer1. -qPrefix=fr1.
    end
'EOF'
    csh makeMaf.csh >&! makeMaf.log &
    tail -100f makeMaf.log

# AFFYMETRIX ZEBRAFISH GENOME ARRAY CHIP 
# (DONE, 2004-07-01, hartera)
    mkdir /projects/compbio/data/microarray/affyZebrafish
    cd /projects/compbio/data/microarray/affyZebrafish
  # Download Zebrafish genome array consensus sequences from Affymetrix
  # http://www.affymetrix.com/support/technical/byproduct.affx?product=zebrafish
    unzip Zebrafish_consensus.zip
    sed -e 's/consensus://' Zebrafish_consensus \
           | sed -e 's/;/ /' > Zebrafish_consensus.fa

    # copy sequences to a directory visible on kkr1u00

    cp /projects/compbio/data/microarray/affyZebrafish/Zebrafish_consensus.fa \
       /cluster/bluearc/affy/
    # Set up cluster job to align Zebrafish consensus sequences to danRer1
    ssh kkr1u00
    cd /cluster/data/danRer1/bed
    mkdir affyZebrafish.2004-06-30
    ln -s /cluster/data/danRer1/bed/affyZebrafish.2004-06-30 \
          /cluster/data/danRer1/bed/affyZebrafish
    cd affyZebrafish
    mkdir -p /iscratch/i/affy
    cp /cluster/bluearc/affy/Zebrafish_consensus.fa /iscratch/i/affy
    iSync

    ssh kk
    cd /cluster/data/danRer1/bed/affyZebrafish
    ls -1 /iscratch/i/affy/Zebrafish_consensus.fa > affy.lst
    ls -1 /iscratch/i/danRer1/trfFa/ > genome.lst

    echo '#LOOP\n/cluster/bin/i386/blat -fine -mask=lower -minIdentity=95 -ooc=/iscratch/i/danRer1/11.ooc /iscratch/i/danRer1/trfFa/$(path1) $(path2) {check out line+ psl/$(root1)_$(root2).psl}\n#ENDLOOP' > template.sub

    gensub2 genome.lst affy.lst template.sub para.spec
    mkdir psl
    para create para.spec
    para try, check, push ... etc.
# para time
# Completed: 310 of 310 jobs
# CPU time in finished jobs:      14931s     248.85m     4.15h    0.17d  0.000 y
# IO & Wait Time:                  1028s      17.13m     0.29h    0.01d  0.000 y
# Average job time:                  51s       0.86m     0.01h    0.00d
# Longest job:                       96s       1.60m     0.03h    0.00d
# Submission to last job:           574s       9.57m     0.16h    0.01d

    # Do sort, best in genome filter, and convert to chromosome coordinates
    # to create affyZebrafish.psl
    pslSort dirs raw.psl tmp psl
    # only use alignments that have at least
    # 95% identity in aligned region.
    # do not use minCover since a lot of sequence is in Un, NA and Finished
    # so genes may be split up so good to see all alignments
    pslReps -minAli=0.95 -nearTop=0.005 raw.psl contig.psl /dev/null
    liftUp affyZebrafish.psl ../../jkStuff/liftAll.lft warn contig.psl
    # shorten names in psl file
    sed -e 's/Zebrafish://' affyZebrafish.psl > affyZebrafish.psl.bak
    mv affyZebrafish.psl.bak affyZebrafish.psl
    pslCheck affyZebrafish.psl
    # psl is good                                                              
    # load track into database
    ssh hgwdev
    cd /cluster/data/danRer1/bed/affyZebrafish
    hgLoadPsl danRer1 affyZebrafish.psl

    # Add consensus sequences for Zebrafish chip
    # Copy sequences to gbdb if they are not there already
    mkdir -p /gbdb/hgFixed/affyProbes
    ln -s \
       /projects/compbio/data/microarray/affyZebrafish/Zebrafish_consensus.fa \
       /gbdb/hgFixed/affyProbes

    hgLoadSeq -abbr=Zebrafish: danRer1 \
              /gbdb/hgFixed/affyProbes/Zebrafish_consensus.fa

    # Clean up
    rm batch.bak contig.psl raw.psl
    # add entry to trackDb.ra in ~kent/src/hg/makeDb/trackDb/zebrafish/danRer1

# ADD CONTIGS TRACK, ctgPos2 (DONE, 2004-07-02, hartera)
# make ctgPos2 (contig name, size, chrom, chromStart, chromEnd) from lift
    ssh kksilo
    cd /cluster/data/danRer1/bed
    mkdir ctgPos2
    cd ctgPos2
    # make a .as file for ctgPos2 which is an extended ctgPos with
    # a field for clone type
    cat << '_EOF_' > $HOME/kent/src/hg/lib/ctgPos2.as
table ctgPos2
"Where a contig is inside of a chromosome including contig type information."
    (
    string chrom;       "Chromosome name"
    uint chromStart;    "Start in chromosome"
    uint chromEnd;      "End in chromosome"
    string name;        "Name of contig"
    char[1] type;       "(W)GS contig, (F)inished, (P)redraft, (D)raft, (O)ther"    uint size;          "Size of contig"
    )
'_EOF_'
    cd $HOME/kent/src/hg/lib/
    autoSql ctgPos2.as ctgPos2
    mv ctgPos2.h $HOME/kent/src/hg/inc
    # do make to check it works and commit the .as, .sql, .c and .h file to CVS
    foreach c (1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \
              21 22 23 24 25)
         awk 'BEGIN {OFS="\t"} \
         {if ($5 != "N") print $6, $3-$2+1, $1, $2-1, $3, $5}' \
         /cluster/data/danRer1/$c/contigs/chr${c}.agp >> ctgPos2.tab
    end
                                                                                
    # need to parse with chrom, chromStart, chromEnd first for liftUp to work
    foreach c (ctg NA Finished)
       awk 'BEGIN {OFS="\t"} \
         {if (($1 ~ /^'"${c}"'[0-9]+$/) && $5 != "N" ) \
         print $1, $2-1, $3, $5, $6, $3-$2+1}' \
         /cluster/data/danRer1/Zv3.contigs.agp >> chrUns.tmp
    end
                                                                                
    # Use liftUp to change co-ordinates to those of virtual chroms
    # make lift file from all 3 virtual chroms
    foreach c (NA Un Finished)
       cat /cluster/data/danRer1/$c/chr${c}.lft >> chrAllUns.lft
    end
                                                                                
    liftUp -type=.bed chrUns.lifted chrAllUns.lft warn chrUns.tmp
                                                                                
    # once have lifted co-ordinates, need to parse to change to table format
    awk 'BEGIN {OFS="\t"} {print $5, $6, $1, $2, $3, $4}' \
        chrUns.lifted > chrUns.tab
    # add this to ctgPos2.tab
    cat chrUns.tab >> ctgPos2.tab
                                                                                
    ssh hgwdev
    cd /cluster/data/danRer1/bed/ctgPos2
    echo "drop table ctgPos2" | hgsql danRer1
    hgsql danRer1 < ~/kent/src/hg/lib/ctgPos2.sql
    echo "load data local infile 'ctgPos2.tab' into table ctgPos2" \
         | hgsql danRer1

# WZ EST ALIGNMENTS (DONE, 2004-07-09, hartera)
# WZ ESTs are compiled ESTs from WashU. They were provided by
# Anthony DiBiase from Leonard Zon's lab at the Children's Hospital, Harvard
# Contact: adibiase@enders.tch.harvard.edu
# http://zon.tchlab.org
     ssh hgwdev
     mkdir -p /cluster/data/danRer1/bed/ZonLab/wzESTs
     cd /cluster/data/danRer1/bed/ZonLab/wzESTs
     # put WZ ESTs in this directory as wzcontigs.txt -
     # obtained from the Zon Lab, these are unmasked.
     # There are 42857 ESTs in this file.
     # Translate to upper case
     tr '[a-z]' '[A-Z]' < wzcontigs.txt > wzcontigs.fa
     ssh kkr1u00
     mkdir -p /iscratch/i/danRer1/wzESTs
     cd /cluster/data/danRer1/bed/ZonLab/wzESTs
     faSplit sequence wzcontigs.fa 20 /iscratch/i/danRer1/wzESTs/wzcontigs
     iSync
                                                                                
     ssh kk
     cd /cluster/data/danRer1/bed/ZonLab/wzESTs
     mkdir psl
     ls -1 /iscratch/i/danRer1/wzESTs/*.fa > est.lst
     ls -1S /iscratch/i/danRer1/trfFa/*.fa > genome.lst
# REdO without masking - not used for native ESTs
     cat << '_EOF_' > gsub
#LOOP
/cluster/bin/i386/blat {check in line+ $(path1)} {check in line+ $(path2)} -ooc={check in exists /iscratch/i/danRer1/11.ooc} {check out line+ psl/$(root1)_$(root2).psl}
#ENDLOOP
'_EOF_'
     gensub2 genome.lst est.lst gsub spec
     para create spec
     para try
     para check
# para time
# Completed: 6200 of 6200 jobs
# CPU time in finished jobs:      50309s     838.48m    13.97h    0.58d  0.002 y# IO & Wait Time:                 19334s     322.23m     5.37h    0.22d  0.001 y# Average job time:                  11s       0.19m     0.00h    0.00d
# Submission to last job:           326s       5.43m     0.09h    0.00d
                                                                                
    # Do sort, best in genome filter, and convert to chromosome coordinates
    # to create wzEsts.psl
    pslSort dirs raw.psl tmp psl
    # only use alignments that have at least
    # 96% identity in aligned region. use parameters used by auto
    # GenBank update for native ESTs
    pslReps -minAli=0.96 -nearTop=0.005 raw.psl contig.psl /dev/null
    liftUp wz_ests.psl /cluster/data/danRer1/jkStuff/liftAll.lft warn contig.psl                                                                                
    # Load EST alignments into database.
    ssh hgwdev
    cd /cluster/data/danRer1/bed/ZonLab/wzESTs
    hgLoadPsl danRer1 wz_ests.psl
                                                                                
    # Add WZ EST sequences
    # Copy sequences to gbdb if they are not there already
    mkdir -p /gbdb/danRer1/wzESTs
    ln -s \
       /cluster/data/danRer1/bed/ZonLab/wzESTs/wzcontigs.fa \
       /gbdb/danRer1/wzESTs
                                                                                
    hgLoadSeq danRer1 /gbdb/danRer1/wzESTs/wzcontigs.fa

# BACENDS (DONE, 2004-07-28, hartera)
# Added bacEndPairsBad table (DONE, 2004-08-02, hartera)
# provided by Anthony DiBiase, Yi Zhou and Leonard Zon at the Boston
# Children's Hospital. Collaborated with them on this track.
# Anthony DiBiase:adibiase@enders.tch.harvard.edu
# Yi Zhou:yzhou@enders.tch.harvard.edu 
# BAC clone end sequences are from Robert Geisler's lab, 
# Max Planck Institute for Developmental Biology, Tuebingen, Germany
    ssh kksilo
    cd /cluster/data/danRer1/bed/ZonLab
    mkdir /cluster/data/danRer1/bed/ZonLab/bacends
    cp cloneEnd.txt ./bacends/bacends.fa
    cd bacends
    # copy BAC clone ends seqeunce file to this directory
    faSize bacends.fa
    # 486978153 bases (39070196 N's 447907957 real) in 594614 sequences in
    # 1 files Total size: mean 819.0 sd 230.3 min 0 (zKp108-H09.za)
    # max 5403 (zC259G13.zb) median 796
    # N count: mean 65.7 sd 154.7
    mkdir /iscratch/i/danRer1/bacends
    faSplit sequence bacends.fa 20 /iscratch/i/danRer1/bacends/bacends
    ls -1S /iscratch/i/danRer1/bacends/*.fa > bacends.lst
    ls -1S /iscratch/i/danRer1/trfFa/*.fa > genome.lst
    cat << '_EOF_' > template
#LOOP
/cluster/bin/i386/blat $(path1) $(path2) -tileSize=10 -ooc=/iscratch/i/danRer1/10.ooc {check out line+ psl/$(root1)_$(root2).psl}
#ENDLOOP
'_EOF_'
    # << this line keeps emacs coloring happy
    mkdir psl
    gensub2 genome.lst bacends.lst template jobList
    ssh kkr1u00
    # iSync bacends to kilokluster
    iSync
    ssh kk
    cd /cluster/data/danRer1/bed/ZonLab/bacends
    para create jobList
    para try, check, push, check, ...
# para time
# Completed: 6200 of 6200 jobs
# CPU time in finished jobs:   4833892s   80564.87m  1342.75h   55.95d  0.153y
# IO & Wait Time:                31979s     532.98m     8.88h    0.37d  0.001y
# Average job time:                785s      13.08m     0.22h    0.01d
# Longest job:                    2300s      38.33m     0.64h    0.03d
# Submission to last job:        14904s     248.40m     4.14h    0.17d
    # back on kksilo, filter and lift results:
    ssh kksilo
    cd /cluster/data/danRer1/bed/ZonLab/bacends
    pslCheck ./psl/*.psl >& pslCheck.log
    # No problems reported by pslCheck
    pslSort dirs raw.psl temp psl
    # tried minCover=0.6 but only 55% of sequences were aligned
    # but with minCover=0.4, 77% of seqeunces are aligned
    # No minCover, aligns 90% of sequences but with a large number
    # of extra alignments - noise
    pslReps -nearTop=0.02 -minCover=0.40 -minAli=0.85 -noIntrons raw.psl \
      bacEnds.psl /dev/null
    liftUp bacEnds.lifted.psl /cluster/data/danRer1/jkStuff/liftAll.lft \
           warn bacEnds.psl
    wc -l bacEnds.lifted.psl
    # 3717153 bacEnds.lifted.psl
    rm -r temp

# Need to add accession information and BAC end sequence pairs
    mkdir -p /cluster/data/danRer1/bed/ZonLab/bacends/bacends.1
    cd /cluster/data/danRer1/bed/ZonLab/bacends/bacends.1
    grep ">" ../bacends.fa > bacEnds
    # remove '>' at beginning of each line
    perl -pi.bak -e 's/>//' bacEnds
    # remove ^M char from end of lines
    perl stripEndChar.pl < bacEnds > bacEnds.names
    rm bacEnds bacEnds.bak
    # Kerstin Jekosch at the Sanger Centre: kj2@sanger.ac.uk
    # provided accession data for BAC End clones - zfish_accs.txt
    # and for BAC End pairs- BACEnd_accessions.txt - put these in bacends.1 dir
    # write and use perl script to split BAC Ends into pairs and singles
    # bacEndPairs.txt and bacEndSingles.txt
    # For pslPairs, the reverse primer (T7 or .z*) should in the first column
    # and the forward primer (SP6 or .y*) should be in the second column
    # There are several reads for some sequences and these have similar names.
    # Reads for the same sequencee should be in a comma separated list.
    # Sequence read names can be translated to external clone names as found
    # in NCBI's clone registry using the name without the suffix and the 
    # translation of prefixes as follows after removal of dashes 
    # and extra zeros in the name:
    # library         external prefix         internal prefix         
    # CHORI-211		CH211-                  zC
    # DanioKey        	DKEY-                   zK
    # DanioKey Pilot  	DKEYP-                  zKp
    # RZPD-71         	RP71-                   bZ
    # BUSM1 (PAC)      	BUSM1-                  dZ
    # e.g. zC001-A03.za becomes CH211-1A03
    perl getBacEndInfo.pl bacEnds.names BACEnd_accessions.txt zfish_accs.txt
    
    # 180280 bacEndPairs.txt  - 571985 sequences
    # 16020  bacEndSingles.txt - 22629 sequences
    # 47048  bacEndAccs.aliases
    # bacEndAccs.aliases contains sequence read names and their
    # Genbank accessions.  
    
    # First process BAC end alignments
    cd /cluster/data/danRer1/bed/ZonLab/bacends/pairs
    set bacDir = /cluster/data/danRer1/bed/ZonLab/bacends/bacends.1
    # these bacEnds vary in size from around 2800 bp to 626,000 bp
    ~/bin/i386/pslPairs -tInsert=10000 -minId=0.91 -noBin -min=2000 \
     -max=650000 -slopval=10000 -hardMax=800000 -slop -short -long -orphan \
     -mismatch -verbose ../bacEnds.lifted.psl $bacDir/bacEndPairs.txt \
     all_bacends bacEnds
     wc -l bacEnds.*
     # 495 bacEnds.long
     # 13046 bacEnds.mismatch
     # 200235 bacEnds.orphan
     # 72820 bacEnds.pairs
     # 0 bacEnds.short
     # 139 bacEnds.slop

    # create header required by "rdb" tools
    echo 'chr\tstart\tend\tclone\tscore\tstrand\tall\tfeatures\tstarts\tsizes'\
          > ../header
    echo '10\t10N\t10N\t10\t10N\t10\t10\t10N\t10\t10' >> ../header
    # make pairs bed file
    cat header bacEnds.pairs | row score ge 300 | sorttbl chr start \
               | headchg -del > bacEndPairs.bed
    # also need to process bacEndSingles.txt into a database table
    # for singles in bacEndSingles.txt, create a dummy file where they
    # are given zJA11B12T7 as dummy sequence pair. If the single is a forward
    # sequence, put the dummy sequence in the second column, if the single is
    # a reverse sequence put in first column. use a perl script to do this.
    cd /cluster/data/danRer1/bed/ZonLab/bacends
    set bacDir = /cluster/data/danRer1/bed/ZonLab/bacends/bacends.1
    mkdir singles
    cd singles
    perl formatSingles.pl $bacDir/bacEndSingles.txt > \
                          $bacDir/bacEndSingles.format
    # then run pslPairs on this formatted sequence
    ~/bin/i386/pslPairs -tInsert=10000 -minId=0.91 -noBin -min=2000 \
     -max=650000 -slopval=10000 -hardMax=800000 -slop -short -long -orphan \
     -mismatch -verbose ../bacEnds.lifted.psl $bacDir/bacEndSingles.format \
     all_bacends bacEnds
     wc -l bacEnds*
    # 0 bacEnds.long
    # 0 bacEnds.mismatch
    # 14126 bacEnds.orphan
    # 0 bacEnds.pairs
    # 0 bacEnds.short
    # 0 bacEnds.slop
    # there are 14126 orphans here and 200235 from pair analysis 
    # so a total of 214361 orphans
    cat bacEnds.orphan ../bacEnds.orphan > bacEnds.singles
    wc -l bacEnds.singles
    # 214361 bacEnds.singles
    # make singles bed file
    cat ../header bacEnds.singles | row score ge 300 | sorttbl chr start \
                  | headchg -del > bacEndSingles.bed
    cp bacEndSingles.bed ../pairs
    cd ../pairs
    # all slop, short, long, mismatch and orphan pairs go into bacEndPairsBad
    cat header bacEnds.slop bacEnds.short bacEnds.long bacEnds.mismatch \
        bacEnds.orphan | row score ge 300 | sorttbl chr start \
        | headchg -del > bacEndPairsBad.bed
    # add bacEndsSingle.bed to bacEnds.load.psl - must not add pair orphans 
    # twice so create a bed file of bacEndPairsBadNoOrphans.bed without orphans
 
    cat header bacEnds.slop bacEnds.short bacEnds.long bacEnds.mismatch \
        | row score ge 300 | sorttbl chr start \
        | headchg -del > bacEndPairsBadNoOrphans.bed
    extractPslLoad -noBin bacEnds.lifted.psl bacEndPairs.bed \
                bacEndPairsBadNoOrphans.bed  bacEndSingles.bed | \
                        sorttbl tname tstart | headchg -del > bacEnds.load.psl
   
    # load BAC end sequences into seq table so alignments may be viewed
    mkdir -p /gbdb/danRer1/bacends
    ln -s /cluster/data/danRer1/bed/ZonLab/bacends/bacends.fa \
                                /gbdb/danRer1/bacends/BACends.fa
    hgLoadSeq danRer1 /gbdb/danRer1/bacends/BACends.fa
    # when loaded into danRer1, Heather found there were rows where
    # the aligments were the same but the lfNames were different. This is 
    # due to the presence of multiple reads for the same BAC end sequence.
    # Sometimes they are slightly different lenghths so the alignments are 
    # a little different. It would be good to consolidate all of these and
    # just pick the best alignment. However, here only the identical rows
    # were merged into one with a list of all the lfNames with that alignment.
   
    # load all alignments into temporary database
    ssh hgwdev
    echo "create database bacs_rah;" | hgsql danRer1
    cd /cluster/data/danRer1/bed/ZonLab/bacends/pairs
    hgLoadBed bacs_rah bacEndPairs bacEndPairs.bed \
                -sqlTable=$HOME/kent/src/hg/lib/bacEndPairs.sql
    # Loaded 72272 elements of size 11
    # create a bacEndSingles table like bacEndPairs
    cp $HOME/kent/src/hg/lib/bacEndPairs.sql ./singles/bacEndSingles.sql
    # edit to give correct table name
    hgLoadBed bacs_rah bacEndSingles bacEndSingles.bed \
                 -sqlTable=singles/bacEndSingles.sql
    # Loaded 203683 elements of size 11
    # load all_bacends later

    # note - this track isn't pushed to RR, just used for assembly QA
    hgLoadBed bacs_rah bacEndPairsBad bacEndPairsBad.bed \
                 -sqlTable=$HOME/kent/src/hg/lib/bacEndPairsBad.sql
    # Loaded 203709 elements of size 11
    # RECREATE BED FILES AND RELOAD TABLES
    # Need to consolidate similar rows for bacEndPairs and bacEndSingles - same
    # name, different lfNames and same alignments.
    mkdir -p /cluster/data/danRer1/bed/ZonLab/bacends/duplicates
    cd /cluster/data/danRer1/bed/ZonLab/bacends/duplicates
    hgsql -N -e "select chrom, chromStart, chromEnd, name, strand from \
          bacEndPairs order by name, chrom, chromStart;" bacs_rah > pairs.txt
    sort pairs.txt | uniq -c > pairs.txt.uniq
    wc -l pairs*
    # 72272 pairs.txt
    # 53992 pairs.txt.uniq
    # for replicate rows, find all the unique lfNames and put these
    # in one row with the relevant lfStarts, lfSizes and correct lfCount
    perl removeReplicates2.pl pairs.txt.uniq bacEndPairs > pairsNoReps.bed
    # repeat for singles
    hgsql -N -e "select chrom, chromStart, chromEnd, name, strand from \
       bacEndSingles order by name, chrom, chromStart;" bacs_rah > singles.txt
    sort singles.txt | uniq -c > singles.txt.uniq
    wc -l singles*
    # 203683 singles.txt
    # 179170 singles.txt.uniq
    perl removeReplicates.pl singles.txt.uniq bacEndSingles \
                             > singlesNoReps.bed                                   hgsql -N -e "select chrom, chromStart, chromEnd, name, strand from \
          bacEndPairsBad order by name, chrom, chromStart;" bacs_rah \
          > badPairs.txt
    sort badPairs.txt | uniq -c > badPairs.txt.uniq
    wc -l badPairs*
    # 203709 badPairs.txt
    # 178304 badPairs.txt.uniq
    perl removeReplicates.pl badPairs.txt.uniq bacEndPairsBad \
         > badPairsNoReps.bed
                                                                               
    # reload sequences into tables in danRer1
    # bed files are already sorted by chrom and chromStart
    # so no need to re-sort                                                                                
    ssh hgwdev
    cd /cluster/data/danRer1/bed/ZonLab/bacends/duplicates
    echo "drop table bacEndPairs;" | hgsql danRer1
    echo "drop table bacEndPairsBad;" | hgsql danRer1
    echo "drop table bacEndSingles;" | hgsql danRer1
    echo "drop table all_bacends;" | hgsql danRer1
                                                                              
    hgLoadBed danRer1 bacEndPairs pairsNoReps.bed \
                 -sqlTable=$HOME/kent/src/hg/lib/bacEndPairs.sql
    # Loaded 53992 elements of size 11
    hgLoadBed danRer1 bacEndPairsBad badPairsNoReps.bed \
                 -sqlTable=$HOME/kent/src/hg/lib/bacEndPairsBad.sql
    # Loaded 178304 elements of size 11
    # edit to give correct table name
    hgLoadBed danRer1 bacEndSingles singlesNoReps.bed \
                 -sqlTable=../singles/bacEndSingles.sql
    # Loaded 179170 elements of size 11
    cd /cluster/data/danRer1/bed/ZonLab/bacends/pairs
    hgLoadPsl danRer1 -table=all_bacends bacEnds.load.psl
    # 2835212 record(s), 0 row(s) skipped, 706 warning(s) loading psl.tab
    # after merging rows with the same BAC name, the scoring is now
    # wrong in bacEndPairs, bacEndSingles and bacEndSingles.
    # Scores should be 1000 if there is 1 row for that name, else
    # 1500/number of rows for that sequence name - calculated by pslPairs.
    # Correct the scores.
    
    ssh hgwdev
    mkdir -p /cluster/data/danRer1/bed/ZonLab/bacends/scores
    cd /cluster/data/danRer1/bed/ZonLab/bacends/scores
    hgsql -N -e "select name from bacEndPairs;" danRer1 \
                 | sort | uniq -c > pairs.hits
    # download bacEndPairs table
    hgsql -N -e "select * from bacEndPairs;" danRer1 > bacEndPairs.out
    # use perl script to change scores from this file and write to new file
        perl correctScores.pl bacEndPairs.out pairs.hits \
                          > bacEndPairsGoodScores.bed
    # same for singles
    hgsql -N -e "select name from bacEndSingles;" danRer1 \
                 | sort | uniq -c > singles.hits
    # dowload bacEndSingles table
    hgsql -N -e "select * from bacEndSingles;" danRer1 > bacEndSingles.out
    perl correctScores.pl bacEndSingles.out singles.hits \
                         > bacEndSinglesGoodScores.bed
    # add bacEndPairsBad -  (hartera, 2004-08-02)
    # this was reloaded temporarily into bacs_rah
    hgsql -N -e "select name from bacEndPairsBad;" bacs_rah \
                 | sort | uniq -c > badPairs.hits
    hgsql -N -e "select * from bacEndPairsBad;" bacs_rah > bacEndPairsBad.out 
    perl correctScores.pl bacEndPairsBad.out badPairs.hits \
                         > bacEndPairsBadGoodScores.bed
    # check scores are correct now
    # reload BAC ends database tables
    hgsql -e "drop table bacEndPairs;" danRer1
    hgsql -e "drop table bacEndSingles;" danRer1
    hgsql -e "drop table bacEndPairsBad;" danRer1
    hgLoadBed danRer1 bacEndPairs bacEndPairsGoodScores.bed \
                 -sqlTable=$HOME/kent/src/hg/lib/bacEndPairs.sql
    # Loaded 53992 elements of size 11
    hgLoadBed danRer1 bacEndSingles bacEndSinglesGoodScores.bed \
                 -sqlTable=../singles/bacEndSingles.sql

    # featureBits danRer1 bacEndPairs gap
    # 18271004 bases of 1459132082 (1.252%) in intersection
    # featureBits danRer1 bacEndSingles gap
    # 2718421 bases of 1459132082 (0.186%) in intersection
    # featureBits danRer1 bacEndSingles bacEndPairs
    # 158155304 bases of 1459132082 (10.839%) in intersection

    # Loaded 179170 elements of size 11
    # 179170 record(s), 0 row(s) skipped, 23 warning(s) loading bed.tab
    # loaded bacEndPairsBad table (hartera, 2004-08-02)
    hgLoadBed danRer1 bacEndPairsBad bacEndPairsBadGoodScores.bed \
                 -sqlTable=$HOME/kent/src/hg/lib/bacEndPairsBad.sql
    # Loaded 178304 elements of size 11
    # 178304 record(s), 0 row(s) skipped, 23 warning(s) loading bed.tab

    # From getBacEndInfo.pl, created a bacEndsAcc.aliases file
    # this has the format <alias> <id> <trueName>
    # where trueName is the accession provided by Sanger in
    # BACEnd_accessions.txt and aliases are the read names for sequences
    # Create a bacEndAlias table.
    cat > bacEndAlias.as << $HOME/kent/src/hg/lib
table bacEndAlias
"BAC ends aliases and associated identification numbers"
    (
    string alias;       "BAC end read name"
    uint identNo;       "Identification number of BAC End"
    string acc;         "GenBank accession for the BAC End"
    )
'_EOF_'
    # << this line makes emacs coloring happy
    cd $HOME/kent/src/hg/lib
    autoSql bacEndAlias.as bacEndAlias
    cp bacEndAlias.h ../inc
    # edit bacEndAlias.sql to add INDEX(identNo) to indices
    # add and commit to cvs bacEndAlias.as, .sql, .c and .h
    ssh hgwdev
    cd /cluster/data/danRer1/bed/ZonLab/bacends/bacends.1
    echo "drop table bacEndAlias" | hgsql danRer1
    hgsql danRer1 < $HOME/kent/src/hg/lib/bacEndAlias.sql
    echo "load data local infile 'bacEndAccs.aliases' into table bacEndAlias"\
         | hgsql danRer1
   # Add a xref table to give external clone registry names, internal names
   # and Genbank accessions
    cat << '_EOF_' > $HOME/kent/src/hg/lib/bacCloneXRef.as
table bacCloneXRef
"BAC clones external names, internal sequence names and Genbank accessions"
    (
    string extName;       "External name for BAC clone"
    string intName;       "Internal sequncing name for BAC clone"
    string acc;         "Genbank accession for the BAC End"
    )
'_EOF_'
    # << this line makes emacs coloring happy
    cd $HOME/kent/src/hg/lib
    autoSql bacCloneXRef.as bacCloneXRef
    cp bacCloneXRef.h ../inc
    # edit bacCloneXRef.sql to add INDEX(extName) to indices
    # edit bacCloneXRef.sql to add INDEX(extName) to indices
    # add and commit to cvs bacCloneXRef.as, .sql, .c and .h
    cd /cluster/data/danRer1/bed/ZonLab/bacends
    # use zfish_accs.txt provided by the Sanger Centre to create a list
    # of BAC clone names, internal names and Genbank accessions
    perl getBACInfo.pl zfish_accs.txt > bacCloneXRef.txt
    # there are 35 clones with different prefixes to those in our track
    # these are in BACerror.log and are not loaded into the XRef table
    echo "drop table bacCloneXRef" | hgsql danRer1
    hgsql danRer1 < $HOME/kent/src/hg/lib/bacCloneXRef.sql
    echo "load data local infile 'bacCloneXRef.txt' into table bacCloneXRef"\
         | hgsql danRer1
    mv bacCloneXRef.txt ./bacends.1
    # edit doLinkedFeaturesSeries in hgc.c to use this information
    # so that links to clone and clone end accessions can be made on the
    # details pages.
  
    # ALIGNED BAC END SEQUENCES TO CHECK THAT ONES WITH SIMILAR NAMES
    # ARE READS FROM THE SAME SEQUENCES.
    # also the sequences ending in .z or .Z and .y or .Y were of unknown
    # direction. Sanger thought that z may be forward but aligning the
    # sequences suggested that z was actually reverse knowing that SP6 
    # is a forward sequence and T7 is reverse.
    # Anthony DiBiase wrote script : countns.pl to filter out sequences with
    # too many Ns. threshold is 80% Ns
    cat bacends.fa | perl countns.pl > bacends.percentNs80
    grep '>' bacends.percentNs80 | sed -e 's/>//' > bacends.remove
    perl removeNames.pl ./bacends.1/bacEnds.names bacends.remove \
         > bacends.noseqswithlotsNs
    # get these good sequences with < 80% Ns
    $HOME/bin/i386/faSomeRecords bacends.fa \
          bacends.noseqswithlotsNs bacends_good.fa
    # blast
    ssh kkr1u00
    mkdir -p /iscratch/i/danRer1/bacends/clusterBlastdb
    cp /cluster/data/danRer1/bed/ZonLab/bacends/bacends_good.fa \
       /iscratch/i/danRer1/bacends/clusterBlastdb
    cd /iscratch/i/danRer1/bacends/clusterBlastdb
    # do all against all blast
    /scratch/blast/formatdb -i bacends_good.fa -t bacendsgood \
                            -n bacendsgood -p F
    mkdir -p /iscratch/i/danRer1/bacends/clusterBlast
    faSplit sequence /cluster/data/danRer1/bed/ZonLab/bacends/bacends_good.fa \
            500 /iscratch/i/danRer1/bacends/clusterBlast/bacendsgood
    iSync
    # do kilokluster run
    ssh kk
    cd /cluster/data/danRer1/bed/ZonLab/bacends
    mkdir clusterBlast
    cd clusterBlast
    mkdir blastout
    ls -1S /iscratch/i/danRer1/bacends/clusterBlast/*.fa > bacs.lst
    ls -1S /iscratch/i/danRer1/bacends/clusterBlastdb/*.nsq \
           | sed "s/.nsq//" > bacsBlastdb.lst
    # Make blast script
    cat > blastSome << end
#!/bin/csh
setenv BLASTMAT /scratch/blast/data
/scratch/blast/blastall -p blastn -d \$1 -i \$2 -o \$3 -e 0.0
end
    # << this line keeps emacs coloring happy
    chmod a+x blastSome
    # Make gensub2 file
    cat > blastGsub <<end
#LOOP
blastSome $(path1) {check in line+ $(path2)} {check out line+ blastout/$(root1)_$(root2).out}
#ENDLOOP
end
    # << this line keeps emacs coloring happy
    gensub2 bacsBlastdb.lst bacs.lst blastGsub blastSpec
    para create blastSpec
    para try, check, push etc.
    # convert to psl format
    ssh kksilo
    cd /cluster/data/danRer1/bed/ZonLab/bacends/clusterBlast
    mkdir psl scores
    foreach b (blastout/*.out)
      set t = $b:t
      set s = $t:r
      echo "Creating ${s}.psl"
      /cluster/bin/i386/blastToPsl $b psl/${s}.psl -scores=scores/${s}.scores \
                     >>& blastToPsl.log
    end
    pslSort dirs raw.psl tmp psl
    mv raw.psl clusterBacEnds.psl
    awk '{print $10, $14, $9;}' clusterBacEnds.psl > clusterBacEnds.out
    # look at blast results for e-value threshold of 0
    # 

# MAKE HGCENTRALTEST BLATSERVERS ENTRY FOR DANRER1
# (DONE, 2004-07-13, hartera)
                                                                                
    ssh hgwdev
    # DNA port is "0", trans prot port is "1"
    echo 'insert into blatServers values("danRer1", "blat8", "17779", "0", "0"); \
          insert into blatServers values("danRer1", "blat8", "17778", "1", "0");' \
    | hgsql -h genome-testdb hgcentraltest
    # if you need to delete those entries
    echo 'delete from blatServers where db="danRer1";' \
    | hgsql -h genome-testdb hgcentraltest
    # to check the entries:
    echo 'select * from blatServers where db="danRer1";' \
    | hgsql -h genome-testdb hgcentraltest

# MAKE DOWNLOADABLE SEQUENCE FILES (DONE, 2004-07-20, hartera)
    ssh kksilo
    cd /cluster/data/danRer1
    #- Build the .zip files
    cat << '_EOF_' > jkStuff/zipAll.csh
rm -rf zip
mkdir zip
# chrom AGP's
zip -j zip/chromAgp.zip [0-9A-Z]*/chr*.agp
# chrom RepeatMasker out files
zip -j zip/chromOut.zip */chr*.fa.out
# soft masked chrom fasta
zip -j zip/chromFa.zip */chr*.fa
# hard masked chrom fasta
zip -j zip/chromFaMasked.zip */chr*.fa.masked
# chrom TRF output files
cd bed/simpleRepeat
zip ../../zip/chromTrf.zip trfMask/chr*.bed
cd ../..
# get GenBank native mRNAs
cd /cluster/data/genbank
./bin/i386/gbGetSeqs -db=danRer1 -native GenBank mrna \
        /cluster/data/danRer1/zip/mrna.fa
# get GenBank xeno mRNAs
./bin/i386/gbGetSeqs -db=danRer1 -xeno GenBank mrna \
        /cluster/data/danRer1/zip/xenoMrna.fa
# get native RefSeq mRNAs
./bin/i386/gbGetSeqs -db=danRer1 -native refseq mrna \
/cluster/data/danRer1/zip/refMrna.fa
# get native GenBank ESTs
./bin/i386/gbGetSeqs -db=danRer1 -native GenBank est \
/cluster/data/danRer1/zip/est.fa
                                                                                
cd /cluster/data/danRer1/zip
# zip GenBank native and xeno mRNAs, native ESTs and RefSeq mRNAs
zip -j mrna.zip mrna.fa
zip -j xenoMrna.zip xenoMrna.fa
zip -j refMrna.zip refMrna.fa
zip -j est.zip est.fa
'_EOF_'
    # << this line makes emacs coloring happy
    csh ./jkStuff/zipAll.csh |& tee ./jkStuff/zipAll.log
    cd zip
    #- Look at zipAll.log to make sure all file lists look reasonable.
    # Make upstream files and Copy the .zip files to
    # hgwdev:/usr/local/apache/...
    ssh hgwdev
    cd /cluster/data/danRer1/zip
    # make upstream files for zebrafish RefSeq
    featureBits danRer1 refGene:upstream:1000 -fa=upstream1000.fa
    zip upstream1000.zip upstream1000.fa
    featureBits danRer1 refGene:upstream:2000 -fa=upstream2000.fa
    zip upstream2000.zip upstream2000.fa
    #- Check zip file integrity:
    foreach f (*.zip)
      unzip -t $f > $f.test
      tail -1 $f.test
    end
    wc -l *.zip.test
                                                                                
    set gp = /usr/local/apache/htdocs/goldenPath/danRer1
    mkdir -p $gp/bigZips
    cp -p *.zip $gp/bigZips
    mkdir -p $gp/chromosomes
    foreach f (../*/chr*.fa) 
       zip -j $gp/chromosomes/$f:t.zip $f
    end
    cd $gp/bigZips
    md5sum *.zip > md5sum.txt
    cd $gp/chromosomes
    md5sum *.zip > md5sum.txt
    # Take a look at bigZips/* and chromosomes/*, update their README.txt's

# MAKE VSHG17 DOWNLOADABLES (DONE, 2004-07-20, hartera)
    ssh hgwdev
    cd /cluster/data/danRer1/bed/blastz.hg17/axtChrom
    set gp = /usr/local/apache/htdocs/goldenPath/danRer1
    mkdir -p $gp/vsHg17/axtChrom
    cp -p *.axt $gp/vsHg17/axtChrom
    cd $gp/vsHg17/axtChrom
    gzip *.axt
    md5sum *.gz > md5sum.txt

    zip -j /cluster/data/danRer1/zip/hg17.chain.zip hg17.chain
    rm hg17.chain
    cp humanhg17.net hg17.net
    zip -j /cluster/data/danRer1/zip/hg17.net.zip hg17.net
    rm hg17.net
    cd $gp/vsHg17
    mv /cluster/data/danRer1/zip/hg17*.zip .
    md5sum *.zip > md5sum.txt
    # Copy over & edit README.txt w/pointers to chain, net formats.
                                                                                
# MAKE VSFR1 DOWNLOADABLES (DONE, 2004-07-26, hartera)
    ssh hgwdev
    cd /cluster/data/danRer1/bed/blastz.fr1/axtChrom
    set gp = /usr/local/apache/htdocs/goldenPath/danRer1
    mkdir -p $gp/vsFr1/axtChrom
    cp -p *.axt $gp/vsFr1/axtChrom
    cd $gp/vsFr1/axtChrom
    gzip *.axt
    md5sum *.gz > md5sum.txt
    # add the axtNet *.axt in blastz.fr1/axtNet
    cd /cluster/data/danRer1/bed/blastz.fr1/axtNet
    set gp = /usr/local/apache/htdocs/goldenPath/danRer1
    mkdir -p $gp/vsFr1/axtNet
    cp -p *.axt $gp/vsFr1/axtNet
    cd $gp/vsFr1/axtNet
    gzip *.axt
    md5sum *.gz > md5sum.txt
                                                                                
    cd /cluster/data/danRer1/bed/blastz.fr1/axtChain
    cp all.chain fr1.chain
    zip -j /cluster/data/danRer1/zip/fr1.chain.zip fr1.chain
    rm fr1.chain
    zip -j /cluster/data/danRer1/zip/fr1.net.zip fr1.net
    cd $gp/vsFr1
    mv /cluster/data/danRer1/zip/fr1*.zip .
    md5sum *.zip > md5sum.txt
    # Copy over & edit README.txt w/pointers to chain, net formats.
                                                                                
# TBLASTN HG16 KNOWN GENES TRACK (DONE, 2004-07-23, hartera)
    # Collaboration with Anthony DiBiase and Yi Zhou of Leonard Zon's lab at 
    # the Childrens Hospital, Boston - part of the Zebrafish Genome Initiative
    # Protein set created by Anthony DiBiase: adibiase@enders.tch.harvard.edu
    # tblastn of proteins from hg16 knownGenePep table as query
    # against a database of repeatmasked Zv3 zebrafish assembly
    # Originally received tblastn output in html format and a 
    # blastHg16.psl file which has an extra accession column at
    # the beginning - this was created from the html
    # Use perl script to move accession to the end of each row
    ssh hgwdev
    # Create a new .as for an extended psl table with a
    # query ID column for the accession
    cat << '_EOF_' > $HOME/kent/src/hg/lib/pslWQueryID.as
table pslWQueryID
"Summary info about a patSpace alignment with a query ID addition"
    (
    uint    matches;      "Number of bases that match that aren't repeats"
    uint    misMatches;   "Number of bases that don't match"
    uint    repMatches;   "Number of bases that match but are part of repeats"
    uint    nCount;       "Number of 'N' bases"
    uint    qNumInsert;   "Number of inserts in query"
    uint     qBaseInsert;  "Number of bases inserted in query"
    uint    tNumInsert;   "Number of inserts in target"
    uint     tBaseInsert;  "Number of bases inserted in target"
    char[2] strand;       "+ or - for query strand. For mouse second +/- for genomic strand"
    string  qName;        "Query sequence name"
    uint    qSize;        "Query sequence size"
    uint    qStart;       "Alignment start position in query"
    uint    qEnd;         "Alignment end position in query"
    string  tName;        "Target sequence name"
    uint    tSize;        "Target sequence size"
    uint    tStart;       "Alignment start position in target"
    uint    tEnd;         "Alignment end position in target"
    uint    blockCount;   "Number of blocks in alignment"
    uint[blockCount] blockSizes;  "Size of each block"
    uint[blockCount] qStarts;     "Start of each block in query."
    uint[blockCount] tStarts;     "Start of each block in target."
    string    queryID;         "query ID field"
    )
'_EOF_'
    cd $HOME/kent/src/hg/lib/
    autoSql pslWQueryID.as pslWQueryID
    mv pslWQueryID.h ../inc
    # edit pslWQueryID.sql to have:
    # index(tName(32),tStart,tEnd) instead of PRIMARY KEY (matches)
    # commit to CVS the .as, .sql, .h and .c files
    
    ssh hgwdev
    cd /cluster/data/danRer1/bed/ZonLab/blast
    echo "drop table pslWQueryID" | hgsql danRer1
    hgsql danRer1 < $HOME/kent/src/hg/lib/pslWQueryID.sql
    # rename table to blastHg16KG
    echo "alter table pslWQueryID rename blastHg16KG" | hgsql danRer1
    # process data to add to the table
    mkdir -p /cluster/data/danRer1/bed/ZonLab/blast
    cd /cluster/data/danRer1/bed/ZonLab/blast
    # put blastHg16.psl into this directory
    perl formatPsl.pl < blastHg16.psl > blastHg16.format.tab
    # use pslCheck.c to check file after removing accessions
    pslCheck blastHg16NoAcc.psl
    # there are many errors. the psl file was not correctly formed.
    # Easiest to re-run tblasn and get regular output and use blastToPsl
    # to get a correctly formatted psl file.

    mkdir -p /cluster/data/danRer1/bed/ZonLab/tblastn
    cd tblastn
    # copy hg16_kg_proteinseq.fa here. This FASTA file was provided by
    # Anthony DiBiase and it contains 40115 predicted protein sequences from
    # hg16 known genes - from the knownGenePep table. There are 42026 
    # peptide sequences in hg16 knownGenePep. The FASTA file sequences were
    # obtained proteins by merging annotation data from the kgXref table with 
    # the sequence and name found in the knownGenePep table.
    
    # split known Genes proteins into smaller files
    ssh kkr1u00
    mkdir -p /cluster/data/danRer1/bed/ZonLab/tblastn
    cd tblastn
    # copy hg16_kg_proteinseq.fa here
    mkdir /iscratch/i/danRer1/hg16kgPep
    faSplit sequence hg16_kg_proteinseq.fa 20 \
              /iscratch/i/danRer1/hg16kgPep/hg16kg
    cd /iscratch/i/danRer1/
    mkdir blastdb
    cd blastdb
    # make formatted database for each
    foreach c (/iscratch/i/danRer1/trfFa/*.fa)
       set h = $c:t
       /scratch/blast/formatdb -i $c -t  $h -n $h -p F
    end
    iSync
    ssh kk
    cd /cluster/data/danRer1/bed/ZonLab/tblastn
    mkdir blastout
    ls -1S /iscratch/i/danRer1/hg16kgPep/*.fa > hg16kg.lst
    ls -1S /iscratch/i/danRer1/blastdb/*.nsq | sed "s/.nsq//" > zv3.lst
                                                                                
    # Make blast script
    cat > blastSome << end
#!/bin/csh
setenv BLASTMAT /scratch/blast/data
/scratch/blast/blastall -p tblastn -d \$1 -i \$2 -o \$3 -e 0.01
end
    # << this line keeps emacs coloring happy
    chmod a+x blastSome
    # Make gensub2 file
    cat > blastGsub <<end
#LOOP
blastSome $(path1) {check in line+ $(path2)} {check out line+ blastout/$(root1)_\$(root2).out}
#ENDLOOP
end
    # << this line keeps emacs coloring happy
    gensub2 zv3.lst hg16kg.lst blastGsub blastSpec
    para create blastSpec
    para try, check, push etc.
# para time
# Completed: 5270 of 5270 jobs
# CPU time in finished jobs:   10936672s  182277.86m  3037.96h  126.58d  0.347 y# IO & Wait Time:                707198s   11786.64m   196.44h    8.19d  0.022 y# Average job time:                2209s      36.82m     0.61h    0.03d
# Longest job:                    11825s     197.08m     3.28h    0.14d
# Submission to last job:         72712s    1211.87m    20.20h    0.84d
   ssh kksilo
   cd /cluster/data/danRer1/bed/ZonLab/tblastn
   # convert blast results to psl
   mkdir psl scores
   # some query names are too long and are on 2 separate lines so blastToPsl
   # can not parse them properly so parse out blast results so get only the
   # nucleotide accession name for the query
    cat << '_EOF_' > shortenQueryName.pl
#!/usr/bin/perl -w
use strict;
my $found = "false"; # flag set when query line found
open(QUERY, ">>queries.log") || die "Can not create error.log:$!";
while (<STDIN>) {
   if (/^(Query=\s?[A-Z]{1,2}[0-9]{4,})\|\w+/) {
      my $query = $1;
      print QUERY;
      $found = "true";
      print "$query\n";
   }
   elsif ($found eq "true") {
      if (/^\s+\([0-9]+\s?letters\)/) { print; }
      else { }
      $found = "false";
   }
   else {
      print;
   }
}
'_EOF_'
   foreach c (blastout/*.out)
      set t = $c:t
      set s = $t:r
      echo "Processing $c"
      perl shortenQueryName.pl < $c > blastout/${s}.format
   end
   foreach b (blastout/*.format)
      set t = $b:t
      set s = $t:r
      echo "Creating ${s}.psl"
      /cluster/bin/i386/blastToPsl $b psl/${s}.psl -scores=scores/${s}.scores \
                     >>& blastToPsl.log
   end
    # need to filter so that only results with e-value <= 1e-100 are kept
    # edit blastToPsl.c so can add e-value option and only get results for
    # e-values less than or equal to that value for filtering
    mkdir filtered/Blast
    cd filteredBlast
    mkdir psl scores
                                                                                
    foreach b (../blastout/*.format)
      set t = $b:t
      set s = $t:r
      echo "Creating ${s}.psl"
      $HOME/bin/i386/blastToPsl $b psl/${s}.psl -scores=scores/${s}.scores \
                     >>& blastToPsl.log
    end

    mkdir -p /cluster/data/danRer1/bed/ZonLab/blast
        lSort dirs raw.psl tmp psl
    liftUp -type=".psl" tblastnHg16KG.psl \
          /cluster/data/danRer1/jkStuff/liftAll.lft warn raw.psl
    # add protein IDs to the qName
    $HOME/bin/i386/protDat tblastnHg16KG.psl \
            /cluster/data/hg16/bed/blat.hg16KG/hg16KG.psl \
            /cluster/data/hg16/bed/blat.hg16KG/kg.mapNames tblastnHg16KGPep.psl
    pslCheck tblastnHg16KGPep.psl
    # psl is good
    # load psl into database
    hgLoadPsl danRer1 tblastnHg16KGPep.psl
    # there are 173355 rows in the table
    # need to do further filtering
    # rename as tblastnHg16KGEval
    hgsql -e "alter table tblastnHg16KGPep rename tblastnHg16KGEval;" danRer1
    mv tblastnHg16KGPep.psl tblastnHg16KGEval.psl
    foreach s (scores/*.scores)
      cat $s >> all.scores
    end
    sort -k2 all.scores > all.scores.sort
    # filter by taking just top 5 hits so for each protein query, take all the
    # alignments associated with its top 5 hits to the whole zebrafish genome
    # using the scores directory created by blastToPsl after filtering at an
    # e-value threshold of 1e-100. 
    # Heather wrote a Java program and used database tables to do this.
    # ScoreFilter.java is in ~kent/java/src/edu/ucsc/genome/qa/filter/
    ssh hgwdev
    cd /cluster/data/danRer1/bed/ZonLab/tblastn/filteredBlast/top5
    hgsql -e "create database qa"; danRer1
    cat << end > blast.sql
CREATE TABLE blast (
    strand char(2) not null,
    qName varchar(255) not null,
    qStart int unsigned not null,
    qEnd int unsigned not null,
    tName varchar(255) not null,
    tStart int unsigned not null,
    tEnd int unsigned not null,
    tSize int unsigned not null,
    score double not null
);
end
    hgsql qa < blast.sql
    echo "load data local infile 'all.scores' into table blast" \
         | hgsql qa
    cat << end > blast2.sql
CREATE TABLE blast2 (
    strand char(2) not null,
    qName varchar(255) not null,
    qStart int unsigned not null,
    qEnd int unsigned not null,
    tName varchar(255) not null,
    tStart int unsigned not null,
    tEnd int unsigned not null,
    tSize int unsigned not null,
    score double not null
);
end
    hgsql qa < blast2.sql
    # load data into blast2 and sort
    echo "insert into blast2 select * from blast order by qName, score, tName" \         | hgsql qa
    # create index
    echo "create index qName on blast2(qName)" | hgsql qa
    cat << end > blast3.sql
CREATE TABLE blast3 (
    strand char(2) not null,
    qName varchar(255) not null,
    qStart int unsigned not null,
    qEnd int unsigned not null,
    tName varchar(255) not null,
    tStart int unsigned not null,
    tEnd int unsigned not null,
    tSize int unsigned not null,
    score double not null
);
end
    hgsql qa < blast3.sql
    mkdir data
    echo "select distinct(qName) from blast" | hgsql qa -N > data/idlist
    wc -l data/idlist
    # 1987 qNames
    # then run ScoreFilter program - this reads from blast2 and
    # writes to blast3 and takes the list of qNames as input
    source javaenv
    javac ScoreFilter.java
    java ScoreFilter qNames.blast.out
    # Test case
    echo "select score, tName from blast2 where qName = 'AB002324' limit 12;" \
         | hgsql qa
    echo "select * from blast3 where qName = 'AB002324';" | hgsql qa
    # get list of qNames and tNames from blast3 to query psl
    echo "select qName, tName from blast3;" | hgsql qa -N \
         > data/qNametName.blast3.out
    wc -l data/qNametName.blast3.out
    # 5201 data/qNametName.blast3.out
    # create psl table in qa
    cat << end > pslBlast.sql
CREATE TABLE pslBlast (
    matches int unsigned not null,
    misMatches int unsigned not null,
    repMatches int unsigned not null,
    nCount int unsigned not null,
    qNumInsert int unsigned not null,
    qBaseInsert int not null,
    tNumInsert int unsigned not null,
    tBaseInsert int not null,
    strand char(2) not null,
    qName varchar(255) not null,
    qSize int unsigned not null,
    qStart int unsigned not null,
    qEnd int unsigned not null,
    tName varchar(255) not null,
    tSize int unsigned not null,
    tStart int unsigned not null,
    tEnd int unsigned not null,
    blockCount int unsigned not null,
    blockSizes longblob not null,
    qStarts longblob not null,
    tStarts longblob not null,
    index(tName(32),tStart,tEnd)
);
end
    hgsql qa < pslBlast.sql
    # cat together psl files
    foreach s (../psl/*.psl)
      cat $s >> all.blast.psl
    end
    wc -l all.blast.psl
    # 181405 all.blast.psl
    # load psl data into table
    echo "load data local infile 'all.blast.psl' into table pslBlast" \
         | hgsql qa
    # check all data loaded, correct number of rows
    # select all lines with qName and tName in data/qNametName.blast3.out
    # from this pslBlast table
    cat << end > getSelectedPsl.pl
#!/usr/bin/perl -w
use strict;
                                                                                
while(<>) {
    chomp;
    my @names = split(/\t/);
    my ($qName, $tName) = @names;
    print STDERR "Querying pslBlast for query: $qName and target:$tName ... \n";    system "hgsql -N -e 'select * from pslBlast where qName = \"$qName\" and tName = \"$tName\";' qa";
                                                                                
}
end
    chmod +x getSelectedPsl.pl
    perl getSelectedPsl.pl < data/qNametName.blast3.out > blastTop5.psl
    # Need to remove long names
    cat << end > shortenNames.pl
#!/usr/bin/perl -w
use strict;
                                                                               
while(<>) {
   my $line = $_;
   my @row = split(/|/);
   # qName is the 10th field in a psl file
   if ($row[9] =~ /\|/) {
      my @accs = split(/\|/, $row[9]);
      my $newAcc = $accs[0];
      foreach (my $i = 0; $i <= $#row; $i++ ) {
         if ($i == 9) {
            print "$newAcc\t";
         }
         elsif ($i == $#row) {
            print "$row[$i]\n";
         }
         else {
            print "$row[$i]\t";
         }
      }
   }
   else {
      print $line;
     }
}
end
   chmod +x shortenNames.pl
   perl shortenNames.pl < blastTop5.psl > blastTop5Format.psl
   wc -l blastTop*
   # 29091 blastTop5.psl
   # 29091 blastTop5Format.psl
    mkdir psl
    mv blastTop5Format.psl ./psl
    pslSort dirs blastTop5Sort.psl tmp psl
    # lift up to chrom co-ordinates
    liftUp -type=".psl" tblastnHg16KG.psl \
          /cluster/data/danRer1/jkStuff/liftAll.lft warn blastTop5Sort.psl
    # add protein IDs to the qName
    $HOME/bin/i386/protDat tblastnHg16KG.psl \
            /cluster/data/hg16/bed/blat.hg16KG/hg16KG.psl \
            /cluster/data/hg16/bed/blat.hg16KG/kg.mapNames tblastnHg16KGPep.psl    pslCheck tblastnHg16KGPep.psl
    # psl is good
    # load psl into database
    hgLoadPsl danRer1 tblastnHg16KGPep.psl
    # previous table of alignments filtered on 1e-100 only is tblastnHg16KGEval    # compare this to current tblastnHg16KGPep table with featureBits
    # featureBits danRer1 tblastnHg16KGEval
    # 2783971 bases of 1459132082 (0.191%) in intersection
    # featureBits danRer1 tblastnHg16KGPep
    # 2397991 bases of 1459132082 (0.164%) in intersection
    # featureBits danRer1 refGene:cds tblastnHg16KGEval
    # 246994 bases of 1459132082 (0.017%) in intersection
    # featureBits danRer1 refGene:cds tblastnHg16KGPep
    # featureBits danRer1 refGene:cds tblastnHg16KGPep
    # 250847 bases of 1459132082 (0.017%) in intersection
    # featureBits danRer1 tblastnHg16KGPepTop3
    # 2223873 bases of 1459132082 (0.152%) in intersection
    # featureBits danRer1 refGene:cds tblastnHg16KGPepTop3
    # 236309 bases of 1459132082 (0.016%) in intersection

    # load sequences into table hg16_kg_proteinseq.fa has 2833 sequences
    # less than in knownGenePep for hg16 - load all knownGenePep sequences
    ssh hgwdev
    cd /cluster/data/danRer1/bed/ZonLab/tblastn
    echo "select * from knownGenePep;" | hgsql -N hg16 > knownGenePepHg16.out
    # make FASTA file and add a "p" to accessions so hgc.c does not get
    # confused with the Genbank sequences of the same name
    awk '{ print ">"$1"p\n"$2}' knownGenePepHg16.out > knownGenePepHg16.fa
    grep '>' knownGenePepHg16.fa | wc
    # 42026 sequences
    # Copy sequences to gbdb if they are not already there
    mkdir -p /gbdb/hgFixed/tblastnKGHg16
    ln -s /cluster/data/danRer1/bed/ZonLab/tblastn/knownGenePepHg16.fa \
          /gbdb/hgFixed/tblastnKGHg16
                                                                               
    # load sequences
    hgLoadSeq danRer1 /gbdb/hgFixed/tblastnKGHg16/knownGenePepHg16.fa
                                                                               
    # try taking top 3 instead of top 5
    ssh hgwdev
    mkdir -p /cluster/data/danRer1/bed/ZonLab/tblastn/filteredBlast/top3
    cd /cluster/data/danRer1/bed/ZonLab/tblastn/filteredBlast/top3
    mkdir data
    cp ../top5/data/idlist data/
    cat << '_EOF_' > getTop3.csh
#!/bin/tcsh
                                                                               
set db = "qa"
set table = "blast3"
set acc = $1
                                                                               
hgsql -N -e 'select qName, tName from '$table' where qName = "'$acc'" order by
score limit 3;' $db >> data/qNametName.blast3.out
'_EOF_'
    # << this line makes emacs coloring happy
     chmod +x getTop3.csh
     cat << '_EOF_' > gsub
#LOOP
./getTop3.csh $(path1)
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy
    gensub2 data/idlist single gsub jobList.csh
    chmod +x jobList.csh
    jobList.csh

    perl ../top5/getSelectedPsl.pl < data/qNametName.blast3.out > blastTop3.psl    perl ../top5/shortenNames.pl < blastTop3.psl > blastTop3Format.psl
    wc -l blastTop*
    # 19823 blastTop5.psl
    # 19823 blastTop5Format.psl
    mkdir psl
    mv blastTop3Format.psl ./psl
    pslSort dirs blastTop3Sort.psl tmp psl
    # lift up to chrom co-ordinates
    liftUp -type=".psl" tblastnHg16KGTop3.psl \
          /cluster/data/danRer1/jkStuff/liftAll.lft warn blastTop3Sort.psl
    # add protein IDs to the qName
    $HOME/bin/i386/protDat tblastnHg16KGTop3.psl \
            /cluster/data/hg16/bed/blat.hg16KG/hg16KG.psl \
            /cluster/data/hg16/bed/blat.hg16KG/kg.mapNames \
            tblastnHg16KGPepTop3.psl
    pslCheck tblastnHg16KGPepTop3.psl
    # psl is good
    # load psl into database
    hgLoadPsl danRer1 tblastnHg16KGPepTop3.psl
    # previous table of alignments filtered on 1e-100 only is tblastnHg16KGEval
    # featureBits danRer1 tblastnHg16KGPepTop3
    # 2223873 bases of 1459132082 (0.152%) in intersection
    # Decided to release the Top 5 hits with tblastnHg16KGPep
    # This could be improved upon. Brian Raney's method below is better and
    # finding exons and reduces noise in the alignments.
    # At a later date, only one of these tracks should be displayed.

    # small changes -- sort by chrom, chromStart, chromEnd; add tName index
    mkdir /cluster/data/danRer1/bed/ZonLab/tblastn/aug12
    cd /cluster/data/danRer1/bed/ZonLab/tblastn/aug12
    hgsqldump danRer1 tblastnHg16KGPep --no-data > tblastnHg16KGPepSorted.sql
    # edit the tablename to be tblastnHg16KGPepSorted.sql
    # also don't need the indices in tblastnHg16KGPepSorted.sql
    hgsql danRer1 < tblastnHg16KGPepSorted.sql
    hgsql danRer1
    insert into tblastnHg16KGPepSorted select * from tblastnHg16KGPep order by tName, tStart, tEnd;
    rename table tblastnHg16KGPep to tblastnHg16KGPepNotSorted;
    rename table tblastnHg16KGPepSorted to tblastnHg16KGPep;
    # would prefer dual indices, but convention is single
    create index bin on tblastnHg16KGPep(bin);
    create index tStart on tblastnHg16KGPep(tStart);
    create index qName on tblastnHg16KGPep(qName);
    create index tEnd on tblastnHg16KGPep(tEnd);
    create index tName on tblastnHg16KGPep(tName);

# Human Proteins - tBLASTn of hg16 knownGene (2004-07-27, braney)
# blastHg16KG track.
    ssh kksilo
    bash

    mkdir -p /cluster/data/danRer1/bed/tblastn.hg16KG
    cd /cluster/data/danRer1/bed/tblastn.hg16KG

    ls -1S /iscratch/i/danRer1/blastdb/*.nsq | sed "s/\.nsq//" >  fish.lst
    mkdir kgfas
    split -l 68 /cluster/data/hg16/bed/blat.hg16KG.2004-05-27/hg16KG.psl kgfas/kg
    cd kgfas
    for i in *
    do
	pslxToFa $i $i.fa
	rm $i
    done
    ls -1S kgfas/*.fa > kg.lst

    # use bluearc to hold our temp output
    mkdir -p /cluster/bluearc/danRer1/bed/tblastn.hg16KG/blastOut
    ln -s  /cluster/bluearc/danRer1/bed/tblastn.hg16KG/blastOut .
    for i in `cat kg.lst`
    do
	mkdir blastOut/$i
    done

    cat << '_EOF_' > blastGsub
#LOOP
blastSome $(path1) {check in line $(path2)} {check out exists blastOut/$(root2)/q.$(root1).psl } {check out exists blastOut/$(root2)/q.$(root1).bscore} {check out exists blastOut/$(root2)/t.$(root1).psl } {check out exists blastOut/$(root2)/t.$(root1).bscore}
#ENDLOOP
'_EOF_'

    gensub2 fish.lst kg.lst blastGsub blastSpec

    ssh kk
    cd /cluster/data/danRer1/bed/tblastn.hg16KG
    para create blastSpec
    para push

    cd blastOut
    for i in kg??
    do
	cat $i/q.*.psl > q.$i.psl
	cat $i/q.*.bscore > q.$i.bscore
	cat $i/t.*.psl > t.$i.psl
	cat $i/t.*.bscore > t.$i.bscore
	simpleChain -prot -maxGap=27000 -outPsl q.$i.psl c.$i.psl
	sort -rn c.$i.psl | pslUniq stdin stdout | awk "((\$13 - \$12) / \$11) > 0.6 {print}" > u.$i.psl
	awk "((\$1 / \$11) ) > 0.60 { print   }" c.$i.psl > m60.$i.psl
	echo $i
    done
    cat u.*.psl m60* | sort -T /tmp -k 14,14 -k 16,16n -k 17,17n  | uniq  > preblastHg16KG.psl

    blatDir=/cluster/data/hg16/bed/blat.hg16KG.2004-05-27
    protDat preblastHg16KG.psl $blatDir/hg16KG.psl $blatDir/kg.mapNames blastHg16KG.psl

    ssh hgwdev
    cd /cluster/data/danRer1/bed/tblastn.hg16KG/blastOut
    hgLoadPsl danRer1 blastHg16KG.psl
#end Human Proteins
