#!/bin/csh -f # set emacs mode
exit; # don't actually run this like a script :)

# This file describes how we made the browser database on 
# NCBI build 35 (May 2004 freeze)

# HOW TO BUILD A ASSEMBLY FROM NCBI FILES 
# ---------------------------------------

    # Make gs.18 directory, gs.18/build35 directory, and gs.18/ffa directory.
    ssh eieio
    mkdir /cluster/store5/gs.18
    mkdir /cluster/store5/gs.18/build35
    mkdir /cluster/store5/gs.18/agp
    mkdir /cluster/store5/gs.18/ffa

    #    Make a symbolic link from /cluster/store1 to this location
    #	(I assume there is some use for this later ?)
	
    cd /cluster/store1
    ln -s /cluster/store5/gs.18 ./gs.18
    ln -s /cluster/store5/gs.18/build35 /cluster/data/hg17

    #    Make a symbolic link from your home directory to the build dir:
    #	(Investigate what this is used for, may no longer be necessary)

    ln -s /cluster/store5/gs.18/build35 ~/oo

# NCBI download site, fetch everything into this one directory:

    #	with the machine and password in your $HOME/.netrc file, this
    #	wget command will require no login.  Your $HOME/.netrc file
    #	is set to 'chmod 600 .netrc' to prevent anyone from finding
    #	the data.  (There were some early files that later moved
    #		into an OLD subdirectory.  They were broken.)
    mkdir /cluster/store5/gs.18/ncbi
    cd /cluster/store5/gs.18/ncbi
    wget --timestamping ftp://ftp.ncbi.nlm.nih.gov/build_35/*

    # FYI: agp file format documented at:
    #	http://www.ncbi.nlm.nih.gov/Genbank/WGS.agpformat.html
    # fixup a couple of names for our own purposes here
    cd /cluster/store5/gs.18/agp
    ln -s ../ncbi/chr*.agp ../ncbi/chr*.fa.gz .
    sed -e "s#MT/NC_001807.4#NC_001807#" ../ncbi/chrMT.agp > chrM.agp
    sed -e "s/NG_002392.2/NG_002392/" ../ncbi/DR52.agp > chr6_hla_hap1.agp
    sed -e "s/NG_002433.1/NG_002433/" ../ncbi/DR53.agp > chr6_hla_hap2.agp
    zcat ../ncbi/DR52.fa.gz | \
	sed -e "s/gi|29124352|ref|NG_002392.2/ref|NG_002392/" | \
	gzip > chr6_hla_hap1.fa.gz
    zcat ../ncbi/DR53.fa.gz | \
	sed -e "s/gi|28212470|ref|NG_002433.1/ref|NG_002433/" | \
	gzip > chr6_hla_hap2.fa.gz
    zcat ../ncbi/chrMT.fa.gz | \
	sed -e "s/gi|17981852|ref|NC_001807.4/ref|NC_001807/" | \
	gzip > chrM.fa.gz
    

    #  Put all the agp files together into one.
    cd /cluster/store5/gs.18/build35
    #	The chrM sequence now has its own agp, remove it from
    #	ref_placed.agp
    sed -e "/^NC_001807/d" ../ncbi/ref_placed.agp > ref_placed.agp
    cat ref_placed.agp ../agp/chrM.agp ../ncbi/ref_unplaced.agp \
	../agp/chr6_hla_hap1.agp ../agp/chr6_hla_hap2.agp \
	../ncbi/PAR.agp > ncbi_build35.agp

    #	and into ffa
    cd /cluster/store5/gs.18/ffa
    #	There is a single bogus line at the end of ref_placed.fa.gz
    #	declaring the NC_001807 MT sequence, this was later replaced by
    #	chrMT.fa.gz, so remove that one line:
    zcat ../ncbi/ref_placed.fa.gz | sed -e "/^>ref|NC_001807/d" | \
	gzip > ref_placed.fa.gz
    #	(That's a 40 minute job)

    #	sequence.inf is usually here, symlink it
    ln -s ../ncbi/sequence.inf
    #	put all the fa.gz files together in one big fa.gz
    time zcat ref_placed.fa.gz ../agp/chrM.fa.gz ../ncbi/ref_unplaced.fa.gz \
	../agp/chr6_hla_hap?.fa.gz ../ncbi/PAR.fa.gz | gzip \
	> ncbi_build35.fa.gz
    #	real    37m42.208s
    #	user    37m3.490s
    #	sys     0m31.430s

    #	Make a listing of all the fasta record headers, just FYI:
    cd /cluster/store5/gs.18/ffa
    zcat ffa/ncbi_build35.fa.gz | grep "^>" > ncbi.fa.headers


    #	New to this build is the sequence: NC_001807 which is the
    #	mitochondria sequence.  This prefix NC_ is new to the process
    #	and will have to be accounted for below.  The other two special
    #	prefixes are similar to what was seen before:
    #	from DR52.agp NG_002392
    #	Homo sapiens major histocompatibility complex, class II,
    #		DR52 haplotype (DR52) on chromosome 6
    #	and from DR53.agp NG_002433
    #	Homo sapiens major histocompatibility complex, class II,
    #		DR53 haplotype (DR53) on chromosome 6

    #	Fixup seq_contig.md
    #
    #	It has a bunch of stuff belonging to the Celera
    #	genome assembly.  Filter those out.  I don't know what the
    #	NT_07959[0-7] items are, but there are no definitions for them
    #	in the agp files and no sequence in any fa.gz file.
    #	Fixup the names for the NG_ items, and change chrom MT to be M
    cd /cluster/store5/gs.18/build35
    egrep -v "Celera|NT_07959[0-7]" ../ncbi/seq_contig.md | \
	sed -e "s/6|NG_002392/6_hla_hap1/" \
	-e "s/6|NG_002433/6_hla_hap2/" \
	-e "s/^9606\tMT|NC_001807/9606\tM/" \
	> temp_contig.md

    #	get the randoms sorted in proper order.  The createNcbiLifts
    #	does not work correctly if the randoms are not grouped together
    #	by chromosome
    grep -v "|" temp_contig.md  > seq_contig.md
    #	This pulls out all the randoms and groups them within the
    #	same chrom but leaving them in the same order as they orginally
    #	were  (warning this is BASH code ...)
    grep "|" temp_contig.md | awk -F"|" '{print $1}' | \
        awk '{print $2}' | sort -n -u | while read CHR
do
        grep "[^0-9]${CHR}|" temp_contig.md
done >> seq_contig.md


    # Sanity check, checkYbr was updated to handle the NC_ identifier
    time zcat ../ffa/ncbi_build35.fa.gz | $HOME/bin/i386/checkYbr \
	ncbi_build35.agp stdin seq_contig.md > check.seq_contig
    #	real    2m34.143s
    #	user    2m24.970s
    #	sys     0m8.900s
    #	result should be clean:
    cat check.seq_contig
    #	Read 380 contigs from ncbi_build35.agp
    #	Verifying sequence sizes in stdin
    #	0 problems detected


    # Convert fa files into UCSC style fa files and place in "contigs"
    # directory inside the gs.18/build35 directory 
    #	(a check that can be done here is make a list of the contigs
    #	in this ./contigs directory before and compare it with the
    #	list of distributed contigs created after they have been
    #	disbursed.)
    #	faNcbiToUcsc was fixed to handle the NC_ identifier

    cd /cluster/store5/gs.18/build35
    #	We've been through this often
    mv contigs contigs.0
    time zcat ../ffa/ncbi_build35.fa.gz | $HOME/bin/i386/faNcbiToUcsc \
	-split -ntLast stdin contigs
    #	real    5m10.938s
    #	user    2m20.070s
    #	sys     0m51.020s
    #	If you want to compare anything to previous work, check now, then:
    rm -fr contigs.0

    # Determine the chromosome sizes from agps
    #	Watch carefully how chrY gets constructed.  I'm not sure
    #	this chrom_sizes represents the whole length of chrY with
    #	the PAR added.  We will see about that.
    #	Script updated to handle new chrom names:
    #	my @chroms = (1 .. 22, 'X', 'Y', 'M', '6_hla_hap1', '6_hla_hap2');

    cd /cluster/store5/gs.18/build35
    /cluster/bin/scripts/getChromSizes ../agp
    #	Create chrom.lst list for use in foreach() loops
    awk '{print $1}' chrom_sizes | sed -e "s/chr//" > chrom.lst

    # Create lift files (this will create chromosome directory structure) and
    #	inserts file
  
    /cluster/bin/scripts/createNcbiLifts -s chrom_sizes seq_contig.md .

    # Create contig agp files (will create contig directory structure)
	
    /cluster/bin/scripts/createNcbiCtgAgp seq_contig.md ncbi_build35.agp .

    # Create chromsome random agp files.

    /cluster/bin/scripts/createNcbiChrAgp -randomonly .

    # Copy the original chrN.agp files from the gs.18/agp directory 
    #    into each of the chromosome directories since they contain better 
    #    gap information. Delete the comments at top from these.
    cd /cluster/store5/gs.18/build35
    foreach c ( `cat chrom.lst` )
	sed -e "/^#.*/d" ../agp/chr${c}.agp > ./${c}/chr${c}.agp
    end
    #	chrM needs a name fixup
    sed -e "s#NC_001807#chrM#" ../agp/chrM.agp > M/chrM.agp

    # Distribute contig .fa to appropriate directory (assumes all files
    # are in "contigs" directory).

    # create global data link for everyone.  No more home directory
    # links required.
    ln -s /cluster/store5/gs.18/build35 /cluster/data/hg17
    cd /cluster/data/hg17
    /cluster/bin/scripts/distNcbiCtgFa contigs .
    #	Verify that everything was moved properly, the contigs directory
    #	should be empty:
    ls contigs
    #	Nothing there, then remove it
    rmdir  contigs

    #	Make a list of the contigs for use later
    rm contig.lst
    touch contig.lst
    foreach chrom ( `cat chrom.lst` )
	foreach c ( $chrom/N{C,G,T}_?????? )
	    set contig = $c:t
	    echo "${chrom}/${contig}/${contig}.fa" >> contig.lst
	end
    end
    #   For later comparisons, this is how many contigs we have:
    wc -l contig.lst
    #	380


    # FILES ARE NOW READY FOR REPEAT MASKING - start that process as
    #	other steps here can proceed in parallel.

    #	Previous practice used to copy everything over for jkStuff from a
    #	previous build.  Rather than do that, pick up whatever is needed
    #	at the time it is needed and verify that it is going to do what
    #	you expect.

    cd /cluster/data/hg17
    mkdir jkStuff

    # Create contig gl files - XXX - NCBI doesn't deliver
    # contig_overlaps.agp
    XXXX /cluster/bin/i386/agpToGl contig_overlaps.agp . -md=seq_contig.md
    # Create chromosome gl files
    XXXX jkStuff/liftGl.csh contig.gl

# CREATING DATABASE  (DONE - 2004-05-20 - Hiram)
    #	RE-DONE for new NIBS - 2004-06-03
    ssh hgwdev
    # Make sure there is at least 5 gig free on hgwdev:/var/lib/mysql
    df -h /var/lib/mysql
#	Filesystem            Size  Used Avail Use% Mounted on
#	/dev/sdc1             1.8T  303G  1.4T  19% /var/lib/mysql

    # Create the database.
    hgsql -e 'create database hg17' mysql
    # Copy over grp table (for track grouping) from another database:
    hgsql -e "create table grp (PRIMARY KEY(NAME)) select * from hg16.grp" hg17

# MAKE CHROMINFO TABLE WITH (TEMPORARILY UNMASKED) NIBS
#	(DONE - 2004-05-21 - Hiram)
    #	RE-DONE with new NIBS - 2004-06-03
    # Make nib/, unmasked until RepeatMasker and TRF steps are done.
    # Do this now so that the chromInfo table will exist and thus the
    #	trackDb tables can be built in the next step.
    #	These unmasked nibs will be replaced by the masked nibs after
    #	repeat mask and trf are done.
    ssh eieio
    cd /cluster/data/hg17
    # Make chr*.fa from contig .fa
    #  Copied chrFa.sh from hg16/jkStuff, renamed it to chrFa.csh
    time ./jkStuff/chrFa.csh
    #	real    13m24.710s
    #	user    9m0.360s
    #	sys     1m15.820s

    mkdir nib
    foreach c (`cat chrom.lst`)
      foreach f ($c/chr${c}{,_random}.fa)
        if (-e $f) then
          echo "nibbing $f"
          /cluster/bin/i386/faToNib $f nib/$f:t:r.nib
        endif
      end
    end

    # Make symbolic links from /gbdb/hg17/nib to the real nibs.
    ssh hgwdev
    mkdir -p /gbdb/hg17/nib
    ln -s /cluster/data/hg17/nib/chr*.nib /gbdb/hg17/nib
    # Load /gbdb/hg17/nib paths into database and save size info.
    cd /cluster/data/hg17
    hgsql hg17  < $HOME/kent/src/hg/lib/chromInfo.sql
    hgNibSeq -preMadeNib hg17 /gbdb/hg17/nib */chr*.fa
    hgsql -N -e "select chrom,size from chromInfo order by chrom" hg17 \
	> chrom.sizes
    # You can compare this chrom.sizes with the previously created
    # chrom_sizes.  Should be no difference
    sort chrom_sizes > s0
    sort chrom.sizes | grep -v random > s1
    diff s0 s1
    rm s0 s1

# MAKE HGCENTRALTEST ENTRY AND TRACKDB TABLE (DONE - 2004-05-21 - Hiram)
    #	dbDb orderKey updated 2004-06-08 - Hiram
    ssh hgwdev
    #	reset dbDb orderKey - these have never been ordered properly
    #	before, this will get them on the program.
    hgsql -e 'update dbDb set orderKey=11 where name = "hg16";' \
	-h genome-testdb hgcentraltest
    hgsql -e 'update dbDb set orderKey=12 where name = "hg15";' \
	-h genome-testdb hgcentraltest
    hgsql -e 'update dbDb set orderKey=13 where name = "hg13";' \
	-h genome-testdb hgcentraltest

    # Enter hg17 into hgcentraltest.dbDb so test browser knows about it:
    hgsql -e 'INSERT INTO dbDb (name, description, nibPath, organism, \
	defaultPos, active, orderKey, genome, scientificName, \
	htmlPath, hgNearOk) \
	VALUES("hg17", "May 2004", "/gbdb/hg17/nib", "Human", \
	"chr4:56214201-56291736", 1, 10, "Human", "Homo sapiens", \
	"/gbdb/hg17/html/description.html", 0);' \
	-h genome-testdb hgcentraltest
    # Make trackDb table so browser knows what tracks to expect:
    cd ~kent/src/hg/makeDb/trackDb
    cvs up -d -P .
    # Edit the makefile to add hg17 in all the right places and do
    make update
    make alpha
    cvs commit makefile

# MAKE LIFTALL.LFT, NCBI.LFT (DONE - 2004-05-21 - Hiram)
    #	Re-DONE with new randoms - 2004-06-03 - Hiram)
    cd /cluster/data/hg17
    mkdir -p jkStuff
    cat */lift/{ordered,random}.lft > jkStuff/liftAll.lft
    # Create jkStuff/ncbi.lft for lifting stuff built with the NCBI assembly.
    # Note: this ncbi.lift will not lift floating contigs to chr_random coords,
    # but it will show the strand orientation of the floating contigs 
    # (grep for '|').
    #   mdToNcbiLift seq_contig.md jkStuff/ncbi.lft 
    #	XXXX - appears to be unused, not done - Hiram

# REPEAT MASKING (DONE - 2004-05-24 - Hiram)
    #	The randoms were rearranged after this was first done,
    #	they are re-made below 2004-06-02)

    # Split contigs, run RepeatMasker, lift results

    #	This split takes about 8 minutes
    ssh eieio
    cd /cluster/data/hg17
    foreach chrom ( `cat chrom.lst` )
	foreach c ( $chrom/N{C,G,T}_?????? )
	    set contig = $c:t
	    echo "splitting ${chrom}/${contig}/${contig}.fa"
	    faSplit size ${chrom}/${contig}/$contig.fa 500000 \
		${chrom}/${contig}/${contig}_ \
		-lift=${chrom}/${contig}/$contig.lft -maxN=500000
	end
    end

    #- Make the run directory and job list:
    cd /cluster/data/hg17
    mkdir -p jkStuff
    #  According to RepeatMasker help file, no arguments are required to
    #	specify species because its default is set for primate (human)
    #  This run script saves the .tbl file to be sent to Arian.  He uses
    # those for his analysis.  Sometimes he needs the .cat and .align files for
    # checking problems.  Krish needs the .align files, they are large.

    cat << '_EOF_' > jkStuff/RMHuman
#!/bin/csh -fe

cd $1
pushd .
/bin/mkdir -p /tmp/hg17/$2
/bin/cp $2 /tmp/hg17/$2/
cd /tmp/hg17/$2
/cluster/bluearc/RepeatMasker/RepeatMasker -ali -s $2
popd
/bin/cp /tmp/hg17/$2/$2.out ./
 if (-e /tmp/hg17/$2/$2.align) /bin/cp /tmp/hg17/$2/$2.align ./
if (-e /tmp/hg17/$2/$2.tbl) /bin/cp /tmp/hg17/$2/$2.tbl ./
# if (-e /tmp/hg17/$2/$2.cat) /bin/cp /tmp/hg17/$2/$2.cat ./
/bin/rm -fr /tmp/hg17/$2/*
/bin/rmdir --ignore-fail-on-non-empty /tmp/hg17/$2
/bin/rmdir --ignore-fail-on-non-empty /tmp/hg17
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x jkStuff/RMHuman

    ssh eieio
    cd /cluster/data/hg17
    mkdir RMRun
    rm -f RMRun/RMJobs
    touch RMRun/RMJobs
    foreach d ( `cat chrom.lst` )
     foreach c ( ${d}/N{C,G,T}_*/N{C,G,T}_*_*.fa )
        set f = $c:t
        set cc = $c:h
        set contig = $cc:t
        echo /cluster/store5/gs.18/build35/jkStuff/RMHuman \
   		/cluster/store5/gs.18/build35/${d}/${contig} $f \
   '{'check out line+ /cluster/store5/gs.18/build35/${d}/${contig}/$f.out'}' \
          >> RMRun/RMJobs
      end
    end

    # We have 5971 jobs in RMJobs:
    wc RMRun/RMJobs
    #	5970   41790 1105804 RMRun/RMJobs

    #- Do the run
    ssh kk
    cd /cluster/data/hg17/RMRun
    para create RMJobs
    para try, para check, para check, para push, para check,...

    #- While that is running, you can run TRF (simpleRepeat) on the small
    # cluster.  See SIMPLE REPEAT section below
# Completed: 5970 of 5970 jobs
# CPU time in finished jobs:   45189516s  753158.60m 12552.64h  523.03d  1.433 y
# IO & Wait Time:                141333s    2355.55m    39.26h    1.64d  0.004 y
# Average job time:                7593s     126.55m     2.11h    0.09d
# Longest job:                    10268s     171.13m     2.85h    0.12d
# Submission to last job:         81484s    1358.07m    22.63h    0.94d

    #	Lift up the split-contig .out's to contig-level .out's
    #
    #	If a mistake is made in the following it would be possible to
    #	destroy all the RM output.  So, just to be paranoid, save all
    #	the RM output in bluearc for the time being:
    ssh eieio

    cd /cluster/data/hg17
    mkdir /cluster/bluearc/hg17/RMOutput
    foreach c ( `cat chrom.lst` )
     foreach d ( ${c}/N{C,G,T}_* )
	set T = /cluster/bluearc/hg17/RMOutput/${d}
	mkdir -p ${T}
        cd ${d}
        set contig = $d:t
        cp -p ${contig}_?{,?,??}.fa.out ${T}
        cd ../..
	echo "${d} done"
     end
    end
    #	Make sure we got them all:
    #	(this doesn't work later since there are more *.fa.out files
    #	after the lifting.  More explicitly to find just these:
    #		find . -name "N?_*_*.fa.out" -print | wc -l
    find . -name "*.fa.out" -print | wc -l
    #	5970
    find /cluster/bluearc/hg17/RMOutput -type f | wc -l
    #	5970
    #	same count

    #	OK, now you can try this operation, do it in a script like this
    #	and save the output of the script for a record of what happened.

    cat << '_EOF_' > jkStuff/liftRM.csh
#!/bin/csh -fe
foreach c ( `cat chrom.lst` )
 foreach d ( ${c}/N{C,G,T}_* )
    cd $d
    set contig = $d:t
    liftUp $contig.fa.out $contig.lft warn ${contig}_?{,?,??}.fa.out 
    cd ../..
 end
end
'_EOF_'
    chmod +x jkStuff/liftRM.csh
    mkdir scriptsOutput
    time jkStuff/liftRM.csh > scriptsOutput/liftRM.1 2>&1
    #	real    4m37.572s
    #	user    1m19.130s
    #	sys     0m32.950s
    #	Check that they all were done:
    grep "fa.out" scriptsOutput/liftRM.1 | wc -l
    #	5959
    #	same count as above

    #- Lift up RepeatMask .out files to chromosome coordinates via
    # picked up jkStuff/liftOut2.sh from the hg16 build.  Renamed to
    # liftOut2.csh, changed the line that does the chrom listing

    time ./jkStuff/liftOut2.csh > scriptsOutput/liftOut2 2>&1
    #	real    9m46.780s
    #	user    1m18.900s
    #	sys     7m33.990s

    #- By this point, the database should have been created (above):
    ssh hgwdev
    cd /cluster/data/hg17
    time hgLoadOut hg17 ?/*.fa.out ??/*.fa.out 6_hla_hap?/*.fa.out > \
	scriptsOutput/hgLoadOut 2>&1
    #	real    5m59.137s
    #	user    1m47.550s
    #	sys     0m15.410s

    # errors during this load:  (there are always a couple of these)
    #	Strange perc. field -6.1 line 243543 of 2/chr2.fa.out
    #	Strange perc. field -5.6 line 243543 of 2/chr2.fa.out
    #	Strange perc. field -6.1 line 243545 of 2/chr2.fa.out
    #	Strange perc. field -5.6 line 243545 of 2/chr2.fa.out
    #	Strange perc. field -0.2 line 30322 of 17/chr17.fa.out
    #	Strange perc. field -0.2 line 30324 of 17/chr17.fa.out
    #	Strange perc. field -0.2 line 30326 of 17/chr17.fa.out
    #	Strange perc. field -0.2 line 30328 of 17/chr17.fa.out
    #	Strange perc. field -18.6 line 77034 of 19/chr19.fa.out

    #	Verify we have similar results to previous assembly:
    #	featureBits hg17 rmsk
    #	1391378842 bases of 2867328468 (48.525%) in intersection
    #	featureBits hg16 rmsk
    #	1388770568 bases of 2865248791 (48.469%) in intersection
    #	Now proceed to MASK SEQUENCE BOTH REPEATMASKER AND SIMPLE REPEAT/TRF
    #	following the SIMPLE REPEAT sections below

# Re-Running REPEAT_MASKER on the new Randoms (DONE - 2004-06-02 - Hiram)
    ssh eieio
    cd /cluster/data/hg17
    grep "|" seq_contig.md | awk '{print $2}' | sed -e "s#|#/#" > randoms.lst

    mkdir /cluster/data/hg17/RMRandoms
    foreach r ( `cat randoms.lst` )
	set d = $r:h
	set contig = $r:t
	foreach c ( ${r}/N{C,G,T}_*_*.fa )
	    set f = $c:t
	    echo /cluster/store5/gs.18/build35/jkStuff/RMHuman \
   		/cluster/store5/gs.18/build35/${d}/${contig} $f \
   '{'check out line+ /cluster/store5/gs.18/build35/${d}/${contig}/$f.out'}' \
          >> RMRandoms/RMJobs
	end
    end

    ssh kk
    cd /cluster/data/hg17/RMRandoms
    para create RMJobs
    para try, para check, para check, para push, para check,...
# Completed: 94 of 94 jobs
# CPU time in finished jobs:     221454s    3690.91m    61.52h    2.56d  0.007 y
# IO & Wait Time:                   866s      14.43m     0.24h    0.01d  0.000 y
# Average job time:                2365s      39.42m     0.66h    0.03d
# Longest job:                     9062s     151.03m     2.52h    0.10d
# Submission to last job:          9106s     151.77m     2.53h    0.11d

    #	Continuing with the paranoia theme, let's backup all the RM output
    #
    ssh eieio

    cd /cluster/data/hg17
    mkdir /cluster/bluearc/hg17/RMRandoms
    foreach c ( `cat chrom.lst` )
     foreach d ( ${c}/N{C,G,T}_* )
	set T = /cluster/bluearc/hg17/RMRandoms/${d}
	mkdir -p ${T}
        cd ${d}
        set contig = $d:t
        cp -p ${contig}_?{,?,??}.fa.out ${T}
        cd ../..
	echo "${d} done"
     end
    end
    #	Make sure we got them all:
    find . -name "N?_*_*.fa.out" -print | wc -l
    #	5959
    find /cluster/bluearc/hg17/RMRandoms -type f | wc -l
    #	5959
    #	same count


    time jkStuff/liftRM.csh > scriptsOutput/liftRM2.1 2>&1
    #	real    4m46.302s
    #	user    1m18.260s
    #	sys     0m18.000s
    #	Check that they all were done:
    grep "fa.out" scriptsOutput/liftRM2.1 | wc -l
    #	5959
    #	same count as above

    #- Lift up RepeatMask .out files to chromosome coordinates via
    # picked up jkStuff/liftOut2.sh from the hg16 build.  Renamed to
    # liftOut2.csh, changed the line that does the chrom listing

    time ./jkStuff/liftOut2.csh > scriptsOutput/liftOut2.1 2>&1
    #	real    2m46.347s
    #	user    1m18.650s
    #	sys     0m15.990s

    #- By this point, the database should have been created (above):
    ssh hgwdev
    cd /cluster/data/hg17
    time hgLoadOut hg17 ?/*.fa.out ??/*.fa.out 6_hla_hap?/*.fa.out > \
	scriptsOutput/hgLoadOut 2>&1
    #	real    5m59.137s
    #	user    1m47.550s
    #	sys     0m15.410s

    # errors during this load:  (there are always a couple of these)
    #	Strange perc. field -6.1 line 243543 of 2/chr2.fa.out
    #	Strange perc. field -5.6 line 243543 of 2/chr2.fa.out
    #	Strange perc. field -6.1 line 243545 of 2/chr2.fa.out
    #	Strange perc. field -5.6 line 243545 of 2/chr2.fa.out
    #	Strange perc. field -0.2 line 30322 of 17/chr17.fa.out
    #	Strange perc. field -0.2 line 30324 of 17/chr17.fa.out
    #	Strange perc. field -0.2 line 30326 of 17/chr17.fa.out
    #	Strange perc. field -0.2 line 30328 of 17/chr17.fa.out
    #	Strange perc. field -18.6 line 77034 of 19/chr19.fa.out

    #	Verify we have similar results to previous assembly:
    #	featureBits hg17 rmsk
    #	1390952984 bases of 2866216770 (48.529%) in intersection
    #	featureBits hg17 rmsk  #with previous randoms:
    #	1391378842 bases of 2867328468 (48.525%) in intersection
    #	featureBits hg16 rmsk
    #	1388770568 bases of 2865248791 (48.469%) in intersection
    #	Now proceed to MASK SEQUENCE BOTH REPEATMASKER AND SIMPLE REPEAT/TRF
    #	following the SIMPLE REPEAT sections below

# SIMPLE REPEAT [TRF] TRACK (DONE - 2004-05-21 - Hiram)
    #	Re-done with new randoms, 2004-06-02 - Hiram
    #	Copy the contigs, first to the bluearc, then to /iscratch/i
    ssh eieio
    mkdir /cluster/bluearc/hg17
    mkdir /cluster/bluearc/hg17/contigs

    cd /cluster/data/hg17
    foreach ctg ( `cat contig.lst` )
	set c = $ctg:t
 	echo "$ctg > /cluster/bluearc/hg17/contigs/$c"
	cp -p $ctg /cluster/bluearc/hg17/contigs/$c
    end
    #	Check how much is there:
    #	du -hsc /cluster/bluearc/hg17/contigs
    #	2.8G    /cluster/bluearc/hg17/contigs

    # Distribute contigs to /iscratch/i
    ssh kkr1u00
    mkdir /iscratch/i/gs.18/build35/unmaskedContigs
    cd /iscratch/i/gs.18/build35/unmaskedContigs
    cp -p /cluster/bluearc/hg17/contigs/* .

    # Verify same amount made it there:
    #	du -hsc /iscratch/i/gs.18/build35/unmaskedContigs
    #	2.8G    /iscratch/i/gs.18/build35/unmaskedContigs
    #	Then send them to the other 7 Iservers
    /cluster/bin/iSync

    #	Go to the small cluster for this business:
    ssh kki

    mkdir -p /cluster/data/hg17/bed/simpleRepeat
    cd /cluster/data/hg17/bed/simpleRepeat
    mkdir trf
    cat << '_EOF_' > runTrf
#!/bin/csh -fe
#
set path1 = $1
set inputFN = $1:t
set outpath = $2
set outputFN = $2:t
mkdir -p /tmp/$outputFN
cp $path1 /tmp/$outputFN
pushd .
cd /tmp/$outputFN
/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $inputFN /dev/null -bedAt=$outputFN -tempDir=/tmp
popd
rm -f $outpath
cp -p /tmp/$outputFN/$outputFN $outpath
rm -fr /tmp/$outputFN/*
rmdir --ignore-fail-on-non-empty /tmp/$outputFN
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x runTrf

    cat << '_EOF_' > gsub
#LOOP
./runTrf {check in line+ $(path1)}  {check out line trf/$(root1).bed}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    ls -1S /iscratch/i/gs.18/build35/unmaskedContigs/*.fa > genome.lst
    gensub2 genome.lst single gsub jobList
    para create jobList
    para try
    para check
    para push
    para check
# Completed: 380 of 380 jobs
# CPU time in finished jobs:      13230s     220.49m     3.67h    0.15d  0.000 y
# IO & Wait Time:                  2078s      34.64m     0.58h    0.02d  0.000 y
# Average job time:                  40s       0.67m     0.01h    0.00d
# Longest job:                     1590s      26.50m     0.44h    0.02d
# Submission to last job:          2504s      41.73m     0.70h    0.03d

    liftUp simpleRepeat.bed /cluster/data/hg17/jkStuff/liftAll.lft \
	warn trf/*.bed  > lu.out 2>&1

    # Load into the database:
    ssh hgwdev
    cd /cluster/data/hg17/bed/simpleRepeat
    /cluster/bin/i386/hgLoadBed hg17 simpleRepeat simpleRepeat.bed \
      -sqlTable=$HOME/kent/src/hg/lib/simpleRepeat.sql
    #	Loaded 629076 elements of size 16
    #	Compare with previous assembly
    featureBits hg17 simpleRepeat
    #	54952425 bases of 2866216770 (1.917%) in intersection

    #	with previous randoms
    featureBits hg17 simpleRepeat
    #	54964044 bases of 3096628158 (1.775%) in intersection
    featureBits hg16 simpleRepeat
    #	54320136 bases of 2865248791 (1.896%) in intersection
    #	GAPS weren't in hg17 yet at this point, after gaps added:
    #	featureBits hg17 simpleRepeat
    #	54964044 bases of 2867328468 (1.917%) in intersection
    #	featureBits -countGaps hg17 simpleRepeat
    #	54964044 bases of 3096628158 (1.775%) in intersection


# PROCESS SIMPLE REPEATS INTO MASK (DONE - 2004-05-21 - Hiram)
    #	re-done with new randoms - 2004-06-03 - Hiram
    # After the simpleRepeats track has been built, make a filtered version 
    # of the trf output: keep trf's with period <= 12:
    ssh eieio
    cd /cluster/data/hg17/bed/simpleRepeat
    mkdir -p trfMask
    foreach f (trf/*.bed)
      awk '{if ($5 <= 12) print;}' $f > trfMask/$f:t
    end

    #	EXPERIMENT, at a filter of <= 12, we have coverage:
    #	20904399 bases of 2867328468 (0.729%) in intersection
    #	at a filter of <= 9, we have coverage:
    #	19271270 bases of 2867328468 (0.672%) in intersection


    # Lift up filtered trf output to chrom coords as well:
    cd /cluster/data/hg17
    mkdir bed/simpleRepeat/trfMaskChrom
    foreach c ( `cat chrom.lst` )
      if (-e $c/lift/ordered.lst) then
        perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
          $c/lift/ordered.lst > $c/lift/oTrf.lst
        liftUp bed/simpleRepeat/trfMaskChrom/chr$c.bed \
          jkStuff/liftAll.lft warn `cat $c/lift/oTrf.lst`
      endif
      if (-e $c/lift/random.lst) then
        perl -wpe 's@(\S+)@bed/simpleRepeat/trfMask/$1.bed@' \
           $c/lift/random.lst > $c/lift/rTrf.lst
        liftUp bed/simpleRepeat/trfMaskChrom/chr${c}_random.bed \
          jkStuff/liftAll.lft warn `cat $c/lift/rTrf.lst`
      endif
    end

# MASK SEQUENCE BOTH REPEATMASKER AND SIMPLE REPEAT/TRF (DONE - 2004-05-25)
#							 -Hiram
    #	re-done with new randoms - 2004-06-03 - Hiram
    # This used to be done right after RepeatMasking.  Now, we mask with 
    # TRF as well, so do this after the "PROCESS SIMPLE REPEATS" step above,
    #	and after Repeat Masker is complete.
    ssh eieio
    cd /cluster/data/hg17

    # copied these scripts from hg16 - reset the lines that make
    # the chrom list to work on, reset the wild cards that find all the
    # contig .fa's

    # Make chr*.fa from contig .fa
    #  Copied chrFa.sh from hg16/jkStuff, renamed it to chrFa.csh
    time ./jkStuff/chrFa.csh > scriptsOutput/chrFa.out 2>&1 &
    #	real    13m18.512s
    #	user    9m1.670s
    #	sys     1m7.290s

    #- Soft-mask (lower-case) the contig and chr .fa's
    time ./jkStuff/makeFaMasked.csh > scriptsOutput/maFaMasked.out 2>&1
    #	real    29m31.623s
    #	user    13m49.700s
    #	sys     5m58.750s
    #- Make hard-masked .fa.masked files as well:
    time ./jkStuff/makeHardMasked.csh > scriptsOutput/maHardMasked.out 2>&1

    #- Create the bothMasksNib/ directory
    time ./jkStuff/makeNib.csh > scriptsOutput/maNib.out 2>&1
    #	real    14m41.694s
    #	user    6m28.000s
    #	sys     1m42.500s

    # Make symbolic links from /gbdb/hg17/nib to the real nibs.
    ssh hgwdev
    mv nib nib.raw
    mv bothMasksNib nib
    rm /gbdb/hg17/nib/*.nib
    ln -s `pwd`/nib/* /gbdb/hg17/nib

    # Load /gbdb/hg17/nib paths into database and save size info.
    hgsql hg17  < ~/kent/src/hg/lib/chromInfo.sql
    cd /cluster/data/hg17
    hgNibSeq -preMadeNib hg17 /gbdb/hg17/nib */chr*.fa
    #	3096628158 total bases

    #	Should be the same size as before
    hgsql -N -e "select chrom,size from chromInfo order by chrom" hg17 \
	> chrom.sizes.masked
    diff chrom.sizes chrom.sizes.masked
    #	should be no output at all, thus:
    rm chrom.sizes.masked

    # Copy the masked contig fa to /scratch and /iscratch
    #	And everything else we will need for blastz runs, etc ...
    #	Best to do this sequence first to /cluster/bluearc/scratch,
    #	which is going to be the source for the /scratch copy.
    #	And then from there to the /iscratch
    #	Make sure you are on the fileserver for the original source:
    ssh eieio
    mkdir -p /cluster/bluearc/scratch/hg/gs.18/build35
    cd /cluster/bluearc/scratch/hg/gs.18/build35

    #	these copies take less than 2 minutes each
    mkdir bothMaskedNibs
    cp -p /cluster/data/hg17/nib/*.nib ./bothMaskedNibs
    mkdir maskedContigs
    foreach chrom ( `cat /cluster/data/hg17/chrom.lst` )
	cp -p /cluster/data/hg17/${chrom}/N{C,G,T}_*/N{C,G,T}_??????.fa \
		./maskedContigs
	echo "done ${chrom}"
    end
    #	make sure you have them all:
    ls maskedContigs | wc -l
    #	380
    wc -l /cluster/data/hg17/contig.lst
    #	380
    mkdir rmsk
    foreach chrom ( `cat /cluster/data/hg17/chrom.lst` )
	cp -p /cluster/data/hg17/${chrom}/*.out ./rmsk
	echo "done ${chrom}"
    end

    #	Now, go to the destination for /iscratch and copy from the
    #	bluearc
    ssh kkr1u00
    mkdir -p /iscratch/i/gs.18/build35
    cd /iscratch/i/gs.18/build35
    #	This takes about 5 minutes
    rsync -arlv /cluster/bluearc/scratch/hg/gs.18/build35/ .

    time /cluster/bin/iSync
    #	real    7m27.649s

    # request rsync of /cluster/bluearc/scratch to the KiloKluster /scratch

# GOLD AND GAP TRACKS (DONE - 2004-05-21 - Hiram)
    #	RE-DONE with new randoms - 2004-06-03 - Hiram
    ssh hgwdev
    cd /cluster/data/hg17
    hgGoldGapGl -noGl -chromLst=chrom.lst hg17 /cluster/data/hg17 .
    #	Disappointing to see this create so many tables ...
    #	_gap and _gold for each chrom

# LOAD ctgPos table - Contig position track (DONE - 2004-06-08 - Hiram)
    #	After fixing up hgCtgPos to accept the -chromLst argument, simply:
    cd /cluster/data/hg17
    hgCtgPos -chromLst=chrom.lst hg17 .

XXX - Come back to this after contig_overlaps.agp has been created
# O+O: ASSEMBLY [GOLD], GAP, COVERAGE, MAP CONTIGS TRACKS (WAIT - 2004-05-21)
# Store o+o info in database.
    ssh eieio
    cd /cluster/data/hg17
    if (-f contig_overlaps.agp) then
      jkStuff/liftGl.sh contig.gl
    else
      ssh hgwdev
      hgGoldGapGl -noGl hg17 /cluster/store5/gs.18 build35 
      echo ""
      echo "*** Note from makeHg15.doc:"
      echo "Come back to this step later when we have contig_overlaps.agp\!"
    endif
    ssh hgwdev
    cd /cluster/store5/gs.18/build35
    if (-f contig_overlaps.agp) then
      hgGoldGapGl hg17 /cluster/store5/gs.18 build35 
      cd /cluster/store5/gs.18
      /cluster/bin/i386/hgClonePos hg17 build35 ffa/sequence.inf /cluster/store5/gs.18 -maxErr=3
    end 
    cd /cluster/store5/gs.18
    # (2/27/04 angie) re-loaded -- chr{1,4,8,15}_random lift files changed
    # 7/30/04.
    hgCtgPos hg17 build35 

#  gc5Base wiggle TRACK (DONE - 2004-05-22 - Hiram)
    #	This previously was a script that ran through each nib
    #	Recently transformed into a mini cluster run.
    #	Re-DONE with the new randoms - 2004-06-04
    ssh kki
    mkdir /cluster/data/hg17/bed/gc5Base
    cd /cluster/data/hg17/bed/gc5Base

    mkdir wigData5 dataLimits5 wigData5_1K dataLimits5_1K

    cat << '_EOF_' > kkRun.sh
#!/bin/sh
NIB=$1

chr=${NIB/.nib/}
chrom=${chr#chr}

hgGcPercent -chr=${chr} -doGaps -file=stdout -win=5 hg17 \
        /iscratch/i/gs.18/build35/bothMaskedNibs | \
    grep -w GC | \
    awk '{if (($3-$2) >= 5) {printf "%d\t%.1f\n", $2+1, $5/10.0} }' | \
    wigAsciiToBinary -dataSpan=5 -chrom=${chr} \
        -wibFile=wigData5/gc5Base_${chrom} \
            -name=${chrom} stdin 2> dataLimits5/${chr}
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x kkRun.sh

    ls /iscratch/i/gs.18/build35/bothMaskedNibs > nibList
    cat << '_EOF_' > gsub
#LOOP
./kkRun.sh $(path1)
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    gensub2 nibList single gsub jobList
    para create jobList
    para try, check, ... etc
# Completed: 46 of 46 jobs
# CPU time in finished jobs:       5251s      87.51m     1.46h    0.06d  0.000 y
# IO & Wait Time:                   130s       2.17m     0.04h    0.00d  0.000 y
# Average job time:                 117s       1.95m     0.03h    0.00d
# Longest job:                      413s       6.88m     0.11h    0.00d
# Submission to last job:           475s       7.92m     0.13h    0.01d

    # load the .wig files back on hgwdev:
    ssh hgwdev
    cd /cluster/data/hg17/bed/gc5Base
    hgLoadWiggle -pathPrefix=/gbdb/hg17/wib/gc5Base hg17 gc5Base wigData5/*.wig
    # and symlink the .wib files into /gbdb
    mkdir /gbdb/hg17/wib/gc5Base
    ln -s `pwd`/wigData5/*.wib /gbdb/hg17/wib/gc5Base

    #	And then the zoomed data view
    ssh kki
    cd /cluster/data/hg17/bed/gc5Base
    mkdir wigData5_1K dataLimits5_1K

    cat << '_EOF_' > kkRunZoom.sh
#!/bin/sh
NIB=$1

chr=${NIB/.nib/}
chrom=${chr#chr}

hgGcPercent -chr=${chr} -doGaps -file=stdout -win=5 hg17 \
        /iscratch/i/gs.18/build35/bothMaskedNibs | \
    grep -w GC | \
    awk '{if (($3-$2) >= 5) {printf "%d\t%.1f\n", $2+1, $5/10.0} }' | \
    wigZoom -dataSpan=1000 stdin | wigAsciiToBinary -dataSpan=1000 \
	-chrom=${chr} -wibFile=wigData5_1K/gc5Base_${chrom}_1K \
            -name=${chrom} stdin 2> dataLimits5_1K/${chr}
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x kkRunZoom.sh

    cat << '_EOF_' > gsubZoom
#LOOP
./kkRunZoom.sh $(path1)
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    gensub2 nibList single gsubZoom jobListZoom
    para create jobListZoom
    para try ... check ... etc ...
# Completed: 46 of 46 jobs
# CPU time in finished jobs:       5216s      86.93m     1.45h    0.06d  0.000 y
# IO & Wait Time:                    34s       0.57m     0.01h    0.00d  0.000 y
# Average job time:                 114s       1.90m     0.03h    0.00d
# Longest job:                      415s       6.92m     0.12h    0.00d
# Submission to last job:           469s       7.82m     0.13h    0.01d

    #	Then load these .wig files into the same database as above
    ssh hgwdev
    hgLoadWiggle -pathPrefix=/gbdb/hg17/wib/gc5Base \
	-oldTable hg17 gc5Base wigData5_1K/*.wig
    # and symlink these .wib files into /gbdb
    mkdir -p /gbdb/hg17/wib/gc5Base
    ln -s `pwd`/wigData5_1K/*.wib /gbdb/hg17/wib/gc5Base

# AUTO UPDATE GENBANK MRNA RUN  (DONE - 2004-06-08 - Hiram)
    ssh eieio
    cd /cluster/data/genbank
    # This is a new organism, edit the etc/genbank.conf file and add:
	# hg17
	hg17.genome = /scratch/hg/gs.18/build35/bothMaskedNibs/chr*.nib
	hg17.lift = /cluster/store5/gs.18/build35/jkStuff/liftAll.lft
	hg17.genbank.est.xeno.load = yes
	hg17.mgcTables.default = full
	hg17.mgcTables.mgc = all
	hg17.downloadDir = hg17

    #	Do the refseq's first, they are the quick ones
    ssh eieio
    cd /cluster/data/genbank
    nice bin/gbAlignStep -srcDb=refseq -type=mrna -verbose=1 -initial hg17
    #	 logFile: var/build/logs/2004.05.25-13:41:07.hg17.initalign.log
    #	checking that log, or watching the batch on kk, you can find
    #	where the batch is running and after it is done get the time:
    cd /cluster/store6/genbank/work/initial.hg17/align
    para time > time
    cat time
# Completed: 9500 of 9500 jobs
# CPU time in finished jobs:      62241s    1037.35m    17.29h    0.72d  0.002 y
# IO & Wait Time:                 33719s     561.98m     9.37h    0.39d  0.001 y
# Average job time:                  10s       0.17m     0.00h    0.00d
# Longest job:                     1062s      17.70m     0.29h    0.01d
# Submission to last job:          1063s      17.72m     0.30h    0.01d

    # Load the results from the above
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 -drop -initialLoad hg17

#	To get the genbank started, the above results need to be
#	moved out of the way.  These things can be removed if there are
#	no problems to debug
    ssh eieio
    cd /cluster/data/genbank/work
    mv initial.hg17 initial.hg17.refseq.mrna

    cd /cluster/data/genbank
    nice bin/gbAlignStep -srcDb=genbank -type=mrna -verbose=1 -initial hg17
    #	logFile: var/build/logs/2004.06.04-10:47:21.hg17.initalign.log
    #	One job was hung up, after killing it on its node, the batch
    #	finished in a few minutes.
# Completed: 35720 of 35720 jobs
# CPU time in finished jobs:    5161424s   86023.74m  1433.73h   59.74d  0.164 y
# IO & Wait Time:                144149s    2402.48m    40.04h    1.67d  0.005 y
# Average job time:                 149s       2.48m     0.04h    0.00d
# Longest job:                    18306s     305.10m     5.08h    0.21d
# Submission to last job:         35061s     584.35m     9.74h    0.41d

    ssh hgwdev
    cd /cluster/data/genbank
    #	some kind of error happened here, had to remove a lock file to
    #	get this to proceed  (this same thing happened again the second
    #	time around)
    nice bin/gbDbLoadStep -verbose=1 -drop -initialLoad hg17

    ssh eieio
    cd /cluster/data/genbank/work
    mv initial.hg17 initial.hg17.genbank.mrna
    cd /cluster/data/genbank
    nice bin/gbAlignStep -srcDb=genbank -type=est -verbose=1 -initial hg17
# Completed: 189240 of 189240 jobs
# CPU time in finished jobs:   97172120s 1619535.33m 26992.26h 1124.68d  3.081 y
# IO & Wait Time:               1507789s   25129.82m   418.83h   17.45d  0.048 y
# Average job time:                 521s       8.69m     0.14h    0.01d
# Longest job:                    33165s     552.75m     9.21h    0.38d
# Submission to last job:        126988s    2116.47m    35.27h    1.47d

    ssh hgwdev
    cd /cluster/data/genbank
    time nice bin/gbDbLoadStep -verbose=1 -drop -initialLoad hg17
    #	real    440m42.750s
    #	user    69m7.810s
    #	sys     23m18.640s
    #	This is ~7.5 hours

    #	If the above is all OK, ask Mark to put this assembly on
    #	the daily updates.

# CPGISLANDS (DONE - 2004-05-25 - Hiram)
    #	Re-DONE with new randoms - 2004-06-04 - Hiram
    ssh hgwdev
    mkdir -p /cluster/data/hg17/bed/cpgIsland
    cd /cluster/data/hg17/bed/cpgIsland

    # Build software from Asif Chinwalla (achinwal@watson.wustl.edu)
    cvs co hg3rdParty/cpgIslands
    cd hg3rdParty/cpgIslands
    make
    #	gcc readseq.c cpg_lh.c -o cpglh.exe
    mv cpglh.exe /cluster/data/hg17/bed/cpgIsland/
    
    # cpglh.exe requires hard-masked (N) .fa's.  
    # There may be warnings about "bad character" for IUPAC ambiguous 
    # characters like R, S, etc.  Ignore the warnings.  
    ssh eieio
    cd /cluster/data/hg17/bed/cpgIsland
    foreach f (../../*/chr*.fa.masked)
      set fout=$f:t:r:r.cpg
      echo running cpglh on $f to $fout
      ./cpglh.exe $f > $fout
    end
    #	the warnings:
    # Bad char 0x52 = 'R' at line 2046, base 102229, sequence chr16_random
    # Bad char 0x4d = 'M' at line 1216113, base 60805573, sequence chr3
    # Bad char 0x52 = 'R' at line 1216118, base 60805801, sequence chr3
    # Bad char 0x52 = 'R' at line 1216118, base 60805801, sequence chr3
    #	real    21m47.823s
    #	user    18m30.810s
    #	sys     1m13.420s

    # Transform cpglh output to bed +
    cat << '_EOF_' > filter.awk
/* Input columns: */
/* chrom, start, end, len, CpG: cpgNum, perGc, cpg:gpc, observed:expected */
/* chr1\t 41776\t 42129\t 259\t CpG: 34\t 65.8\t 0.92\t 0.94 */
/* Output columns: */
/* chrom, start, end, name, length, cpgNum, gcNum, perCpg, perGc, obsExp */
/* chr1\t41775\t42129\tCpG: 34\t354\t34\t233\t19.2\t65.8\to0.94 */
{
$2 = $2 - 1;
width = $3 - $2;
printf("%s\t%d\t%s\t%s %s\t%s\t%s\t%0.0f\t%0.1f\t%s\t%s\n",
       $1, $2, $3, $5,$6, width,
       $6, width*$7*0.01, 100.0*2*$6/width, $7, $9);
}
'_EOF_'
    # << this line makes emacs coloring happy
    awk -f filter.awk chr*.cpg > cpgIsland.bed

    ssh hgwdev
    cd /cluster/data/hg17/bed/cpgIsland
    hgLoadBed hg17 cpgIslandExt -tab -noBin \
      -sqlTable=$HOME/kent/src/hg/lib/cpgIslandExt.sql cpgIsland.bed
    #	Reading cpgIsland.bed
    #	Loaded 27801 elements of size 10
    #	Sorted
    #	Saving bed.tab
    #	Loading hg17

# MAKE HGCENTRALTEST BLATSERVERS ENTRY (DONE - 2004-05-25 - Heather)
    ssh hgwdev
    hgsql -e 'INSERT INTO blatServers (db, host, port, isTrans) \
	VALUES("hg17", "blat12", "17778", "1"); \
	INSERT INTo blatServers (db, host, port, isTrans) \
	VALUES("hg17", "blat12", "17779", "0");' \
	-h genome-testdb hgcentraltest

# PREPARE CLUSTER FOR BLASTZ RUNS (DONE - 2004-05-26 - Hiram)
    #	Re-DONE with new randoms - 2004-06-03 - Hiram

    ssh eieio
    mkdir /cluster/bluearc/scratch/hg/gs.18/build35/rmsk.spec
    cd /cluster/bluearc/scratch/hg/gs.18/build35/rmsk.spec
    ln -s ../rmsk/*.out .
    #	This takes 40 minutes run as a script, to hurry it up it has
    #	been converted to a mini cluster run
    cat << '_EOF_' > runArian.sh
#!/bin/sh
for FN in *.out
do
    echo /cluster/bluearc/RepeatMasker030619/DateRepsinRMoutput.pl \
	${FN} -query human -comp rat -comp mouse
done
'_EOF_'
    chmod +x runArian.sh
    ssh kki
    cd /cluster/bluearc/scratch/hg/gs.18/build35/rmsk.spec
    ./runArian.sh > jobList
    para create jobList
    para try, ... check ... push ... etc ...
# Completed: 46 of 46 jobs
# CPU time in finished jobs:        668s      11.14m     0.19h    0.01d  0.000 y
# IO & Wait Time:                   514s       8.56m     0.14h    0.01d  0.000 y
# Average job time:                  26s       0.43m     0.01h    0.00d
# Longest job:                       86s       1.43m     0.02h    0.00d
# Submission to last job:           108s       1.80m     0.03h    0.00d

    #	Now extract each one, 1 = Rat, 2 = Mouse
    ssh eieio
    cd /cluster/bluearc/scratch/hg/gs.18/build35

    mkdir linSpecRep.notInRat linSpecRep.notInMouse
    foreach f (rmsk.spec/*.out_rat_mus)
        set base = $f:t:r:r
        echo "$f -> $base.out.spec"
        /cluster/bin/scripts/extractLinSpecReps 1 $f > \
                        linSpecRep.notInRat/$base.out.spec
        /cluster/bin/scripts/extractLinSpecReps 2 $f > \
                        linSpecRep.notInMouse/$base.out.spec
    end
    #	There is actually no difference at all between these two results.
    #	copy to iscratch
    ssh kkr1u00
    cd /iscratch/i/gs.18/build35
    rsync -arlv /cluster/bluearc/scratch/hg/gs.18/build35/ .
    /cluster/bin/iSync
    # request rsync of /cluster/bluearc/scratch to the KiloKluster /scratch

# COPY DATA TO GOLDEN PATH LOCATIONS (DONE - 2004-06-04 - Hiram)
    ssh hgwdev
    mkdir -p /usr/local/apache/htdocs/goldenPath/hg17/chromosomes
    cd /cluster/data/hg17
    #	Beware, this backgrounding of the gzips can be hard on hgwdev.
    #	You could wait until after the copy then run one gzip to do them all
    foreach chrom ( `cat chrom.lst` )
	cp -p ${chrom}/*.fa /usr/local/apache/htdocs/goldenPath/hg17/chromosomes
	gzip \
	/usr/local/apache/htdocs/goldenPath/hg17/chromosomes/chr${chrom}*.fa &
	echo "done ${chrom}"
    end
    cd /usr/local/apache/htdocs/goldenPath/hg17/chromosomes
    gzip *.fa


# FOSMID END PAIRS TRACK (WORKING - 2004-06-09 kate)

    # Use latest fosmid ends data prepared by Terry Furey.
    # He says there is no on-going work on fosmid ends, so this
    # should suffice indefinitely ?  Move/link this stuff into
    # central data area.
    ssh eieio
    cd /cluster/data/ncbi
    mkdir -p fosends/human
    ln -s /cluster/store1/fosends.3 fosends/human
    cd fosends/human/fosends.3
    faSize fosEnds.fa
       # 579735181 bases (369769 N's 579365412 real) in 1087670 sequences 
       # 580M bases in 1M sequences
    # create link in /gbdb/ncbi/fosends/human ?

    # use pre-split fosend files, and associated list for cluster run
    # Sequences are in /cluster/bluearc/hg/fosEnds
    cp /cluster/bluearc/booch/fosends/fosEnds.lst /cluster/bluearc/hg/fosEnds
    
    # run on rack9 since kilokluster is busy
    ssh kk9
    cd /cluster/data/hg17
    mkdir -p bed/fosends
    cd bed/fosends
    mkdir -p run
    cd run
    ls -1S /scratch/hg/gs.18/build35/maskedContigs/*.fa \
                > contigs.lst
    cp /cluster/bluearc/hg/fosEnds/fosEnds.lst fosEnds.lst
        # 380 contigs vs 97 fosEnd files -> 40K jobs
    # send output to kksilo, as it can better handle the NFS load
    mkdir -p /cluster/store7/kate/hg17/fosends/out
    ln -s /cluster/store7/kate/hg17/fosends/out ../out
cat > gsub << 'EOF'
#LOOP
/cluster/bin/i386/blat $(path1) $(path2) -ooc=/scratch/hg/h/11.ooc {check out line+ /cluster/data/hg17/bed/fosends/out/$(root2)/$(root1).$(root2).psl}
#ENDLOOP
'EOF'
    gensub2 contigs.lst fosEnds.lst gsub jobList
    foreach f (`cat fosEnds.lst`)
        set d = $f:r:t
        echo $d
        mkdir -p /cluster/data/hg17/bed/fosends/out/$d
    end

    para create jobList
        # 36860 jobs
    para try
    para check
    para push
# CPU time in finished jobs:    1655943s   27599.05m   459.98h   19.17d  0.053 y
# IO & Wait Time:                101145s    1685.75m    28.10h    1.17d  0.003 y
# Average job time:                  48s       0.79m     0.01h    0.00d
# Longest job:                     1294s      21.57m     0.36h    0.01d
# Submission to last job:         19269s     321.15m     5.35h    0.22d

    # sort, filter, and lift alignments
    ssh eieio
    cd /cluster/data/hg17/bed/fosends
    pslSort dirs raw.psl temp out/fosEnds*
    pslReps  -nearTop=0.01 -minCover=0.70 -minAli=0.85 -noIntrons raw.psl \
                        fosEnds.psl /dev/null
    rm -r temp
    rm raw.psl
    liftUp

    /cluster/home/booch/bin/i386/pslPairs -tInsert=5000 -minId=0.94 -min=30000 -max=50000 -slop -short -long -orphan -mismatch -verbose fosEnds.psl.lifted $(PAIRS) all_fosends fosEnds
    cat $(HEADER) fosEnds.pairs | row score ge 300 | sorttbl chr start | headchg -del > fosEndPairs.bed


# BAC END PAIRS TRACK (DONE - 2004-06-09 kate)

    # Use latest BAC ends data from NCBI
    # Checked  ftp.ncbi.nih.gov/genomes/BACENDS/homo_sapiens,
    #  and files were unchanged from Terry's last download
    #  (to /cluster/store1/bacends.4)
    # Link this stuff into central data area.
    ssh eieio
    cd /cluster/data/ncbi
    mkdir -p bacends/human
    ln -s /cluster/store1/bacends.4 bacends/human
    cd bacends/human/bacends.4
    faSize BACends.fa
        # 400230494 bases (2743171 N's 397487323 real) in 832614 sequences
        # 400M bases in 800K sequences

    # use pre-split bacends files, and associated list for cluster run
    ssh kk
    cd /cluster/data/hg17
    mkdir -p bed/bacends
    cd bed/bacends
    mkdir run
    cd run
    ls -1S /scratch/hg/gs.18/build35/maskedContigs/*.fa > contigs.lst
    ls -1S /cluster/bluearc/hg/bacEnds/hs/*.fa > bacends.lst
        # 380 contigs vs 98 bacends files -> 40K jobs

    # send output to kksilo, as it can better handle the NFS load
    # (these are quick jobs)
    mkdir -p /cluster/store7/kate/hg17/bacends/out
    ln -s /cluster/store7/kate/hg17/bacends/out ../out
cat > gsub << 'EOF'
#LOOP
/cluster/bin/i386/blat $(path1) $(path2) -ooc=/scratch/hg/h/11.ooc {check out line+ /cluster/data/hg17/bed/bacends/out/$(root2)/$(root1).$(root2).psl}
#ENDLOOP
'EOF'
    gensub2 contigs.lst bacends.lst gsub jobList
    foreach f (`cat bacends.lst`)
        set d = $f:r:t
        echo $d
        mkdir -p /cluster/data/hg17/bed/bacends/out/$d
    end

    para create jobList
        # 37240 jobs written to batch
    para try
    para check
    para push
# CPU time in finished jobs:    1573932s   26232.19m   437.20h   18.22d  0.050 y
# IO & Wait Time:                122751s    2045.86m    34.10h    1.42d  0.004 y
# Average job time:                  46s       0.76m     0.01h    0.00d
# Longest job:                     3312s      55.20m     0.92h    0.04d
# Submission to last job:          7148s     119.13m     1.99h    0.08d

    cd ../out/BACends000
    pslCheck *.psl
#Error: invalid PSL: AZ519021:1-575 NT_004559:1306426-1608347 - NT_004559.BACends000.psl:1101
#AZ519021 query block 3 start 283 < previous block end 575
    # NOTE: inquired with JK regarding these results

    # lift alignments
    ssh eieio
    cd /cluster/data/hg17/bed/bacends
    pslSort dirs raw.psl temp out/BACends*
    # takes hours ?

        # 37240 files in 98 dirs
        # Got 37240 files 193 files per mid file
    pslReps -nearTop=0.02 -minCover=0.60 -minAli=0.85 -noIntrons \
                raw.psl  bacEnds.psl /dev/null
        # Processed 52291246 alignments
    mkdir lifted
    liftUp lifted/bacEnds.lifted.psl \
                /cluster/data/hg17/jkStuff/liftAll.lft warn bacEnds.psl
    pslSort dirs bacEnds.sorted.psl temp lifted
    rmdir temp
    wc -l *.sorted.psl
        # 2497227 bacEnds.sorted.psl

    set ncbiDir = /cluster/data/ncbi/bacends/human/bacends.4
    ~/bin/i386/pslPairs -tInsert=10000 -minId=0.91 -noBin -min=25000 -max=350000 -slopval=10000 -hardMax=500000 -slop -short -long -orphan -mismatch -verbose bacEnds.sorted.psl $ncbiDir/bacEndPairs.txt all_bacends bacEnds

    # create header required by "rdb" tools
    # TODO: replace w/ awk & sort
    echo 'chr\tstart\tend\tclone\tscore\tstrand\tall\tfeatures\tstarts\tsizes' > header
    echo '10\t10N\t10N\t10\t10N\t10\t10\t10N\t10\t10' >> header
    cat header bacEnds.pairs | row score ge 300 | sorttbl chr start | headchg -del > bacEndPairs.bed
    cat header  bacEnds.slop bacEnds.short bacEnds.long bacEnds.mismatch bacEnds.orphan \
        | row score ge 300 | sorttbl chr start | headchg -del > bacEndPairsBad.bed

    extractPslLoad -noBin bacEnds.sorted.psl bacEndPairs.bed \
                bacEndPairsBad.bed | \
                        sorttbl tname tstart | headchg -del > bacEnds.load.psl

    # load into database
    ssh hgwdev
    cd /cluster/data/hg17/bed/bacends
    hgLoadBed hg17 bacEndPairs bacEndPairs.bed \
                 -sqlTable=/cluster/home/kate/kent/src/hg/lib/bacEndPairs.sql 
        # Loaded 201380  
    # note - this track isn't pushed to RR, just used for assembly QA
    hgLoadBed hg17 bacEndPairsBad bacEndPairsBad.bed \
                 -sqlTable=/cluster/home/kate/kent/src/hg/lib/bacEndPairsBad.sql
        # Loaded 81773
    #hgLoadPsl hg17 -nobin -table=all_bacends bacEnds.load.psl
    # NOTE: truncates file to 0 if -nobin is used
    hgLoadPsl hg17 -table=all_bacends bacEnds.load.psl
        #load of all_bacends did not go as planned: 441072 record(s), 0 row(s) skipped, 30 warning(s) loading psl.tab
    # load BAC end sequences

    mkdir -p /gbdb/hg17/bacends
    ln -s /cluster/data/ncbi/bacends/human/bacends.4/BACends.fa \
                                /gbdb/hg17/bacends/BACends.fa
    hgLoadSeq hg17 /gbdb/hg17/bacends/BACends.fa
        # 158588 sequences


# PLACE ASSEMBLY CLONES ON CONTIGS AND SEQUENCE (WORKING - 2004-06-04 - Hiram)
    ssh eieio
    mkdir /cluster/data/hg17/bed/contig_overlaps
    cd /cluster/data/hg17/bed/contig_overlaps
    #	find all the clones that were used in the assembly
    sed -e "/^#.*/d" /cluster/data/hg17/ncbi_build35.agp | \
        awk '{if (!match($5,"N")) {print $6}}' | \
        sort -u > placed_in_assembly.list
    wc -l placed_in_assembly.list
    #	26872 placed_in_assembly.list
    #	These may be available from the phases files at:
    #	ftp://ftp.ncbi.nih.gov/genbank/genomes/H_sapiens
    #	Which are easily fetched with wget.  However I took a look
    #	at those and could not find all the clones in them.  There may
    #	be a versioning problem because these phases files are often
    #	updated.
    #	Fetch them from Genbank with the following three PERL scripts:
    #	[hiram@hgwdev /cluster/data/hg17/bed/contig_overlaps] ls -og *.pl
    #	-rwxrwxr-x    1     3047 May 24 18:43 bioPerlFetch.pl
    #	-rwxrwxr-x    1     2370 Jun  4 15:21 fetchGenbank.pl
    #	-rwxrwxr-x    1      700 May 24 21:47 foldEm.pl

    #	Which takes about 4 days ...
    #	Example, 
    cat << '_EOF_' > terrys.list
AC011841.7
AC018692.9
AC018743.27
AC037482.14
AL163540.11
'_EOF_'
    # << this line makes emacs coloring happy
    #	only works on hgwdev
    ssh hgwdev
    cd /cluster/data/hg17/bed/contig_overlaps
    mkdir fasta
    time ./fetchGenbank.pl terrys.list > fetchResult.out 2>&1

    #	There is a bit of behind the scenes hocus pocus going on here.
    #	This is a tedious task of comparing various lists with each
    #	other and making sure everything matches.  Manual fixups are
    #	done for the newly named 6_hla_hap* items, copies of the PAR
    #	business were duplicated so that X and Y both have the same set
    #	of clones for that.  The end result should be a directory hierarchy
    #	here with a directory for each chrom, each random, the 6_hla_hap?
    #	items and each directory contains the clones that belong to that
    #	chromosome.  The leftovers are the unplaced clones which end up
    #	in the directory called: unPlaced.  The instructions here are
    #	merely a guideline of possibilities.  Care should be taken to
    #	make sure all listings are correct and everything gets in the
    #	right place.
    ssh eieio
    #	And then make a list of all clones considered for assembly:
    sed -e "/^#.*/d" /cluster/store5/gs.18/ncbi/sequence.inf | \
	grep for_assembly | awk '{print $1}' | sort -u > sequence.list
    wc -l sequence.list
    #	46733 sequence.list
    #	Verify overlaps are correct:
    comm -12 placed_in_assembly.list sequence.list > inBoth
    comm -23 placed_in_assembly.list sequence.list > inAssemblyNotSequence
    comm -13 placed_in_assembly.list sequence.list > inSequenceNotAssembly
    wc in*
    #	    1       1      12 inAssemblyNotSequence
    #	26871   26871  301709 inBoth
    #	19862   19862  219050 inSequenceNotAssembly
    #	46734   46734  520771 total
    #	This stray one is from Terry's five additions in the final fixup
    #	phase with Greg:
    cat inAssemblyNotSequence
    #	AC018743.27
    #	Terry added: AC011841.7 AC018692.9 AC018743.27 AC037482.14 AL163540.11
    #
    #	Generate a listing that relates clones to their contigs
    sed -e "/^#.*/d" /cluster/store5/gs.18/build35/ncbi_build35.agp | \
	./contigAcc.pl > disburseEm.list
    #
    #	Using that list, sort the downloaded clones into their
    #	respective chrom directories:
    ./disburse.sh

    #	Check the number of sequences obtained:
    find ./? ./?? ./*_random ./6_hla* -type f | wc -l
    #	26872
    #	So, why is this number one more than the inBoth list ?
    #	Because, the official NCBI sequence.inf file is missing one of
    #	the clones that Terry added: AC018743.27
    #	And it shows up in our check list above as inAssemblyNotSequence
    #	It isn't exactly missing, it just isn't marked "for_assembly"

    #	OK, with everything in place, we are ready to try and find
    #	all these items in the assembly.  To run a Kluster job on one of
    #	the chroms, matching the items that are supposed to be included
    #	in that chrom.  We need to get things set up on the Iservers,
    #	psLayout is heavy into disk I/O and it brings everything down if
    #	allowed to work on any NFS filesystems for input.

    #	It appears that psLayout wants an ooc file of tile size 10
    #	I tried making one for the whole assembly but it seemed to
    #	include too much for some contigs and it caused a lot of
    #	alignments to be missed.  Thus, create an ooc file for each
    #	contig

    ssh eieio
    mkdir /cluster/bluearc/scratch/hg/gs.18/build35/contigOoc10
    cd /cluster/bluearc/scratch/hg/gs.18/build35/contigOoc10
    ls ../maskedContigs | sed -e "s/.fa//" | while read CONTIG
    do
	blat -repMatch=256 -makeOoc=${CONTIG}.10.ooc -tileSize=10 \
	    ../maskedContigs/${CONTIG}.fa \
	    ../maskedContigs/${CONTIG}.fa /dev/null
	echo "done: ${CONTIG}"
    done

    #	Copy that result to the Iservers:
    ssh kkr1u00
    mkdir /iscratch/i/gs.18/build35/contigOoc10
    cd /iscratch/i/gs.18/build35/contigOoc10
    rsync -arlv /cluster/bluearc/scratch/hg/gs.18/build35/contigOoc10/ .
    #	And, copy the clone sequences:
    mkdir /iscratch/i/gs.18/build35/clones
    cd /cluster/store5/gs.18/build35/bed/contig_overlaps
    for D in ? ?? *_random 6_hla_hap?
    do
	rsync -arlv `pwd`/${D} /iscratch/i/gs.18/build35/clones
    done
    
    /cluster/bin/iSync

    ssh kk
    cd /cluster/data/hg17/bed/contig_overlaps
    mkdir psl
    cat << '_EOF_' > runPsLayout.sh
#!/bin/sh
#       kkiPsLayout.sh <chrom> <clone> <contig>
#       where <chrom> is the chrom this contig is on
#       <clone> is one of the .fa.gz files in
#               /cluster/data/hg17/bed/contig_overlaps/*/<clone>.fa.gz
#               without the .fa.gz extension
#               This stuff has been mirrored to:
#               /iscratch/i/gs.18/clones/*/<clone>.fa.gz
#       <contig> is one of the contigs found in:
#               /cluster/store5/gs.18/build35/<chrom>/<contig>/<contig>.fa
#
CHROM=$1
CLONE=$2
CONTIG=$3
TARGET=/iscratch/i/gs.18/build35/maskedContigs/${CONTIG}.fa
FAZ=/iscratch/i/gs.18/build35/clones/${CHROM}/${CLONE}.fa.gz
OOC=/iscratch/i/gs.18/build35/contigOoc10/${CONTIG}.10.ooc
mkdir -p psl/${CONTIG}
if [ ! -s ${FAZ} ]; then
        echo "Can not find: ${FAZ}"
        exit 255
fi
if [ ! -s ${TARGET} ]; then
        echo "Can not find: ${TARGET}"
        exit 255
fi
if [ ! -s ${OOC} ]; then
        echo "Can not find: ${OOC}"
        exit 255
fi
zcat ${FAZ} > /tmp/${CLONE}.fa
$HOME/bin/i386/psLayout ${TARGET} \
        /tmp/${CLONE}.fa genomic ${OOC} psl/${CONTIG}/${CLONE}.psl
RET=$?
rm -f /tmp/${CLONE}.fa
exit ${RET}
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x runPsLayout.sh

    #	make up a listing of chrom, clone, contig from:
    grep -v "^#" disburseEm.list | sed -e "s/.fa.gz//" > chr.clone.contig.list
    wc -l chr.clone.contig.list
    #	26872 chr.clone.contig.list
    awk '{
printf "./runPsLayout.sh %s %s %s {check out line+ psl/%s/%s.psl}\n",
        $1, $2, $3, $3, $2
}' chr.clone.contig.list > jobList
    # << this line makes emacs coloring happy
    #	To do a quick test, run just chr22:
    grep -v "^22" chr.clone.contig.list | awk '{
printf "./runPsLayout.sh %s %s %s {check out line+ psl/%s/%s.psl}\n",
        $1, $2, $3, $3, $2
}' > jobList
    para create jobList
    para try ... check ... etc ...
    #	One run on chr22 took:
# Completed: 561 of 561 jobs
# CPU time in finished jobs:     927068s   15451.14m   257.52h   10.73d  0.029 y
# IO & Wait Time:                  6295s     104.91m     1.75h    0.07d  0.000 y
# Average job time:                1664s      27.73m     0.46h    0.02d
# Longest job:                    69745s    1162.42m    19.37h    0.81d
# Submission to last job:         69780s    1163.00m    19.38h    0.81d


    #	put the results together, filter, lift and load:
    cd /cluster/data/hg17/bed/contig_overlaps/psl
    pslSort dirs raw.psl tmp N*
    pslReps -singleHit raw.psl repsSingle.psl /dev/null
    liftUp chr22.psl /cluster/data/hg17/jkStuff/liftAll.lft \
	warn repsSingle.psl
    hgLoadPsl -table=cloneTest hg17 chr22.psl

    #	There are a number of clones listed in the sequence.inf file
    #	as status W with names beginning AACC AADB AADC AADD
    #	These are the Whole shotgun assemblies for the Celera genome.
    #	A few of them were used in the assembly of the NCBI genome, namely:
./11/AADB01066164.1.fa.gz
./11/AADC01095577.1.fa.gz
./11/AADD01116830.1.fa.gz
./11/AADD01118406.1.fa.gz
./11/AADD01116787.1.fa.gz
./11/AADD01112371.1.fa.gz
./11/AADD01116788.1.fa.gz
./11/AADD01115518.1.fa.gz
./11/AADD01118410.1.fa.gz
./11/AADD01117999.1.fa.gz
./21/AADD01172789.1.fa.gz
./21/AADD01172788.1.fa.gz
./21/AADD01209098.1.fa.gz
./21/AADD01172902.1.fa.gz
    #	And these have been distributed properly in their corresponding
    #	chromosome.  The rest of them, 26, all with names starting AACC are in
    #	the directory here: celeraOnly

    #	To run the unPlaced alignments.
    #	Prepare scratch and iscratch
    ssh eieio
    mkdir /cluster/bluearc/scratch/hg/gs.18/build35/clones/unPlaced
    rsync -arlv /cluster/data/hg17/bed/contig_overlaps/unPlaced/ \
	/cluster/bluearc/scratch/hg/gs.18/build35/clones/unPlaced
    #	request scratch sync to cluster admins

    ssh kkr1u00
    mkdir /iscratch/i/gs.18/build35/clones/unPlaced
    rsync -arlv /cluster/data/hg17/bed/contig_overlaps/unPlaced/ \
	/iscratch/i/gs.18/build35/clones/unPlaced
    /cluster/bin/iSync

    ssh hgwdev
    cd /cluster/data/hg17/bed/contig_overlaps/unPlacedBatch
    #	There are too many to try them all, obtain guildelines from hg16
    #	of clone to contig mapping:
    hgsql -N -e "select name,chrom from clonePos;" hg16 > hg16.clone.chrom
    hgsql -N -e "select contig,chrom from ctgPos;" hg16 > hg16.contig.chrom

    ssh kk
    mkdir /cluster/data/hg17/bed/contig_overlaps/unPlacedBatch
    cd /cluster/data/hg17/bed/contig_overlaps/unPlacedBatch
    ls ../unPlaced | sed -e "s/.fa.gz//" > unPlaced.clone.list
    wc -l unPlaced.clone.list
    #	19836 unPlaced.clone.list
    ls -1S /scratch/hg/gs.18/build35/maskedContigs > contig.list
    wc -l contig.list
    #	380 contig.list

    cat << '_EOF_' > runPsLayout.sh
#!/bin/sh
#       kkiPsLayout.sh <clone> <contig>
#       <clone> is one of the .fa.gz files in
#               /scratch/hg/gs.18/build35/clones/unPlaced
#               without the .fa.gz extension
#       <contig> is one of the contigs found in:
#               /iscratch/i/gs.18/build35/maskedContigs
#
CLONE=$1
CONTIG=$2
TARGET=/iscratch/i/gs.18/build35/maskedContigs/${CONTIG}.fa
FAZ=/scratch/hg/gs.18/build35/clones/unPlaced/${CLONE}.fa.gz
OOC=/iscratch/i/gs.18/build35/contigOoc10/${CONTIG}.10.ooc
mkdir -p psl/${CONTIG}
if [ ! -s ${FAZ} ]; then
        echo "Can not find: ${FAZ}"
        exit 255
fi
if [ ! -s ${TARGET} ]; then
        echo "Can not find: ${TARGET}"
        exit 255
fi
if [ ! -s ${OOC} ]; then
        echo "Can not find: ${OOC}"
        exit 255
fi
zcat ${FAZ} > /tmp/${CLONE}.fa
$HOME/bin/i386/psLayout ${TARGET} \
        /tmp/${CLONE}.fa genomic ${OOC} psl/${CONTIG}/${CLONE}.psl
RET=$?
rm -f /tmp/${CLONE}.fa
exit ${RET}
'_EOF_'
    # << this line makes emacs coloring happy
    chmod +x runPsLayout.sh

    cat << '_EOF_' > gsub
#LOOP
./runPsLayout.sh $(path1) $(path2) {check out line+ psl/$(path2)/$(path1).psl}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

-
    gensub2 unPlaced.clone.list contig.list gsub jobList


# BUILD KNOWN GENES TABLES (DONE 6/8/04 Fan)

  Build sp040515 and proteins040515 DBs first.
  
  hgsql hg17 -e "create database kgHg17"
  
  cd /cluster/store6/kgDB/bed
  mkdir kgHg17
  cd /cluster/store6/kgDB/bed/kgHg17

  ~/src/hg/protein/KGprocess.sh kgHg17 hg17 040515
  
  The script was run successfully with the last message:

  	Tue Jun  8 15:36:52 PDT 2004 DONE =========================

  After initial inspection of tables in kgHg17, do the following
  from mySql prompt:

  alter table kgHg17.cgapAlias rename as hg17.cgapAlias;
  alter table kgHg17.cgapBiocDesc rename as hg17.cgapBiocDesc;
  alter table kgHg17.cgapBiocPathway rename as hg17.cgapBiocPathway;
  alter table kgHg17.dupSpMrna rename as hg17.dupSpMrna;
  alter table kgHg17.keggMapDesc rename as hg17.keggMapDesc;
  alter table kgHg17.keggPathway rename as hg17.keggPathway;
  alter table kgHg17.kgAlias rename as hg17.kgAlias;
  alter table kgHg17.kgProtAlias rename as hg17.kgProtAlias;
  alter table kgHg17.kgXref rename as hg17.kgXref;
  alter table kgHg17.knownGene rename as hg17.knownGene;
  alter table kgHg17.knownGeneLink rename as hg17.knownGeneLink;
  alter table kgHg17.knownGeneMrna rename as hg17.knownGeneMrna;
  alter table kgHg17.knownGenePep rename as hg17.knownGenePep;
  alter table kgHg17.mrnaRefseq rename as hg17.mrnaRefseq;
  alter table kgHg17.spMrna rename as hg17.spMrna;

  hg17.knownGene has 43,401 entries and hg16.knownGene has 43,232 entries.
  and running featireBits shows:
  
   	featureBits hg17 knownGene
   	63983072 bases of 2866216770 (2.232%) in intersection
   
   	featureBits hg16 knownGene
   	63781799 bases of 2865248791 (2.226%) in intersection
  
  Connect to genome-testdb and use hgcentraltest DB.
  Add a new entry in gdbPdb table:
 
        insert into gdbPdb values('hg17', 'proteins040515');


# CREATE LINEAGE-SPECIFIC REPEATS FOR BLASTZ WITH ZEBRAFISH
# (DONE, 2004-06-08, hartera)
    # Treat all repeats as lineage-specific
    mkdir /iscratch/i/gs.18/build35/linSpecRep.notInZebrafish
    foreach f (/iscratch/i/gs.18/build35/rmsk/chr*.fa.out)
 cp -p $f /iscratch/i/gs.18/build35/linSpecRep.notInZebrafish/$f:t:r:r.out.spec
    end
    iSync
  

# PREP FOR LIFTOVER CHAINS TO THIS ASSEMBLY (2004-06-10 kate)

    # split into 3K chunks
    ssh eieio
    set tempDir = /cluster/data/hg17/bed/liftOver/liftSplit
    mkdir -p $tempDir
    cd $tempDir
cat > split.csh << 'EOF'
    set split = /iscratch/i/hg17/liftOver/split
    mkdir -p $split
    set tempDir = /cluster/data/hg17/bed/liftOver/liftSplit
    foreach i (`cat /cluster/data/hg17/chrom.lst`)
        echo chr$i
        faSplit -lift=$tempDir/chr$i.lft size /cluster/data/hg17/$i/chr$i.fa -oneFile 3000 $split/chr$i
    end
'EOF'
    csh split.csh >&! split.log &
    tail -100f split.log

    ssh kkr1u00
    iSync


# STS MARKERS (2004-06-09 kate)

   # update from NCBI
    ssh eieio
   ln -s /cluster/store5/sts.2004-06 /cluster/data/ncbi
   cd /cluster/data/ncbi/sts.2004-06
    ln -s /cluster/data/ncbi/sts.2004-06 sts.9
    wget ftp://ftp.ncbi.nih.gov/repository/dbSTS/dbSTS.sts
    wget ftp://ftp.ncbi.nih.gov/repository/dbSTS/dbSTS.aliases
    wget ftp://ftp.ncbi.nih.gov/blast/db/sts.tar.gz
    mv sts.tar.gz dbSTS.FASTA.dailydump.Z
    gunzip dbSTS.FASTA.dailydump.Z

    # incremental update from previous build
    # NOTE: could mysql dump this, unless hand-updated (like hg16)
    # First - copy from Terry's dir
    ssh eieio
    ln -s /cluster/store1/sts.8 /cluster/data/ncbi
    cd /cluster/data/ncbi/sts.9
    cp ~booch/tracks/update/stsInfo2.bed .
    # reuse hg16's fasta file
    cp /cluster/data/ncbi/sts.8/all.STS.fa.new all.STS.fa
    checkStsIds stsInfo2.bed dbSTS.aliases dbSTS.sts \
                        dbSTS.FASTA.dailydump all.STS.fa
    
    # run alignment
    ssh kk
    cd /cluster/data/hg17
    mkdir -p bed/sts
    cd bed/sts
    mkdir run
    cd run
    ls -1S /scratch/hg/hg17/maskedContigs/*.fa > contigs.lst
    ls -1S /cluster/bluearc/sts.9/all.STS.fa > sts.lst

    mkdir -p /cluster/store7/kate/hg17/sts/out
    ln -s /cluster/store7/kate/hg17/sts/out ../out
cat > gsub << 'EOF'
#LOOP
/cluster/bin/i386/blat $(path1) $(path2) -ooc=/scratch/hg/h/11.ooc {check out line+ /cluster/data/hg17/bed/sts/out/(root1).psl}
#ENDLOOP
'EOF'
    gensub2 contigs.lst sts.lst gsub jobList
    foreach f (`cat sts.lst`)
        set d = $f:r:t
        echo $d
        mkdir -p /cluster/data/hg17/bed/bacends/out/$d
    end


# LOAD AFFYRATIO (WORKING - 2004-06-09 - Hiram)
#	Copied from Hg16 doc
    # Set up cluster job to align consenesus/exemplars to hg17
    ssh eieio
    mkdir /cluster/bluearc/hg17/affyGnf
    cp -p /projects/compbio/data/microarray/affyGnf/sequences/HG-U95/HG-U95Av2_all.fa /cluster/bluearc/hg17/affyGnf

    ssh kkr1u00
    mkdir -p /iscratch/i/affyGnf
    cp -p /cluster/bluearc/hg17/affyGnf/* /iscratch/i/affyGnf
    /cluster/bin/iSync

    ssh kki
    mkdir /cluster/data/hg17/bed/affyGnf.2004-06-09
    cd /cluster/data/hg17/bed/affyGnf.2004-06-09
    ls -1 /iscratch/i/affyGnf/* > affy.lst
    ls -1 /iscratch/i/gs.18/build35/maskedContigs/* > allctg.lst
    cat << '_EOF_' > template.sub
#LOOP
/cluster/bin/i386/blat -fine -mask=lower -minIdentity=95 -ooc=/iscratch/i/gs.18/build35/hg17.11.ooc  $(path1) $(path2) {check out line+ psl/$(root1)_$(root2).psl}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    gensub2 allctg.lst affy.lst template.sub jobList
    mkdir psl
    para create jobList
# Completed: 380 of 380 jobs
# CPU time in finished jobs:       2922s      48.70m     0.81h    0.03d  0.000 y
# IO & Wait Time:                  1146s      19.10m     0.32h    0.01d  0.000 y
# Average job time:                  11s       0.18m     0.00h    0.00d
# Longest job:                       80s       1.33m     0.02h    0.00d
# Submission to last job:           333s       5.55m     0.09h    0.00d

XXXX - pause here while going to fishClone mapping

Also do:
# Load AFFYUCLANORM, extended version of affyUcla track. Hopefully
# final freeze of data set.
    cp /projects/compbio/data/microarray/affyUcla/sequences/HG-U133AB_all ./
and
# GNF ATLAS 2  [Done jk 3/29/2004]
    # Align probes from GNF1H chip.

######	 A second attempt at clone alignment###
    #	Split the clones into 3K pieces into about 1000 fa files

    #	Example:
zcat Z99916.1.fa.gz Z99774.1.fa.gz Z99756.7.fa.gz | faSplit size stdin 3000 /tmp/name.fa -lift=/tmp/name.lft -oneFile

    #	Trying this idea in unPlacedBatch
    ssh kk0
    mkdir /cluster/data/hg17/bed/contig_overlaps/unPlacedBatch
    cd /cluster/data/hg17/bed/contig_overlaps/unPlacedBatch
    ls -1S /scratch/hg/gs.18/build35/bothMaskedNibs > nibList
    ls -1S /cluster/data/hg17/bed/contig_overlaps/blatClones > cloneList
cat << '_EOF_' > gsub
#LOOP
/cluster/bin/i386/blat -fastMap -ooc=/scratch/hg/h/11.ooc -q=dna -t=dna {check in exists /scratch/hg/gs.18/build35/bothMaskedNibs/$(path1)} {check in exists+ /cluster/data/hg17/bed/contig_overlaps/blatClones/$(path2)} {check out line+ psl/$(root1)/$(root2).psl}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    mkdir psl
    cat nibList | sed -e "s/.nib//" | while read D
do
mkdir psl/$D
done

    gensub2 nibList cloneList gsub jobList
    para create jobList

# MAKE LINEAGE-SPECIFIC REPEATS FOR CHICKEN & FUGU (DONE 2004-06-10 kate)
    # In an email 2/13/04 to Angie, Arian said we could treat all 
    # human repeats as 
    # lineage-specific for human-chicken blastz.  
    # and Angie did the same for fugu.
    # Scripts expect *.out.spec filenames.
    ssh kkr1u00
    cd /cluster/data/hg17
    mkdir /iscratch/i/hg17/linSpecRep.chicken
    foreach f (/iscratch/i/hg17/rmsk/chr*.fa.out)
      cp -p $f /iscratch/i/hg17/linSpecRep.chicken/$f:t:r:r.out.spec
    end
    ln -s /iscratch/i/hg17/linSpecRep.chicken \
          /iscratch/i/hg17/linSpecRep.fugu
    iSync


# BLASTZ FUGU (FR1) (WORKING 2004-06-10 kate)
    ssh kk
    mkdir -p /cluster/data/hg17/bed/blastz.fr1.2004-06-10
    ln -s /cluster/data/hg17/bed/blastz.fr1.2004-06-10 \
            /cluster/data/hg17/bed/blastz.fr1
    cd /cluster/data/hg17/bed/blastz.fr1
    # Set L=6000 (more relaxed than chicken) and abridge repeats.
    # Treat all repeats as lineage-specific (reuse linSpecRep.Chicken).
    cat << '_EOF_' > DEF
# human vs. fugu
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin

ALIGN=blastz-run
BLASTZ=blastz

# Reuse parameters from human-chicken.
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q
BLASTZ_ABRIDGE_REPEATS=1

# TARGET: Human
SEQ1_DIR=/iscratch/i/hg17/bothMaskedNibs
SEQ1_RMSK=
SEQ1_FLAG=
SEQ1_SMSK=/iscratch/i/hg17/linSpecRep.fugu
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY: Fugu
SEQ2_DIR=/iscratch/i/fr1/nib
SEQ2_RMSK=
SEQ2_FLAG=
SEQ2_SMSK=/iscratch/i/fr1/linSpecRep
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/cluster/data/hg17/bed/blastz.fr1

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len
'_EOF_'
    # << this line keeps emacs coloring happy

    bash # if a csh/tcsh user
    source DEF
    mkdir $RAW run.0
    /cluster/home/angie/hummus/make-joblist $DEF > $BASE/run.0/j
    # GOT HERE
    sh ./xdir.sh
    cd run.0
    sed -e 's@^blastz-run@/cluster/bin/penn/blastz-run@' j > jobList
    para create jobList
        # 11935 jobs
    para try
    para check
    para push
# Completed: 11935 of 11935 jobs
# CPU time in finished jobs:    4673316s   77888.60m  1298.14h   54.09d  0.148 y
# IO & Wait Time:                329249s    5487.48m    91.46h    3.81d  0.010 y
# Average job time:                 419s       6.99m     0.12h    0.00d
# Longest job:                      714s      11.90m     0.20h    0.01d
# Submission to last job:          5575s      92.92m     1.55h    0.06d

    # second cluster run: lift raw alignments -> lav dir
    ssh kki
    cd /cluster/data/hg17/bed/blastz.fr1
    bash # if a csh/tcsh user
    source DEF
    mkdir run.1 lav
    /cluster/bin/scripts/blastz-make-out2lav $DEF $BASE > $BASE/run.1/jobList
    cd run.1
    wc -l jobList
    para create jobList
        # 341 jobs
    para try
    para check 
    para push
# CPU time in finished jobs:        315s       5.26m     0.09h    0.00d  0.000 y
# IO & Wait Time:                  4451s      74.18m     1.24h    0.05d  0.000 y
# Average job time:                  14s       0.23m     0.00h    0.00d
# Longest job:                      107s       1.78m     0.03h    0.00d
# Submission to last job:           368s       6.13m     0.10h    0.00d

    # third run: lav -> axt
    ssh kki
    cd /cluster/data/hg17/bed/blastz.fr1
    mkdir axtChrom pslChrom run.2
    cd run.2
    cat << 'EOF' > do.csh
#!/bin/csh -ef
cd $1
set chr = $1:t
cat `ls -1 *.lav | sort -g` \
| lavToAxt stdin \
        /iscratch/i/hg17/bothMaskedNibs /iscratch/i/fr1/nib stdout \
| axtSort stdin ../../axtChrom/$chr.axt 
axtToPsl ../../axtChrom/$chr.axt ../../S1.len ../../S2.len \
        ../../pslChrom/$chr.psl
'EOF'
    # << this line keeps emacs coloring happy
    chmod a+x do.csh
    cp /dev/null jobList
    foreach d (../lav/chr*)
      echo "do.csh $d" >> jobList
    end
    para create jobList
        # 41 jobs
    para try
    para check
    para push


<<<<<<< makeHg17.doc
# CHAIN FUGU BLASTZ (2004-06-11 kate)
    # Run axtChain on little cluster
    ssh kki
    cd /cluster/data/hg17/bed/blastz.fr1
    mkdir -p axtChain/run1
    cd axtChain/run1
    mkdir out chain
    ls -1S /cluster/data/hg17/bed/blastz.fr1/axtChrom/*.axt \
      > input.lst
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out line+ out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    # Reuse gap penalties from chicken run.
    cat << '_EOF_' > temp.gap
tablesize	11
smallSize	111
position	1	2	3	11	111	2111	12111	32111	72111	152111	252111
qGap	325	360	400	450	600	1100	3600	7600	15600	31600	56600
tGap	325	360	400	450	600	1100	3600	7600	15600	31600	56600
bothGap	625	660	700	750	900	1400	4000	8000	16000	32000	57000
'_EOF_'
    # << this line makes emacs coloring happy
    sed 's/  */\t/g' temp.gap > ../../fuguHumanTuned.gap
    rm -f temp.gap

    cat << '_EOF_' > doChain
#!/bin/csh
axtChain -scoreScheme=/cluster/data/blastz/HoxD55.q \
                      -linearGap=../../fuguHumanTuned.gap \
                      -minScore=5000 $1 \
    /iscratch/i/hg17/bothMaskedNibs \
    /iscratch/i/fr1/nib $2 > $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doChain
    gensub2 input.lst single gsub jobList
    para create jobList
        # 46 jobs
    para try
    para check
    para push
        # 1 crashed job -- chr6_hla_hap1.chain is empty
# CPU time in finished jobs:        610s      10.16m     0.17h    0.01d  0.000 y
# IO & Wait Time:                  1644s      27.40m     0.46h    0.02d  0.000 y
# Average job time:                  50s       0.83m     0.01h    0.00d
# Longest job:                      233s       3.88m     0.06h    0.00d
# Submission to last job:           339s       5.65m     0.09h    0.00d

    # now on the cluster server, sort chains
    ssh eieio
    cd /cluster/data/hg17/bed/blastz.fr1/axtChain
    chainMergeSort run1/chain/*.chain > all.chain
    chainSplit chain all.chain
    rm run1/chain/*.chain

    # Load chains into database
    ssh hgwdev
    cd /cluster/data/hg17/bed/blastz.fr1/axtChain/chain
    foreach i (*.chain)
        set c = $i:r
        echo loading $c
        hgLoadChain hg17 ${c}_chainFr1 $i
    end
    featureBits hg16 chainFr1Link
        # 50709290 bases of 2865248791 (1.770%) in intersection


# ANCIENT REPEAT TABLE (2004-06-11 kate)

    # The netClass operations requires an "ancientRepeat" table in one 
    # of the databases.
    # This is a hand curated table obtained from Arian.

    ssh hgwdev
    mkdir -p /cluster/data/hg17/bed/ancientRepeat
    cd /cluster/data/hg17/bed/ancientRepeat
    # mysqldump needs write permission to this directory
    chmod 777 .
    hgsqldump --all --tab=. hg15 ancientRepeat
    chmod 775 .
    hgsql hg17 < ancientRepeat.sql
    echo "LOAD DATA LOCAL INFILE 'ancientRepeat.txt' into table ancientRepeat"\
                | hgsql hg17


# NET FUGU BLASTZ (2004-06-11 kate)
    ssh eieio
    cd /cluster/data/hg17/bed/blastz.fr1/axtChain
    chainPreNet all.chain ../S1.len ../S2.len stdout \
    | chainNet stdin -minSpace=1 ../S1.len ../S2.len stdout /dev/null \
    | netSyntenic stdin noClass.net

    # Add classification info using db tables:
    ssh hgwdev
    cd /cluster/data/hg17/bed/blastz.fr1/axtChain
    netClass noClass.net hg17 fr1 human.net

    # Make a 'syntenic' subset:
    ssh eieio
    cd /cluster/data/hg17/bed/blastz.fr1/axtChain
    rm noClass.net
    # Make a 'syntenic' subset of these with
    netFilter -syn human.net > humanSyn.net

    # Load the nets into database 
    ssh hgwdev
    cd /cluster/data/hg17/bed/blastz.fr1/axtChain
    netFilter -minGap=10 human.net |  hgLoadNet hg17 netFr1 stdin
    #netFilter -minGap=10 humanSyn.net | hgLoadNet hg17 netSyntenyFr1 stdin


# EXTRACT AXT'S AND MAF'S FROM THE NET
    ssh eieio
    cd /cluster/data/hg17/bed/blastz.fr1/axtChain
    netSplit human.net humanNet
    mkdir -p ../axtNet
cat > axtNet.csh << 'EOF'
    foreach f (humanNet/chr*.net)
        set c = $f:t:r
        echo "axtNet on $c"
        netToAxt -maxGap=300 humanNet/$c.net chain/$c.chain /cluster/data/hg17/nib /cluster/data/fr1/nib ../axtNet/$c.axt
    end
'EOF'
    csh axtNet.csh >&! axtNet.log &
    tail -100f axtNet.log
    # 10 minutes or so

    ssh eieio
    cd /cluster/data/hg17/bed/blastz.fr1
    cd axtNet
    mkdir ../mafNet
cat > makeMaf.csh << 'EOF'
    foreach f (chr*.axt)
      set maf = $f:t:r.fr1.maf
      echo translating $f to $maf
      axtToMaf $f \
            /cluster/data/hg17/chrom.sizes /cluster/data/fr1/chrom.sizes \
            ../mafNet/$maf -tPrefix=hg17. -qPrefix=fr1.
    end
'EOF'
    csh makeMaf.csh >&! makeMaf.log &
    tail -100f makeMaf.log

#  BLASTZ RAT RN3 (WORKING - 2004-06-11 - Hiram)

    ssh kk
    mkdir -p /cluster/data/hg17/bed/blastz.rn3.2004-06-11
    cd /cluster/data/hg17/bed
    ln -s  blastz.rn3.2004-06-11 blastz.rn3
    cd blastz.rn3

    cat << '_EOF_' > DEF
# rat vs. human
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/home/angie/schwartzbin:/cluster/home/kent/bin/i386

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
BLASTZ_ABRIDGE_REPEATS=1

# TARGET
# Human
SEQ1_DIR=/iscratch/i/gs.18/build35/bothMaskedNibs
# not used
SEQ1_RMSK=
# not used
SEQ1_FLAG=
SEQ1_SMSK=/iscratch/i/gs.18/build35/linSpecRep.notInRat
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY
# Rat
SEQ2_DIR=/iscratch/i/rn3/bothMaskedNibs
# not currently used
SEQ2_RMSK=
# not currently used
SEQ2_FLAG=
SEQ2_SMSK=/cluster/bluearc/rat/rn3/linSpecRep.notInHuman
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=30000000
SEQ2_LAP=0

BASE=/cluster/store5/gs.18/build35/bed/blastz.rn3

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len
'_EOF_'
    # << this line makes emacs coloring happy

    # prepare first cluster run
    ssh kk
    cd /cluster/data/hg17/bed/blastz.rn3
    source DEF
    # script copied over from /cluster/data/mm4/jkStuff/BlastZ_run0.sh
    #	it is a generic script and works for any assembly
    /cluster/data/hg17/jkStuff/BlastZ_run0.sh
    cd run.0
    para try, check, push, check, ....
Completed: 41943 of 41943 jobs
CPU time in finished jobs:   15330421s  255507.02m  4258.45h  177.44d  0.486 y
IO & Wait Time:                673809s   11230.15m   187.17h    7.80d  0.021 y
Average job time:                 382s       6.36m     0.11h    0.00d
Longest job:                     4651s      77.52m     1.29h    0.05d
Submission to last job:        169197s    2819.95m    47.00h    1.96d

    #	Second cluster run to convert the .out's to .lav's
    #	You do NOT want to run this on the big cluster.  It brings
    #	the file server to its knees.  Run this on the small cluster.
    ssh kki
    cd /cluster/data/hg17/bed/blastz.rn3
    source DEF
    # script copied over from /cluster/data/mm4/jkStuff/BlastZ_run1.sh
    #	fixup machine check, should be kki, not kk
    /cluster/data/hg17/jkStuff/BlastZ_run1.sh
    cd run.1
    para try, check, push, etc ...
# Completed: 341 of 341 jobs
# CPU time in finished jobs:       1894s      31.56m     0.53h    0.02d  0.000 y
# IO & Wait Time:                  6271s     104.52m     1.74h    0.07d  0.000 y
# Average job time:                  24s       0.40m     0.01h    0.00d
# Longest job:                      131s       2.18m     0.04h    0.00d
# Submission to last job:           590s       9.83m     0.16h    0.01d

    #	Third cluster run to convert lav's to axt's
    source DEF
    cd /cluster/data/hg17/bed/blastz.rn3
    #	The copy of this in mm4 was broken, fixed here
    /cluster/data/hg17/jkStuff/BlastZ_run2.sh
    cd run.2
    para try, check, push, etc ...
# Completed: 46 of 46 jobs
# CPU time in finished jobs:        426s       7.09m     0.12h    0.00d  0.000 y
# IO & Wait Time:                  7283s     121.39m     2.02h    0.08d  0.000 y
# Average job time:                 168s       2.79m     0.05h    0.00d
# Longest job:                      642s      10.70m     0.18h    0.01d
# Submission to last job:           642s      10.70m     0.18h    0.01d

    # translate sorted axt files into psl
    ssh eieio
    cd /cluster/data/hg17/bed/blastz.rn3
    mkdir pslChrom
    set tbl = "blastzRn3"
    foreach f (axtChrom/chr*.axt)
      set c=$f:t:r
      echo "Processing chr $c"
      /cluster/bin/i386/axtToPsl $f S1.len S2.len pslChrom/${c}_${tbl}.psl
    end
XXXX - running - 2004-06-13 14:35
    #	That takes about 30 minutes

    # Load database tables
    ssh hgwdev
    cd /cluster/data/hg17/bed/blastz.rn3/pslChrom
    /cluster/bin/i386/hgLoadPsl hg17 chr*_blastzRn3.psl
    # this is a 55 minute job

# CHAIN Rn3 BLASTZ (WORKING - 2004-06-13 - Hiram)

# The axtChain is best run on the small kluster, or the kk9 kluster
    ssh kki
    mkdir -p /cluster/data/hg17/bed/blastz.rn3/axtChain/run1
    cd /cluster/data/hg17/bed/blastz.rn3/axtChain/run1
    mkdir out chain

    ls -1S /cluster/data/hg17/bed/blastz.rn3/axtChrom/*.axt > input.lst
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} out/$(root1).out
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    cat << '_EOF_' > doChain
#!/bin/csh
    axtFilter -notQ_random $1 | axtChain stdin \
	/iscratch/i/gs.18/build35/bothMaskedNibs \
	/iscratch/i/rn3/bothMaskedNibs $2 > $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doChain

    # 46 jobs
    gensub2 input.lst single gsub jobList
    para create jobList
    para try
    para push # ... etc ...
# Completed: 46 of 46 jobs
# CPU time in finished jobs:       4675s      77.91m     1.30h    0.05d  0.000 y
# IO & Wait Time:                  5429s      90.49m     1.51h    0.06d  0.000 y
# Average job time:                 220s       3.66m     0.06h    0.00d
# Longest job:                      989s      16.48m     0.27h    0.01d
# Submission to last job:           989s      16.48m     0.27h    0.01d

    # now on the file server, sort chains
    ssh eieio
    cd /cluster/data/hg17/bed/blastz.rn3/axtChain
    time chainMergeSort run1/chain/*.chain > all.chain
    # real    10m5.525s
    # user    8m9.350s
    # sys     0m48.450s

    time chainSplit chain all.chain
    # real    10m23.201s
    # user    7m51.930s
    # sys     0m53.910s

    # these steps take ~20 minutes
    # optionally: rm run1/chain/*.chain

    # Load chains into database
    # next machine
    ssh hgwdev
    cd /cluster/data/hg17/bed/blastz.rn3/axtChain/chain
    foreach i (*.chain)
        set c = $i:r
        hgLoadChain hg17 ${c}_chainRn3 $i
        echo done $c
    end
