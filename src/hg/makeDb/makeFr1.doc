#!/bin/csh -f # set emacs mode
exit; # don't actually run this like a script :)

# This file describes how we made the browser database on the 
# Fugu Rubripes (Japanese pufferfish), whole genome shotgun assembly dated 
# 26 August 2002, from the DOE Joint Genome Institute (v3.0).
# This release contains ~320Mbases, in 20,378 scaffolds.
# Size distribution is: 2 scaffolds ~1Mbase, 17 are 500-1000kbase.
# Kate Rosenbloom 4/03

# DOWNLOAD THE SEQUENCE FROM JGI (04/22/03 KRR)

    ssh eieio
    set fugudir = /cluster/store5/Fugu_Rubripes_V3
    mkdir $fugudir
    cd $fugudir

    # NOTE: there are two downloads available on the site -- one
    # is named indicating it is masked, and contains additional N's.
    # The other file (the one we are using) contains lower-case,
    # indicating it is soft-masked. (As an aside, the mask locations are not
    # consistent between the two files).
    # We use the soft-masked file, since it has more sequence; however
    # we will translate to upper case and repeat mask ourselves, 
    # so that we can generate a Repeats track.
    #
    # The file consists of fasta records, each named scaffold_#.
    # Unfortunately, the ordering of scaffolds is alphabetic, not numeric,
    # so scaffold_1 is followed by scaffold_10, not scaffold_2.
    # Also, the scaffold numbering has holes -- the highest numbered
    # scaffold is 32607. 

    wget ftp://ftp.jgi-psf.org/pub/JGI_data/Fugu/fugu_v3.fasta.Z
    ln -s $fugudir ~/fr1
    cd ~/fr1
    gunzip fugu_v3.fasta.Z

    # trim off garbage at end of file (list of scaffolds),
    # this appears to be an artifact of the release process

    mv fugu_v3.fasta fugu_v3.orig.fasta
    grep -v '^scaffold' fugu_v3.orig.fasta > fugu_v3.fasta

    # force to upper case (see above), but preserve lower case headers
    tr '[a-z]' '[A-Z]' < fugu_v3.fasta | \
        sed 's/>SCAFFOLD_/>scaffold_/' > fugu_v3.upper.fasta


# SPLIT THE FASTA FILE INTO SCAFFOLDS FOR FURTHER PROCESSING (DONE KRR)

    ssh eieio
    set fugudir = /cluster/store5/Fugu_Rubripes_V3
    cd $fugudir
    mkdir scaffolds

    # NOTE: must give a record count larger than the number of scaffolds
    # next time, use "faSplit byName" to do this
    faSplit sequence fugu_v3.upper.fasta 30000 scaffolds/

    # rename files to match scaffold (in fasta header)
    # and store in directories (00 .. 32) for convenience and readability
    # This produces directories with 200 - 1000 files per dir.
    # The directory is the leading two digits of the scaffold number,
    # (when represented as a zero-filled 5-digit number)
    cd scaffolds
    foreach d1 (0 1 2 3)
        foreach d2 (0 1 2 3 4 5 6 7 8 9)
            foreach file ($d1$d2*.fa)
                set num = `sed -n '1s/>scaffold_//p' $file`
                set dir = `printf "%05d" $num | sed 's/...$//'`
                mkdir -p $dir
                echo "moving $file to $dir/scaffold_$num"
                mv $file $dir/scaffold_${num}.fa
            end
        end
    end

    # remake a whole-genome fasta file, with the scaffold records
    # ordered more sensibly (numeric, not alphabetic)
    set ordered_file = fugu_v3.ordered.fa
    cp /dev/null $ordered_file
    cd scaffolds
    foreach d ([0-9][0-9])
        set filelist = `(cd $d; ls *.fa | sort -n -k 1.10)`
        foreach f ($filelist) 
            echo "concatenating $f"
            cat $d/$f >> ../$ordered_file
        end
    end
    cd ..

    # split into ~500KB "super-scaffolds"
    # each file has one or more scaffolds. Fasta records are preserved
    # This creates 577 files, size range ~51000 to ~1.2Mb
    rm -fr superscaffolds
    mkdir superscaffolds

    faSplit about $ordered_file 500000 superscaffolds/ss_



# RUN REPEAT MASKER ON THE SUPER-SCAFFOLDS (6/2/03 KRR)
    # note: fugu library ("puffer.lib") is dated 7/9/2002
    ssh eieio
    cd ~/fr1

    # make the run directory, output directory, and job list
    mkdir -p RMRun/Un
    cp /dev/null RMRun/RMJobs
    set fugudir = /cluster/store5/Fugu_Rubripes_V3
    cd superscaffolds
    foreach f (ss_*.fa)
        echo /cluster/bin/scripts/RMFugu \
                $fugudir/superscaffolds $f $fugudir/RMRun/Un \
               '{'check out line+ $fugudir/RMRun/Un/$f.out'}' \
              >> ../RMRun/RMJobs
    end

    # do the run
    ssh kk
    cd ~/fr1/RMRun
    para create RMJobs
    para try
    para check
    para push
    para check,...


# PROCESS SUPER-SCAFFOLDS FOR SIMPLE REPEATS (6/2/03 KRR)
    # TRF runs pretty quickly now... it takes a few hours total runtime, 
    # so instead of binrsyncing and para-running, just do this on the
    # local fileserver
    ssh eieio
    mkdir ~/fr1/bed/simpleRepeat
    cd ~/fr1/bed/simpleRepeat
    mkdir trf
    cp /dev/null jobs.csh
    set fugudir = /cluster/store5/Fugu_Rubripes_V3
    foreach f ($fugudir/superscaffolds/*.fa)
        set fout = $f:t:r.bed
        echo $fout
        echo "/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $f /dev/null -bedAt=trf/$fout -tempDir=/tmp" \
        >> jobs.csh
    end
    tcsh jobs.csh >&! jobs.log &
    # check on this with
    tail -f jobs.log
    wc -l jobs.csh
    ls -1 trf | wc -l


# FILTER SIMPLE REPEATS INTO MASK (6/3/03 KRR)
    # make a filtered version # of the trf output: 
    # keep trf's with period <= 12:
    ssh eieio
    cd ~/fr1/bed/simpleRepeat
    mkdir -p trfMask
    foreach f (trf/*.bed)
        echo "filtering $f"
        awk '{if ($5 <= 12) print;}' $f > trfMask/$f:t
    end


# MASK SUPER-SCAFFOLDS USING REPEATMASKER AND FILTERED TRF FILES (6/3/03 KRR)
    ssh eieio
    cd ~/fr1
    mkdir superscaffolds.masked
    foreach p (superscaffolds/ss_*.fa)
        set f = $p:t
        echo "masking $f"
        maskOutFa $p RMRun/Un/$f.out superscaffolds.masked/$f -soft
        maskOutFa superscaffolds.masked/$f bed/simpleRepeat/trfMask/${f:r}.bed superscaffolds.masked/$f -softAdd
    end


# CREATE FASTA FOR UNORDERED CHROM FROM MASKED SUPER-SCAFFOLDS (6/3/03 KRR)
# Note: scaffolds are separated by 1000 Bp gaps
    ssh eieio
    cd ~/fr1

    # remake a whole-genome fasta file from the masked supser-scaffolds
    set masked_file = fugu_v3.masked.fa
    cd superscaffolds.masked
    cat `ls ss_*.fa | sort -n -k 1.4` > ../$masked_file
    cd ..
    
    # generate AGP and lift files from the chromosome fasta 

    scaffoldFaToAgp $masked_file
    # gap size is 1000, total gaps: 20379
    # chrom size is 349519338

    mkdir Un
    mv fugu_v3.masked.agp Un/chrUn.agp
    mv fugu_v3.masked.lft Un/chrUn.lft

    # Create chromosome FA file from AGP and file of masked scaffolds
    cd Un
    agpToFa -simpleMultiMixed chrUn.agp chrUn chrUn.fa ../fugu_v3.masked.fa


# CREATING DATABASE (5/2/03 KRR)
    # Create the database.
    ssh hgwdev
    echo 'create database fr1' | hgsql ''
    # Make a semi-permanent read-only alias:
    alias fr1 "mysql -u hguser -phguserstuff -A fr1"
    # Make sure there is at least 5 gig free for the database
    df -h /var/lib/mysql


# STORE SEQUENCE AND ASSEMBLY INFORMATION (6/3/03 KRR)

    # Translate to nib
    ssh eieio
    cd ~/fr1
    mkdir nib
    faToNib -softMask Un/chrUn.fa nib/chrUn.nib

    # Make symbolic links from /gbdb/fr1/nib to the real nibs.
    ssh hgwdev
    mkdir -p /gbdb/fr1/nib
    set fugudir = /cluster/store5/Fugu_Rubripes_V3
    ln -s $fugudir/nib/chrUn.nib  /gbdb/fr1/nib

    # Load /gbdb/fr1/nib paths into database and save size info.
     ssh hgwdev
     hgsql fr1  < ~/src/hg/lib/chromInfo.sql
     cd ~/fr1

    # NOTE: last arg here may be in error
     hgNibSeq -preMadeNib fr1 /gbdb/fr1/nib chrUn.nib
     echo "select chrom,size from chromInfo" | hgsql -N fr1 > chrom.sizes

    # create assembly and gap tracks
    ssh hgwdev
    hgGoldGapGl -noGl fr1 ~ fr1 


# CREATING GRP TABLE FOR TRACK GROUPING (5/2/03 KRR)
    # Copy all the data from the table "grp" 
    # in the existing database "rn1" to the new database
    ssh hgwdev
    echo "create table grp (PRIMARY KEY(NAME)) select * from rn1.grp" \
      | hgsql fr1


# CREATE REPEAT TRACKS (6/3/03 KRR)

    ssh eieio
    cd ~/fr1

    # merge and lift up the repeatmasker output files to chrom coordinates
    liftUp Un/chrUn.fa.out Un/chrUn.lft warn RMRun/Un/*.out

    # load into the database (chrUn_rmsk table)
    ssh hgwdev
    hgLoadOut fr1 Un/chrUn.fa.out

    # lift the simple repeat output to chrom coordinates
    ssh eieio
    cd ~/fr1
    cd bed/simpleRepeat
    liftUp simpleRepeat.bed ~/fr1/Un/chrUn.lft warn trf/*.bed

    # load into the database (simpleRepeat table)
    ssh hgwdev
    cd ~/fr1/bed/simpleRepeat
    hgLoadBed fr1 simpleRepeat simpleRepeat.bed \
      -sqlTable=$HOME/src/hg/lib/simpleRepeat.sql



# MAKE GCPERCENT (6/3/03 KRR)
     ssh hgwdev
    set fugudir = /cluster/store5/Fugu_Rubripes_V3
     mkdir -p $fugudir/bed/gcPercent
     cd $fugudir/bed/gcPercent
     hgsql fr1  < ~/src/hg/lib/gcPercent.sql

     # load gcPercent table
     hgGcPercent fr1 ../../nib


# MAKE HGCENTRALTEST ENTRY AND TRACKDB TABLE FOR FUGU (5/6/03 KRR)
    echo 'INSERT INTO defaultDb VALUES ("Fugu", "fr1");' \
      | hgsql -h genome-testdb hgcentraltest

    # Warning: must genome and organism fields must correspond
    # with defaultDb values
    # Note: for next assembly, set scientificName column to "Fugu rubripes"
    echo 'INSERT INTO dbDb \
        (name, description, nibPath, organism, \
                defaultPos, active, orderKey, genome) values \
        ("fr1", "Aug. 2002", "/gbdb/fr1/nib", "Fugu", \
               "chrUn:827700-845800", 1, 10, "Fugu");' \
      | hgsql -h genome-testdb hgcentraltest

    # Make trackDb table so browser knows what tracks to expect:
    ssh hgwdev
    cd ~/src/hg/makeDb/trackDb
    cvs up -d -P

    # Edit that makefile to add fr1 in all the right places and do
    make update

    # go public on genome-test
    #make alpha
    cvs commit makefile

    # Add trackDb directories
    mkdir fugu
    mkdir fugu/fr1
    cvs add fugu
    cvs add fugu/fr1
    cvs commit fugu


# MAKE RELATIONAL RNA TABLES

    hgLoadRna new fr1


# MAKE HGCENTRALTEST BLATSERVERS ENTRY FOR FUGU (6/4/03 KRR)
    ssh hgwdev
    # Get appropriate hostname from cluster admins
    echo 'insert into blatServers values("fr1", "blat11", "17783", "1"); \
          insert into blatServers values("fr1", "blat11", "17782", "0");' \
      | hgsql -h genome-testdb hgcentraltest


# MAKE AND STORE mRNA ALIGNMENTS  (6/8/03 KRR)

    # Load up the local disks of the small cluster with  mrna.fa
    # and masked super-scaffolds
    # from /cluster/store5/mrna.134/org/Takifugu_rubripes
    # note: there are 76 mrna sequences, and 24398 EST's
    ssh kkr1u00
    cd /iscratch/i/fugu
    set mrnaDir = /cluster/store5/mrna.134/org/Takifugu_rubripes
    mkdir mrna 
    cp -p $mrnaDir/mrna.fa mrna
    mkdir trfFa 
    cp -p ~/fr1/superscaffolds.masked/*.fa trfFa
    # note: this isn't currently working for me 
    # (permission errors) -- Hiram did it
    ~kent/bin/iSync

    # generate alignment job 
    # TODO: try aligning whole mrna file against whole chrUn nib
    # on file server (eieio), instead of using cluster.
    # this would be simpler, and probably fast enough
    cd ~/fr1/bed
    mkdir mrna
    cd mrna
    mkdir psl
    ls -1S /iscratch/i/fugu/trfFa/*.fa > genome.lst
    ls -1S /iscratch/i/fugu/mrna/*.fa > mrna.lst
    cat << '_EOF_' > gsub
#LOOP
/cluster/bin/i386/blat -mask=lower -ooc={check in exists /scratch/hg/h/11.ooc} {check in exists+ $(path1)} {check in line+ $(path2)} {check out line+ psl/$(root1)_$(root2).psl}
#ENDLOOP 
'_EOF_' 
    gensub2 genome.lst mrna.lst gsub spec
    para create spec
    para try
    para check
    para push
    # this just takes a few minutes

    ssh eieio
    cd ~/fr1/bed/mrna
    pslSort dirs raw.psl /tmp psl
        # psl with 578 files
        # 578 files in 1 dirs
        # Got 578 files 24 files per mid file
        # Writing /tmp/tmp0.psl
        # ...
        # Writing /tmp/tmp24.psl
        # writing raw.psl
        # Cleaning up temp files

    pslReps -minAli=0.98 -sizeMatters -nearTop=0.005 raw.psl scaffolds.psl \
      /dev/null
        # Processing raw.psl to scaffolds.psl and /dev/null
        # Processed 161 alignments

    # lift up PSL files to chrom coordinates
    liftUp -nohead all_mrna.psl ~/fr1/Un/chrUn.lft warn scaffolds.psl
        # Got 40758 lifts in /cluster/home/kate/fr1/Un/chrUn.lft
        # Lifting scaffolds.psl

    pslSortAcc nohead chrom /tmp all_mrna.psl
        # Processing all_mrna.psl
        # Processed 89 lines into 1 temp files

    # load mrna tables
    ssh hgwdev
    cd ~/fr1/bed/mrna/chrom

    # rename psl file to required format (must be chr*_mrna)
    mv chrUn.psl chrUn_mrna.psl

    # load alignments
    hgLoadPsl -noTNameIx fr1 chrUn_mrna.psl
    cd ..
    hgLoadPsl fr1 all_mrna.psl -nobin

    # prepare for data load
    set mrnaDir = mrna.134
    mkdir /gbdb/fr1/mrna.134
    #ln -s /cluster/store5/$mrnaDir/org/Takifugu_rubripes /gbdb/fr1/$mrnaDir
    ln -s /cluster/store5/$mrnaDir/org/Takifugu_rubripes/mrna.fa /gbdb/fr1/$mrnaDir
    # load mRna into database
    # WARNING: had to use -ignore flag... check on this
    hgLoadRna add -type=mRNA fr1 /gbdb/fr1/$mrnaDir/mrna.fa \
        /cluster/store5/$mrnaDir/org/Takifugu_rubripes/mrna.ra
        # Adding data of type: mRNA ...


# MAKE AND STORE EST ALIGNMENTS  (6/8/03 KRR)

    # setup BlueArc with EST files
    ssh kk
    mkdir -p /cluster/bluearc/fugu/fr1/est
    cd /cluster/bluearc/fugu/fr1/est
    
    # split up est file into 100Kb chunks 
    # (makes ~120 query files * ~600 target superscaffolds = 70000 jobs)
    faSplit about  /cluster/store5/mrna.134/org/Takifugu_rubripes/est.fa \
                                100000 e

    # TODO: try aligning est files against whole chrUn nib
    # this would be easier on the filesystem, and probably fast enough
    cd ~/fr1/bed
    mkdir est
    cd est
    mkdir psl
    ls -1S /iscratch/i/fugu/trfFa/*.fa > genome.lst
    ls -1S /cluster/bluearc/fugu/fr1/est/*.fa > est.lst
    cat << '_EOF_' > gsub
#LOOP
/cluster/bin/i386/blat -mask=lower -ooc={check in exists /scratch/hg/h/11.ooc} {check in exists+ $(path1)} {check in line+ $(path2)} {check out line+ psl/$(root1)_$(root2).psl}
#ENDLOOP 
'_EOF_' 
    gensub2 genome.lst est.lst gsub spec
    para create spec
    para try
    para check
    para push
    para check...
    # just a few minutes to complete

    # process alignments
    ssh eieio
    cd ~/fr1/bed/est

    pslSort dirs raw.psl /tmp psl
    pslReps -minAli=0.98 -sizeMatters -nearTop=0.005 raw.psl scaffolds.psl \
      /dev/null

    # lift up PSL files to chrom coordinates
    liftUp -nohead all_est.psl ~/fr1/Un/chrUn.lft warn scaffolds.psl
        # Got 40758 lifts in /cluster/home/kate/fr1/Un/chrUn.lft
        # Lifting scaffolds.psl

    pslSortAcc nohead chrom /tmp all_est.psl
        # Processing all_est.psl
        # Processed 20501 lines into 1 temp files

    # load into database
    ssh hgwdev
    cd ~/fr1/bed/est/chrom

    # rename psl file to required format (must be chr*_est)
    mv chrUn.psl chrUn_est.psl

    # load alignments
    hgLoadPsl -noTNameIx fr1 chrUn_est.psl
    cd ..
    hgLoadPsl fr1 all_est.psl -nobin

    set mrnaDir = mrna.134
    ln -s /cluster/store5/$mrnaDir/org/Takifugu_rubripes/est.fa /gbdb/fr1/$mrnaDir
    
    cd ~/fr1/bed/est
    rm *.tab
    hgLoadRna add -type=EST fr1 /gbdb/fr1/$mrnaDir/est.fa \
        /cluster/store5/$mrnaDir/org/Takifugu_rubripes/est.ra
    unset mrnaDir

    # note: there is no Genbank RefSeq directory for this organism

# PRODUCE CROSS_SPECIES MRNA ALIGNMENT (6/9/03 KRR)
    # Aligns non-Fugu mRNA's  against the masked genome
    # This uses Genbank mRNA files already downloaded 
    # to /cluster/store5/genbank.134
    # Files are unpacked to /cluster/store5/mrna.134

    # use fileserver where genbank.134 resides
    ssh eieio
    set genbankdir = /cluster/store5/genbank.134
    set mrnadir = /cluster/store5/mrna.134
    cd $mrnaDir

    # generate filter file for unpacking entries
    # note: skip this if fuguXenoRna.fil exists
    cat << '_EOF_' > fuguXenoRna.fil
restrict mol=mRNA & !(org="Takifugu rubripes")
hide mol
'_EOF_' 

    # unpack non-Fugu mRNAs
    gunzip -c $genbankdir/gb{pri,rod,v,mam,inv,bct,htc,pat,phg,pln}* \
        | gbToFaRa fuguXenoRna.fil \
                fuguXenoRna.fa fuguXenoRna.ra fuguXenoRna.ta stdin

    # copy masked chromosome nib to bluearc for cluster run
    # NOTE: probably want to do this earlier
    # so it is available for all alignments
    ssh kk
    cd /cluster/bluearc/fugu/fr1
    mkdir chromNib
    cp ~/fr1/nib/* chromNib

    # split masked scaffold file into 50Mb chunks
    # to produce 7 files
    mkdir trfFa
    faSplit about ~/fr1/fugu_v3.masked.fa 50000000 trfFa/s

    # split big xeno mrna file into 700Kb chunks on bluearc for cluster run
    # (makes ~1000 query files * 7 target files = ~7000 jobs)
    # This is not too many to handle in a single directory (psl)
    mkdir xenoRnaSplit
    faSplit about $mrnadir/fuguXenoRna.fa 700000 xenoRnaSplit/m

    cd ~/fr1/bed
    mkdir xenoMrna
    cd xenoMrna
    mkdir psl
    ls -1S /cluster/bluearc/fugu/fr1/trfFa/s*.fa > genome.lst
    ls -1S /cluster/bluearc/fugu/fr1/xenoRnaSplit/m*.fa > mrna.lst
    cat << '_EOF_' > gsub
#LOOP
/cluster/bin/i386/blat -q=rnax -t=dnax -mask=lower {check in exists+ $(path1)} {check in line+ $(path2)} {check out line+ psl/$(root1)_$(root2).psl}
#ENDLOOP 
'_EOF_'
    gensub2 genome.lst mrna.lst gsub spec
    para create spec
    para try
    para check
    para push
    para check...
        # Completed: 6629 of 6629 jobs
        # Avg. job: 2.7 min
        # Longest job: 4.7 min
        # Submission to last job: 49 min

    # process alignments
    ssh eieio
    cd ~/fr1/bed/xenoMrna

    # sort alignmnents
    pslSort dirs raw.psl /tmp psl
        # Got 6629 files 81 files per mid file

    # analyse repeats and generate genome-wide best alignment
    # note different parameters from previous alignments
    pslReps -minAli=0.25 raw.psl scaffolds.psl /dev/null
        # ...........Processed 1675350 alignments

    # lift up PSL files to chrom coordinates
    liftUp -nohead chrom.psl ~/fr1/Un/chrUn.lft warn scaffolds.psl
        # Got 40758 lifts in /cluster/home/kate/fr1/Un/chrUn.lft

    # extract psl file for each accession, into a dir
    pslSortAcc nohead chrom /tmp chrom.psl
    
    # merge psl files
    pslCat -dir chrom > xenoMrna.psl

    # load alignments into database
    ssh hgwdev
    cd ~/fr1/bed/xenoMrna
    rm *.tab
    hgLoadPsl fr1 xenoMrna.psl

    # load xeno rna into database
    set mrnadir = mrna.134
    set datadir = /cluster/store5
    ln -s $datadir/$mrnadir/fuguXenoRna.fa /gbdb/fr1/$mrnaDir
    hgLoadRna add -type=xenoRna fr1 /gbdb/fr1/$mrnadir/fuguXenoRna.fa \
         $datadir/$mrnadir/fuguXenoRna.ra


# HUMAN HG16 BLAT ALIGNMENT (DONE 2003-09-09 kate)
    ssh kk
    mkdir /cluster/data/fr1/bed/blatHg16
    cd /cluster/data/fr1/bed/blatHg16
    # create output dir and subdirs (prevent directory overload)
    mkdir psl
    foreach f (/iscratch/i/fugu/trfFa/*.fa)
      set c=$f:t:r
      echo $c
      mkdir -p psl/$c
    end
    mkdir run
    cd run
    ls -1S /iscratch/i/fugu/trfFa/*.fa > fugu.lst
        # 578 files
    ls -1S /scratch/hg/gs.17/build34/trfFa/*.fa > human.lst
        # 491 files
    cat << 'EOF' > gsub
#LOOP
/cluster/bin/i386/blat -mask=lower -qMask=lower -q=dnax -t=dnax {check in line+ $(path1)} {check in line+ $(path2)} {check out line+ /cluster/data/fr1/bed/blatHg16/psl/$(root1)/$(root1)_$(root2).psl}
#ENDLOOP
'EOF'

    gensub2 fugu.lst human.lst gsub spec
        # 283798 lines
    para create spec
    para try
    para check
    para push
    para check

    # merge to single Fugu chrUn chrom
    ssh eieio
    cd /cluster/data/fr1/bed/blastHg16
    pslCat -dir psl/ss_* | \
        liftUp -type=.psl stdout \
                /cluster/data/fr1/Un/chrUn.lft warn stdin | \
        pslSortAcc nohead chrom temp stdin
        # 15 minutes ?
    
    # load into database
    cd $fr1dir/chrom
    hgLoadPsl -noTNameIx fr1 chrUn_blatHg15.psl


# TETRAODON BLAT ALIGNMENT (IN PROGRESS 2003-09-11 kate)
# NOTE: too many alignments -- must repeat/trf-mask tetra sequence
    ssh kk
    mkdir /cluster/data/fr1/bed/blatTetra
    cd /cluster/data/fr1/bed/blatTetra
    # create output dir and subdirs (prevent directory overload)
    mkdir psl
    foreach f (/iscratch/i/fugu/trfFa/*.fa)
      set c=$f:t:r
      echo $c
      mkdir -p psl/$c
    end
    mkdir run
    cd run
    ls -1S /iscratch/i/fugu/trfFa/*.fa > fugu.lst
        # 578 files
    cp -rp /cluster/data/tetra /cluster/bluearc
    ls -1S /cluster/bluearc/tetra/*.fa > tetra.lst
        # 36 files
    cat << 'EOF' > gsub
#LOOP
/cluster/bin/i386/blat -mask=lower -q=dnax -t=dnax {check in line+ $(path1)} {check in line+ $(path2)} {check out line+ /cluster/data/fr1/bed/blatTetra/psl/$(root1)/$(root1)_$(root2).psl}
#ENDLOOP
'EOF'

    gensub2 fugu.lst tetra.lst gsub spec
        # 23120 jobs
    para create spec
    para try
    para check
    para push
    para check

    # merge to single Fugu chrUn chrom
    ssh eieio
    cd /cluster/data/fr1/bed/blatTetra
    pslCat -dir psl/ss_* | \
        liftUp -type=.psl stdout \
                /cluster/data/fr1/Un/chrUn.lft warn stdin | \
        pslSortAcc nohead chrom temp stdin
        # 15 minutes ?
    
    # load into database
    ssh hgwdev
    cd /cluster/data/fr1/bed/blatTetra/chrom
    mv chrUn.psl chrUn_blatTetra.psl
    hgLoadPsl -noTNameIx fr1 chrUn_blatTetra.psl

    # Add entry to trackDb.ra

    # make hg16 symlink in /gbdb and load human sequence data
    set gbdbdir=/gbdb/fr1/tetra
    mkdir $gbdbdir
    cd $gbdbdir
    foreach f (/cluster/data/tetra/*)
        ln -s $f .
    end

    # load human sequences into database
    ssh hgwdev
    cd /cluster/data/fr1/bed/blatTetra
    hgLoadSeq fr1 /gbdb/fr1/tetra/*.fa


# HUMAN HG16 BLAT SWAPPED FROM HUMAN BROWSER

    ssh eieio
    set hg16dir = /cluster/data/hg16/bed/blatFr1
    set fr1dir = /cluster/data/fr1/bed/blatHg16.swp
    mkdir -p $fr1dir/{psl,chrom}

    # swap alignments (for each human chrom)
    cd $hg16dir/chrom
    foreach f (chr?{,?}{,_random}_blatFr1.psl)
        pslSwap $f $fr1dir/psl/$f
    end 
    # merge to single Fugu chrUn chrom
    cd $fr1dir/psl
    pslCat chr?{,?}{,_random}_blatFr1.psl > \
                $fr1dir/chrom/scaffolds.psl 

    # lift target side (fugu) to chrom coordinates
    cd $fr1dir/chrom
    liftUp chrUn_blatHg16.psl /cluster/data/fr1/Un/chrUn.lft \
                                        warn scaffolds.psl
        # Got 40758 lifts
    
    # load into database
    ssh hgwdev
    cd /cluster/data/fr1/bed/blatHg16.swp/chrom
    hgLoadPsl -noTNameIx fr1 chrUn_blatHg16.psl

    # Add entry to trackDb.ra

    # make hg16 symlink in /gbdb and load human sequence data
    set gbdbdir=/gbdb/fr1/hg16
    mkdir $gbdbdir
    cd $gbdbdir
    foreach f (/cluster/data/hg16/?{,?})
        set c = $f:t
        if (-e $f/chr${c}.fa) then
            ln -s $f/chr${c}.fa
        endif
        if (-e $f/chr${c}_random.fa) then
            ln -s $f/chr${c}_random.fa
        endif
    end

    # load human sequences into database
    ssh hgwdev
    cd /cluster/data/fr1/bed/blatHg16.swp
    hgLoadSeq fr1 /gbdb/fr1/hg16/*.fa



# PRODUCE TETRAODON BLASTZ (6/13/03 IN PROGRESS KRR)
# NOTE: the tetra sequence wasn't repeatmasked -> must be redone

    cd eieio
    mkdir ~/fr1/bed/blastzTetra

    # translate the Genoscope fasta files to upper-case
    # so sequence doesn't look like it's masked out (all-lower)
    cd /iscratch/i/fish
    foreach f (*.fa)
        echo $f
        tr '[a-z]' '[A-Z]' < $f > /cluster/bluearc/tetra/$f
    end
    (cat README; echo "NOTE: Upper-cased files from /iscratch/i/fish") > \
                                        /cluster/bluearc/tetra/README

    # copy fugu files to cluster filesystem
    set clusterdir = /cluster/bluearc
    mkdir -p $clusterdir/fugu/fr1/rmsk
    cd $clusterdir/fugu/fr1/rmsk
    rm *.out
    cp ~/fr1/Un/chrUn.fa.out .

    # generate DEF file
    cd ~/fr1/bed/blastzTetra
    cat << '_EOF_' > DEF
# Tetraodon vs. Fugu
# TODO: angie's home dir out of this
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/home/angie/schwartzbin:/cluster/bin/i386

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
#BLASTZ_ABRIDGE_REPEATS=1 if SMSK is specified
BLASTZ_ABRIDGE_REPEATS=0

# TARGET - Fugu
# soft-masked chrom nib
SEQ1_DIR=/cluster/bluearc/fugu/fr1/chromNib
# repeat masker output file for chrom
SEQ1_RMSK=/cluster/bluearc/fugu/fr1/rmsk
# lineage-specific repeats -- (repeats in fugu, not in tetraodon)
# we don't have that information for these species
SEQ1_SMSK=/dev/null
# currently unused
SEQ1_FLAG=-fish
SEQ1_IN_CONTIGS=0
# 10 MB chunk for target
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY - Tetraodon
SEQ2_DIR=/cluster/bluearc/tetra
# no repeatmasker output for these sequences
SEQ2_RMSK=/dev/null
SEQ2_SMSK=/dev/null
# currently unused
SEQ2_FLAG=-fish
# set to 1 if multiple contigs are in 1 fasta file
SEQ2_IN_CONTIGS=1
# 10 Mbase for query
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/cluster/store5/Fugu_Rubripes_V3/bed/blastzTetra

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len

#DEBUG=1
'_EOF_' 

    # save the DEF file in the current standard place
    chmod +x DEF
    cp DEF ~angie/hummus/DEF.fr1-tetra.2003-06-17

    # setup cluster run
    ssh kk
    cd ~/fr1/bed/blastzTetra

    # source the DEF file
    bash
    . ./DEF

    # follow the next set of directions slavishly
    mkdir -p $BASE/run
    # give up on avoiding angie's directories
    # tcl script
    # creates xdir.sh and joblist run/j
    ~angie/hummus/make-joblist $DEF > $BASE/run/j
        # Computing genome sizes
        # Writing mkdir script: /cluster/store5/Fugu_Rubripes_V3/bed/blastzTetra/xdir.sh
        # Writing job list

    # xdir.sh makes a bunch of result directories in $BASE/raw/
    # based on chrom name and CHUNK size
    sh $BASE/xdir.sh
    cd $BASE/run

    # now edit j to prefix path to executable name
    # NOTE: we should have a controlled version of schwartz bin executables
    sed -e 's#^#/cluster/home/schwartz/bin/#' j > j2
    wc -l j*
    head j2

    # *** make sure the j2 edits are OK, then use it:
    mv j2 j

    # para create will create the file: 'batch' for the cluster run
    para create j
        # Checking input files
        # 1400 jobs written to batch
    para try
    para check
    para push
    # ... etc ...
    # about 3 hrs. per job

    # post-process blastz
    # normalize
    ssh kk
    cd ~/fr1/bed/blastzTetra
        #   source the DEF file again in case you are coming back to this
    bash
    . ./DEF

    # a new run directory
    mkdir -p $BASE/run.1

    # another obscure script creates a new job list:
    ~angie/hummus/do.out2lav $DEF >$BASE/run.1/j
    cd $BASE/run.1

    # add path to executable in the job list:
    sed -e 's/^/\/cluster\/home\/schwartz\/bin\//' j > j2
    wc -l j*
    head j2

    # make sure the edited j2 is OK, then use it:
    mv j2 j
    para create j
    para try
    para check
    para push
    # etc.
        # 15 minute jobs

    # Translate the .lav files
    # into axt files
    ssh kkstore
    set base="/cluster/store5/Fugu_Rubripes_V3/bed/blastzTetra"
    set tbl="blastzTetra"

    # generate single tetra fa file required for lavToAxt
    cd /cluster/bluearc
    cat tetra/* > tetra-all.fa

    # copy lav files to bluearc to speed up job
    ssh eieio
    mkdir -p /cluster/bluearc/fugu/fr1/blastzTetra/lav
    cp ~/fr1/bed/blastzTetra/lav/chrUn/* /cluster/bluearc/fugu/fr1/blastzTetra/lav
    
    # cluster job for converting lav files
    # these take a long time because query is in fa files
    # (with 880K sequences), not a chrom nib
    ssh kk
    cd ~/fr1/bed/blastzTetra
    mkdir -p axtLav
    mkdir run.lavToAxt
    cd run.lavToAxt
    
    # create file lists
    ls -1S /cluster/bluearc/fugu/fr1/blastzTetra/lav/*.lav > lav.lst
    echo "" > dummy.lst

    # Create template file, gsub, for gensub2. 
    cat << 'EOF' > gsub
#LOOP 
/cluster/bin/i386/lavToAxt {check in line+ $(path1)} /cluster/bluearc/fugu/fr1/chromNib -fa /cluster/bluearc/tetra-all.fa {check out line+ /cluster/store5/Fugu_Rubripes_V3/bed/blastzTetra/axtLav/$(root1).axt}
#ENDLOOP
'EOF'

    gensub2 lav.lst dummy.lst gsub jobList
    para create jobList
    para try 
    para check
    para push

    # 35 hours (too long -- need to split up better).

    cd $base
    mkdir -p axtChrom
    cat axtLav/*.axt | /cluster/bin/i386/axtSort stdin axtChrom/chrUn.axt
    
    #foreach c (lav/*)
      #pushd $c
      #set chr=$c:t
      #set out=$base/axtChrom/$chr.axt
      #echo "out=$out"
      #echo "Translating $chr lav to $out"
      #echo cat `ls -1 *.lav | sort -g` \
        #| /cluster/bin/i386/lavToAxt stdin $fugudir/nib \
                #-fa /cluster/bluearc/tetra-all.fa stdout \
        #| /cluster/bin/i386/axtSort stdin $out
      #popd
    #end

    # Translate the sorted axt files into psl:
    cd $base
    mkdir -p pslChrom
    foreach f (axtChrom/chr*.axt)
      set c=$f:t:r
      /cluster/bin/i386/axtToPsl $f S1.len S2.len pslChrom/${c}_${tbl}.psl
    end

    # load these blastz results
    ssh hgwdev
    cd ~/fr1/bed/blastzTetra/pslChrom
    /cluster/bin/i386/hgLoadPsl -noTNameIx fr1 chr*_*.psl

    # STOPPED HERE -- due to empty output data


# PRODUCING GENSCAN PREDICTIONS (6/17/03 KRR)
#       Note: Genscan input files should be hard-masked
    
    ssh eieio

    # hard mask the super-scaffolds
    mkdir -p ~/fr1/superscaffolds.hardmasked
    cd ~/fr1/superscaffolds
    foreach f (ss_*.fa)
        echo $
        maskOutFa $f hard ../superscaffolds.hardmasked/$f
    end

    # setup the genscan directory
    mkdir -p ~/fr1/bed/genscan
    cd ~/fr1/bed/genscan

    # Make 3 subdirectories for genscan to put output files in
    mkdir -p gtf pep subopt

    # Generate a list file, genome.list, of all the contigs
    # *that do not have pure Ns* (due to heterochromatin, unsequenceable 
    # stuff) which would cause genscan to run forever.
    rm -f genome.list
    touch genome.list
    foreach f ( `ls -1S /cluster/store5/Fugu_Rubripes_V3/superscaffolds.hardmasked/ss_*.fa` )
      egrep '[ACGT]' $f > /dev/null
      if ($status == 0) echo $f >> genome.list
    end

    # Log into kkr1u00 (not kk!).  kkr1u00 is the driver node for the small
    # cluster (kkr2u00 -kkr8u00. Genscan has problem running on the
    # big cluster, due to limitation of memory and swap space on each
    # processing node).
    ssh kkr1u00
    cd ~/fr1/bed/genscan

    # Create template file, gsub, for gensub2. 
    # NOTE: According to the README for Genscan, 
    # the HumanIso.smat parameter files is used for all vertebrates
    cat << '_EOF_' > gsub
#LOOP 
/cluster/home/kent/bin/i386/gsBig {check in line+ $(path1)} {check out line gtf/$(root1).gtf} -trans={check out line pep/$(root1).pep} -subopt={check out line subopt/$(root1).bed} -exe=/cluster/home/fanhsu/projects/compbio/bin/genscan-linux/genscan -par=/cluster/home/fanhsu/projects/compbio/bin/genscan-linux/HumanIso.smat -tmp=/tmp -window=2400000
#ENDLOOP
'_EOF_' 
    echo "" > dummy.list
    gensub2 genome.list dummy.list gsub jobList
    para create jobList
    para try
    para check
    para push

    # Convert output to chrom coordinates
    ssh eieio
    cd ~/fr1/bed/genscan
    liftUp genscan.gtf ~/fr1/Un/chrUn.lft warn gtf/*.gtf
    liftUp genscanSubopt.bed ~/fr1/Un/chrUn.lft warn subopt/*.bed > /dev/null
    cat pep/*.pep > genscan.pep

    # Load into the database
    ssh hgwdev
    cd ~/fr1/bed/genscan
    ldHgGene fr1 genscan genscan.gtf
        # Reading genscan.gtf
        # Read 36106 transcripts in 243984 lines in 1 files
        # 36106 groups 1 seqs 1 sources 1 feature types
        # 36106 gene predictions
    hgPepPred fr1 generic genscanPep genscan.pep
    hgLoadBed fr1 genscanSubopt genscanSubopt.bed > /dev/null


# CPGISLANDS (6/17/03 KRR)
    ssh eieio
    mkdir -p /cluster/data/fr1/bed/cpgIsland
    cd /cluster/data/fr1/bed/cpgIsland

    # Build software emailed from Asif Chinwalla (achinwal@watson.wustl.edu)
    # NOTE: we should keep this centrally
    cp -rp /cluster/data/mm3/bed/cpgIsland/cpg_dist .
    cd cpg_dist
    # gcc readseq.c cpg_lh.c -o cpglh.exe

    # hard-mask the chrom fa 
    # cpglh.exe requires hard-masked (N) .fa's.  
    # There may be warnings about "bad character" for IUPAC ambiguous 
    # characters like R, S, etc.  Ignore the warnings.  
    cd ~/fr1/Un
    maskOutFa chrUn.fa hard chrUn.hardmasked.fa

    # run cpgislands on the hard-masked chrom fa
    cd ~/fr1/bed/cpgIsland
    ./cpglh.exe ~/fr1/Un/chrUn.hardmasked.fa > chrUn.cpg

    cp ~kent/mm3/bed/cpgIsland/filter.awk .
    awk -f filter.awk chr*.cpg > cpgIsland.bed

    # load annotations into database
    ssh hgwdev
    cd ~/fr1/bed/cpgIsland
    # NOTE: this should go into a shared lib dir
    cp $HOME/kent/src/hg/lib/cpgIsland.sql .
    hgLoadBed fr1 cpgIsland -tab -noBin \
      -sqlTable=cpgIsland.sql cpgIsland.bed
        # Reading cpgIsland.bed
        # Loaded 41723 elements
        # Sorted
        # Saving bed.tab
        # Loading fr1

# LOAD ENSEMBL GENE PREDICTIONS (DONE 2003-08-11 kate)
# The downloads were provided by Shawn Hoon (shawnh@fugu-sg.org)
# of the Singapore Institute of Molecular and Cell Biology (IMCB)
# Fugu genome project, part of the international Fugu Genome Consortium
# Other contact is: Venkatesh Byrappa (mcbbv@imcb.a-star.edu.sg)
# In future releases, these annotations will need to be downloaded from Ensembl,
# using Ensmart

    ssh hgwdev
    cd /cluster/data/fr1
    mkdir -p bed/ensembl
    cd bed/ensembl

    # download and load gene predictions
    wget http://www.fugu-sg.org/~shawnh/fugu_download/fugu_rubripes_15_2_gtf.tar.gz
    tar xvfz fugu_rubripes_15_2_gtf.tar.gz
    cd fugu_rubripes_core_15_2
    foreach f (*.gtf)
        echo $f
        sed 's/^Chr_scaffold/scaffold/' $f >> ../ensGene.scaffold.gtf
    end
    cd ..
    /cluster/bin/i386/liftUp ensGene.gtf /cluster/data/fr1/Un/chrUn.lft warn ensGene.scaffold.gtf
    /cluster/bin/i386/ldHgGene fr1 ensGene ensGene.gtf
      # 38510 gene predictions

    # download and load predicted peptides
    #wget http://www.fugu-sg.org/~shawnh/fugu_download/Fugu_rubripes.FUGU2.pep.fa.gz
    #gunzip Fugu_rubripes.FUGU2.pep.fa.gz

    sed 's/Translation:SINFRUP/SINFRUT/g' < Fugu_rubripes.FUGU2.pep.fa > ensembl.pep
    hgPepPred fr1 generic ensPep ensembl.pep
#

# MAKE DOWNLOADABLE SEQUENCE FILES (IN PROGRESS 2003-08-29 kate)
    ssh eieio
    cd /cluster/data/fr1

    # Build the .zip files
    mkdir jkStuff
    cp /cluster/data/hg15/jkStuff/zipAll.sh jkStuff

    # Edit jkStuff/zipAll.sh to point to the correct paths.

    ./jkStuff/zipAll.sh >&! zipAll.log
    tail -f zipAll.log

    # Look at zipAll.log to make sure all file lists look reasonable.  
    # Check zip file integrity:

    mkdir zip
    mv *.zip* zip
    cd zip
    foreach f (*.zip)
      unzip -t $f > $f.test
      tail -1 $f.test
    end
    wc -l *.zip.test
      # 3 chromAgp.zip.test
      # 3 chromFa.zip.test
      # 3 chromFaMasked.zip.test
      # 3 chromOut.zip.test
      # 3 chromTrf.zip.test
      # 580 contigFa.zip.test
      # 580 contigFaMasked.zip.test
      # 580 contigTrf.zip.test
      # 3 liftAll.zip.test
      # 3 mrna.zip.test

    # Copy the .zip files to hgwdev:/usr/local/apache/...  
    ssh hgwdev
    cd /cluster/data/fr1/zip
    set webDir = /usr/local/apache/htdocs/goldenPath/fr1
    mkdir -p $webDir/bigZips
    cp -p *.zip $webDir/bigZips

    # Create rmsk database table dump for early users (Victor Solovyev)
    cd $webDir
    chmod o+w .
    mysqldump --user=SECRET --password=SECRET --all --tab=. \
                                                        fr1 chrUn_rmsk
    # change ownership (was mysql)
    mkdir database
    cp chrUn_rmsk.* database
    rm chrUn_rmsk.*

    # restore dir permission
    chmod o-w .
    unset webDir

    # No RefSeqs for Fugu, so no upstream files.

    # create checksum file
    md5sum *.zip > md5sum.txt

    # Take a look at bigZips/* update the README.txt


# move fr1 to incremental genbank/refseq updates:
    # need to create a lift file to splice chrUn into fragments larger
    # than the scaffolds or there will be far to many jobs
    cd /cluster/data/fr1/Un
    ./makeFragLift chrUn.lft >chrUn-frag.lft

    cd /cluster/data/genbank/
    # edit etc/genbank.conf to add fr1

    # align mrnas, then ests
    nice ./bin/gbAlignStep -initial -type=mrna -clusterRootDir=/cluster/bluearc/genbank -iserver=no fr1 
    nice ./bin/gbAlignStep -initial -type=est -clusterRootDir=/cluster/bluearc/genbank -iserver=no fr1 

# ECORES FROM GENOSCOPE [in progress, hartera, 2004-03-29]
# download data from  http://www.genoscope.cns.fr/externe/tetraodon/Data3/ecores

# ecotigFH - ecores on Fugu, genome conserved with Human, hg16
# ecotigFM - ecores on Fugu, genome conserved with Mouse, mm3
# ecotigFR - ecores on Fugu, genome conserved with Rat, rn3

    ssh hgwdev
    mkdir /cluster/data/fr1/bed/ecores/
    # add parse_ecotig.pl to this directory

# MOUSE

    mkdir /cluster/data/fr1/bed/ecores/mm3
    cd /cluster/data/fr1/bed/ecores/mm3/

    # download data for ecotigFH to this directory 
    # parse ecotig files to produce a bed format file
    perl ../parse_ecotig.pl < ecotigFM > ecotigFM.bed
                                                                                
    # remove "CONTIG:" and change from upper to lower case for "SCAFFOLD"
    perl -pi.bak -e 's/CONTIG://g; s/SCAFFOLD/scaffold/g;' ecotigFM.bed
                                                                                
    # use liftUp to convert scaffold co-ordinates to chrUn co-ordinates
    liftUp -type=.bed ecotigFMlift.bed /cluster/data/fr1/Un/chrUn.lft warn ecotigFM.bed
    # only converts the chromStart and chromEnd fields, use fixLift.pl to 
    # convert rest of the bed file

    perl ../fixLift.pl < ecotigFMlift.bed > ecotigFMliftFixed.bed                                                                       
    hgLoadBed -tab fr1 ecoresMm3 ecotigFMliftFixed.bed
    # clean up
    rm *.bak

# RAT
                                                                                
    mkdir /cluster/data/fr1/bed/ecores/rn3
    cd /cluster/data/fr1/bed/ecores/rn3/
                                                                                
    # download data for ecotigFH to this directory
    # parse ecotig files to produce a bed format file
    perl ../parse_ecotig.pl < ecotigFR > ecotigFR.bed
                                                                                
    # remove "CONTIG:" and change from upper to lower case for "SCAFFOLD"
    perl -pi.bak -e 's/CONTIG://g; s/SCAFFOLD/scaffold/g;' ecotigFR.bed
                                                                                
    # use liftUp to convert scaffold co-ordinates to chrUn co-ordinates
    liftUp -type=.bed ecotigFRlift.bed /cluster/data/fr1/Un/chrUn.lft warn ecotigFR.bed
    # only converts the chromStart and chromEnd fields, use fixLift.pl to 
    # convert rest of the bed file

    perl ../fixLift.pl < ecotigFRlift.bed > ecotigFRliftFixed.bed                                                                       
    hgLoadBed -tab fr1 ecoresRn3 ecotigFRliftFixed.bed
    
    # clean up
    rm *.bak

# HUMAN
    mkdir /cluster/data/fr1/bed/ecores/hg16
    cd /cluster/data/fr1/bed/ecores/hg16/

    # download data for ecotigFH to this directory
    # parse ecotig files to produce a bed format file
    perl ../parse_ecotig.pl < ecotigFH > ecotigFH.bed
                                                                                
    # remove "CONTIG:" and change from upper to lower case for "SCAFFOLD"
    perl -pi.bak -e 's/CONTIG://g; s/SCAFFOLD/scaffold/g;' ecotigFH.bed
   
    # use liftUp to convert scaffold co-ordinates to chrUn co-ordinates
    liftUp -type=.bed ecotigFHlift.bed /cluster/data/fr1/Un/chrUn.lft warn ecotigFH.bed
    # only converts the chromStart and chromEnd fields, use fixLift.pl to 
    # convert rest of the bed file

    perl ../fixLift.pl < ecotigFHlift.bed > ecotigFHliftFixed.bed                                                                       
    hgLoadBed -tab fr1 ecoresHg16 ecotigFHliftFixed.bed
    # clean up
    rm *.bak


    # add entries in kent/src/hg/makeDb/trackDb/fugu/trackDb.ra
    # add html for details pages to this directory:
    # ecoresMm3.html, ecoresRn3.html and ecoresHg16.html

# MAKE Human Proteins track
    ssh eieio
    mkdir -p /cluster/data/fr1/blastDb
    cd /cluster/data/fr1/blastDb
    for i in ../superscaffolds/*.fa; do ln -s $i .; done
    for i in *.fa; do formatdb -i $i -p F 2> /dev/null; done
    rm *.fa *.log

    ssh kkr1u00
    mkdir -p /iscratch/i/fr1/blastDb
    cp /cluster/data/fr1/blastDb/* /iscratch/i/fr1/blastDb
    (~kent/bin/iSync) 2>&1 > sync.out
    
    mkdir -p /cluster/data/fr1/bed/tblastn.hg16KG
    cd /cluster/data/fr1/bed/tblastn.hg16KG
    ls -1S /iscratch/i/fr1/blastDb/*.nsq | sed "s/\.nsq//" > fugu.lst
    exit

    # back to kksilo
    cd /cluster/data/fr1/bed/tblastn.hg16KG
    mkdir kgfa
    # calculate a reasonable number of jobs
    calc `wc /cluster/data/hg16/bed/blat.hg16KG/hg16KG.psl | awk "{print \\\$1}"`/\(150000/`wc fugu.lst | awk "{print \\\$1}"`\)
    # 41117/(150000/578) = 158.437507
    split -l 158 /cluster/data/hg16/bed/blat.hg16KG/hg16KG.psl kgfa/kg
    cd kgfa
    for i in *; do pslxToFa $i $i.fa; rm $i; done
    cd ..
    ls -1S kgfa/*.fa > kg.lst
    mkdir blastOut
    for i in `cat kg.lst`; do  mkdir blastOut/`basename $i .fa`; done
    cat << '_EOF_' > blastGsub
#LOOP
blastSome $(path1) {check in line $(path2)} {check out exists blastOut/$(root2)/q.$(root1).psl } 
#ENDLOOP
'_EOF_'
    cat << '_EOF_' > blastSome
#!/bin/sh
BLASTMAT=/iscratch/i/blast/data
export BLASTMAT
f=/tmp/`basename $3`
for eVal in 0.01 0.001 0.0001 0.00001 0.000001 1E-09 1E-11
do
if /scratch/blast/blastall -M BLOSUM80 -m 0 -F no -e $eVal -p tblastn -d $1 -i $2 -o $f.8
then
        mv $f.8 $f.1
        break;
fi
done
if test -f  $f.1
then
if /cluster/bin/i386/blastToPsl $f.1 $f.2
then
        liftUp -nosort -type=".psl" -pslQ -nohead $3.tmp /cluster/data/hg16/bed/blat.hg16KG/protein.lft warn $f.2
        mv $3.tmp $3
        rm -f $f.1 $f.2 
        exit 0
    fi
fi
rm -f $f.1 $f.2 $3.tmp $f.8
exit 1
'_EOF_'

    chmod +x blastSome
    gensub2 fugu.lst kg.lst blastGsub blastSpec

    ssh kk
    cd /cluster/data/fr1/bed/tblastn.hg16KG
    para create blastSpec
    para shove   # jobs will need to be restarted, but they should all finish
# Completed: 150858 of 150858 jobs
# CPU time in finished jobs:   10404860s  173414.33m  2890.24h  120.43d  0.330 y
# IO & Wait Time:               2468643s   41144.06m   685.73h   28.57d  0.078 y
# Average job time:                  85s       1.42m     0.02h    0.00d
# Longest job:                      411s       6.85m     0.11h    0.00d
# Submission to last job:         26364s     439.40m     7.32h    0.31d

    cat << '_EOF_' > chainGsub
#LOOP
chainSome $(path1)
#ENDLOOP
'_EOF_'

    cat << '_EOF_' > chainSome
(cd $1; cat q.*.psl | simpleChain -prot -outPsl -maxGap=10000 stdin ../c.`basename $1`.psl)
'_EOF_'
    chmod +x chainSome

    ls -1dS `pwd`/blastOut/kg?? > chain.lst
    gensub2 chain.lst single chainGsub chainSpec

    ssh kki
    cd /cluster/data/fr1/bed/tblastn.hg16KG
    para create chainSpec
    para push
# Completed: 261 of 261 jobs
# CPU time in finished jobs:        828s      13.81m     0.23h    0.01d  0.000 y
# IO & Wait Time:                 19521s     325.34m     5.42h    0.23d  0.001 y
# Average job time:                  78s       1.30m     0.02h    0.00d
# Longest job:                      270s       4.50m     0.07h    0.00d
# Submission to last job:          3878s      64.63m     1.08h    0.04d

    exit
    # back to kksilo
    cd /cluster/data/fr1/bed/tblastn.hg16KG/blastOut
    for i in kg??
    do 
	awk "(\$13 - \$12)/\$11 > 0.6 {print}" c.$i.psl > c60.$i.psl
	sort -rn c60.$i.psl | pslUniq stdin u.$i.psl
	awk "((\$1 / \$11) ) > 0.60 { print   }" c60.$i.psl > m60.$i.psl
	echo $i
    done

    cat u.*.psl m60* | liftUp -nosort -type=".psl" -nohead stdout /cluster/data/fr1/jkStuff/liftAll.lft warn stdin |  \
    	sort -T /tmp -k 14,14 -k 16,16n -k 17,17n  | uniq  > ../preblastHg16KG.psl
    cd ..
    blatDir=/cluster/data/hg16/bed/blat.hg16KG
    protDat -kg preblastHg16KG.psl $blatDir/hg16KG.psl $blatDir/kg.mapNames blastHg16KG.psl

    ssh hgwdev
    cd /cluster/data/fr1/bed/tblastn.hg16KG
    hgLoadPsl fr1 blastHg16KG.psl
    exit

    # back to kksilo
    rm -rf blastOut

# End tblastn
