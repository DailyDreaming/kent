#!/bin/csh -f # set emacs mode
exit; # don't actually run this like a script :)

# Drosophila Melanogaster -- 
# 
# Berkeley Drosophila Genome Project (fruitfly.org) release 3.1 (Jan. 2003)
# http://www.fruitfly.org/annot/release3.html
#
# FlyBase (http://flybase.bio.indiana.edu/) last updated 20 Jan 2003
#

# DOWNLOAD SEQUENCE (DONE 10/2/03 angie)
    ssh kksilo
    mkdir /cluster/store6/dm1
    cd /cluster/data
    ln -s /cluster/store6/dm1 dm1
    cd /cluster/data/dm1
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/FASTA/whole_genome_genomic_dmel_RELEASE3-1.FASTA
    faSplit byname whole_genome_genomic_dmel_RELEASE3-1.FASTA dummyArg

    # Follow FlyBase's lead on the chromosome names, but still use our 
    # "chr" prefix:
    foreach c (2L 2R 2h 3L 3R 3h 4 X Xh Yh U)
      mkdir $c
      sed -e 's/^>/>chr/' $c.fa > $c/chr$c.fa
      echo chr$c.fa size:
      faSize $c.fa
      echo $c/chr$c.fa size:
      faSize $c/chr$c.fa
      echo comparison:
      faCmp $c.fa $c/chr$c.fa
      echo ""
    end
    # Carefully review output of those commands, then:
    rm 2L.fa 2R.fa 2h.fa 3L.fa 3R.fa 3h.fa 4.fa X.fa Xh.fa Yh.fa U.fa

    # put away the big download file
    mkdir -p downloads/fruitfly
    mv whole_genome_genomic_dmel_RELEASE3-1.FASTA downloads/fruitfly/


# SPLIT CHROM FA INTO SMALLER CHUNKS BY GAPS (DONE 10/3/03 angie)
    ssh kksilo
    cd /cluster/data/dm1
    foreach c (?{,?})
      faSplit -minGapSize=100 -lift=$c/chr$c.lft \
        gap $c/chr$c.fa 2000000 $c/chr${c}_
    end
    foreach ctgFa (?{,?}/chr*_*.fa)
      set ctg = $ctgFa:r
      mkdir $ctg
      mv $ctgFa $ctg
    end


# CREATING DATABASE (DONE 10/3/03 angie)
    # Create the database.
    ssh hgwdev
    echo 'create database dm1' | hgsql ''
    # Make a semi-permanent read-only alias:
    alias dm1 "mysql -u hguser -phguserstuff -A dm1"
    # Make sure there is at least 5 gig free for the database
    df -h /var/lib/mysql


# EXTRACT GAP INFORMATION FROM FASTA, LOAD GAP TRACK (DONE 10/3/03 angie)
    ssh kksilo
    cd /cluster/data/dm1
    # size up the gap situation -- can we use gaps to extract agp info?
    faGapSizes downloads/fruitfly/whole_genome_genomic_dmel_RELEASE3-1.FASTA 
    # yup.  Jim's verdict:
    # I think that we can probably just show all gaps as bridged
    # in the non-h chromosomes, and as unbridged in the h chromosomes
    # and leave it at that.

    # Extract gaps using scaffoldFaToAgp.  It's really meant for a different 
    # purpose, so clean up its output: remove the .lft and .agp, and remove 
    # the last line of .gap (extra gap added at end).  Also substitute in 
    # the correct chrom name in .gap.  
    foreach c (?{,?})
      set chr = chr$c
      pushd $c
      scaffoldFaToAgp -minGapSize=100 $chr.fa
      rm $chr.{lft,agp}
      set chrSize = `faSize $chr.fa | awk '{print $1;}'`
      set origLines = `cat $chr.gap | wc -l`
      awk '($2 != '$chrSize'+1) {print;}' $chr.gap \
      | sed -e "s/chrUn/$chr/" > $chr.gap2
      set newLines = `cat $chr.gap2 | wc -l`
      if ($newLines == ($origLines - 1)) then
        mv $chr.gap2 $chr.gap
      else
        echo "Error: $chr/$chr.gap2 has wrong number of lines."
      endif
      popd
    end
    # Call the gaps unbridged in chrU and chr*h:
    foreach c (U ?h)
      set chr = chr$c
      sed -e 's/yes/no/' $c/$chr.gap > $c/$chr.gap2
      mv $c/$chr.gap2 $c/$chr.gap
    end
    ssh hgwdev
    hgLoadGap dm1 /cluster/data/dm1


# MAKE DESCRIPTION/SAMPLE POSITION HTML PAGE (DONE 10/13/03 angie/donnak)
    ssh hgwdev
    # Write ~/kent/src/hg/makeDb/trackDb/drosophila/dm1/description.html 
    # with a description of the assembly and some sample position queries.  
    chmod a+r ~/kent/src/hg/makeDb/trackDb/drosophila/dm1/description.html
    # Check it in and copy (perhaps via a make in trackDb??) to 
    # /cluster/data/dm1/html.  
    mkdir -p /gbdb/dm1/html
    ln -s /cluster/data/dm1/html/description.html /gbdb/dm1/html/


# RUN REPEAT MASKER (DONE 10/4/03 angie)
    # Note: drosophila library ("drosophila.lib") is dated May 27 '03.
    # Contigs (*/chr*_*/chr*_*.fa) are split into 500kb chunks to make 
    # RepeatMasker runs manageable on the cluster ==> results need lifting.

    # Split contigs into 500kb chunks:
    ssh kksilo
    cd /cluster/data/dm1
    foreach d ( */chr*_?{,?} )
      cd $d
      set contig = $d:t
      faSplit -minGapSize=100 -lift=$contig.lft -maxN=500000 \
        gap $contig.fa 500000 ${contig}_
      cd ../..
    end

    # make the run directory, output directory, and job list
    mkdir RMRun
    cp /dev/null RMRun/RMJobs
    foreach d ( ?{,?}/chr*_?{,?} )
      set ctg = $d:t
      foreach f ( $d/${ctg}_?{,?}.fa )
        set f = $f:t
        echo /cluster/bin/scripts/RMDrosophila \
             /cluster/data/dm1/$d $f /cluster/data/dm1/$d \
           '{'check out line+ /cluster/data/dm1/$d/$f.out'}' \
        >> RMRun/RMJobs
      end
    end

    # do the run
    ssh kk
    cd /cluster/data/dm1/RMRun
    para create RMJobs
    para try
    para check
    para push
    para check,...
#Completed: 288 of 288 jobs
#CPU time in finished jobs:    1764726s   29412.10m   490.20h   20.43d  0.056 y
#IO & Wait Time:                  3392s      56.53m     0.94h    0.04d  0.000 y
#Average job time:                6139s     102.32m     1.71h    0.07d
#Longest job:                     7256s     120.93m     2.02h    0.08d
#Submission to last job:          7257s     120.95m     2.02h    0.08d

    # Lift up the split-contig .out's to contig-level .out's
    ssh kksilo
    cd /cluster/data/dm1
    foreach d ( ?{,?}/chr*_?{,?} )
      cd $d
      set contig = $d:t
      liftUp $contig.fa.out $contig.lft warn ${contig}_*.fa.out > /dev/null
      cd ../..
    end

    # Lift up the contig-level .out's to chr-level
    foreach c (?{,?})
      cd $c
      if (-e chr$c.lft && ! -z chr$c.lft) then
        echo lifting $c
        /cluster/bin/i386/liftUp chr$c.fa.out chr$c.lft warn \
          `awk '{print $2"/"$2".fa.out";}' chr$c.lft` > /dev/null
      else
        echo Can\'t find $c/chr$c.lft \!
      endif
      cd ..
    end

    # soft-mask contig .fa's with .out's
    foreach c (?{,?})
      foreach j ($c/chr${c}_?{,?}/chr${c}_?{,?}.fa)
        maskOutFa $j $j.out $j -soft
      end
      echo done $c
    end

    # Load the .out files into the database with:
    ssh hgwdev
    hgLoadOut dm1 /cluster/data/dm1/?{,?}/*.fa.out


# SIMPLE REPEATS (TRF) (DONE 10/5/03 angie)
    # TRF runs pretty quickly now... it takes a few hours total runtime, 
    # so instead of binrsyncing and para-running, just do this on the
    # local fileserver
    ssh kksilo
    mkdir /cluster/data/dm1/bed/simpleRepeat
    cd /cluster/data/dm1/bed/simpleRepeat
    mkdir trf
    cp /dev/null jobs.csh
    foreach f (/cluster/data/dm1/?{,?}/chr*_*/chr?{,?}_?{,?}.fa)
        set fout = $f:t:r.bed
        echo $fout
        echo "/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $f /dev/null -bedAt=trf/$fout -tempDir=/tmp" \
        >> jobs.csh
    end
    tcsh jobs.csh >&! jobs.log &
    # check on this with
    tail -f jobs.log
    wc -l jobs.csh
    ls -1 trf | wc -l
    # When job is done do:
    mkdir /cluster/data/dm1/jkStuff
    liftUp simpleRepeat.bed /cluster/data/dm1/jkStuff/liftAll.lft warn \
      trf/*.bed

    # Load this into the database as so
    ssh hgwdev
    hgLoadBed dm1 simpleRepeat \
      /cluster/data/dm1/bed/simpleRepeat/simpleRepeat.bed \
      -sqlTable=$HOME/src/hg/lib/simpleRepeat.sql


# FILTER SIMPLE REPEATS (TRF) INTO MASK (DONE 10/5/03 angie)
    # make a filtered version # of the trf output: 
    # keep trf's with period <= 12:
    ssh kksilo
    cd /cluster/data/dm1/bed/simpleRepeat
    mkdir -p trfMask
    foreach f (trf/*.bed)
        echo "filtering $f"
        awk '{if ($5 <= 12) print;}' $f > trfMask/$f:t
    end
    # Lift up filtered trf output to chrom coords as well:
    cd /cluster/data/dm1
    mkdir bed/simpleRepeat/trfMaskChrom
    foreach c (?{,?})
      liftUp bed/simpleRepeat/trfMaskChrom/chr$c.bed $c/chr$c.lft warn \
        `awk '{print "bed/simpleRepeat/trfMask/"$2".bed";}' $c/chr$c.lft`
    end


# MASK FA USING REPEATMASKER AND FILTERED TRF FILES (DONE 10/5/03 angie)
    ssh kksilo
    cd /cluster/data/dm1
    foreach c (?{,?})
      echo repeat- and trf-masking chr$c.fa
      /cluster/home/kent/bin/i386/maskOutFa -soft \
        $c/chr$c.fa $c/chr$c.fa.out $c/chr$c.fa
      /cluster/home/kent/bin/i386/maskOutFa -softAdd \
        $c/chr$c.fa bed/simpleRepeat/trfMaskChrom/chr$c.bed $c/chr$c.fa
    end
    foreach c (?{,?})
      echo repeat- and trf-masking contigs of chr$c
      foreach ctgFa ($c/chr*/chr${c}_?{,?}.fa)
        set trfMask=bed/simpleRepeat/trfMask/$ctgFa:t:r.bed
        /cluster/home/kent/bin/i386/maskOutFa -soft $ctgFa $ctgFa.out $ctgFa
        /cluster/home/kent/bin/i386/maskOutFa -softAdd $ctgFa $trfMask $ctgFa
      end
    end


# STORE SEQUENCE AND ASSEMBLY INFORMATION (DONE 10/5/03 angie)

    # Translate to nib
    ssh kksilo
    cd /cluster/data/dm1
    mkdir nib
    foreach c (?{,?})
      faToNib -softMask $c/chr$c.fa nib/chr$c.nib
    end

    # Make symbolic links from /gbdb/dm1/nib to the real nibs.
    ssh hgwdev
    mkdir -p /gbdb/dm1/nib
    foreach f (/cluster/data/dm1/nib/chr*.nib)
      ln -s $f /gbdb/dm1/nib
    end

    # Load /gbdb/dm1/nib paths into database and save size info.
    hgsql dm1  < ~/src/hg/lib/chromInfo.sql
    hgNibSeq -preMadeNib dm1 /gbdb/dm1/nib /cluster/data/dm1/?{,?}/chr?{,?}.fa
    echo "select chrom,size from chromInfo" | hgsql -N dm1 \
      > /cluster/data/dm1/chrom.sizes


# CREATING GRP TABLE FOR TRACK GROUPING (DONE 10/5/03 angie)
    # Copy all the data from the table "grp" 
    # in the existing database "rn1" to the new database
    ssh hgwdev
    echo "create table grp (PRIMARY KEY(NAME)) select * from rn1.grp" \
      | hgsql dm1


# MAKE GCPERCENT (DONE 10/5/03 angie)
     ssh hgwdev
     mkdir /cluster/data/dm1/bed/gcPercent
     cd /cluster/data/dm1/bed/gcPercent
     # create and load gcPercent table
     hgsql dm1  < ~/src/hg/lib/gcPercent.sql
     hgGcPercent dm1 ../../nib


# MAKE HGCENTRALTEST ENTRY AND TRACKDB TABLE FOR DROSOPHILA (DONE 10/8/03 angie)
    # Warning: must genome and organism fields must correspond
    # with defaultDb values
    echo 'INSERT INTO dbDb \
        (name, description, nibPath, organism, \
                defaultPos, active, orderKey, genome, scientificName, \
                htmlPath, hgNearOk, hgPbOk, sourceName) values \
        ("dm1", "Jan. 2003", "/gbdb/dm1/nib", "Fruitfly", \
               "chr2L:827700-845800", 1, 55, "Fruitfly", \
                "Drosophila melanogaster", "/gbdb/dm1/html/description.html", \
                0, 0, "BDGP v. 3");' \
      | hgsql -h genome-testdb hgcentraltest
    echo 'INSERT INTO defaultDb (genome, name) values ("Fruitfly", "dm1");' \
      | hgsql -h genome-testdb hgcentraltest

    # Make trackDb table so browser knows what tracks to expect:
    ssh hgwdev
    cd ~/src/hg/makeDb/trackDb
    cvs up -d -P

    # Edit that makefile to add dm1 in all the right places and do
    make update

    # go public on genome-test
    #make alpha
    cvs commit makefile

    # Add trackDb directories
    mkdir drosophila
    mkdir drosophila/dm1
    cvs add drosophila
    cvs add drosophila/dm1
    cvs commit drosophila


# MAKE HGCENTRALTEST BLATSERVERS ENTRY FOR DROSOPHILA (DONE 10/17/03 angie)
    ssh hgwdev
    # Get appropriate hostname from cluster admins
    # 8/26/04: set canPcr=1 for untranslated blat server.
    echo 'insert into blatServers values("dm1", "blat10", 17787, 1, 0); \
          insert into blatServers values("dm1", "blat10", 17786, 0, 1);' \
      | hgsql -h genome-testdb hgcentraltest


# LOAD UP BDGP 3.1 ANNOTATIONS (DONE 10/8/03 angie)
    # fruitfly.org is the Berkeley Drosophila Genome Project.  
    # Their GFF annotations contain FlyBase IDs useful for cross-linking.
    ssh kksilo
    mkdir /cluster/data/dm1/bed/bdgpAnnotations
    cd /cluster/data/dm1/bed/bdgpAnnotations
    # Download all available annotations:
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_3_UTR_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_5_UTR_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_CDS_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_annotation_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_exon_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_intron_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_noncoding-gene_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_protein-coding-gene_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_splice_site_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_tRNA_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_transcript_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_translation_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_transposable_element_dmel_RELEASE3-1.GFF.gz
    gunzip *.gz
    # Protein-coding genes...
    perl -wpe 's/^/chr/; s/translation/CDS/; \
               s/genegrp.*transgrp=(\S+);.*$/$1/; \
               s/genegrp.*transgrp=(\S+)$/$1/;' \
      whole_genome_protein-coding-gene_dmel_RELEASE3-1.GFF \
    > bdgpGene.gff
    # Loading a .tab file caused some lines with super-long go fields 
    # to be skipped.  Generating a .sql file with INSERTs works.
    perl -we 'while (<>) { \
                chop; @fields = split("\t"); \
                if ($fields[2] eq "gene") { \
                  @vars = split("; ", $fields[8]); \
                  $go = "";  $cdna_clone = ""; \
                  foreach $v (@vars) { \
                    @vv = split("=", $v); \
                    if ($vv[0] eq "name") { \
                      $bdgpName = $vv[1]; \
                    } elsif ($vv[0] eq "dbxref") { \
                      if ($vv[1] =~ /^GO:(\d+)/) { \
                        $go .= "$1,"; \
                      } elsif ($vv[1] =~ /FlyBase:(\w+)/) { \
                        $flybase = $1; \
                      } else { \
                        die "unrecognized dbxref $vv[1]"; \
                      } \
                    } elsif ($vv[0] eq "symbol") { \
                      $symbol = $vv[1]; \
                    } elsif ($vv[0] eq "cytorange") { \
                      $cytorange = $vv[1]; \
                    } elsif ($vv[0] eq "cdna_clone") { \
                      $cdna_clone .= "$vv[1],"; \
                    } elsif ($vv[0] eq "genegrp") { \
                    } else { \
                      die "unrecognized var $v" \
                    } \
                  } \
                  print "INSERT INTO bdgpGeneInfo VALUES ( \"$bdgpName\", \"$flybase\", \"$go\", \"$symbol\", \"$cytorange\", \"$cdna_clone\");\n"; \
                } \
              }' \
      whole_genome_protein-coding-gene_dmel_RELEASE3-1.GFF \
    > bdgpGeneInfo.sql
    # Proteins for coding genes:
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/FASTA/whole_genome_translation_dmel_RELEASE3-1.FASTA.gz
    gunzip -c whole_genome_translation_dmel_RELEASE3-1.FASTA.gz \
    | perl -wpe 's/^>(pp-)*(\w+)-\w(\w).*/>$2-R$3/' \
    > bdgpGenePep.fa
    # load up coding genes, proteins and assoc. info:
    ssh hgwdev
    ldHgGene dm1 bdgpGene /cluster/data/dm1/bed/bdgpAnnotations/bdgpGene.gff
    hgPepPred dm1 generic bdgpGenePep \
      /cluster/data/dm1/bed/bdgpAnnotations/bdgpGenePep.fa
    hgsql dm1 < $HOME/src/hg/lib/bdgpGeneInfo.sql
    hgsql dm1 < /cluster/data/dm1/bed/bdgpAnnotations/bdgpGeneInfo.sql
    # Non-coding genes...
    perl -wpe 's/^/chr/; \
               s/genegrp.*transgrp=(\S+);.*$/$1/; \
               s/genegrp.*transgrp=(\S+)$/$1/;' \
      whole_genome_noncoding-gene_dmel_RELEASE3-1.GFF \
    > bdgpNonCoding.gff
    sed -e 's/bdgpGeneInfo/bdgpNonCodingInfo/' \
      ~/kent/src/hg/lib/bdgpGeneInfo.sql \
      > bdgpNonCodingInfo.sql
    perl -we 'while (<>) { \
                chop; @fields = split("\t"); \
                if (($fields[2] ne "exon") && \
                    ($fields[2] ne "transcript") && \
                    ($fields[2] ne "translation")) { \
                  @vars = split("; ", $fields[8]); \
                  $go = "";  $cdna_clone = ""; \
                  foreach $v (@vars) { \
                    @vv = split("=", $v); \
                    if ($vv[0] eq "name") { \
                      $bdgpName = $vv[1]; \
                    } elsif ($vv[0] eq "dbxref") { \
                      if ($vv[1] =~ /^GO:(\d+)/) { \
                        $go .= "$1,"; \
                      } elsif ($vv[1] =~ /FlyBase:(\w+)/) { \
                        $flybase = $1; \
                      } else { \
                        die "unrecognized dbxref $vv[1]"; \
                      } \
                    } elsif ($vv[0] eq "symbol") { \
                      $symbol = $vv[1]; \
                    } elsif ($vv[0] eq "cytorange") { \
                      $cytorange = $vv[1]; \
                    } elsif ($vv[0] eq "cdna_clone") { \
                      $cdna_clone .= "$vv[1],"; \
                    } elsif ($vv[0] eq "genegrp") { \
                    } else { \
                      die "unrecognized var $v" \
                    } \
                  } \
                  print "INSERT INTO bdgpNonCodingInfo VALUES ( \"$bdgpName\", \"$flybase\", \"$go\", \"$symbol\", \"$cytorange\", \"$cdna_clone\");\n"; \
                } \
              }' \
      whole_genome_noncoding-gene_dmel_RELEASE3-1.GFF \
    >> bdgpNonCodingInfo.sql
    ssh hgwdev
    ldHgGene dm1 bdgpNonCoding \
      /cluster/data/dm1/bed/bdgpAnnotations/bdgpNonCoding.gff
    hgsql dm1 < /cluster/data/dm1/bed/bdgpAnnotations/bdgpNonCodingInfo.sql


# FLYBASE GENES (REDONE 6/28/04 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/flyBase
    cd /cluster/data/dm1/bed/flyBase
    wget -O genes.txt.040617 \
      ftp://flybase.bio.indiana.edu/flybase/genes/genes.txt
    # Had to edit genes.txt.040617 to get around these two lines that didn't 
    # start with acode symbols:
    #line 258894
#Allele class: hypom
    #line 459025 (459026 of original)
#uncertain *u FBan0017679; annotated data are available for this gene.
    #line 3128725 (3128726 of original), for Trn-SR / FBgn0031456:
#%f nu
    hgFlyBase dm1 genes.txt.040617


# BDGP GENE DISRUPTION PROJECT/PSCREEN (DONE 7/29/04 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/pscreen
    cd /cluster/data/dm1/bed/pscreen
    # Robin Hiesinger emailed 2 table dumps that need to be joined on ID;
    # saved as atbloom.dump, genetag_gdp.dump.
    # Turns out that this file has a more complete mapping than atbloom --
    # use it to fill in stock numbers missing from atbloom:
    wget http://flybase.bio.indiana.edu/stocks/stock-centers/bloomington/lk/bloomington.csv
    dos2unix atbloom.dump
    # one-shot Perl script:
    chmod a+x mkPscreen.pl
    mkPscreen.pl genetag_gdp.dump atbloom.dump bloomington.csv > pscreen.bed
    hgLoadBed dm1 pscreen -sqlTable=$HOME/kent/src/hg/lib/pscreen.sql -tab \
      pscreen.bed
    # 8/13/04: Galt found some duplicate IDs and Robin/Bob(?) suggested 
    # fixing them as follows:
    hgsql dm1 -e '\
      delete from pscreen where name = "KG05017" limit 1; \
      delete from pscreen where name = "EY14401" or name = "PA07741"; \
      delete from pscreen where name = "EY14474" and chromEnd = "8975785"; \
      delete from pscreen where name = "KV00692" and chromEnd = "4702749";'
    # 8/26/04: Drop geneDeltas column due to incomplete / possibly stale 
    # data.  (removed it from pscreen.as & derivatives, hgc.c first)
    # Also remove 3 items mapped off the end of chrX.
    hgsql dm1 -e 'alter table pscreen drop column geneDeltas;'
    hgsql dm1 -e '\
      delete from pscreen where name = "KG03740"; \
      delete from pscreen where name = "BG01196"; \
      delete from pscreen where name = "EY02003";'
    # 9/16/04: Bob Levis emailed an update... a bunch of deletions, 
    # 1 change and 2 additions:
    # (see email text and UCSC_changes_091604v2.csv)
    hgsql dm1 -e '\
      delete from pscreen where name = "BG02404"; \
      delete from pscreen where name = "EY03152"; \
      delete from pscreen where name = "EY05239"; \
      delete from pscreen where name = "EY05534"; \
      delete from pscreen where name = "EY07100"; \
      delete from pscreen where name = "EY09238"; \
      delete from pscreen where name = "EY11132"; \
      delete from pscreen where name = "EY11235"; \
      delete from pscreen where name = "EY11805"; \
      delete from pscreen where name = "EY12014"; \
      delete from pscreen where name = "EY12019"; \
      delete from pscreen where name = "EY12603"; \
      delete from pscreen where name = "EY13113"; \
      delete from pscreen where name = "EY13711"; \
      delete from pscreen where name = "EY14401"; \
      delete from pscreen where name = "KG03002"; \
      delete from pscreen where name = "KG04889";'
    hgsql dm1 -e 'update pscreen set geneIds = "CG33472," where name = "EY04063";'
    hgsql dm1 -e 'update pscreen set chrom = "chr2L" where name = "EY11972";'
    hgsql dm1 -e 'insert into pscreen values(672, "chrX", 11430596, 11430597, "EY12008", 0, "+", 0, 1, "CG1830,");'
    hgsql dm1 -e 'insert into pscreen values(720, "chr2R", 17764291, 17764292, "EY12526", 0, "-", 0, 1, "CG30092,");'


# FLYBASE IN SITU IMAGES / EXPRESSION (REDONE 6/17/04 angie)
    # FlyBase has downloadable in situ images for BACs:
    # ftp://flybase.net/flybase/images/bac_insitu_pic/*
    # and FBti's:
    # ftp://flybase.net/flybase/images/in-situ-images/insitus.zip
    # but what Jim is interested in is the insitus for expression data.
    # Don't see that in the ftp listing, but they do make it easy to link in.
    ssh hgwdev
    cd /cluster/data/dm1/bed/flyBase
    wget -O summary.txt.040617 \
    'http://www.fruitfly.org/cgi-bin/ex/bquery.pl?qpage=entry&qtype=summarytext'
    hgsql dm1 < ~/kent/src/hg/lib/bdgpExprLink.sql
    hgsql dm1 -e \
      'load data local infile "summary.txt.040617" into table bdgpExprLink'


# SWISSPROT-FLYBASE CROSS-REFERENCING  (REDONE 6/17/04 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/flyBaseSwissProt
    cd /cluster/data/dm1/bed/flyBaseSwissProt
    echo "select extAcc1,acc from extDbRef,extDb where extDbRef.extDb = extDb.id and extDb.val = 'flybase'" \
    | hgsql -N sp040515 \
    > fbSpAcc.tab
    ssh kksilo
    cd /cluster/data/dm1/bed/flyBaseSwissProt
    wget ftp://ftp.ebi.ac.uk/pub/databases/SPproteomes/fasta_files/proteomes/7227.FASTAC
    # Some of those SwissProt "names" are > 255 chars!  trim the few long ones.
    perl -we 'open(F, "fbSpAcc.tab") || die; \
              %sp2fb = (); \
              while (<F>) { \
                chop;  @words = split("\t"); \
                $sp2fb{$words[1]} = $words[0]; \
              } \
              close(F); \
              while (<>) { \
                if (/^>(\w+)\s+\((\w+)\)\s+(.*)/) { \
                  $fbAcc = $sp2fb{$2}; \
                  $spAcc = $3; \
                  $spAcc = substr($3, 0, 250) . "..." if (length($3) > 255); \
                  print "$fbAcc\t$2\t$spAcc\t$1\n" if (defined $fbAcc); \
                } \
              }' \
      7227.FASTAC \
    > flyBaseSwissProt.tab
    rm 7227.FASTAC
    ssh hgwdev
    hgsql dm1 < $HOME/src/hg/lib/flyBaseSwissProt.sql
    echo 'load data local infile "/cluster/data/dm1/bed/flyBaseSwissProt/flyBaseSwissProt.tab" into table flyBaseSwissProt' \
    | hgsql dm1


# AUTO UPDATE GENBANK MRNA RUN  (DONE 10/9/03 angie)

    # Put the nib's on /cluster/bluearc:
    ssh kksilo
    mkdir /cluster/bluearc/drosophila
    mkdir /cluster/bluearc/drosophila/dm1
    cp -pR /cluster/data/dm1/nib /cluster/bluearc/drosophila/dm1

    # Instructions for setting up incremental genbank updates are here:
    # http://www.soe.ucsc.edu/~markd/genbank-update/doc/initial-load.html
    # This time around, Markd handled adding the new species to gbGenome.c 
    # because it's not yet in the kent tree.  

    # Edit /cluster/data/genbank/etc/genbank.conf and add:
# dm1
dm1.genome = /cluster/bluearc/drosophila/dm1/nib/chr*.nib
dm1.lift = /cluster/data/dm1/jkStuff/liftAll.lft
dm1.genbank.mrna.xeno.load = yes
dm1.genbank.est.xeno.load = no
dm1.downloadDir = dm1

    ssh eieio
    cd /cluster/data/genbank
    # This is an -initial run, mRNA only:
    nice bin/gbAlignStep -iserver=no -clusterRootDir=/cluster/bluearc/genbank \
      -srcDb=genbank -type=mrna -verbose=1 -initial dm1

    # Load the results from the above
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 -drop -initialLoad dm1

    ssh eieio
    # To get this next one started, the work directory of the initial run 
    # needs to be moved out of the way.  
    rm -r /cluster/bluearc/genbank/work/initial.dm1
    # Now align refseqs:
    cd /cluster/data/genbank
    nice bin/gbAlignStep -iserver=no -clusterRootDir=/cluster/bluearc/genbank \
      -srcDb=refseq -type=mrna -verbose=1 -initial dm1
    # Load results:
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 dm1

    ssh eieio
    # To get this next one started, the work directory of the initial run 
    # needs to be moved out of the way.  
    rm -r /cluster/bluearc/genbank/work/initial.dm1
    # Now align ESTs:
    nice bin/gbAlignStep -iserver=no -clusterRootDir=/cluster/bluearc/genbank \
      -srcDb=genbank -type=est -verbose=1 -initial dm1
    # Load results:
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 dm1
    # Clean up:
    rm -r /cluster/bluearc/genbank/work/initial.dm1


# PUT NIBS ON ISCRATCH (DONE 10/9/03 angie)
    ssh kkr1u00
    cd /iscratch/i/dm1
    cp -pR /cluster/data/dm1/nib .
    iSync


# PRODUCING FUGU FISH ALIGNMENTS  (DONE 10/9/03 angie)
    # Assumes masked NIBs have been prepared as above
    # and Fugu pieces are already on kluster /iscratch/i.
    # next machine
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blatFugu
    cd /cluster/data/dm1/bed/blatFugu
    mkdir psl
    ls -1S /iscratch/i/fugu/*.fa > fugu.lst
    ls -1S /iscratch/i/dm1/nib/chr*.nib > fly.lst
    cat << '_EOF_' > gsub
#LOOP
/cluster/bin/i386/blat -q=dnax -t=dnax -mask=lower {check in exists+ $(path1)} {check in line+ $(path2)} {check out line+ psl/$(root1)_$(root2).psl}
#ENDLOOP
'_EOF_'
    # << this line keeps emacs coloring happy
    gensub2 fly.lst fugu.lst gsub spec
    para create spec
    para try
    para check
    para push
    para check
#Completed: 1408 of 1408 jobs
#Average job time:                 217s       3.61m     0.06h    0.00d
#Longest job:                     1431s      23.85m     0.40h    0.02d
#Submission to last job:          1433s      23.88m     0.40h    0.02d

    # When cluster run is done, sort alignments
    # append _blatFugu to chrom for .psl file names.
    ssh kksilo
    cd /cluster/data/dm1/bed/blatFugu
    foreach c (2L 2R 2h 3L 3R 3h 4 X Xh Yh U)
      echo -n "chr${c} "
      pslCat psl/chr${c}_*.psl \
      | pslSortAcc nohead chrom temp stdin
      rm -f chrom/chr${c}_blatFugu.psl
      mv chrom/chr${c}.psl chrom/chr${c}_blatFugu.psl
    end

    # Load alignments
    ssh hgwdev
    cd /cluster/data/dm1/bed/blatFugu/chrom
    hgLoadPsl -noTNameIx dm1 chr*_blatFugu.psl

    # Make fugu /gbdb/ symlink and load Fugu sequence data.
    mkdir /gbdb/dm1/fuguSeq
    ln -s /cluster/store3/fuguSeq/fugu_v3_mask.fasta /gbdb/dm1/fuguSeq
    # ! ! !  DO NOT RUN hgLoadSeq in /gbdb - it leaves .tab files
    cd /cluster/data/dm1/bed/blatFugu
    hgLoadSeq dm1 /gbdb/dm1/fuguSeq/fugu_v3_mask.fasta


# BLASTZ D.PSEUDOOBSCURA (DONE 8/3/04 angie)
    ssh kksilo
    mkdir /cluster/data/dm1/bed/blastz.dp2.2004-08-03
    cd /cluster/data/dm1/bed/blastz.dp2.2004-08-03
    ln -s blastz.dp2.2004-08-03 /cluster/data/dm1/bed/blastz.dp2
    cat << '_EOF_' > DEF
# D.melanogaster vs. D.pseudoobscura
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
BLASTZ_ABRIDGE_REPEATS=0

# TARGET - D. melanogaster
SEQ1_DIR=/iscratch/i/dm1/nib
# unused: SEQ1_RMSK=
SEQ1_SMSK=
SEQ1_FLAG=-drosophila
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY - D. pseudoobscura
SEQ2_DIR=/iscratch/i/dp2/nib
# unused: SEQ2_RMSK=
SEQ2_SMSK=
SEQ2_FLAG=-drosophila
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/cluster/data/dm1/bed/blastz.dp2.2004-08-03

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len

'_EOF_'
    # << this line keeps emacs coloring happy
    # run bash shell if you don't already:
    bash
    source DEF
    mkdir run
    ~angie/hummus/make-joblist $DEF > $BASE/run/j
    sh ./xdir.sh
    cd run
    sed -e 's#^#/cluster/bin/penn/#' j > j2
    wc -l j*
    head j2
    mv j2 j
    # cluster run
    ssh kk
    cd /cluster/data/dm1/bed/blastz.dp2.2004-08-03/run
    para create j
    para try, check, push, check, ....
#Completed: 15939 of 15939 jobs
#Average job time:                  13s       0.21m     0.00h    0.00d
#Longest job:                      294s       4.90m     0.08h    0.00d
#Submission to last job:          1118s      18.63m     0.31h    0.01d

    # back in the bash shell on kksilo...
    mkdir /cluster/data/dm1/bed/blastz.dp2.2004-08-03/run.1
    cd /cluster/data/dm1/bed/blastz.dp2.2004-08-03/run.1
    /cluster/bin/scripts/blastz-make-out2lav $DEF $BASE > j
    # small cluster run
    ssh kki
    cd /cluster/data/dm1/bed/blastz.dp2.2004-08-03/run.1
    para create j
    para try, check, push, check, ....
#Completed: 21 of 21 jobs
#Average job time:                  18s       0.30m     0.01h    0.00d
#Longest job:                       25s       0.42m     0.01h    0.00d
#Submission to last job:            55s       0.92m     0.02h    0.00d
    cd ..
    rm -r raw

    # Translate .lav to axt, with dp2 in scaffold coords for collaborators:
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.dp2.2004-08-03
    mkdir axtChrom
    foreach c (lav/*)
      pushd $c
      set chr=$c:t
      set out=axtChrom/$chr.axt
      echo "Translating $chr lav to $out"
      cat `ls -1 *.lav | sort -g` \
        | lavToAxt stdin /cluster/data/dm1/nib /cluster/data/dp2/nib stdout \
        | axtSort stdin ../../$out
      popd
    end


# CHAIN YAKUBA BLASTZ (DONE 8/3/04 angie)
    # Run axtChain on little cluster
    ssh kki
    cd /cluster/data/dm1/bed/blastz.dp2.2004-08-03
    mkdir -p axtChain/run1
    cd axtChain/run1
    mkdir out chain
    ls -1S /cluster/data/dm1/bed/blastz.dp2.2004-08-03/axtChrom/*.axt \
      > input.lst
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out exists out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    cat << '_EOF_' > doChain
#!/bin/csh
axtChain -verbose=0 $1 \
  /iscratch/i/dm1/nib \
  /iscratch/i/dp2/nib $2 > $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doChain
    gensub2 input.lst single gsub jobList
    para create jobList
    para try, check, push, check...
#Completed: 11 of 11 jobs
#Average job time:                   8s       0.13m     0.00h    0.00d
#Longest job:                       14s       0.23m     0.00h    0.00d
#Submission to last job:            14s       0.23m     0.00h    0.00d

    # now on the cluster server, sort chains
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.dp2.2004-08-03/axtChain
    chainMergeSort run1/chain/*.chain > all.chain
    chainSplit chain all.chain
    rm run1/chain/*.chain

    # take a look at score distr's
    foreach f (chain/*.chain)
      grep chain $f | awk '{print $2;}' | sort -nr > /tmp/score.$f:t:r
      echo $f:t:r
      textHistogram -binSize=10000 /tmp/score.$f:t:r
      echo ""
    end

    # Load chains into database
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.dp2.2004-08-03/axtChain/chain
    foreach i (*.chain)
        set c = $i:r
        echo loading $c
        hgLoadChain dm1 ${c}_chainDp2 $i
    end


# NET PSEUDOOBSCURA BLASTZ (DONE 8/3/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.dp2.2004-08-03/axtChain
    chainPreNet all.chain ../S1.len ../S2.len stdout \
    | chainNet stdin -minSpace=1 ../S1.len ../S2.len stdout /dev/null \
    | netSyntenic stdin noClass.net

    # Add classification info using db tables:
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.dp2.2004-08-03/axtChain
    netClass -noAr noClass.net dm1 dp2 pseudoobscura.net \
    |& g -v "table gap doesn't exist"

    # Make a 'syntenic' subset:
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.dp2.2004-08-03/axtChain
    rm noClass.net
    netFilter -syn pseudoobscura.net > pseudoobscuraSyn.net

    # Load the nets into database 
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.dp2.2004-08-03/axtChain
    netFilter -minGap=10 pseudoobscura.net |  hgLoadNet dm1 netDp2 stdin
    netFilter -minGap=10 pseudoobscuraSyn.net \
    | hgLoadNet dm1 netSyntenyDp2 stdin


# MAKE VSDP2 DOWNLOADABLES (DONE 8/17/04 angie)
    ssh hgwdev
    mkdir /usr/local/apache/htdocs/goldenPath/dm1/vsDp2
    cd /usr/local/apache/htdocs/goldenPath/dm1/vsDp2
    gzip -c \
      /cluster/data/dm1/bed/blastz.dp2.2004-08-03/axtChain/all.chain \
      > pseudoobscura.chain.gz
    gzip -c \
      /cluster/data/dm1/bed/blastz.dp2.2004-08-03/axtChain/pseudoobscura.net \
      > pseudoobscura.net.gz
    mkdir axtNet
    foreach f (/cluster/data/dm1/bed/blastz.dp2.2004-08-03/axtNet/chr*axt)
      gzip -c $f > axtNet/$f:t.gz
    end
    md5sum *.gz */*.gz > md5sum.txt
    # Make a README.txt which explains the files & formats.


PRODUCING GENSCAN PREDICTIONS (TODO 11/14/03 angie)
    # Run on small cluster -- genscan needs big mem.
    ssh kkr1u00
    mkdir /cluster/data/dm1/bed/genscan
    cd /cluster/data/dm1/bed/genscan
    # Make 3 subdirectories for genscan to put their output files in
    mkdir gtf pep subopt
    # Make hard-masked contigs
    foreach f (/cluster/data/dm1/?{,?}/chr*/chr?{,?}_?{,?}.fa)
      maskOutFa $f hard $f.masked
    end
    # Generate a list file, contigs.list, of all the hard-masked contigs that 
    # *do not* consist of all-N's (which would cause genscan to blow up)
    rm -f contigs.list
    touch contigs.list
    foreach f ( `ls -1S /cluster/data/dm1/?{,?}/chr*/chr?{,?}{,_random}_?{,?}.fa.masked` )
      egrep '[ACGT]' $f > /dev/null
      if ($status == 0) echo $f >> contigs.list
    end
    cat << '_EOF_' > gsub
#LOOP
/cluster/bin/i386/gsBig {check in line+ $(path1)} {check out line gtf/$(root1).gtf} -trans={check out line pep/$(root1).pep} -subopt={check out line subopt/$(root1).bed} -exe=/cluster/home/fanhsu/projects/compbio/bin/genscan-linux/genscan -par=/cluster/home/fanhsu/projects/compbio/bin/genscan-linux/HumanIso.smat -tmp=/tmp -window=2400000
#ENDLOOP
'_EOF_'
    # << this line keeps emacs coloring happy
    gensub2 contigs.list single gsub jobList
    para create jobList
    para try
    para check
    para push
#Completed: 79 of 79 jobs
#Average job time:                 136s       2.26m     0.04h    0.00d
#Longest job:                      229s       3.82m     0.06h    0.00d
#Submission to last job:          1718s      28.63m     0.48h    0.02d

    # If there are crashes, diagnose with "para problems".  
    # If a job crashes due to genscan running out of memory, re-run it 
    # manually with "-window=1200000" instead of "-window=2400000".
    # chr14_21, chr16_4
    
    # Convert these to chromosome level files as so:
    ssh kksilo
    cd /cluster/data/dm1/bed/genscan
    liftUp genscan.gtf ../../jkStuff/liftAll.lft warn gtf/*.gtf
    liftUp genscanSubopt.bed ../../jkStuff/liftAll.lft warn subopt/*.bed
    cat pep/*.pep > genscan.pep

    # Load into the database as so:
    ssh hgwdev
    cd /cluster/data/dm1/bed/genscan
    ldHgGene dm1 genscan genscan.gtf
    hgPepPred dm1 generic genscanPep genscan.pep
    hgLoadBed dm1 genscanSubopt genscanSubopt.bed


# EXTEND BDGPGENE AND CREATE BDGPNEAR FOR HGNEAR (REDONE 6/18/04 angie)
    ssh hgwdev
    cd /cluster/data/dm1/bed/bdgpAnnotations.3.2
    cp ~/kent/src/hg/lib/bdgpSwissProt.sql .
    perl -we '%bName2all = (); \
              %bName2fb = (); \
              open(P, "echo \"select * from bdgpGeneInfo\" | hgsql -N dm1|") \
                || die; \
              while (<P>) { \
                chop; my @words = split("\t"); \
                $bName2fb{$words[0]} = \@words; \
              } \
              close(P); \
              open(P, "echo \"select bdgpGeneInfo.*,flyBaseSwissProt.* from bdgpGeneInfo,flyBaseSwissProt where bdgpGeneInfo.flyBaseId = flyBaseSwissProt.flyBaseId\" | hgsql -N dm1|") || die; \
              while (<P>) { \
                chop; my @words = split("\t"); \
                $bName2all{$words[0]} = \@words; \
              } \
              close(P); \
              open(P, "echo select name from bdgpGene | hgsql -N dm1|") ||die; \
              while (<P>) { \
                chop; $name = $_; \
                $bName = $name;  $bName =~ s/-R.*//; \
                if (exists($bName2all{$bName})) { \
                  ($bName, $fbID, $go, $symbol, $cyto, undef, \
                   undef, $spID, $spDesc, $spSymb) = @{$bName2all{$bName}}; \
                  print "INSERT INTO bdgpSwissProt VALUES ( \"$name\", \"$fbID\", \"$go\", \"$symbol\", \"$cyto\", \"$spID\", \"$spDesc\", \"$spSymb\");\n"; \
                } elsif (exists($bName2fb{$bName})) { \
                  ($bName, $fbID, $go, $symbol, $cyto, undef) = @{$bName2fb{$bName}}; \
                  $go = "" if (not defined $go); \
                  $cyto = "" if (not defined $cyto); \
                  print "INSERT INTO bdgpSwissProt VALUES ( \"$name\", \"$fbID\", \"$go\", \"$symbol\", \"$cyto\", \"n/a\", \"n/a\", \"n/a\");\n"; \
                } else { die "No info for $name."; } \
              } \
              close(P); ' \
    >> bdgpSwissProt.sql
    hgsql dm1 < bdgpSwissProt.sql

    # Use the above table to add a proteinID field to bdgpGene.  
    hgsql dm1 -e 'create table bdgpGene2 \
                  select bdgpGene.*, bdgpSwissProt.swissProtId as proteinID \
                  from bdgpGene, bdgpSwissProt \
                  where bdgpGene.name = bdgpSwissProt.bdgpName'

    # Now examine bdgpGene2 vs. bdgpGene manually, carefully.  
    # Do they have the same # rows?  
    hgsql dm1 -N -e 'select count(*) from bdgpGene'
    hgsql dm1 -N -e 'select count(*) from bdgpGene2'
    # Are most proteinID fields non-"n/a"? 
    hgsql dm1 -N -e 'select count(*) from bdgpGene2 where proteinID = "n/a"'
    # Spot-check some genes... are the fields of bdgpGene2 identical to 
    # the fields of bdgpGene, except for the new proteinIDs?  
    hgsql dm1 -N -e 'select * from bdgpGene limit 3'
    hgsql dm1 -N -e 'select * from bdgpGene2 limit 3'
    # If so, then go ahead:
    hgsql dm1 -e 'drop table bdgpGene; rename table bdgpGene2 to bdgpGene;'
    # and check the new bdgpGene manually.


#############################################################################
# MAKE HGNEAR
# adapted from makeHgNear.doc; split into sections.  See makeHgFixed.doc 
# for how Arbeitman et al's fly lifecycle expression data were loaded into 
# hgFixed.  

# BLASTP SELF, CLUSTER GENES, MAP TO EXP.DATA FOR HGNEAR (REDONE 6/18/04 angie)
    ssh hgwdev
    # Now that bdgpGene has proteinID, use hgClusterGenes to cluster
    # together various alt-splicing isoforms, creating the tables
    # bdgpIsoforms and bdgpCanonical.  
    hgClusterGenes dm1 bdgpGene bdgpIsoforms bdgpCanonical
    # Extract peptides from bdgpGenes into fasta file
    # and create a blast database out of them.
    mkdir /cluster/data/dm1/bed/blastp
    cd /cluster/data/dm1/bed/blastp
    pepPredToFa dm1 bdgpGenePep bdgp.faa
    formatdb -i bdgp.faa -t bdgp -n bdgp

    # Copy over database to iscratch/i
    ssh kkr1u00
    if (-e /iscratch/i/dm1/blastp) then
      rm -r /iscratch/i/dm1/blastp
    endif
    mkdir -p /iscratch/i/dm1/blastp
    cp /cluster/data/dm1/bed/blastp/bdgp.* /iscratch/i/dm1/blastp

    # Load up iscratch/i with blastp and related files
    # if necessary
    if (! -e /iscratch/i/blast/blastall) then
      mkdir -p /iscratch/i/blast
      cp /projects/compbio/bin/i686/blastall /iscratch/i/blast
      mkdir -p /iscratch/i/blast/data
      cp /projects/compbio/bin/i686/data/* /iscratch/i/blast/data
    endif
    iSync

    # Split up fasta file into bite sized chunks for cluster
    ssh kksilo
    cd /cluster/data/dm1/bed/blastp
    mkdir split
    faSplit sequence bdgp.faa 6000 split/bg

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/self/run/out
    cd /cluster/data/dm1/bed/blastp/self/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/dm1/blastp/bdgp -i \$1 -o \$2 -e 0.01 -m 8 -b 1000
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5821 of 5821 jobs
#Average job time:                  10s       0.16m     0.00h    0.00d
#Longest job:                      326s       5.43m     0.09h    0.00d
#Submission to last job:           655s      10.92m     0.18h    0.01d

    # Load into database.  This took only ~3 minutes for dm1.
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/self/run/out
    time hgLoadBlastTab dm1 bdgpBlastTab *.tab
    # Create table that maps between bdgp genes and RefSeq
    hgMapToGene dm1 refGene bdgpGene bdgpToRefSeq
    # Create table that maps between bdgp genes and LocusLink
    echo "select mrnaAcc,locusLinkId from refLink" | hgsql -N dm1 > refToLl.txt
### NOT DONE (LocusLink info temporarily missing from genbank)
    hgMapToGene dm1 refGene bdgpGene bdgpToLocusLink -lookup=refToLl.txt
###
    # Create table that maps between known genes and Pfam domains
    hgMapViaSwissProt dm1 bdgpGene name proteinID Pfam bdgpToPfam

    # Create a table that maps BDGP root names to canonical transcripts:
    cd /cluster/data/dm1/bed/blastp
    cat > bdgpToCanonical.sql <<end
CREATE TABLE bdgpToCanonical (
  name varchar(255) NOT NULL default '',
  value varchar(255) NOT NULL default '',
  KEY name (name(16)),
  KEY value (value(16))
) TYPE=MyISAM;
end
    perl -we 'open(P, "echo select transcript from bdgpCanonical | hgsql dm1 -N |") || die; \
              while (<P>) { \
                chop; $name = $value = $_; $value =~ s/-R.$//; \
                print "INSERT INTO bdgpToCanonical VALUES (\"$name\", \"$value\");\n"; \
              } \
              close(P);' \
      >> bdgpToCanonical.sql
    hgsql dm1 < bdgpToCanonical.sql
    # Create a table that maps between bdgp genes and the 
    # Stanford Microarray Project expression data. (see makeHgFixed.doc)
    hgExpDistance -lookup=bdgpToCanonical \
      dm1 hgFixed.arbFlyLifeMedianRatio dummyArg arbExpDistance

    # Make sure that GO database is up to date.
    See README in /cluster/store1/geneOntology.


# C.ELEGANS BLASTP FOR HGNEAR (REDONE 6/17/04 angie)
    # Make C. elegans ortholog column using blastp on wormpep.
    # First make C. elegans protein database and copy it to iscratch/i
    # if it doesn't exist already:
    ssh eieio
    mkdir /cluster/data/ce2/bed/blastp
    cd /cluster/data/ce2/bed/blastp
    # Point a web browser at ftp://ftp.sanger.ac.uk/pub/databases/wormpep/
    # to find out the latest version.  Then use that in place of 126 below.
    wget -O wormPep126.faa \
      ftp://ftp.sanger.ac.uk/pub/databases/wormpep/wormpep126/wormpep
    formatdb -i wormPep126.faa -t wormPep126 -n wormPep126
    ssh kkr1u00
    if (-e /iscratch/i/ce2/blastp) then
      rm -r /iscratch/i/ce2/blastp
    endif
    mkdir -p /iscratch/i/ce2/blastp
    cp /cluster/data/ce2/bed/blastp/wormPep126.p?? /iscratch/i/ce2/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/ce2/run/out
    cd /cluster/data/dm1/bed/blastp/ce2/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/ce2/blastp/wormPep126 -i \$1 -o \$2 -e 0.01 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5821 of 5821 jobs
#Average job time:                   8s       0.14m     0.00h    0.00d
#Longest job:                      156s       2.60m     0.04h    0.00d
#Submission to last job:           156s       2.60m     0.04h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/ce2/run/out
    hgLoadBlastTab dm1 ceBlastTab -maxPer=1 *.tab


# MOUSE BLASTP FOR HGNEAR (REDONE 6/17/04 angie)
    # Make mouse ortholog column using blastp on mouse known genes.
    # First make mouse protein database and copy it to iscratch/i
    # if it doesn't exist already:
    ssh hgwdev
    mkdir /cluster/data/mm4/bed/blastp
    cd /cluster/data/mm4/bed/blastp
    pepPredToFa mm4 knownGenePep known.faa
    formatdb -i known.faa -t known -n known
    ssh kkr1u00
    if (-e /iscratch/i/mm4/blastp) then
      rm -r /iscratch/i/mm4/blastp
    endif
    mkdir -p /iscratch/i/mm4/blastp
    cp -p /cluster/data/mm4/bed/blastp/known.p?? /iscratch/i/mm4/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/mm4/run/out
    cd /cluster/data/dm1/bed/blastp/mm4/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/mm4/blastp/known -i \$1 -o \$2 -e 0.001 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5821 of 5821 jobs
#Average job time:                  12s       0.20m     0.00h    0.00d
#Longest job:                      267s       4.45m     0.07h    0.00d
#Submission to last job:           267s       4.45m     0.07h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/mm4/run/out
    hgLoadBlastTab dm1 mmBlastTab -maxPer=1 *.tab


# HUMAN BLASTP FOR HGNEAR (REDONE 6/17/04 angie)
    # Make human ortholog column using blastp on human known genes.
    # First make human protein database and copy it to iscratch/i
    # if it doesn't exist already:
    mkdir /cluster/data/hg16/bed/blastp
    cd /cluster/data/hg16/bed/blastp
    pepPredToFa hg16 knownGenePep known.faa
    formatdb -i known.faa -t known -n known
    ssh kkr1u00
    if (-e /iscratch/i/hg16/blastp) then
      rm -r /iscratch/i/hg16/blastp
    endif
    mkdir -p /iscratch/i/hg16/blastp
    cp /cluster/data/hg16/bed/blastp/known.p?? /iscratch/i/hg16/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/hg16/run/out
    cd /cluster/data/dm1/bed/blastp/hg16/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/hg16/blastp/known -i \$1 -o \$2 -e 0.001 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5821 of 5821 jobs
#Average job time:                  14s       0.24m     0.00h    0.00d
#Longest job:                      327s       5.45m     0.09h    0.00d
#Submission to last job:           327s       5.45m     0.09h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/hg16/run/out
    hgLoadBlastTab dm1 hgBlastTab -maxPer=1 *.tab


# ZEBRAFISH BLASTP FOR HGNEAR (REDONE 6/17/04 angie)
    # Make Danio rerio (zebrafish) ortholog column using blastp on Ensembl.
    # First make protein database and copy it to iscratch/i
    # if it doesn't exist already:
    ssh kkstore
    mkdir /cluster/data/danRer1/bed/blastp
    cd /cluster/data/danRer1/bed/blastp
    wget ftp://ftp.ensembl.org/pub/current_zebrafish/data/fasta/pep/Danio_rerio.ZFISH3.may.pep.fa.gz 
    zcat Dan*.pep.fa.gz > ensembl.faa
    formatdb -i ensembl.faa -t ensembl -n ensembl
    ssh kkr1u00
    if (-e /iscratch/i/danRer1/blastp) then
      rm -r /iscratch/i/danRer1/blastp
    endif
    mkdir -p /iscratch/i/danRer1/blastp
    cp /cluster/data/danRer1/bed/blastp/ensembl.p?? /iscratch/i/danRer1/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/danRer1/run/out
    cd /cluster/data/dm1/bed/blastp/danRer1/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/danRer1/blastp/ensembl -i \$1 -o \$2 -e 0.005 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5821 of 5821 jobs
#Average job time:                  12s       0.20m     0.00h    0.00d
#Longest job:                      273s       4.55m     0.08h    0.00d
#Submission to last job:           273s       4.55m     0.08h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/danRer1/run/out
    hgLoadBlastTab dm1 drBlastTab -maxPer=1 *.tab


# YEAST BLASTP FOR HGNEAR (REDONE 6/17/04 angie)
    # Make Saccharomyces cerevisiae (yeast) ortholog column using blastp on 
    # RefSeq.  First make protein database and copy it to iscratch/i
    # if it doesn't exist already:
    mkdir /cluster/data/sacCer1/bed/blastp
    cd /cluster/data/sacCer1/bed/blastp
    wget ftp://genome-ftp.stanford.edu/pub/yeast/data_download/sequence/genomic_sequence/orf_protein/orf_trans.fasta.gz
    zcat orf_trans.fasta.gz > sgdPep.faa
    formatdb -i sgdPep.faa -t sgdPep -n sgdPep
    ssh kkr1u00
    # Note: sacCer1 is a name conflict with SARS coronavirus... oh well, 
    # fortunately we won't be looking for homologs there.  :)
    if (-e /iscratch/i/sacCer1/blastp) then
      rm -r /iscratch/i/sacCer1/blastp
    endif
    mkdir -p /iscratch/i/sacCer1/blastp
    cp /cluster/data/sacCer1/bed/blastp/sgdPep.p?? /iscratch/i/sacCer1/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/sacCer1/run/out
    cd /cluster/data/dm1/bed/blastp/sacCer1/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/sacCer1/blastp/sgdPep -i \$1 -o \$2 -e 0.01 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5821 of 5821 jobs
#Average job time:                   4s       0.07m     0.00h    0.00d
#Longest job:                       37s       0.62m     0.01h    0.00d
#Submission to last job:            42s       0.70m     0.01h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/sacCer1/run/out
    hgLoadBlastTab dm1 scBlastTab -maxPer=1 *.tab


# MAKE ORGANISM-SPECIFIC HGNEARDATA FILES (DONE 10/17/03 angie)
    cd ~/kent/src/hg/near/hgNear/hgNearData
    # The directory name is the dbDb.genome field, processed by 
    # hdb.c's hgDirForOrg():
    mkdir D_melanogaster
    cp C_elegans/*.{html,ra} D_melanogaster/
    cd D_melanogaster
    mv kimLifeCycleFull.html arbLifeCycleFull.html
    mv kimLifeCycleMedian.html arbLifeCycleMedian.html
    # edit all .ra and .html files as appropriate for D. melanogaster/dm1...
    # cvs add and check in D_melanogaster/ and all .ra and .html files.
    # The "representatives" lines in columnDb.ra are tricky.  
    # For the median reps, I used this, edited to include an extra "-1," 
    # at each boundary between evelopmental stages (embryo -> larva, etc):
    # 29->30, 41->42, 59->60
    echo "select id from arbFlyLifeMedianExps;" | hgsql -N hgFixed \
    | perl -we '$i=0; \
                while (<>) { \
                  chop; print "$_,"; \
                  $i++; if (($i % 5) == 0) { print "-1," }; \
                } \
                print "\n";'
    # For the full reps, I used the output of this command minus the 
    # initial ",-1,".  -1 separators are inserted between each group 
    # that was lumped together in arbMed.ra:
    awk '-F      ' '{print $3;}' \
      ~/kent/src/hg/makeDb/hgMedianMicroarray/arbMed.ra \
    | perl -we 'while (<>) { \
                  @w=split(" "); \
                   print ",-1," . join(",", @w); \
                } \
                print "\n";'
    # Figure out what values to use for expMulti defn's absoluteMax.
    # Report on the max and 99.5%ile absolute score:
    echo select expScores from arbFlyLifeAll | hgsql hgFixed -N \
    | perl -we '$max = 0.0; \
                @all = (); \
                while (<>) { \
                  chop; @nums = split(","); \
                  foreach $num (@nums) { \
                    $max = $num if ($num > $max); \
                    push @all, $num if (defined $num && $num ne ""); \
                  } \
                } \
                $n = scalar(@all); \
                print "max is $max, N is $n\n"; \
                @all = sort { $a <=> $b } @all; \
                $useMax = $all[$n * 0.995 + 1]; \
                print "99.5%ile is $useMax -- use this for max\n";'
# max is 65535.000, N is 797364
# 99.5%ile is 15433.000 -- use this for max
    # N is the product of count(*) of arbFlyLifeAll{,Exps} .  
    # Now repeat the above perl in-liner, but on arbFlyLifeAllRatio, 
    # to determine ratioMax:  
    echo select expScores from arbFlyLifeAllRatio | hgsql hgFixed -N \
    # <paste in perl from above>
# max is 9.862, N is 797364
# 99.5%ile is 2.915 -- use this for max


# ENABLE HGNEAR FOR DM1 IN HGCENTRALTEST (DONE 10/17/03 angie)
    echo "update dbDb set hgNearOk = 1 where name = 'dm1';" \
      | hgsql -h genome-testdb hgcentraltest


# END OF HGNEAR STUFF
#############################################################################


# reload refseqs to pickup a change that uses locus_tag if gene name isn't
# available.  2003/10/16 markd

# First remove refseqs from databases:
    drop table refSeqStatus;
    drop table refLink;
    drop table refSeqSummary;
    drop table refGene;
    drop table refSeqAli;
    drop table refFlat;

    delete from gbSeq where srcDb = 'RefSeq';
    delete from gbStatus where srcDb = 'RefSeq';
    delete from gbExtFile where path like '%/refseq%';
    delete from gbLoaded where srcDb = 'RefSeq';
    delete from mrna where acc like 'NM\_%';
    delete from imageClone where acc like 'NM\_%';
    delete from mrna where acc like 'NR\_%';
    delete from imageClone where acc like 'NR\_%';

# now reload:
    cd /cluster/data/genbank
    ./bin/gbDbLoadStep -type=mrna dm1


# MAKE DOWNLOADABLE FILES (DONE 10/29/03 angie)
    ssh kksilo
    cd /cluster/data/dm1
    mkdir zips
    zip -j zips/chromOut.zip ?{,?}/chr?{,?}.fa.out
    zip -j zips/chromFa.zip ?{,?}/chr?{,?}.fa
    foreach f (?{,?}/chr?{,?}.fa)
      maskOutFa $f hard $f.masked
    end
    zip -j zips/chromFaMasked.zip ?{,?}/chr?{,?}.fa.masked
    cd bed/simpleRepeat
    zip ../../zips/chromTrf.zip trfMaskChrom/chr*.bed
    zip ../../zips/contigTrf.zip trfMask/N{T,G}*.bed
    cd ../..
    # Make a starter mrna.zip -- it will get updated regularly on the RR. 
    /cluster/data/genbank/bin/i386/gbGetSeqs -gbRoot=/cluster/data/genbank \
      -db=dm1 -native genbank mrna mrna.fa
    zip zips/mrna.zip mrna.fa
    rm mrna.fa
    foreach f (zips/*.zip)
      echo $f
      unzip -t $f | tail -1
    end
    ssh hgwdev
    mkdir /usr/local/apache/htdocs/goldenPath/dm1
    cd /usr/local/apache/htdocs/goldenPath/dm1
    mkdir bigZips database
    # Create README.txt files in bigZips/ and database/ to explain the files.
    cp -p /cluster/data/dm1/zips/*.zip bigZips


# ANOPHELES ECORES FROM GENOSCOPE (DONE 11/10/03 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/anophelesEcores
    cd /cluster/data/dm1/bed/anophelesEcores
    # save attachment from Olivier Jaillon's email 11/10/03 to 
    # ecotig.6.4ucsc
    perl -wpe 'if (/^(\w+)\:\d+ (\d+) (\d+) (\w+)\:\d+ (\d+) (\d+)$/) { \
                 if ($1 ne $4) { die "diff chr: $1 $4"; } \
                 $name = "chr$1:$2-$6";  $start = $2 - 1; \
                 $sz1 = $3 - $start;  $sz2 = $6 - ($5 - 1);   $st2 = $5 - $2; \
                 $_ = "chr$1\t$start\t$6\t$name\t0\t+\t$start\t$6\t0\t2\t$sz1,$sz2,\t0,$st2,\n"; \
               } elsif (/^(\w+)\:\d+ (\d+) (\d+)\s*$/) { \
                 $name = "chr$1:$2-$3";  $start = $2 - 1;  $sz1 = $3 - $start; \
                 $_ = "chr$1\t$start\t$3\t$name\t0\t+\t$start\t$3\t0\t1\t$sz1,\t0,\n"; \
               } else { chop; die "cant parse line $.:\n|$_|"; }' \
    < ecotig.6.4ucsc > anophelesEcores.bed
    hgLoadBed -tab dm1 anophelesEcores anophelesEcores.bed


# MASKED-QUERY XENO RNA ALIGNMENTS (DONE 10/22/03 angie)
    # Experimental track... if this TRF-masking works out, then find out 
    # what it would take to add it as an option to Mark's genbank stuff.
    ssh kksilo
    mkdir /cluster/data/dm1/bed/xenoMrnaMasked
    cd /cluster/data/dm1/bed/xenoMrnaMasked
    # Grab the latest full-release (137) genbank mRNA's.  Strip the 
    # .version suffixes so we can use accessions from mrna.gbidx to 
    # pick out the non-D.mel. mrnas.  
    perl -wpe 's/^>(\w+).\d+/>$1/' \
      /cluster/data/genbank/data/processed/genbank.137.0/full/mrna.fa \
      > allMrna.fa
    grep -v "Drosophila melanogaster" \
      /cluster/data/genbank/data/processed/genbank.137.0/full/mrna.gbidx \
        | awk '{print $1;}' | grep -v "^#" \
    > xenoAcc.lst
    faSomeRecords allMrna.fa xenoAcc.lst flyXenoRna.fa
    # Split up the sequences into manageably sized files.  
    mkdir flyXenoRnaSplit
    faSplit about flyXenoRna.fa 10000000 flyXenoRnaSplit/xenoRna
    # Now TRF-mask the sequences... use repeats with period <= 9 to mask.
    mkdir trf
    rm -f trf.log; touch trf.log
    foreach f (flyXenoRnaSplit/*.fa)
      set b = trf/$f:t:r.bed
      echo $f to $b...
      /cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $f /dev/null \
        -bedAt=$b -tempDir=/tmp -maxPeriod=9 >& trf.log
      maskOutFa -soft $f $b $f
    end
    # distribute masked xenoRna sequence on i-servers
    mkdir /cluster/bluearc/dm1/mrna.137
    cp -p /cluster/data/dm1/bed/xenoMrnaMasked/flyXenoRnaSplit/*.fa \
      /cluster/bluearc/dm1/mrna.137/

    ssh kk
    cd /cluster/data/dm1/bed/xenoMrnaMasked
    mkdir psl
    ls -1S /iscratch/i/dm1/nib/chr*.nib > fly.lst
    ls -1S /cluster/bluearc/dm1/mrna.137/xenoRna*.fa > mrna.lst
    echo '#LOOP \
/cluster/bin/i386/blat -maxIntron=50000 $(path1) {check in line+ $(path2)} -q=rnax -t=dnax -mask=lower {check out line+ psl/$(root1)_$(root2).psl} \
#ENDLOOP' > gsub
    gensub2 fly.lst mrna.lst gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 814 of 814 jobs
#Average job time:                 459s       7.65m     0.13h    0.01d
#Longest job:                     1756s      29.27m     0.49h    0.02d
#Submission to last job:          2145s      35.75m     0.60h    0.02d
    ssh kksilo
    cd /cluster/data/dm1/bed/xenoMrnaMasked
    pslSort dirs raw.psl /cluster/store2/temp psl
    pslReps raw.psl cooked.psl /dev/null -minAli=0.25
    # pslFilter -minMatch=60 -gapSizeLogMod=2 -minScore=30 cooked.psl filt.psl
    pslFilter -minAli=250 -minUniqueMatch=15 cooked.psl filt.psl
    pslSortAcc nohead chrom /cluster/store2/temp filt.psl
    pslCat -dir chrom > xenoMrnaMasked.psl
    rm -r chrom raw.psl cooked.psl filt.psl
    # Load into database as so:
    ssh hgwdev
    cd /cluster/data/dm1/bed/xenoMrnaMasked
    hgLoadPsl dm1 xenoMrnaMasked.psl
    # Looks like no need to hgLoadSeq -- seqs are already loaded by Mark's 
    # stuff.


# BLAT HONEYBEE (apiMel0) (DONE 1/13/04 angie)
    ssh kk
    mkdir /cluster/data/dm1/bed/blatApiMel0
    cd /cluster/data/dm1/bed/blatApiMel0
    mkdir psl
    ls -1S /iscratch/i/dm1/nib/*.nib > fly.lst
    ls -1S /iscratch/i/apiMel0/chunks/*.fa > bee.lst
cat << 'EOF' > gsub
#LOOP
/cluster/bin/i386/blat -mask=lower -qMask=lower -q=dnax -t=dnax {check in exists+ $(path1)} {check in line+ $(path2)} {check out line+ /cluster/data/dm1/bed/blatApiMel0/psl/$(root1)_$(root2).psl}
#ENDLOOP
'EOF'
    # << this line makes emacs coloring happy
    gensub2 fly.lst bee.lst gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 495 of 495 jobs
#Average job time:                 802s      13.37m     0.22h    0.01d
#Longest job:                    26590s     443.17m     7.39h    0.31d
#Submission to last job:         27028s     450.47m     7.51h    0.31d
    # postprocess
    pslCat -dir psl > blatApiMel0.psl
    # load
    ssh hgwdev
    cd /cluster/data/dm1/bed/blatApiMel0
    hgLoadPsl dm1 blatApiMel0.psl
    mkdir /gbdb/dm1/apiMel0
    foreach f (/cluster/data/apiMel0/groups/*.fa)
      ln -s $f /gbdb/dm1/apiMel0/$f:t
    end
    hgLoadSeq dm1 /gbdb/dm1/apiMel0/*.fa


#  miRNA track (DONE - 2004-05-04 - Hiram)
    #	data from: Sam Griffiths-Jones <sgj@sanger.ac.uk>
    #	and Michel.Weber@ibcg.biotoul.fr
    #	notify them if this assembly updates to renew this track
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/miRNA
    cd /cluster/data/dm1/bed/miRNA
    wget --timestamping \
    "ftp://ftp.sanger.ac.uk/pub/databases/Rfam/miRNA/genomes/dme_bdgp3.*"
    grep -v "^track " dme_bdgp3.bed | sed -e "s/ /\t/g" > dm1.bed
    hgLoadBed dm1 miRNA dm1.bed
    # entry in trackDb/trackDb.ra already there
    #	featureBits dm1 miRNA
    #	6845 bases of 126527731 (0.005%) in intersection


# BLASTZ D.YAKUBA (DONE 5/22/04 angie)
    ssh kksilo
    mkdir /cluster/data/dm1/bed/blastz.droYak1.2004-05-22
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22
    cat << '_EOF_' > DEF
# D.melanogaster vs. D.yakuba
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
BLASTZ_ABRIDGE_REPEATS=0

# TARGET - D. melanogaster
SEQ1_DIR=/cluster/bluearc/drosophila/dm1/nib
# unused: SEQ1_RMSK=
SEQ1_SMSK=
SEQ1_FLAG=-drosophila
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY - D. yakuba
SEQ2_DIR=/iscratch/i/droYak1/nib
# unused: SEQ2_RMSK=
SEQ2_SMSK=
SEQ2_FLAG=-drosophila
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/cluster/data/dm1/bed/blastz.droYak1.2004-05-22

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len

#DEBUG=1
'_EOF_'
    # << this line keeps emacs coloring happy
    # run bash shell if you don't already:
    bash
    source DEF
    mkdir run
    ~angie/hummus/make-joblist $DEF > $BASE/run/j
    sh ./xdir.sh
    cd run
    sed -e 's#^#/cluster/home/angie/schwartzbin/#' j > j2
    wc -l j*
    head j2
    mv j2 j
    # cluster run
    ssh kk
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/run
    para create j
    para try, check, push, check, ....
#Completed: 672 of 672 jobs
#Average job time:                 164s       2.74m     0.05h    0.00d
#Longest job:                     1836s      30.60m     0.51h    0.02d
#Submission to last job:          2433s      40.55m     0.68h    0.03d

    # back on kksilo...
    mkdir /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/run.1
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/run.1
    ~angie/hummus/do.out2lav ../DEF > j
    # small cluster run
    ssh kki
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/run.1
    para create j
    para try, check, push, check, ....
#Completed: 21 of 21 jobs
#Average job time:                 453s       7.54m     0.13h    0.01d
#Longest job:                      650s      10.83m     0.18h    0.01d
#Submission to last job:           685s      11.42m     0.19h    0.01d
    cd ..
    rm -r raw

    # third run: lav -> axt
    ssh kki
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22
    mkdir axtChrom pslChrom run.2
    cd run.2
    cat << '_EOF_' > do.csh
#!/bin/csh -ef
cd $1
set chr = $1:t
cat `ls -1 *.lav | sort -g` \
| $HOME/bin/x86_64/lavToAxt stdin \
    /cluster/bluearc/drosophila/dm1/nib /iscratch/i/droYak1/nib stdout \
| $HOME/bin/x86_64/axtSort stdin ../../axtChrom/$chr.axt 
$HOME/bin/x86_64/axtToPsl ../../axtChrom/$chr.axt ../../S1.len ../../S2.len \
  ../../pslChrom/$chr.psl
'_EOF_'
    # << this line keeps emacs coloring happy
    chmod a+x do.csh
    cp /dev/null jobList
    foreach d (../lav/chr*)
      echo "do.csh $d" >> jobList
    end
    para create jobList
    para try, check, push, check
#Completed: 11 of 11 jobs
#Average job time:                1516s      25.27m     0.42h    0.02d
#Longest job:                     2903s      48.38m     0.81h    0.03d
#Submission to last job:          2903s      48.38m     0.81h    0.03d

# CHAIN YAKUBA BLASTZ (DONE 5/27/04 angie)
    # Run axtChain on little cluster
    ssh kki
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22
    mkdir -p axtChain/run1
    cd axtChain/run1
    mkdir out chain
    ls -1S /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChrom/*.axt \
      > input.lst
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out exists out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    cat << '_EOF_' > doChain
#!/bin/csh
axtFilter -notQ=chrUn_random $1 \
| axtChain -verbose=0 stdin \
  /cluster/bluearc/drosophila/dm1/nib \
  /iscratch/i/droYak1/nib $2 > $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doChain
    gensub2 input.lst single gsub jobList
    para create jobList
    para try, check, push, check...
#Completed: 11 of 11 jobs
#Average job time:                  18s       0.30m     0.01h    0.00d
#Longest job:                       35s       0.58m     0.01h    0.00d
#Submission to last job:            35s       0.58m     0.01h    0.00d

    # now on the cluster server, sort chains
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    chainMergeSort run1/chain/*.chain > all.chain
    chainSplit chain all.chain
    rm run1/chain/*.chain

    # take a look at score distr's
    foreach f (chain/*.chain)
      grep chain $f | awk '{print $2;}' | sort -nr > /tmp/score.$f:t:r
      echo $f:t:r
      textHistogram -binSize=10000 /tmp/score.$f:t:r
      echo ""
    end

    # Load chains into database
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain/chain
    foreach i (*.chain)
        set c = $i:r
        echo loading $c
        hgLoadChain dm1 ${c}_chainDroYak1 $i
    end


# NET YAKUBA BLASTZ (DONE 5/27/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    chainPreNet all.chain ../S1.len ../S2.len stdout \
    | chainNet stdin -minSpace=1 ../S1.len ../S2.len stdout /dev/null \
    | netSyntenic stdin noClass.net

    # Add classification info using db tables:
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    netClass -noAr noClass.net dm1 droYak1 yakuba.net

    # Make a 'syntenic' subset:
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    rm noClass.net
    netFilter -syn yakuba.net > yakubaSyn.net

    # Load the nets into database 
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    netFilter -minGap=10 yakuba.net |  hgLoadNet dm1 netDroYak1 stdin
    netFilter -minGap=10 yakubaSyn.net | hgLoadNet dm1 netSyntenyDroYak1 stdin


# MAKE VSDROYAK1 DOWNLOADABLES (DONE 10/5/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    gzip -c all.chain > ../../../zips/yakuba.chain.gz
    gzip -c yakuba.net > ../../../zips/yakuba.net.gz
    # axtNet/* already compressed...

    ssh hgwdev
    mkdir /usr/local/apache/htdocs/goldenPath/dm1/vsDroYak1
    cd /usr/local/apache/htdocs/goldenPath/dm1/vsDroYak1
    mv /cluster/data/dm1/zips/yakuba*.gz .
    cp -pR /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtNet .
    md5sum *.gz */*.gz > md5sum.txt
    # Copy over & edit README.txt w/pointers to chain, net formats.


# GENERATE DROYAK1 MAF FOR MULTIZ FROM NET (DONE 5/27/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    netSplit yakuba.net net
    ssh kolossus
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22
    mkdir axtNet
    foreach f (axtChain/net/*)
      set chr = $f:t:r
      netToAxt $f axtChain/chain/$chr.chain /cluster/data/dm1/nib \
        /cluster/data/droYak1/nib stdout \
      | axtSort stdin axtNet/$chr.axt
    end

    mkdir mafNet
    foreach f (axtNet/chr*.axt)
      set maf = mafNet/$f:t:r.my.maf
      axtToMaf $f \
            /cluster/data/dm1/chrom.sizes /cluster/data/droYak1/chrom.sizes \
            $maf -tPrefix=dm1. -qPrefix=droYak1.
    end


# GENERATE DP2 MAF FOR MULTIZ FROM NET (DONE 8/9/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.dp2.2004-08-03/axtChain
    netSplit pseudoobscura.net net
    cd ..
    mkdir axtNet
    foreach f (axtChain/net/chr*.net)
      netToAxt $f axtChain/chain/$f:t:r.chain \
        /cluster/data/dm1/nib /cluster/data/dp2/nib stdout \
      | axtSort stdin axtNet/$f:t:r.axt
    end
    mkdir mafNet
    foreach f (axtNet/chr*.axt)
      set maf = mafNet/$f:t:r.maf
      axtToMaf $f \
            /cluster/data/dm1/chrom.sizes /cluster/data/dp2/chrom.sizes \
            $maf -tPrefix=dm1. -qPrefix=dp2.
    end


# MULTIZ MELANOGASTER/YAKUBA/PSEUDOOBSCURA (DONE 8/9/04 angie)
    # put the MAFs on bluearc
    ssh kksilo
    mkdir -p /cluster/bluearc/multiz.fly/my
    cp /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/mafNet/*.maf \
      /cluster/bluearc/multiz.fly/my
    mkdir -p /cluster/bluearc/multiz.fly/mp
    cp /cluster/data/dm1/bed/blastz.dp2.2004-08-03/mafNet/*.maf \
      /cluster/bluearc/multiz.fly/mp

    ssh kki
    mkdir /cluster/data/dm1/bed/multiz.dm1droYak1dp2
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp2
    mkdir myp
    # Wrapper script required because of stdout redirect:
    cat << '_EOF_' > doMultiz
#!/bin/csh -ef
/cluster/bin/penn/multiz $1 $2 - > $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doMultiz
    rm -f jobList
    foreach file (/cluster/bluearc/multiz.fly/my/*.maf) 
      set root=$file:t:r:r
      echo "doMultiz /cluster/bluearc/multiz.fly/mp/${root}.maf $file /cluster/data/dm1/bed/multiz.dm1droYak1dp2/myp/${root}.maf" >> jobList
    end
    para create jobList
    para try, check, push, check
#Completed: 11 of 11 jobs
#Average job time:                  52s       0.87m     0.01h    0.00d
#Longest job:                      144s       2.40m     0.04h    0.00d
#Submission to last job:           144s       2.40m     0.04h    0.00d

    # clean up bluearc (these are big files!)
    rm -r /cluster/bluearc/multiz.fly

    # no downloadables/db-loading -- just go on and do the 4-way multiz.


# BDGP 3.2 ANNOTATIONS (DONE 6/18/04 angie)
    ss kksilo
    cd /cluster/data/dm1/bed
    mv bdgpAnnotations bdgpAnnotations.3.1
    mkdir bdgpAnnotations.3.2
    cd /cluster/data/dm1/bed/bdgpAnnotations.3.2
    # No annotations for chrU or h's!  probably better that way...
    foreach c (2L 2R 3L 3R 4 X)
      set f = dmel_${c}_translation_r3.2.0.fasta.gz
      wget ftp://flybase.net/genomes/Drosophila_melanogaster/current/fasta/$f
      set f = dmel_${c}_r3.2.0.gff.gz
      wget ftp://flybase.net/genomes/Drosophila_melanogaster/current/gff/$f
    end
    gunzip dmel*.gff.gz
    # This new set is almost-but-not-quite GFF3, and all features (genes,
    # pseudogenes, ESTs, regulatory regions, kitchen sink) are included 
    # in one /gff file.  Perl the gene and non-coding gene parts of it into 
    # bdgp{Gene,NonCoding}.gtf and bdgp{Gene,NonCoding}Info.tab.
    chmod a+x ./extractGenes.pl
    ./extractGenes.pl *.gff
    # Sort GTF by position.
    sort -k1,1 -k4n,5n bdgpGene.gtf > tmp
    mv tmp bdgpGene.gtf
    sort -k1,1 -k4n,5n bdgpNonCoding.gtf > tmp
    mv tmp bdgpNonCoding.gtf

    # Proteins:
    zcat dmel*trans*.gz | perl -wpe 's/^>(CG\d+)(-\S+)?-P(\w)\s.*/>$1-R$3/' \
    > bdgpGenePep.fa

    # Load into test table for now -- don't want to mess up Gene Sorter 
    # stuff that depends on bdgpGene* tables.
    ssh hgwdev
    cd /cluster/data/dm1/bed/bdgpAnnotations.3.2
    ldHgGene -gtf -genePredExt dm1 bdgpGene2 bdgpGene.gtf
    featureBits dm1 bdgpGene
#28262131 bases of 126527731 (22.337%) in intersection
    featureBits dm1 bdgpGene2
#28178241 bases of 126527731 (22.270%) in intersection
    featureBits dm1 bdgpGene bdgpGene2
#27813359 bases of 126527731 (21.982%) in intersection
    # OK, looks OK.  Replace bdgpGene with bdgpGene2, reload bdgpGeneInfo,
    # then go back up and quickly rebuild all the hgNear (Gene Sorter) 
    # tables that depend on bdgpGene.  
    hgsql dm1 -e 'drop table bdgpGene; alter table bdgpGene2 rename bdgpGene'
    hgsql dm1 -e 'delete from bdgpGeneInfo'
    hgsql dm1 -e 'load data local infile "bdgpGeneInfo.tab" into table bdgpGeneInfo'
    # load bdgpNonCoding*
    ldHgGene -gtf -genePredExt dm1 bdgpNonCoding bdgpNonCoding.gtf
    hgsql dm1 -e 'delete from bdgpNonCodingInfo'
    hgsql dm1 -e 'load data local infile "bdgpNonCodingInfo.tab" into table bdgpNonCodingInfo'
    # load proteins
    hgPepPred dm1 generic bdgpGenePep bdgpGenePep.fa


# CYTOBANDS (DONE 6/15/04 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/cytoband
    cd /cluster/data/dm1/bed/cytoband
    foreach c (2L 2R 3L 3R 4 X)
      wget ftp://flybase.net/genomes/Drosophila_melanogaster/current/gnomap/cytomap-$c.tsv
    end
    cp /dev/null cytoBand.tab
    foreach c (2L 2R 3L 3R 4 X)
      grep -v '^#' cytomap-$c.tsv \
      | awk '$3 > 0 && $2 != $3 {print "chr'$c'\t" $2 "\t" $3 "\t" $1 "\t";}' \
      >> cytoBand.tab
    end
    hgsql dm1 < ~/kent/src/hg/lib/cytoBand.sql
    hgsql dm1 -e 'load data local infile "cytoBand.tab" into table cytoBand'
    # Normally we would just create cytoBandIdeo by select * from cytoBand --
    # but that gives too high of a resolution for any bands to be visible!  
    # So make a boiled-down cytoBandIdeo that just has {number, letter} 
    # instead of {number, letter, number}.
    perl -we 'while (<>) { \
                chomp; @w=split(" "); \
                if ($w[3] =~ /(\d+[A-Z]+)\d+/) { \
                  $b = $1; \
                } else { die "doh!" } \
                if (! defined $lastB) { \
                  $start = $w[1]; \
                } elsif ($lastB ne $b) { \
                  print "$chrom\t$start\t$end\t$lastB\n"; \
                  $start = $w[1]; \
                } \
                ($chrom, $end, $lastB) = ($w[0], $w[2], $b); \
              } \
              print "$chrom\t$start\t$end\t$lastB\n";' cytoBand.tab \
     > cytoBandIdeo.tab
    hgsql dm1 -e 'create table cytoBandIdeo select * from cytoBand where 0'
    hgsql dm1 -e 'load data local infile "cytoBandIdeo.tab" into table \
                  cytoBandIdeo'
    # 6/23/04: trim the entries that fall off the ends of their respective 
    # chromosomes.
    foreach c (`echo select chrom from chromInfo | hgsql -N dm1`)
      set size = `echo select size from chromInfo where chrom = '"'$c'"' \
                  | hgsql -N dm1`
      echo $c $size
      foreach table (cytoBand cytoBandIdeo)
        echo delete from $table where chrom = \"$c\" and chromStart \> $size \
        | hgsql -N dm1
        echo update $table set chromEnd = $size where chrom = \"$c\" and \
             chromEnd \> $size \
        | hgsql -N dm1
      end
    end


# BLASTZ ANOPHELES (DONE 6/10/04 angie)
    # Will give human-fugu params a try... but without abridging repeats 
    # since I don't know which are lin-spec for fly vs. mosquito, and don't 
    # want to bother Arian or speculate.
    ssh kksilo
    mkdir /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10
    cat << '_EOF_' > DEF
# D.melanogaster vs. A. gambiae
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q
BLASTZ_ABRIDGE_REPEATS=0

# TARGET - D. melanogaster
SEQ1_DIR=/cluster/bluearc/drosophila/dm1/nib
# unused: SEQ1_RMSK=
SEQ1_SMSK=
SEQ1_FLAG=-drosophila
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY - A. gambiae
SEQ2_DIR=/cluster/bluearc/anoGam1/nib
# unused: SEQ2_RMSK=
SEQ2_SMSK=
SEQ2_FLAG=-anopheles
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/cluster/data/dm1/bed/blastz.anoGam1.2004-06-10

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len

#DEBUG=1
'_EOF_'
    # << this line keeps emacs coloring happy
    # run bash shell if you don't already:
    bash
    source DEF
    mkdir run
    ~angie/hummus/make-joblist $DEF > $BASE/run/j
    sh ./xdir.sh
    cd run
    sed -e 's#^#/cluster/bin/penn/#' j > j2
    wc -l j*
    head j2
    mv j2 j
    # cluster run -- use rack 9 to avoid getting in the way of hg17.
    ssh kk9
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/run
    para create j
    para try, check, push, check, ....
#Completed: 693 of 693 jobs
#Average job time:                 208s       3.47m     0.06h    0.00d
#Longest job:                      675s      11.25m     0.19h    0.01d
#Submission to last job:          1884s      31.40m     0.52h    0.02d

    # back on kksilo...
    mkdir /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/run.1
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/run.1
    ~angie/hummus/do.out2lav ../DEF > j
    # small cluster run
    ssh kki
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/run.1
    para create j
    para try, check, push, check, ....
#Completed: 21 of 21 jobs
#Average job time:                   5s       0.09m     0.00h    0.00d
#Longest job:                       12s       0.20m     0.00h    0.00d
#Submission to last job:            14s       0.23m     0.00h    0.00d
    cd ..
    rm -r raw

    # third run: lav -> axt
    ssh kki
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10
    mkdir axtChrom pslChrom run.2
    cd run.2
    cat << '_EOF_' > do.csh
#!/bin/csh -ef
cd $1
set chr = $1:t
cat `ls -1 *.lav | sort -g` \
| $HOME/bin/x86_64/lavToAxt stdin \
    /cluster/bluearc/drosophila/dm1/nib /iscratch/i/anoGam1/nib stdout \
| $HOME/bin/x86_64/axtSort stdin ../../axtChrom/$chr.axt 
$HOME/bin/x86_64/axtToPsl ../../axtChrom/$chr.axt ../../S1.len ../../S2.len \
  ../../pslChrom/$chr.psl
'_EOF_'
    # << this line keeps emacs coloring happy
    chmod a+x do.csh
    cp /dev/null jobList
    foreach d (../lav/chr*)
      echo "do.csh $d" >> jobList
    end
    para create jobList
    para try, check, push, check
#Completed: 11 of 11 jobs
#Average job time:                   9s       0.16m     0.00h    0.00d
#Longest job:                       18s       0.30m     0.01h    0.00d
#Submission to last job:            18s       0.30m     0.01h    0.00d


# CHAIN ANOPHELES BLASTZ (DONE 6/10/04 angie)
    # Run axtChain on little cluster
    ssh kki
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10
    mkdir -p axtChain/run1
    cd axtChain/run1
    mkdir out chain
    ls -1S /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChrom/*.axt \
      > input.lst
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out exists out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    # Make our own linear gap file with reduced gap penalties:
    cat << '_EOF_' > ../../chickenHumanTuned.gap
tablesize	11
smallSize	111
position	1	2	3	11	111	2111	12111	32111	72111	152111	252111
qGap	325	360	400	450	600	1100	3600	7600	15600	31600	56600
tGap	325	360	400	450	600	1100	3600	7600	15600	31600	56600
bothGap	625	660	700	750	900	1400	4000	8000	16000	32000	57000
'_EOF_'
    # << this line makes emacs coloring happy

    cat << '_EOF_' > doChain
#!/bin/csh
axtChain -scoreScheme=/cluster/data/blastz/HoxD55.q \
         -linearGap=../../chickenHumanTuned.gap \
         -verbose=0 $1 \
  /cluster/bluearc/drosophila/dm1/nib \
  /iscratch/i/anoGam1/nib $2 > $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doChain
    gensub2 input.lst single gsub jobList
    para create jobList
    para try, check, push, check...
#Completed: 11 of 11 jobs
#Average job time:                  13s       0.22m     0.00h    0.00d
#Longest job:                       19s       0.32m     0.01h    0.00d
#Submission to last job:            19s       0.32m     0.01h    0.00d

    # now on the cluster server, sort chains
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain
    chainMergeSort run1/chain/*.chain > all.chain
    chainSplit chain all.chain
    rm run1/chain/*.chain

    # take a look at score distr's
    foreach f (chain/*.chain)
      grep chain $f | awk '{print $2;}' | sort -nr > /tmp/score.$f:t:r
      echo $f:t:r
      textHistogram -binSize=10000 /tmp/score.$f:t:r
      echo ""
    end

    # Load chains into database
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain/chain
    foreach i (*.chain)
        set c = $i:r
        echo loading $c
        hgLoadChain dm1 ${c}_chainAnoGam1 $i
    end


# NET ANOPHELES BLASTZ (DONE 6/10/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain
    chainPreNet all.chain ../S1.len ../S2.len stdout \
    | chainNet stdin -minSpace=1 ../S1.len ../S2.len stdout /dev/null \
    | netSyntenic stdin noClass.net

    # Add classification info using db tables:
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain
    netClass -noAr noClass.net dm1 anoGam1 anopheles.net

    # Make a 'syntenic' subset:
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain
    rm noClass.net
    netFilter -syn anopheles.net > anophelesSyn.net

    # Load the nets into database 
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain
    netFilter -minGap=10 anopheles.net |  hgLoadNet dm1 netAnoGam1 stdin
    netFilter -minGap=10 anophelesSyn.net | hgLoadNet dm1 netSyntenyAnoGam1 stdin


# MAKE VSANOGAM1 DOWNLOADABLES (DONE 10/5/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain
    gzip -c all.chain > ../../../zips/anopheles.chain.gz
    gzip -c anopheles.net > ../../../zips/anopheles.net.gz
    cd ..
    mkdir ../../zips/axtNet
    foreach f (axtNet/chr*.axt)
      gzip -c $f > ../../zips/axtNet/$f:t.gz
    end

    ssh hgwdev
    mkdir /usr/local/apache/htdocs/goldenPath/dm1/vsAnoGam1
    cd /usr/local/apache/htdocs/goldenPath/dm1/vsAnoGam1
    mv /cluster/data/dm1/zips/anopheles*.gz .
    mv /cluster/data/dm1/zips/axtNet .
    md5sum *.gz */*.gz > md5sum.txt
    # Copy over & edit README.txt w/pointers to chain, net formats.


# GENERATE ANOGAM1 MAF FOR MULTIZ FROM NET (DONE 6/10/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain
    netSplit anopheles.net net
    ssh kolossus
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10
    mkdir axtNet
    foreach f (axtChain/net/*)
      set chr = $f:t:r
      netToAxt $f axtChain/chain/$chr.chain /cluster/data/dm1/nib \
        /cluster/data/anoGam1/nib stdout \
      | axtSort stdin axtNet/$chr.axt
    end

    mkdir mafNet
    foreach f (axtNet/chr*.axt)
      set maf = mafNet/$f:t:r.ma.maf
      axtToMaf $f \
            /cluster/data/dm1/chrom.sizes /cluster/data/anoGam1/chrom.sizes \
            $maf -tPrefix=dm1. -qPrefix=anoGam1.
    end


# MULTIZ MELANOGASTER/YAKUBA/PSEUDOOBSCURA/ANOPHELES (DONE 8/9/04 angie)
    # put the MAFs on bluearc
    ssh kksilo
    mkdir -p /cluster/bluearc/multiz.flymo/myp
    cp /cluster/data/dm1/bed/multiz.dm1droYak1dp2/myp/*.maf \
      /cluster/bluearc/multiz.flymo/myp
    mkdir -p /cluster/bluearc/multiz.flymo/ma
    cp /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/mafNet/*.maf \
      /cluster/bluearc/multiz.flymo/ma

    ssh kki
    mkdir /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1
    mkdir mypa
    # Wrapper script required because of stdout redirect:
    cat << '_EOF_' > doMultiz
#!/bin/csh -fe
/cluster/bin/penn/multiz $1 $2 - > $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doMultiz
    rm -f jobList
    foreach file (/cluster/bluearc/multiz.flymo/myp/*.maf) 
      set root=$file:t:r:r
      echo "doMultiz /cluster/bluearc/multiz.flymo/ma/${root}.ma.maf $file /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/mypa/${root}.maf" >> jobList
    end
    para create jobList
    para try, check, push, check
#Completed: 11 of 11 jobs
#Average job time:                  27s       0.45m     0.01h    0.00d
#Longest job:                       74s       1.23m     0.02h    0.00d
#Submission to last job:            74s       1.23m     0.02h    0.00d
    du -sh mypa
#415M    mypa

    # clean up bluearc
    rm -r /cluster/bluearc/multiz.flymo

    # setup external files for database reference
    ssh hgwdev
    mkdir /gbdb/dm1/mzDy1Dp2Ag1_phast
    ln -s /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/mypa/chr*.maf \
      /gbdb/dm1/mzDy1Dp2Ag1_phast/
    # load into database
    hgLoadMaf -warn dm1 mzDy1Dp2Ag1_phast
    # The previous Conservation wigMaf had "pairwise mypa" in its trackDb.
    # Now we need to define a new suffix for the pairwise tables and 
    # /gbdb/ locations, in order to avoid a clash between dp1 and dp2 
    # d_pseudoobscura_$pairwise tables/gbdb.  
    mkdir /gbdb/dm1/{d_yakuba,d_pseudoobscura,a_gambiae}_myp2a
    cd /tmp
    ln -s /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/mafNet/*.maf \
      /gbdb/dm1/d_yakuba_myp2a
    hgLoadMaf -WARN dm1 d_yakuba_myp2a
    ln -s /cluster/data/dm1/bed/blastz.dp2.2004-08-03/mafNet/*.maf \
      /gbdb/dm1/d_pseudoobscura_myp2a
    hgLoadMaf -WARN dm1 d_pseudoobscura_myp2a
    ln -s /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/mafNet/*.maf \
      /gbdb/dm1/a_gambiae_myp2a
    hgLoadMaf -WARN dm1 a_gambiae_myp2a

    # put it out for download (10/5/04)
    ssh kksilo
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1
    mkdir ../../zips/mzMafs
    foreach f (mypa/*.maf)
      gzip -c $f > ../../zips/mzMafs/$f:t.gz
    end
    ssh hgwdev
    mkdir /usr/local/apache/htdocs/goldenPath/dm1/multizDroYak1Dp2AnoGam1
    cd /usr/local/apache/htdocs/goldenPath/dm1/multizDroYak1Dp2AnoGam1
    mv /cluster/data/dm1/zips/mzMafs/* .
    rmdir /cluster/data/dm1/zips/mzMafs
    md5sum *.gz > md5sum.txt


# PHASTCONS MELANOGASTER/YAKUBA/PSEUDOOBSCURA (first cut 6/1/04 acs)

    # make sure /cluster/bin/phast is in your path
    # commands below use bash shell
    # watch for typos -- copying from a Makefile and editing a bit

    # there are three steps to go through: fitting a phylogenetic
    # model to the data set with phyloFit, then running phastCons for
    # conservation scores, then running phastCons for predictions of
    # conserved elements.

    # step 1: fit phylogenetic model, with rate variation (discrete
    # gamma model) first we need to extract the sufficient stats for
    # the phylogenetic model from the MAFs.  In this case, we don't
    # care about the order of the tuples (allows for much more compact
    # representation)

    ssh eieio
    mkdir /cluster/data/dm1/bed/phastCons
    cd /cluster/data/dm1/bed/phastCons
    MAF=/cluster/data/dm1/bed/multiz.dm1droYak1dp2/myp
    # first extract chromosome by chromosome
    for file in ${MAF}/*.maf ; do \
	msa_view $file --in-format MAF --out-format SS --order dm1,droYak1,dp2 \
	    --unordered-ss  > `basename $file .maf`.ss ;\
    done
    # now aggregate for whole genome
    ls chr*.ss > fnames
    msa_view "*fnames" -i SS --aggregate dm1,droYak1,dp2 -o SS --unordered-ss > all.ss

    # now estimate parameters; should be very fast in this case
    phyloFit --msa all.ss --tree "((1,2),3)" --msa-format SS --subst-mod REV --nrates 10 --out-root rev-dg --output-tree

    # make sure model looks reasonable
    cat rev-dg.mod
#ALPHABET: A C G T
#ORDER: 0
#SUBST_MOD: REV
#NRATECATS: 10
#ALPHA: 1.273963
#TRAINING_LNL: -290963642.026431
#BACKGROUND: 0.279634 0.220483 0.220439 0.279444
#RATE_MAT:
#  -0.929128    0.196259    0.495954    0.236916
#   0.248910   -1.089402    0.212511    0.627981
#   0.629134    0.212554   -1.090600    0.248913
#   0.237077    0.495481    0.196354   -0.928911
#TREE: ((1:0.057640,2:0.073631):0.166542,3:0.166542);

    # beware of zero branch lengths (indicates bad topology) or very
    # large branch lengths (much greater than one; indicates
    # convergence problems).  Can try without --nrates or with
    # --subst-mod HKY85 to double check (parameter estimates should be
    # in the same ballpark, REV + dG likelihood should be a bit
    # better).  Also can write a log file to monitor convergence
    # (--log).  Also watch out for alpha much less than one.
    # Sometimes it's helpful to produce a rendering of the tree using
    # draw_tree (run on the *.nh file)

    # step 2: run phastCons.  First partition the alignments into
    # bite-sized chunks.  This time we need the ordered version of the
    # SS format.

    # some vars used below
    MAF=/cluster/data/dm1/bed/multiz.dm1droYak1dp2/myp
    FA=/cluster/data/dm1
    WINSIZE=1000000
    WINOVERLAP=0

    for file in ${MAF}/*.maf ; do \
	root=`basename $file .maf` ;\
	chr=`echo $root | sed 's/chr//'` ;\
	echo $file $root $chr ;\
	mkdir -p SS/$chr ;\
	msa_split $file -i MAF -o SS -O dm1,droYak1,dp2 -M ${FA}/$chr/$root.fa \
	    -w ${WINSIZE},${WINOVERLAP} -r SS/$chr/$$root -I 1000 -d 1 -B 5000 ;\
    done
    # (this is worth doing as a little cluster job with mammalian genomes,
    # but it's pretty fast with fly)

    ssh hgwdev
    cd /cluster/data/dm1/bed/phastCons

    # now set up cluster job.  Make a little wrapper for phastCons
    cat << '_EOF_' > doPostProbs
#!/bin/sh

PHAST=/cluster/bin/phast
TMP=/scratch/phastCons

file=$1
root=`basename $file .ss`
chrom=`echo $root | awk -F\. '{print $1}'`

mkdir -p $TMP
$PHAST/phastCons $file rev-dg.mod --cut-at 2 --nrates 10 --transitions 0.030,0.015 --quiet > ${TMP}/$root.pp
mkdir -p POSTPROBS/$chrom
gzip -c $TMP/$root.pp > POSTPROBS/$chrom/$root.pp.gz
rm $TMP/$root.pp
'_EOF_'
    # << this line makes emacs coloring happy
    chmod 775 doPostProbs

    # Note: the --cut-params arguments above are approximate
    # likelihood estimates obtained by running phastCons *without* the
    # --cut-params argument on four or five different windows (all
    # gave similar results).  They may need to change for different
    # data sets.  Careful, though: the parameter estimation procedure
    # is a little unstable.  Be sure to create a log file and monitor
    # for convergence.

    # 2nd note: manually tried various parameter settings and settled
    # on --cut-params 0.010,0.005.  Seems to give best results for
    # both post probs and viterbi path.  At least until we settle on a
    # better principle for setting the params, these are the
    # recommended ones to use.

    # set up a jobs list
    rm -f jobs.lst
    for file in `find SS -name "*.ss"` ; do echo doPostProbs $file >> jobs.lst ; done

    # run cluster job
    ssh kk
    cd /cluster/data/dm1/bed/phastCons
    para create jobs.lst ; para try ; para push ; etc....
#Completed: 134 of 134 jobs
#CPU time in finished jobs:       2584s      43.07m     0.72h    0.03d  0.000 y
#IO & Wait Time:                  2090s      34.83m     0.58h    0.02d  0.000 y
#Average job time:                  35s       0.58m     0.01h    0.00d
#Longest job:                       46s       0.77m     0.01h    0.00d
#Submission to last job:            62s       1.03m     0.02h    0.00d
    logout

    # now create wiggle track
    # NOTE: might want to integrate with the multiz track instead of
    # keeping separate
    mkdir -p wib
    for dir in POSTPROBS/* ; do \
	echo $dir ;\
	chr=`basename $dir` ;\
	zcat `ls $dir/*.pp.gz | sort -t\. -k2,2n` | \
	    wigAsciiToBinary -chrom=$chr \
	    -wibFile=wib/${chr}_phastCons stdin ;\
    done
    hgLoadWiggle dm1 phastCons wib/chr*_phastCons.wig
    mkdir -p /gbdb/dm1/wib
    rm -f /gbdb/dm1/wib/*phastCons.wib
    ln -s `pwd`/wib/*.wib /gbdb/dm1/wib
    chmod 775 . wib
    chmod 664 wib/*.wib

    # trackDb.ra entry
#track phastCons
#shortLabel phastCons 
#longLabel phastCons Conservation Score, melanogaster/yakuba/pseudoobscura
#group compGeno
#priority 103
#visibility hide
#color 0,10,100
#maxHeightPixels 40
#type wig 0.0 1.0
#autoScaleDefault off

    # step 3:  predictions of conserved elements
    # (could do these at the same time as step 2, but we want to use
    # different --rates-cut params)
    cat << '_EOF_' > doViterbi
#!/bin/sh

PHAST=/cluster/home/acs/phast/bin
TMP=/scratch/phastCons

file=$1
root=`basename $file .ss`
chrom=`echo $root | awk -F\. '{print $1}'`

mkdir -p PREDICTIONS/$chrom
$PHAST/phastCons $file rev-dg.mod --nrates 10 --viterbi PREDICTIONS/$chrom/$root.bed --score --no-post-probs --transitions 0.030,0.015 --quiet --seqname $chrom
'_EOF_'
    # << this line makes emacs coloring happy
    chmod 775 doViterbi

    # see note above regarding --cut-params (0.010,0.005 recommended)

    rm -f jobs.viterbi.lst
    for file in `find SS -name "*.ss"` ; do echo doViterbi $file >> jobs.viterbi.lst ; done
    logout

    ssh kk
    cd /cluster/data/dm1/bed/phastCons
    para create jobs.viterbi.lst ; para try ; para push ; etc....
#CPU time in finished jobs:        320s       5.34m     0.09h    0.00d  0.000 y
#IO & Wait Time:                   427s       7.11m     0.12h    0.00d  0.000 y
#Average job time:                   6s       0.09m     0.00h    0.00d
#Longest job:                       10s       0.17m     0.00h    0.00d
#Submission to last job:            52s       0.87m     0.01h    0.00d
    logout

    # create track; we want to tweak the scores and the names
    sed 's/id //' PREDICTIONS/*/*.bed | \
	awk '{printf "%s\t%s\t%s\tlod=%d\t%d\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n", \
	    $1, $2, $3, $5, 221.65 * log($5) - 352.64, $6, $7, $8, $9, \
	    $10, $11, $12}' > all.bed
    hgLoadBed dm1 phastConsElements all.bed

    # Scores are transformed as follows, for a reasonable-looking 
    # "spectrum".  Let x_max be the maximum (believable) score (here
    # x_max  = 447) and let x_med be the median score (here x_med =
    # 19).  The  scores are transformed via the function f(x) = a *
    # log x + b, s.t. f(x_med) = 300 and f(x_max) = 1000.  Solving
    # for a and b, you obtain  b = (300 log x_max - 1000 log x_med) /
    # (log x_max - log x_med), a = (1000 - b) / log x_max.  Here a =
    # 221.65, b = -352.64.

    # trackDb.ra file
#track phastConsElements
#shortLabel phastConsElements
#longLabel phastCons Conserved Elements, melanogaster/yakuba/pseudoobscura
#group compGeno
#priority 105
#visibility hide
#spectrum on
#color 0,60,120
#altColor 200,220,255
#exonArrows off
#type bed 12 .

    # should gzip or even delete contents of SS directory when done


# PHASTCONS MELANOGASTER/YAKUBA/PSEUDOOBSCURA/ANOPHELES (DONE 8/10/04 angie)
# NOTE - REPLACED, SEE "NEW PHASTCONS" SECTION BELOW
    # step 1: fit phylogenetic model, with rate variation (discrete
    # gamma model) first we need to extract the sufficient stats for
    # the phylogenetic model from the MAFs.  In this case, we don't
    # care about the order of the tuples (allows for much more compact
    # representation)
    ssh kksilo
    mkdir /cluster/data/dm1/bed/phastCons4way
    cd /cluster/data/dm1/bed/phastCons4way
    mkdir unordered
    foreach f (/cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/mypa/chr*.maf)
      /cluster/bin/phast/msa_view $f --in-format MAF --out-format SS \
            --order dm1,droYak1,dp2,anoGam1 \
            --unordered-ss  > unordered/$f:t:r.ss
    end
    # aggregate for whole genome
    ls unordered/chr*.ss > fnames
    /cluster/bin/phast/msa_view "*fnames" -i SS \
      --aggregate dm1,droYak1,dp2,anoGam1 -o SS --unordered-ss \
      > all.ss
    # estimate parameters; should be very fast in this case
    /cluster/bin/phast/phyloFit --msa all.ss \
      --tree "(((dm1,droYak1),dp2),anoGam1)" \
      --msa-format SS --subst-mod REV --nrates 10 --out-root rev-dg
    # make sure model looks reasonable
    cat rev-dg.mod
#ALPHABET: A C G T 
#ORDER: 0
#SUBST_MOD: REV
#NRATECATS: 10
#ALPHA: 1.343144
#TRAINING_LNL: -311520966.123671
#BACKGROUND: 0.276938 0.223190 0.223142 0.276730 
#RATE_MAT:
#  -0.926281    0.197688    0.496369    0.232223 
#   0.245294   -1.090671    0.230605    0.614772 
#   0.616038    0.230655   -1.092265    0.245573 
#   0.232398    0.495831    0.198018   -0.926247 
#TREE: (((dm1:0.056418,droYak1:0.074515):0.111186,dp2:0.219098):0.288564,anoGam1:0.288564);

    # Since anopheles is at such a large distance (est. divergence of 
    # branches that became drosophila & anopheles: ~250mya, as opposed to 
    # 35-40mya for mel-pseudo and 10-15mya for mel-yakuba), the branch length
    # between anopheles and pseudoobscura is underestimated above.  Adam 
    # scaled the lengths according to the published est. mya's, so 
    # edit rev-dg.mod's TREE line to this:
#TREE: (((dm1:0.058,droYak1:0.074):0.133,dp2:0.200):0,anoGam1:2.66);
    # A new way to arrive at that: put Mya estimates in 4way.nh, and 
    # merge with the (more reliable) 3-way estimated tree:
    echo "(((dm1:10,droYak1:10):30,dp2:40):210,anoGam1:250);" \
    > 4wayMya.nh
    /cluster/bin/phast/tree_doctor /cluster/data/dm1/bed/phastCons/rev-dg.mod \
      --rename '1->dm1; 2->droYak1; 3->dp2; 4->anoGam1' \
    | /cluster/bin/phast/tree_doctor - --merge 4wayMya.nh
#TREE: (((dm1:0.057640,droYak1:0.073631):0.142750,dp2:0.190334):1.083495,anoGam1:1.28987
    # Very similar to Adam's tree with a little manual redistribution of 
    # weights from ancestors to outgroups at the outermost 2 nodes.

    # beware of zero branch lengths (indicates bad topology) or very
    # large branch lengths (much greater than one; indicates
    # convergence problems).  Can try without --nrates or with
    # --subst-mod HKY85 to double check (parameter estimates should be
    # in the same ballpark, REV + dG likelihood should be a bit
    # better).  Also can write a log file to monitor convergence
    # (--log).  Also watch out for alpha much less than one.
    # Sometimes it's helpful to produce a rendering of the tree using
    # draw_tree (run on the *.nh file)

    # step 2: run phastCons.  First partition the alignments into
    # bite-sized chunks.  This time we need the ordered version of the
    # SS format.
    set WINSIZE=1000000
    set WINOVERLAP=0
    mkdir SS
    foreach f (/cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/mypa/*.maf)
      set chr=$f:t:r
      set c=`echo $chr | sed 's/chr//'`
      echo $f $chr $c
      mkdir SS/$c
      /cluster/bin/phast/msa_split $f -i MAF -o SS \
        -O dm1,droYak1,dp2,anoGam1 -M /cluster/data/dm1/$c/$chr.fa \
        -w ${WINSIZE},${WINOVERLAP} -r SS/$c/$chr -I 1000 -d 1 -B 5000
    end
    # save some space and I/O time:
    gzip SS/*/*.ss
    ssh kk
    cd /cluster/data/dm1/bed/phastCons4way
    cat << '_EOF_' > doPostProbs
#!/bin/csh -ef

set PHAST=/cluster/bin/phast
set TMP=/scratch/phastCons

set file=$1
set root = $file:t:r:r
set chrom = $root:r

mkdir -p $TMP
cp $file $TMP/$root.ss.gz
zcat $TMP/$root.ss.gz \
| $PHAST/phastCons - rev-dg.mod --cut-at 2 --nrates 10 \
  --transitions 0.030,0.015 --quiet \
  > ${TMP}/$root.pp
mkdir -p POSTPROBS/$chrom
gzip -c $TMP/$root.pp > POSTPROBS/$chrom/$root.pp.gz
rm $TMP/$root.pp
rm $TMP/$root.ss.gz
'_EOF_'
    # << this line makes emacs coloring happy
    chmod 775 doPostProbs
    # Note: the --cut-params arguments above are approximate
    # likelihood estimates obtained by running phastCons *without* the
    # --cut-params argument on four or five different windows (all
    # gave similar results).  They may need to change for different
    # data sets.  Careful, though: the parameter estimation procedure
    # is a little unstable.  Be sure to create a log file and monitor
    # for convergence.
    rm -f jobs.lst
    foreach f (`ls -1S SS/*/*.ss.gz`)
      echo './doPostProbs {check in exists+ '$f'}' \
      >> jobs.lst
    end
    para create jobs.lst 
    para try ; para push ; etc....
#Completed: 134 of 134 jobs
#Average job time:                  23s       0.38m     0.01h    0.00d
#Longest job:                       30s       0.50m     0.01h    0.00d
#Submission to last job:           187s       3.12m     0.05h    0.00d

    # now create wiggle track
    # NOTE: might want to integrate with the multiz track instead of
    # keeping separate
    ssh kksilo
    cd /cluster/data/dm1/bed/phastCons4way
    mkdir wib
    foreach dir (POSTPROBS/*)
        echo $dir
        set chr=$dir:t
        zcat `ls -1 $dir/*.pp.gz | sort -t\. -k2,2n` | \
            wigAsciiToBinary -chrom=$chr \
            -wibFile=wib/${chr}_phastCons4way stdin
    end
    ssh hgwdev
    mkdir -p /gbdb/dm1/wib/mzDy1Dp2Ag1_phast
    cd /cluster/data/dm1/bed/phastCons4way
    chmod 775 . wib
    chmod 664 wib/*.wib
    ln -s `pwd`/wib/*.wib /gbdb/dm1/wib/mzDy1Dp2Ag1_phast/
    hgLoadWiggle dm1 mzDy1Dp2Ag1_phast_wig \
      -pathPrefix=/gbdb/dm1/wib/mzDy1Dp2Ag1_phast wib/*.wig

    # step 3:  predictions of conserved elements
    # (could do these at the same time as step 2, but we want to use
    # different --rates-cut params)
    ssh kk
    cd /cluster/data/dm1/bed/phastCons4way
    cat << '_EOF_' > doElements
#!/bin/csh -fe

set PHAST=/cluster/home/acs/phast/bin
set TMP=/scratch/phastCons

set file=$1
set root = $file:t:r:r
set chrom = $root:r

mkdir -p $TMP
mkdir -p PREDICTIONS/$chrom
cp $file $TMP/$root.ss.gz
zcat $TMP/$root.ss.gz \
| $PHAST/phastCons - rev-dg.mod --nrates 10 \
  --viterbi PREDICTIONS/$chrom/$root.bed --score --no-post-probs \
  --transitions 0.030,0.015 --quiet --seqname $chrom
rm $TMP/$root.ss.gz
'_EOF_'
    # << this line makes emacs coloring happy
    chmod 775 doElements
    rm -f jobs.els.lst
    foreach f (SS/*/*.ss.gz)
      echo 'doElements {check in exists+ '$f'}' >> jobs.els.lst
    end
    para create jobs.els.lst
    para try ; para push ; etc....
#Completed: 134 of 134 jobs
#Average job time:                   5s       0.09m     0.00h    0.00d
#Longest job:                        9s       0.15m     0.00h    0.00d
#Submission to last job:           287s       4.78m     0.08h    0.00d

    # Reloaded 8/24/04 with corrected transform / new script:
    # Create bed track: tweak scores and names, trim the unused strand column.
    cat PREDICTIONS/*/*.bed \
    | awk '{printf "%s\t%d\t%d\tlod=%d\t%s\n", $1, $2, $3, $5, $5;}' \
    | /cluster/bin/scripts/lodToBedScore \
    > all.bed

    hgLoadBed dm1 phastConsElements all.bed
    featureBits dm1 phastConsElements
#22859943 bases of 126527731 (18.067%) in intersection

    # make top-5000 list and launcher on Adam's home page:
    sort -k5,5nr raw.bed | head -5000 > top5000.bed
    /cluster/home/acs/bin/make-launcher-with-scores.sh top5000.bed \
      /cse/grads/acs/public_html/dm-top5000-4way \
      "top 5000 conserved elements (4way)" dm1


# NEW PHASTCONS MELANOGASTER/YAKUBA/PSEUDOOBSCURA/ANOPHELES (DONE 9/24/04 angie)
    ssh kksilo
    # copy chrom fa to bluearc, break up the genome-wide MAFs into pieces
    mkdir -p /cluster/bluearc/dm1/chrom
    cp -p /cluster/data/dm1/?{,?}/chr*.fa /cluster/bluearc/dm1/chrom/
    ssh kki
    mkdir /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/phastCons
    mkdir /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/phastCons/run.split
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/phastCons/run.split
    set WINDOWS = /cluster/bluearc/dm1/phastCons/WINDOWS
    rm -fr $WINDOWS
    mkdir -p $WINDOWS
    cat << 'EOF' > doSplit.sh
#!/bin/csh -ef

set PHAST=/cluster/bin/phast
set FA_SRC=/cluster/bluearc/dm1/chrom
set WINDOWS=/cluster/bluearc/dm1/phastCons/WINDOWS

set maf=$1
set c = $maf:t:r
set tmpDir = /scratch/msa_split/$c
rm -rf $tmpDir
mkdir -p $tmpDir
${PHAST}/msa_split $maf -i MAF -M ${FA_SRC}/$c.fa -O dm1,droYak1,dp2,anoGam1 \
   -w 1000000,0 -r $tmpDir/$c -o SS -I 1000 -B 5000
cd $tmpDir
foreach file ($c.*.ss)
  gzip -c $file > ${WINDOWS}/$file.gz
end
rm -f $tmpDir/$c.*.ss
rmdir $tmpDir
'EOF'
# << for emacs
    chmod a+x doSplit.sh
    rm -f jobList
    foreach file (/cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/mypa/*.maf) 
      if (-s $file) then
        echo "doSplit.sh {check in line+ $file}" >> jobList
      endif
    end
    para create jobList
    para try,  check, push, check...
#Completed: 11 of 11 jobs
#Average job time:                  25s       0.42m     0.01h    0.00d
#Longest job:                       60s       1.00m     0.02h    0.00d
#Submission to last job:            60s       1.00m     0.02h    0.00d
    cd ..

    # use the model previously estimated (seein phastCons4way as a 
    # starting model.
    cp /cluster/data/dm1/bed/phastCons4way/rev-dg.mod starting-tree.mod
    # -- Because of the very long branch length to anoGam1 being pretty 
    # much impossible to estimate from alignment data, edit that file to 
    # reduce the anoGam1 branch length from 2.66 to 0.5.  Otherwise 
    # estimation process blows up.  So our starting tree becomes
#TREE: (((dm1:0.058,droYak1:0.074):0.133,dp2:0.200):0,anoGam1:0.5);

    # Get genome-wide average GC content (for all species together,
    # not just the reference genome).  If you have a globally
    # estimated tree model, as above, you can get this from the
    # BACKGROUND line in the .mod file.  E.g.,
# ALPHABET: A C G T
# ...
# BACKGROUND: 0.276938 0.223190 0.223142 0.276730
    # add up the C and G:
    awk '$1 == "BACKGROUND:" {printf "%0.3f\n", $3 + $4;}' starting-tree.mod
#0.446
    # Great, use 0.446 as the --gc parameter in phastCons below:.

    # Now set up cluster job to estimate model parameters.  
    # Use parameters iteratively determined for dm2 -- see makeDm2.doc:
    # --target-coverage 0.20 --expected-lengths 25
    # Parameters will be estimated separately for each alignment fragment 
    # then will be combined across fragments.

    mkdir run.estimate
    cd run.estimate
    cat << '_EOF_' > doEstimate.sh
#!/bin/csh -ef
zcat $1 \
| /cluster/bin/phast/phastCons - ../starting-tree.mod --gc 0.446 --nrates 1,1 \
    --no-post-probs --ignore-missing --expected-lengths 25 \
    --target-coverage 0.20 --quiet --log $2 --estimate-trees $3
'_EOF_'
# << for emacs
    chmod a+x doEstimate.sh
    rm -fr LOG TREES
    mkdir -p LOG TREES
    rm -f jobList
    foreach f (/cluster/bluearc/dm1/phastCons/WINDOWS/*.ss.gz)
      set root = $f:t:r:r
      echo doEstimate.sh $f LOG/$root.log TREES/$root >> jobList
    end
    # run cluster job
    ssh kk9
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/phastCons/run.estimate
    para create jobList
    para try, check, push, check, ...
#Completed: 134 of 134 jobs
#Average job time:                 135s       2.26m     0.04h    0.00d
#Longest job:                      258s       4.30m     0.07h    0.00d
#Submission to last job:           350s       5.83m     0.10h    0.00d

    # Now combine parameter estimates.  We can average the .mod files
    # using phyloBoot.  This must be done separately for the conserved
    # and nonconserved models
    ssh kksilo
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/phastCons/run.estimate
    ls -1 TREES/*.cons.mod > cons.txt
    /cluster/bin/phast/phyloBoot --read-mods '*cons.txt' \
      --output-average ave.cons.mod > cons_summary.txt
    ls -1 TREES/*.noncons.mod > noncons.txt
    /cluster/bin/phast/phyloBoot --read-mods '*noncons.txt' \
      --output-average ave.noncons.mod > noncons_summary.txt
    grep TREE ave*.mod
#ave.cons.mod:TREE: (((dm1:0.019916,droYak1:0.025549):0.038378,dp2:0.068605):0.085662,anoGam1:0.085662);
#ave.noncons.mod:TREE: (((dm1:0.081971,droYak1:0.105403):0.161271,dp2:0.290165):0.361849,anoGam1:0.361849);
    cat cons_summary.txt 
    # look over the files cons_summary.txt and noncons_summary.txt.
    # The means and medians should be roughly equal and the stdevs
    # should be reasonably small compared to the means, particularly
    # for rate matrix parameters (at bottom) and for branches to the
    # leaves of the tree.  The stdevs may be fairly high for branches
    # near the root of the tree; that's okay.  Some min values may be
    # 0 for some parameters.  That's okay, but watch out for very large
    # values in the max column, which might skew the mean.  If you see
    # any signs of bad outliers, you may have to track down the
    # responsible .mod files and throw them out.  I've never had to do
    # this; the estimates generally seem pretty well behaved.

    # NOTE: Actually, a random sample of several hundred to a thousand
    # alignment fragments (say, a number equal to the number of
    # available cluster nodes) should be more than adequate for
    # parameter estimation.  If pressed for time, use this strategy.

    # Now we are ready to set up the cluster job for computing the
    # conservation scores and predicted elements.  It's all downhill
    # from here.
    ssh kk9
    mkdir /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/phastCons/run.phast
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/phastCons/run.phast
    cat << 'EOF' > doPhastCons.sh
#!/bin/csh -ef
set pref = $1:t:r:r
set chr = `echo $pref | awk -F\. '{print $1}'`
set tmpfile = /scratch/phastCons.$$
zcat $1 \
| /cluster/bin/phast/phastCons - \
    ../run.estimate/ave.cons.mod,../run.estimate/ave.noncons.mod \
    --expected-lengths 25 --target-coverage 0.20 --quiet --seqname $chr \
    --idpref $pref \
    --viterbi /cluster/bluearc/dm1/phastCons/ELEMENTS/$pref.bed --score \
    --require-informative 0 \
  > $tmpfile
gzip -c $tmpfile > /cluster/bluearc/dm1/phastCons/POSTPROBS/$pref.pp.gz
rm $tmpfile
'EOF'
# << for emacs
    chmod a+x doPhastCons.sh
    rm -fr /cluster/bluearc/dm1/phastCons/{POSTPROBS,ELEMENTS}
    mkdir -p /cluster/bluearc/dm1/phastCons/{POSTPROBS,ELEMENTS}
    rm -f jobList
    foreach f (/cluster/bluearc/dm1/phastCons/WINDOWS/*.ss.gz)
      echo doPhastCons.sh $f >> jobList
    end
    para create jobList
    para try, check, push, check, ...
#Completed: 134 of 134 jobs
#Average job time:                  26s       0.43m     0.01h    0.00d
#Longest job:                       53s       0.88m     0.01h    0.00d
#Submission to last job:            55s       0.92m     0.02h    0.00d

    # back on kksilo:
    # combine predictions and transform scores to be in 0-1000 interval
    # do in a way that avoids limits on numbers of args
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/phastCons
    awk '{printf "%s\t%d\t%d\tlod=%d\t%s\n", $1, $2, $3, $5, $5;}' \
      /cluster/bluearc/dm1/phastCons/ELEMENTS/*.bed \
    | /cluster/bin/scripts/lodToBedScore > all.bed

    ssh hgwdev
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/phastCons
    featureBits dm1 all.bed
#37105867 bases of 126527731 (29.326%) in intersection
    # OK, close enough.
    hgLoadBed dm1 phastConsElements all.bed

    # Create wiggle on the small cluster
    ssk kki
    mkdir /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/phastCons/run.wib
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/phastCons/run.wib
    rm -rf /cluster/bluearc/dm1/phastCons/wib
    mkdir -p /cluster/bluearc/dm1/phastCons/wib
    cat << 'EOF' > doWigAsciiToBinary
#!/bin/csh -ef
set chr = $1
zcat `ls -1 /cluster/bluearc/dm1/phastCons/POSTPROBS/$chr.*.pp.gz \
      | sort -t\. -k2,2n` \
| wigAsciiToBinary -chrom=$chr \
    -wibFile=/cluster/bluearc/dm1/phastCons/wib/${chr}_phastCons stdin 
'EOF'
# << for emacs
    chmod a+x doWigAsciiToBinary
    rm -f jobList
    foreach chr (`ls -1 /cluster/bluearc/dm1/phastCons/POSTPROBS \
                  | awk -F\. '{print $1}' | sort -u`)
      echo doWigAsciiToBinary $chr >> jobList
    end
    para create jobList
    para try, check, push, check, ...
#Completed: 11 of 11 jobs
#Average job time:                  19s       0.31m     0.01h    0.00d
#Longest job:                       47s       0.78m     0.01h    0.00d
#Submission to last job:            47s       0.78m     0.01h    0.00d

    # back on kksilo, copy wibs, wigs and POSTPROBS (people sometimes want 
    # the raw scores) from bluearc
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/phastCons
    rm -rf wib POSTPROBS
    rsync -av /cluster/bluearc/dm1/phastCons/wib .
    rsync -av /cluster/bluearc/dm1/phastCons/POSTPROBS .

    # load wiggle component of Conservation track
    ssh hgwdev
    mkdir -p /gbdb/dm1/wib/mzDy1Dp2Ag1_phast
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp2anoGam1/phastCons
    chmod 775 . wib
    chmod 664 wib/*.wib
    ln -s `pwd`/wib/*.wib /gbdb/dm1/wib/mzDy1Dp2Ag1_phast/
    hgLoadWiggle dm1 mzDy1Dp2Ag1_phast_wig \
      -pathPrefix=/gbdb/dm1/wib/mzDy1Dp2Ag1_phast wib/*.wig

    # make top-5000 list and launcher on Adam's home page:
    sed -e 's/lod=//' all.bed | sort -k4,4nr | head -5000 \
    | awk '{printf "%s\t%d\t%d\tlod=%d\t%d\n", $1, $2, $3, $4, $4}' \
    > top5000.bed
    /cluster/home/acs/bin/make-launcher-with-scores.sh top5000.bed \
      /cse/grads/acs/public_html/dm-top5000-4way \
      "top 5000 conserved elements (4way)" dm1

    # and clean up bluearc.
    rm -r /cluster/bluearc/dm1/phastCons
    rm -r /cluster/bluearc/dm1/chrom


# LOAD ENSEMBL GENES (NEVER MIND 6/16/04 angie)
    # OK, never mind, Ensembl just imported the BDGP / Flybase genes.

# BLAT FlyBase predicted DM proteins against DM
    ssh hgwdev
    cd /cluster/data/dm1/bed
    mkdir blat.dm1FB
    cd blat.dm1FB
    pepPredToFa dm1 bdgpGenePep dm1FB.fa
    hgPepPred dm1 generic blastFBPep00 dm1FB.fa
    ssh kk
    cd /cluster/data/hg17/bed/blat.dm1FB
    cat << '_EOF_' > blatSome
#!/bin/csh -fe
/cluster/bin/i386/blat -t=dnax -q=prot -out=pslx $1 $2 $3
'_EOF_'
    # << keep emacs happy
    chmod +x blatSome
    ls -1S /iscratch/i/dm1/nib/*.nib > bug.lst
    mkdir fbfas
    cd fbfas
    faSplit sequence ../dm1FB.fa 5000 kg
    cd ..
    ls -1S fbfas/*.fa > fb.lst
    cat << '_EOF_' > blatGsub
#LOOP
blatSome $(path1) {check in line $(path2)} {check out line psl/$(root1)/$(root2).psl}
#ENDLOOP
'_EOF_'
    # << keep emacs happy
    gensub2 human.lst kg.lst blatGsub blatSpec
    mkdir psl
    cd psl
    foreach i (`cat ../bug.lst`)
	mkdir `basename $i .nib`
    end
    cd ..
    para create blatSpec
    para push

53361 jobs in batch
Completed: 53361 of 53361 jobs
CPU time in finished jobs:     640171s   10669.52m   177.83h    7.41d  0.020 y
IO & Wait Time:                144105s    2401.75m    40.03h    1.67d  0.005 y
Average job time:                  15s       0.24m     0.00h    0.00d
Longest job:                     1924s      32.07m     0.53h    0.02d
Submission to last job:          3506s      58.43m     0.97h    0.04d

    ssh eieio
    cd /cluster/data/dm1/bed/blat.dm1FB
    pslSort dirs raw.psl /tmp psl/*
    pslReps -nohead -minCover=0.9 -minAli=0.9 raw.psl cov90.psl /dev/null
    sort -rn cov90.psl | pslUniq stdin dm1FB.psl
    pslxToFa dm1FB.psl dm1FB_ex.fa -liftTarget=genome.lft -liftQuery=protein.lft
    fbName dm1 dm1FB.psl blastFBRef00
    ssh hgwdev
    cd /cluster/data/dm1/bed/blat.dm1FB
    hgsql dm1 < ~/kent/src/lib/hg/blastRef.sql
    echo "rename table blastRef to blastFBRef00" | hgsql dm1
    echo "load data local infile 'blastFBRef00' into table blastFBRef00" | hgsql dm1

# MITOPRED DATA FOR HGGENE (DONE 8/10/04 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/mitopred
    cd /cluster/data/dm1/bed/mitopred
    wget http://mitopred.sdsc.edu/data/fly_30.out
    perl -wpe 's/^(\S+)\s+\S+\s+(.*)/$1\t$2/' fly_30.out > mitopred.tab
    cat > mitopred.sql << '_EOF_'
# Prediction of nuclear-encoded mito. proteins from http://mitopred.sdsc.edu/
CREATE TABLE mitopred (
    name varchar(10) not null,      # SwissProt ID
    confidence varchar(8) not null, # Confidence level
              #Indices
    PRIMARY KEY(name(6))
);
'_EOF_'
    # << this line makes emacs coloring happy
    hgsql dm1 < mitopred.sql
    hgsql dm1 -e 'load data local infile "mitopred.tab" into table mitopred'

# MAKE Human Proteins track
    mkdir -p /cluster/data/dm1/bed/tblastn.hg16KG
    cd /cluster/data/dm1/bed/tblastn.hg16KG
    ls -1S /iscratch/i/dm1/blastDb/*.nsq | sed "s/\.nsq//" > bug.lst
    exit

    # back to kksilo
    cd /cluster/data/dm1/bed/tblastn.hg16KG
    mkdir kgfa
    # calculate a reasonable number of jobs
    calc `wc /cluster/data/hg16/bed/blat.hg16KG/hg16KG.psl | awk "{print \\\$1}"`/\(150000/`wc bug.lst | awk "{print \\\$1}"`\)
    # 41117/(150000/578) = 158.437507
    split -l 158 /cluster/data/hg16/bed/blat.hg16KG/hg16KG.psl kgfa/kg
    cd kgfa
    for i in *; do pslxToFa $i $i.fa; rm $i; done
    cd ..
    ls -1S kgfa/*.fa > kg.lst
    mkdir blastOut
    for i in `cat kg.lst`; do  mkdir blastOut/`basename $i .fa`; done
    cat << '_EOF_' > blastGsub
#LOOP
blastSome $(path1) {check in line $(path2)} {check out exists blastOut/$(root2)/q.$(root1).psl } 
#ENDLOOP
'_EOF_'
    cat << '_EOF_' > blastSome
#!/bin/sh
BLASTMAT=/iscratch/i/blast/data
export BLASTMAT
f=/tmp/`basename $3`
for eVal in 0.01 0.001 0.0001 0.00001 0.000001 1E-09 1E-11
do
if /scratch/blast/blastall -M BLOSUM80 -m 0 -F no -e $eVal -p tblastn -d $1 -i $2 -o $f.8
then
        mv $f.8 $f.1
        break;
fi
done
if test -f  $f.1
then
if /cluster/bin/i386/blastToPsl $f.1 $f.2
then
        liftUp -nosort -type=".psl" -pslQ -nohead $3.tmp /cluster/data/hg16/bed/blat.hg16KG/protein.lft warn $f.2
	if pslCheck -prot $3.tmp
	then
	    mv $3.tmp $3
	    rm -f $f.1 $f.2 
	    exit 0
	fi
    fi
fi
rm -f $f.1 $f.2 $3.tmp $f.8
exit 1
'_EOF_'

    chmod +x blastSome
    gensub2 bug.lst kg.lst blastGsub blastSpec

    ssh kk
    cd /cluster/data/dm1/bed/tblastn.hg16KG
    para create blastSpec
    para push

    cat << '_EOF_' > chainGsub
#LOOP
chainSome $(path1)
#ENDLOOP
'_EOF_'

    cat << '_EOF_' > chainSome
(cd $1; cat q.*.psl | simpleChain -prot -outPsl -maxGap=25000 stdin ../c.`basename $1`.psl)
'_EOF_'
    chmod +x chainSome

    ls -1dS `pwd`/blastOut/kg?? > chain.lst
    gensub2 chain.lst single chainGsub chainSpec

    ssh kki
    cd /cluster/data/dm1/bed/tblastn.hg16KG
    para create chainSpec
    para push

    exit
    # back to kksilo
    cd /cluster/data/dm1/bed/tblastn.hg16KG/blastOut
    for i in kg??
    do 
	awk "(\$13 - \$12)/\$11 > 0.6 {print}" c.$i.psl > c60.$i.psl
	sort -rn c60.$i.psl | pslUniq stdin u.$i.psl
	awk "((\$1 / \$11) ) > 0.60 { print   }" c60.$i.psl > m60.$i.psl
	echo $i
    done

    cat u.*.psl m60* | sort -T /tmp -k 14,14 -k 16,16n -k 17,17n  | uniq  > ../preblastHg16KG.psl
    cd ..
    blatDir=/cluster/data/hg16/bed/blat.hg16KG
    protDat -kg preblastHg16KG.psl $blatDir/hg16KG.psl $blatDir/kg.mapNames blastHg16KG.psl

    ssh hgwdev
    cd /cluster/data/dm1/bed/tblastn.hg16KG
    hgLoadPsl dm1 blastHg16KG.psl
    exit

    # back to kksilo
    rm -rf blastOut

# End tblastn


# BLASTZ DM2 (FOR LIFTOVER EXPERIMENT) (DONE 9/15/04 angie)
    ssh kksilo
    mkdir /cluster/data/dm1/bed/blastz.dm2.2004-09-15
    cd /cluster/data/dm1/bed/blastz.dm2.2004-09-15
    cat << '_EOF_' > DEF
# dm1 vs. dm2 (for liftOver)
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin

ALIGN=/cluster/bin/penn/blastz-run
BLASTZ=/cluster/bin/penn/blastz
BLASTZ_H=2000
BLASTZ_ABRIDGE_REPEATS=0

SEQ1_DIR=/iscratch/i/dm1/nib
SEQ1_SMSK=
SEQ1_FLAG=-drosophila
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

SEQ2_DIR=/iscratch/i/dm2/nib
SEQ2_SMSK=
SEQ2_FLAG=-drosophila
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/cluster/data/dm1/bed/blastz.dm2.2004-09-15

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len

'_EOF_'
    # << this line keeps emacs coloring happy
    # run bash shell if you don't already:
    bash
    source DEF
    mkdir run
    /cluster/bin/scripts/blastz-make-joblist $DEF > $BASE/run/j
    sh ./xdir.sh
    # cluster run -- use rack 9 to avoid getting in the way of hg17.
    ssh kk9
    cd /cluster/data/dm1/bed/blastz.dm2.2004-09-15/run
    para create j
    para try, check, push, check, ....
#Completed: 483 of 483 jobs
#Average job time:                 119s       1.99m     0.03h    0.00d
#Longest job:                     1238s      20.63m     0.34h    0.01d
#Submission to last job:          1625s      27.08m     0.45h    0.02d

    # back on kksilo...
    mkdir /cluster/data/dm1/bed/blastz.dm2.2004-09-15/run.1
    cd /cluster/data/dm1/bed/blastz.dm2.2004-09-15/run.1
    /cluster/bin/scripts/blastz-make-out2lav $DEF $BASE > jobList
    # small cluster run
    ssh kki
    cd /cluster/data/dm1/bed/blastz.dm2.2004-09-15/run.1
    para create jobList
    para try, check, push, check, ....
#Completed: 21 of 21 jobs
#Average job time:                   8s       0.13m     0.00h    0.00d
#Longest job:                       23s       0.38m     0.01h    0.00d
#Submission to last job:            35s       0.58m     0.01h    0.00d
    cd ..
    rm -r raw

    # third run: lav -> axt
    ssh kki
    cd /cluster/data/dm1/bed/blastz.dm2.2004-09-15
    mkdir axtChrom run.2
    cd run.2
    cat << '_EOF_' > do.csh
#!/bin/csh -ef
cd $1
set chr = $1:t
cat `ls -1 *.lav | sort -g` \
| lavToAxt stdin \
    /cluster/bluearc/drosophila/dm1/nib /iscratch/i/dm2/nib stdout \
| axtSort stdin ../../axtChrom/$chr.axt 
'_EOF_'
    # << this line keeps emacs coloring happy
    chmod a+x do.csh
    cp /dev/null jobList
    foreach d (../lav/chr*)
      echo "do.csh $d" >> jobList
    end
    para create jobList
    para try, check, push, check
#Completed: 11 of 11 jobs
#Average job time:                  41s       0.68m     0.01h    0.00d
#Longest job:                       62s       1.03m     0.02h    0.00d
#Submission to last job:            62s       1.03m     0.02h    0.00d


# CHAIN DM2 BLASTZ (FOR LIFTOVER EXPERIMENT) (DONE 9/15/04 angie)
    # Run axtChain on little cluster
    ssh kki
    cd /cluster/data/dm1/bed/blastz.dm2.2004-09-15
    mkdir -p axtChain/run1
    cd axtChain/run1
    mkdir out chain
    ls -1S /cluster/data/dm1/bed/blastz.dm2.2004-09-15/axtChrom/*.axt \
      > input.lst
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out exists out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    cat << '_EOF_' > doChain
#!/bin/csh -ef
axtChain -verbose=0 $1 \
  /iscratch/i/dm1/nib \
  /iscratch/i/dm2/nib $2 > $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doChain
    gensub2 input.lst single gsub jobList
    para create jobList
    para try, check, push, check...
#Completed: 11 of 11 jobs
#Average job time:                  20s       0.33m     0.01h    0.00d
#Longest job:                       62s       1.03m     0.02h    0.00d
#Submission to last job:            62s       1.03m     0.02h    0.00d

    # now on the file server, sort chains
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.dm2.2004-09-15/axtChain
    chainMergeSort run1/chain/*.chain > all.chain
    chainSplit chain all.chain
    rm run1/chain/*.chain

    # take a look at score distr's [looks like could use some filtering,
    # didn't abridge repeats]
    foreach f (chain/*.chain)
      grep chain $f | awk '{print $2;}' | sort -nr > /tmp/score.$f:t:r
      echo $f:t:r
      textHistogram -binSize=10000 /tmp/score.$f:t:r
      echo ""
    end

    # Don't load chains into database -- just doing this for liftOver.


# NET DM2 BLASTZ (FOR LIFTOVER EXPERIMENT) (DONE 9/15/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.dm2.2004-09-15/axtChain
    chainPreNet all.chain ../S1.len ../S2.len stdout \
    | chainNet stdin -minSpace=1 ../S1.len ../S2.len stdout /dev/null \
    | netSyntenic stdin noClass.net

    # Don't bother with classification and db-loading -- just make 
    # liftOver-suitable (single-coverage from net) chains:
    netSplit noClass.net net
    mkdir subset
    foreach f (chain/*.chain)
      echo subsetting net/$f:t:r.net, $f to subset/$f:t
      netChainSubset net/$f:t:r.net $f subset/$f:t
    end
    cat subset/*.chain > dm1ToDm2.over.chain

    # clean up
    rm -r net subset


# MAKE AN 11.OOC FILE FOR BLAT (DONE 9/16/04 angie)
    # Use -repMatch=100 (based on size -- for human we use 1024, and 
    # fly size is 126527731 / 2866216770 = ~4.4% of human judging by 
    # gapless genome size from featureBits -- we would use 45, but 
    # bump that up a bit to be more conservative)
    ssh kolossus
    mkdir /cluster/data/dm1/bed/ooc
    cd /cluster/data/dm1/bed/ooc
    ls -1 /cluster/data/dm1/nib/chr*.nib > nib.lst
    blat nib.lst /dev/null /dev/null -tileSize=11 \
      -makeOoc=/cluster/bluearc/dm1/11.ooc -repMatch=100
#Wrote 2917 overused 11-mers to /cluster/bluearc/dm1/11.ooc
    ssh kkr1u00
    cp -p /cluster/bluearc/dm1/*.ooc /iscratch/i/dm1/
    iSync


# BLAT DM2 TO GET LIFTOVER CHAINS (DONE 9/17/04 angie)
    # Note: this process makes /cluster/data/dm1/bed/blat.dm2.2004-09-17/ .
    # Previously I made a blat.dm2.2004-09-16/ using this process (mostly),
    # except for the split and lift steps.  I used the dm2 2Mb chunks and 
    # dm2/jkStuff/liftAll.lft.  Blat run was much slower... liftOver 
    # performance was about the same, so use the quicker smaller chunks.
    ssh kkr1u00
    ~/kent/src/hg/makeDb/makeLoChain/makeLoChain-split.csh \
      dm2 /iscratch/i/dm2/nib
    # Do what its output says to do next:
    ssh kk (or kk9)
    ~/kent/src/hg/makeDb/makeLoChain/makeLoChain-align.csh \
      dm1 /iscratch/i/dm1/nib dm2 /iscratch/i/dm2/split3k \
      /iscratch/i/dm1/11.ooc
    # Do what its output says to do next:
    cd /cluster/data/dm1/bed/blat.dm2.2004-09-16/run
    para try, check, push, check, ...
#Completed: 143 of 143 jobs
#Average job time:                  37s       0.62m     0.01h    0.00d
#Longest job:                      282s       4.70m     0.08h    0.00d
#Submission to last job:           283s       4.72m     0.08h    0.00d
    ssh kksilo
    ~/kent/src/hg/makeDb/makeLoChain/makeLoChain-lift.csh dm1 dm2 \
      /iscratch/i/dm2/split3k
    # Do what its output says to do next:
    ssh kki
    ~/kent/src/hg/makeDb/makeLoChain/makeLoChain-chain.csh \
      dm1 /iscratch/i/dm1/nib dm2 /iscratch/i/dm2/nib
    # Do what its output says to do next:
    cd /cluster/data/dm1/bed/blat.dm2.2004-09-16/chainRun
    para try, check, push, check, ...
#Completed: 13 of 13 jobs
#Average job time:                   7s       0.12m     0.00h    0.00d
#Longest job:                       11s       0.18m     0.00h    0.00d
#Submission to last job:            11s       0.18m     0.00h    0.00d
    ssh kksilo
    ~/kent/src/hg/makeDb/makeLoChain/makeLoChain-net.csh dm1 dm2
    # Do what its output says to do next:
    ssh hgwdev
    ~/kent/src/hg/makeDb/makeLoChain/makeLoChain-load.csh dm1 dm2


