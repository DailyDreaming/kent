#!/bin/csh -f # set emacs mode
exit; # don't actually run this like a script :)

# Drosophila Melanogaster -- 
# 
# Berkeley Drosophila Genome Project (fruitfly.org) release 3.1 (Jan. 2003)
# http://www.fruitfly.org/annot/release3.html
#
# FlyBase (http://flybase.bio.indiana.edu/) last updated 20 Jan 2003
#

# DOWNLOAD SEQUENCE (DONE 10/2/03 angie)
    ssh kksilo
    mkdir /cluster/store6/dm1
    cd /cluster/data
    ln -s /cluster/store6/dm1 dm1
    cd /cluster/data/dm1
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/FASTA/whole_genome_genomic_dmel_RELEASE3-1.FASTA
    faSplit byname whole_genome_genomic_dmel_RELEASE3-1.FASTA dummyArg

    # Follow FlyBase's lead on the chromosome names, but still use our 
    # "chr" prefix:
    foreach c (2L 2R 2h 3L 3R 3h 4 X Xh Yh U)
      mkdir $c
      sed -e 's/^>/>chr/' $c.fa > $c/chr$c.fa
      echo chr$c.fa size:
      faSize $c.fa
      echo $c/chr$c.fa size:
      faSize $c/chr$c.fa
      echo comparison:
      faCmp $c.fa $c/chr$c.fa
      echo ""
    end
    # Carefully review output of those commands, then:
    rm 2L.fa 2R.fa 2h.fa 3L.fa 3R.fa 3h.fa 4.fa X.fa Xh.fa Yh.fa U.fa

    # put away the big download file
    mkdir -p downloads/fruitfly
    mv whole_genome_genomic_dmel_RELEASE3-1.FASTA downloads/fruitfly/


# SPLIT CHROM FA INTO SMALLER CHUNKS BY GAPS (DONE 10/3/03 angie)
    ssh kksilo
    cd /cluster/data/dm1
    foreach c (?{,?})
      faSplit -minGapSize=100 -lift=$c/chr$c.lft \
        gap $c/chr$c.fa 2000000 $c/chr${c}_
    end
    foreach ctgFa (?{,?}/chr*_*.fa)
      set ctg = $ctgFa:r
      mkdir $ctg
      mv $ctgFa $ctg
    end


# CREATING DATABASE (DONE 10/3/03 angie)
    # Create the database.
    ssh hgwdev
    echo 'create database dm1' | hgsql ''
    # Make a semi-permanent read-only alias:
    alias dm1 "mysql -u hguser -phguserstuff -A dm1"
    # Make sure there is at least 5 gig free for the database
    df -h /var/lib/mysql


# EXTRACT GAP INFORMATION FROM FASTA, LOAD GAP TRACK (DONE 10/3/03 angie)
    ssh kksilo
    cd /cluster/data/dm1
    # size up the gap situation -- can we use gaps to extract agp info?
    faGapSizes downloads/fruitfly/whole_genome_genomic_dmel_RELEASE3-1.FASTA 
    # yup.  Jim's verdict:
    # I think that we can probably just show all gaps as bridged
    # in the non-h chromosomes, and as unbridged in the h chromosomes
    # and leave it at that.

    # Extract gaps using scaffoldFaToAgp.  It's really meant for a different 
    # purpose, so clean up its output: remove the .lft and .agp, and remove 
    # the last line of .gap (extra gap added at end).  Also substitute in 
    # the correct chrom name in .gap.  
    foreach c (?{,?})
      set chr = chr$c
      pushd $c
      scaffoldFaToAgp -minGapSize=100 $chr.fa
      rm $chr.{lft,agp}
      set chrSize = `faSize $chr.fa | awk '{print $1;}'`
      set origLines = `cat $chr.gap | wc -l`
      awk '($2 != '$chrSize'+1) {print;}' $chr.gap \
      | sed -e "s/chrUn/$chr/" > $chr.gap2
      set newLines = `cat $chr.gap2 | wc -l`
      if ($newLines == ($origLines - 1)) then
        mv $chr.gap2 $chr.gap
      else
        echo "Error: $chr/$chr.gap2 has wrong number of lines."
      endif
      popd
    end
    # Call the gaps unbridged in chrU and chr*h:
    foreach c (U ?h)
      set chr = chr$c
      sed -e 's/yes/no/' $c/$chr.gap > $c/$chr.gap2
      mv $c/$chr.gap2 $c/$chr.gap
    end
    ssh hgwdev
    hgLoadGap dm1 /cluster/data/dm1


# MAKE DESCRIPTION/SAMPLE POSITION HTML PAGE (DONE 10/13/03 angie/donnak)
    ssh hgwdev
    # Write ~/kent/src/hg/makeDb/trackDb/drosophila/dm1/description.html 
    # with a description of the assembly and some sample position queries.  
    chmod a+r ~/kent/src/hg/makeDb/trackDb/drosophila/dm1/description.html
    # Check it in and copy (perhaps via a make in trackDb??) to 
    # /cluster/data/dm1/html.  
    mkdir -p /gbdb/dm1/html
    ln -s /cluster/data/dm1/html/description.html /gbdb/dm1/html/


# RUN REPEAT MASKER (DONE 10/4/03 angie)
    # Note: drosophila library ("drosophila.lib") is dated May 27 '03.
    # Contigs (*/chr*_*/chr*_*.fa) are split into 500kb chunks to make 
    # RepeatMasker runs manageable on the cluster ==> results need lifting.

    # Split contigs into 500kb chunks:
    ssh kksilo
    cd /cluster/data/dm1
    foreach d ( */chr*_?{,?} )
      cd $d
      set contig = $d:t
      faSplit -minGapSize=100 -lift=$contig.lft -maxN=500000 \
        gap $contig.fa 500000 ${contig}_
      cd ../..
    end

    # make the run directory, output directory, and job list
    mkdir RMRun
    cp /dev/null RMRun/RMJobs
    foreach d ( ?{,?}/chr*_?{,?} )
      set ctg = $d:t
      foreach f ( $d/${ctg}_?{,?}.fa )
        set f = $f:t
        echo /cluster/bin/scripts/RMDrosophila \
             /cluster/data/dm1/$d $f /cluster/data/dm1/$d \
           '{'check out line+ /cluster/data/dm1/$d/$f.out'}' \
        >> RMRun/RMJobs
      end
    end

    # do the run
    ssh kk
    cd /cluster/data/dm1/RMRun
    para create RMJobs
    para try
    para check
    para push
    para check,...
#Completed: 288 of 288 jobs
#CPU time in finished jobs:    1764726s   29412.10m   490.20h   20.43d  0.056 y
#IO & Wait Time:                  3392s      56.53m     0.94h    0.04d  0.000 y
#Average job time:                6139s     102.32m     1.71h    0.07d
#Longest job:                     7256s     120.93m     2.02h    0.08d
#Submission to last job:          7257s     120.95m     2.02h    0.08d

    # Lift up the split-contig .out's to contig-level .out's
    ssh kksilo
    cd /cluster/data/dm1
    foreach d ( ?{,?}/chr*_?{,?} )
      cd $d
      set contig = $d:t
      liftUp $contig.fa.out $contig.lft warn ${contig}_*.fa.out > /dev/null
      cd ../..
    end

    # Lift up the contig-level .out's to chr-level
    foreach c (?{,?})
      cd $c
      if (-e chr$c.lft && ! -z chr$c.lft) then
        echo lifting $c
        /cluster/bin/i386/liftUp chr$c.fa.out chr$c.lft warn \
          `awk '{print $2"/"$2".fa.out";}' chr$c.lft` > /dev/null
      else
        echo Can\'t find $c/chr$c.lft \!
      endif
      cd ..
    end

    # soft-mask contig .fa's with .out's
    foreach c (?{,?})
      foreach j ($c/chr${c}_?{,?}/chr${c}_?{,?}.fa)
        maskOutFa $j $j.out $j -soft
      end
      echo done $c
    end

    # Load the .out files into the database with:
    ssh hgwdev
    hgLoadOut dm1 /cluster/data/dm1/?{,?}/*.fa.out


# SIMPLE REPEATS (TRF) (DONE 10/5/03 angie)
    # TRF runs pretty quickly now... it takes a few hours total runtime, 
    # so instead of binrsyncing and para-running, just do this on the
    # local fileserver
    ssh kksilo
    mkdir /cluster/data/dm1/bed/simpleRepeat
    cd /cluster/data/dm1/bed/simpleRepeat
    mkdir trf
    cp /dev/null jobs.csh
    foreach f (/cluster/data/dm1/?{,?}/chr*_*/chr?{,?}_?{,?}.fa)
        set fout = $f:t:r.bed
        echo $fout
        echo "/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $f /dev/null -bedAt=trf/$fout -tempDir=/tmp" \
        >> jobs.csh
    end
    tcsh jobs.csh >&! jobs.log &
    # check on this with
    tail -f jobs.log
    wc -l jobs.csh
    ls -1 trf | wc -l
    # When job is done do:
    mkdir /cluster/data/dm1/jkStuff
    liftUp simpleRepeat.bed /cluster/data/dm1/jkStuff/liftAll.lft warn \
      trf/*.bed

    # Load this into the database as so
    ssh hgwdev
    hgLoadBed dm1 simpleRepeat \
      /cluster/data/dm1/bed/simpleRepeat/simpleRepeat.bed \
      -sqlTable=$HOME/src/hg/lib/simpleRepeat.sql


# FILTER SIMPLE REPEATS (TRF) INTO MASK (DONE 10/5/03 angie)
    # make a filtered version # of the trf output: 
    # keep trf's with period <= 12:
    ssh kksilo
    cd /cluster/data/dm1/bed/simpleRepeat
    mkdir -p trfMask
    foreach f (trf/*.bed)
        echo "filtering $f"
        awk '{if ($5 <= 12) print;}' $f > trfMask/$f:t
    end
    # Lift up filtered trf output to chrom coords as well:
    cd /cluster/data/dm1
    mkdir bed/simpleRepeat/trfMaskChrom
    foreach c (?{,?})
      liftUp bed/simpleRepeat/trfMaskChrom/chr$c.bed $c/chr$c.lft warn \
        `awk '{print "bed/simpleRepeat/trfMask/"$2".bed";}' $c/chr$c.lft`
    end


# MASK FA USING REPEATMASKER AND FILTERED TRF FILES (DONE 10/5/03 angie)
    ssh kksilo
    cd /cluster/data/dm1
    foreach c (?{,?})
      echo repeat- and trf-masking chr$c.fa
      /cluster/home/kent/bin/i386/maskOutFa -soft \
        $c/chr$c.fa $c/chr$c.fa.out $c/chr$c.fa
      /cluster/home/kent/bin/i386/maskOutFa -softAdd \
        $c/chr$c.fa bed/simpleRepeat/trfMaskChrom/chr$c.bed $c/chr$c.fa
    end
    foreach c (?{,?})
      echo repeat- and trf-masking contigs of chr$c
      foreach ctgFa ($c/chr*/chr${c}_?{,?}.fa)
        set trfMask=bed/simpleRepeat/trfMask/$ctgFa:t:r.bed
        /cluster/home/kent/bin/i386/maskOutFa -soft $ctgFa $ctgFa.out $ctgFa
        /cluster/home/kent/bin/i386/maskOutFa -softAdd $ctgFa $trfMask $ctgFa
      end
    end


# STORE SEQUENCE AND ASSEMBLY INFORMATION (DONE 10/5/03 angie)

    # Translate to nib
    ssh kksilo
    cd /cluster/data/dm1
    mkdir nib
    foreach c (?{,?})
      faToNib -softMask $c/chr$c.fa nib/chr$c.nib
    end

    # Make symbolic links from /gbdb/dm1/nib to the real nibs.
    ssh hgwdev
    mkdir -p /gbdb/dm1/nib
    foreach f (/cluster/data/dm1/nib/chr*.nib)
      ln -s $f /gbdb/dm1/nib
    end

    # Load /gbdb/dm1/nib paths into database and save size info.
    hgsql dm1  < ~/src/hg/lib/chromInfo.sql
    hgNibSeq -preMadeNib dm1 /gbdb/dm1/nib /cluster/data/dm1/?{,?}/chr?{,?}.fa
    echo "select chrom,size from chromInfo" | hgsql -N dm1 \
      > /cluster/data/dm1/chrom.sizes


# CREATING GRP TABLE FOR TRACK GROUPING (DONE 10/5/03 angie)
    # Copy all the data from the table "grp" 
    # in the existing database "rn1" to the new database
    ssh hgwdev
    echo "create table grp (PRIMARY KEY(NAME)) select * from rn1.grp" \
      | hgsql dm1


# MAKE GCPERCENT (DONE 10/5/03 angie)
     ssh hgwdev
     mkdir /cluster/data/dm1/bed/gcPercent
     cd /cluster/data/dm1/bed/gcPercent
     # create and load gcPercent table
     hgsql dm1  < ~/src/hg/lib/gcPercent.sql
     hgGcPercent dm1 ../../nib


# MAKE HGCENTRALTEST ENTRY AND TRACKDB TABLE FOR DROSOPHILA (DONE 10/8/03 angie)
    # Warning: must genome and organism fields must correspond
    # with defaultDb values
    echo 'INSERT INTO dbDb \
        (name, description, nibPath, organism, \
                defaultPos, active, orderKey, genome, scientificName, \
                htmlPath, hgNearOk) values \
        ("dm1", "Jan. 2003", "/gbdb/dm1/nib", "Fruitfly", \
               "chr2L:827700-845800", 1, 55, "Fruitfly", \
                "Drosophila melanogaster", "/gbdb/dm1/html/description.html", \
                0);' \
      | hgsql -h genome-testdb hgcentraltest
    echo 'INSERT INTO defaultDb (genome, name) values ("Fruitfly", "dm1");' \
      | hgsql -h genome-testdb hgcentraltest

    # Make trackDb table so browser knows what tracks to expect:
    ssh hgwdev
    cd ~/src/hg/makeDb/trackDb
    cvs up -d -P

    # Edit that makefile to add dm1 in all the right places and do
    make update

    # go public on genome-test
    #make alpha
    cvs commit makefile

    # Add trackDb directories
    mkdir drosophila
    mkdir drosophila/dm1
    cvs add drosophila
    cvs add drosophila/dm1
    cvs commit drosophila


# MAKE HGCENTRALTEST BLATSERVERS ENTRY FOR DROSOPHILA (DONE 10/17/03 angie)
    ssh hgwdev
    # Get appropriate hostname from cluster admins
    echo 'insert into blatServers values("dm1", "blat10", "17787", "1"); \
          insert into blatServers values("dm1", "blat10", "17786", "0");' \
      | hgsql -h genome-testdb hgcentraltest


# LOAD UP BDGP ANNOTATIONS (DONE 10/8/03 angie)
    # fruitfly.org is the Berkeley Drosophila Genome Project.  
    # Their GFF annotations contain FlyBase IDs useful for cross-linking.
    ssh kksilo
    mkdir /cluster/data/dm1/bed/bdgpAnnotations
    cd /cluster/data/dm1/bed/bdgpAnnotations
    # Download all available annotations:
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_3_UTR_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_5_UTR_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_CDS_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_annotation_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_exon_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_intron_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_noncoding-gene_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_protein-coding-gene_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_splice_site_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_tRNA_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_transcript_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_translation_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_transposable_element_dmel_RELEASE3-1.GFF.gz
    gunzip *.gz
    # Protein-coding genes...
    perl -wpe 's/^/chr/; s/translation/CDS/; \
               s/genegrp.*transgrp=(\S+);.*$/$1/; \
               s/genegrp.*transgrp=(\S+)$/$1/;' \
      whole_genome_protein-coding-gene_dmel_RELEASE3-1.GFF \
    > bdgpGene.gff
    # Loading a .tab file caused some lines with super-long go fields 
    # to be skipped.  Generating a .sql file with INSERTs works.
    perl -we 'while (<>) { \
                chop; @fields = split("\t"); \
                if ($fields[2] eq "gene") { \
                  @vars = split("; ", $fields[8]); \
                  $go = "";  $cdna_clone = ""; \
                  foreach $v (@vars) { \
                    @vv = split("=", $v); \
                    if ($vv[0] eq "name") { \
                      $bdgpName = $vv[1]; \
                    } elsif ($vv[0] eq "dbxref") { \
                      if ($vv[1] =~ /^GO:(\d+)/) { \
                        $go .= "$1,"; \
                      } elsif ($vv[1] =~ /FlyBase:(\w+)/) { \
                        $flybase = $1; \
                      } else { \
                        die "unrecognized dbxref $vv[1]"; \
                      } \
                    } elsif ($vv[0] eq "symbol") { \
                      $symbol = $vv[1]; \
                    } elsif ($vv[0] eq "cytorange") { \
                      $cytorange = $vv[1]; \
                    } elsif ($vv[0] eq "cdna_clone") { \
                      $cdna_clone .= "$vv[1],"; \
                    } elsif ($vv[0] eq "genegrp") { \
                    } else { \
                      die "unrecognized var $v" \
                    } \
                  } \
                  print "INSERT INTO bdgpGeneInfo VALUES ( \"$bdgpName\", \"$flybase\", \"$go\", \"$symbol\", \"$cytorange\", \"$cdna_clone\");\n"; \
                } \
              }' \
      whole_genome_protein-coding-gene_dmel_RELEASE3-1.GFF \
    > bdgpGeneInfo.sql
    # Proteins for coding genes:
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/FASTA/whole_genome_translation_dmel_RELEASE3-1.FASTA.gz
    gunzip -c whole_genome_translation_dmel_RELEASE3-1.FASTA.gz \
    | perl -wpe 's/^>(pp-)*(\w+)-\w(\w).*/>$2-R$3/' \
    > bdgpGenePep.fa
    # load up coding genes, proteins and assoc. info:
    ssh hgwdev
    ldHgGene dm1 bdgpGene /cluster/data/dm1/bed/bdgpAnnotations/bdgpGene.gff
    hgPepPred dm1 generic bdgpGenePep \
      /cluster/data/dm1/bed/bdgpAnnotations/bdgpGenePep.fa
    hgsql dm1 < $HOME/src/hg/lib/bdgpGeneInfo.sql
    hgsql dm1 < /cluster/data/dm1/bed/bdgpAnnotations/bdgpGeneInfo.sql
    # Non-coding genes...
    perl -wpe 's/^/chr/; \
               s/genegrp.*transgrp=(\S+);.*$/$1/; \
               s/genegrp.*transgrp=(\S+)$/$1/;' \
      whole_genome_noncoding-gene_dmel_RELEASE3-1.GFF \
    > bdgpNonCoding.gff
    sed -e 's/bdgpGeneInfo/bdgpNonCodingInfo/' \
      ~/kent/src/hg/lib/bdgpGeneInfo.sql \
      > bdgpNonCodingInfo.sql
    perl -we 'while (<>) { \
                chop; @fields = split("\t"); \
                if (($fields[2] ne "exon") && \
                    ($fields[2] ne "transcript") && \
                    ($fields[2] ne "translation")) { \
                  @vars = split("; ", $fields[8]); \
                  $go = "";  $cdna_clone = ""; \
                  foreach $v (@vars) { \
                    @vv = split("=", $v); \
                    if ($vv[0] eq "name") { \
                      $bdgpName = $vv[1]; \
                    } elsif ($vv[0] eq "dbxref") { \
                      if ($vv[1] =~ /^GO:(\d+)/) { \
                        $go .= "$1,"; \
                      } elsif ($vv[1] =~ /FlyBase:(\w+)/) { \
                        $flybase = $1; \
                      } else { \
                        die "unrecognized dbxref $vv[1]"; \
                      } \
                    } elsif ($vv[0] eq "symbol") { \
                      $symbol = $vv[1]; \
                    } elsif ($vv[0] eq "cytorange") { \
                      $cytorange = $vv[1]; \
                    } elsif ($vv[0] eq "cdna_clone") { \
                      $cdna_clone .= "$vv[1],"; \
                    } elsif ($vv[0] eq "genegrp") { \
                    } else { \
                      die "unrecognized var $v" \
                    } \
                  } \
                  print "INSERT INTO bdgpNonCodingInfo VALUES ( \"$bdgpName\", \"$flybase\", \"$go\", \"$symbol\", \"$cytorange\", \"$cdna_clone\");\n"; \
                } \
              }' \
      whole_genome_noncoding-gene_dmel_RELEASE3-1.GFF \
    >> bdgpNonCodingInfo.sql
    ssh hgwdev
    ldHgGene dm1 bdgpNonCoding \
      /cluster/data/dm1/bed/bdgpAnnotations/bdgpNonCoding.gff
    hgsql dm1 < /cluster/data/dm1/bed/bdgpAnnotations/bdgpNonCodingInfo.sql


# FLYBASE GENES (REDONE 11/21/03 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/flyBase
    cd /cluster/data/dm1/bed/flyBase
    wget ftp://flybase.bio.indiana.edu/flybase/genes/genes.txt
    # Note: this time around I had to edit out the first word of line 448798
    #  uncertain *u FBan0017679; annotated data are available for this gene.
    # so that it started with *u.  
    # Lines 739149-739150 looked corrupted too -- edited out extra words:
    #  $E Lewis, Cited in Lindsley and Grell, 19680561where symbol = ""wing
    #  Cut wi
    hgFlyBase dm1 genes.txt


# BDGP GENE DISRUPTION PROJECT/PSCREEN (DONE 5/18/04 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/pscreen
    cd /cluster/data/dm1/bed/pscreen
    wget http://flypush.imgen.bcm.tmc.edu/pscreen/primarycollection1.xls.gz
    gunzip primarycollection1.xls.gz
    # Use ooffice  or similar tool to translate MS Excel format to 
    # tab-separated text file.
    ssh yall
    ooffice /cluster/data/dm1/bed/pscreen/primarycollection1.xls
    # save as CSV -- select tab as delimiter, no quotes.
    # one-shot Perl script!  Also output rows with strange chrom values.
    chmod a+x mkPscreen.pl
    (mkPscreen.pl primarycollection1.csv > pscreen.bed) >& no-chroms.txt
    hgLoadBed dm1 pscreen -sqlTable=$HOME/kent/src/hg/lib/pscreen.sql -tab \
      pscreen.bed

# FLYBASE IN SITU IMAGES / EXPRESSION (DONE 11/24/03 angie)
    # FlyBase has downloadable in situ images for BACs:
    # ftp://flybase.net/flybase/images/bac_insitu_pic/*
    # and FBti's:
    # ftp://flybase.net/flybase/images/in-situ-images/insitus.zip
    # but what Jim is interested in is the insitus for expression data.
    # Don't see that in the ftp listing, but they do make it easy to link in.
    ssh hgwdev
    cd /cluster/data/dm1/bed/flyBase
    wget -O summary.txt \
    'http://www.fruitfly.org/cgi-bin/ex/bquery.pl?qpage=entry&qtype=summarytext'
    hgsql dm1 < ~/kent/src/hg/lib/bdgpExprLink.sql
    echo 'load data local infile "summary.txt" into table bdgpExprLink' \
    | hgsql dm1


# SWISSPROT-FLYBASE CROSS-REFERENCING  (DONE 10/7/03 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/flyBaseSwissProt
    cd /cluster/data/dm1/bed/flyBaseSwissProt
    echo "select extAcc1,acc from extDbRef,extDb where extDbRef.extDb = extDb.id and extDb.val = 'flybase'" \
    | hgsql -N sp092903 \
    > fbSpAcc.tab
    ssh kksilo
    cd /cluster/data/dm1/bed/flyBaseSwissProt
    wget ftp://ftp.ebi.ac.uk/pub/databases/SPproteomes/fasta_files/proteomes/7227.FASTAC
    # Some of those SwissProt "names" are > 255 chars!  trim the few long ones.
    perl -we 'open(F, "fbSpAcc.tab") || die; \
              %sp2fb = (); \
              while (<F>) { \
                chop;  @words = split("\t"); \
                $sp2fb{$words[1]} = $words[0]; \
              } \
              close(F); \
              while (<>) { \
                if (/^>(\w+)\s+\((\w+)\)\s+(.*)/) { \
                  $fbAcc = $sp2fb{$2}; \
                  $spAcc = $3; \
                  $spAcc = substr($3, 0, 250) . "..." if (length($3) > 255); \
                  print "$fbAcc\t$2\t$spAcc\t$1\n" if (defined $fbAcc); \
                } \
              }' \
      7227.FASTAC \
    > flyBaseSwissProt.tab
    rm 7227.FASTAC
    ssh hgwdev
    hgsql dm1 < $HOME/src/hg/lib/flyBaseSwissProt.sql
    echo 'load data local infile "/cluster/data/dm1/bed/flyBaseSwissProt/flyBaseSwissProt.tab" into table flyBaseSwissProt' \
    | hgsql dm1


# AUTO UPDATE GENBANK MRNA RUN  (DONE 10/9/03 angie)

    # Put the nib's on /cluster/bluearc:
    ssh kksilo
    mkdir /cluster/bluearc/drosophila
    mkdir /cluster/bluearc/drosophila/dm1
    cp -pR /cluster/data/dm1/nib /cluster/bluearc/drosophila/dm1

    # Instructions for setting up incremental genbank updates are here:
    # http://www.soe.ucsc.edu/~markd/genbank-update/doc/initial-load.html
    # This time around, Markd handled adding the new species to gbGenome.c 
    # because it's not yet in the kent tree.  

    # Edit /cluster/data/genbank/etc/genbank.conf and add:
# dm1
dm1.genome = /cluster/bluearc/drosophila/dm1/nib/chr*.nib
dm1.lift = /cluster/data/dm1/jkStuff/liftAll.lft
dm1.genbank.mrna.xeno.load = yes
dm1.genbank.est.xeno.load = no
dm1.downloadDir = dm1

    ssh eieio
    cd /cluster/data/genbank
    # This is an -initial run, mRNA only:
    nice bin/gbAlignStep -iserver=no -clusterRootDir=/cluster/bluearc/genbank \
      -srcDb=genbank -type=mrna -verbose=1 -initial dm1

    # Load the results from the above
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 -drop -initialLoad dm1

    ssh eieio
    # To get this next one started, the work directory of the initial run 
    # needs to be moved out of the way.  
    rm -r /cluster/bluearc/genbank/work/initial.dm1
    # Now align refseqs:
    cd /cluster/data/genbank
    nice bin/gbAlignStep -iserver=no -clusterRootDir=/cluster/bluearc/genbank \
      -srcDb=refseq -type=mrna -verbose=1 -initial dm1
    # Load results:
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 dm1

    ssh eieio
    # To get this next one started, the work directory of the initial run 
    # needs to be moved out of the way.  
    rm -r /cluster/bluearc/genbank/work/initial.dm1
    # Now align ESTs:
    nice bin/gbAlignStep -iserver=no -clusterRootDir=/cluster/bluearc/genbank \
      -srcDb=genbank -type=est -verbose=1 -initial dm1
    # Load results:
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 dm1
    # Clean up:
    rm -r /cluster/bluearc/genbank/work/initial.dm1


# PUT NIBS ON ISCRATCH (IN PROGRESS 10/9/03 angie)
    ssh kkr1u00
    cd /iscratch/i/dm1
    cp -pR /cluster/data/dm1/nib .
    iSync
    # kkr3u00 and kkr4u00 are down, so iSync should be run again when 
    # they're back up.


# PRODUCING FUGU FISH ALIGNMENTS  (DONE 10/9/03 angie)
    # Assumes masked NIBs have been prepared as above
    # and Fugu pieces are already on kluster /iscratch/i.
    # next machine
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blatFugu
    cd /cluster/data/dm1/bed/blatFugu
    mkdir psl
    ls -1S /iscratch/i/fugu/*.fa > fugu.lst
    ls -1S /iscratch/i/dm1/nib/chr*.nib > fly.lst
    cat << '_EOF_' > gsub
#LOOP
/cluster/bin/i386/blat -q=dnax -t=dnax -mask=lower {check in exists+ $(path1)} {check in line+ $(path2)} {check out line+ psl/$(root1)_$(root2).psl}
#ENDLOOP
'_EOF_'
    # << this line keeps emacs coloring happy
    gensub2 fly.lst fugu.lst gsub spec
    para create spec
    para try
    para check
    para push
    para check
#Completed: 1408 of 1408 jobs
#Average job time:                 217s       3.61m     0.06h    0.00d
#Longest job:                     1431s      23.85m     0.40h    0.02d
#Submission to last job:          1433s      23.88m     0.40h    0.02d

    # When cluster run is done, sort alignments
    # append _blatFugu to chrom for .psl file names.
    ssh kksilo
    cd /cluster/data/dm1/bed/blatFugu
    foreach c (2L 2R 2h 3L 3R 3h 4 X Xh Yh U)
      echo -n "chr${c} "
      pslCat psl/chr${c}_*.psl \
      | pslSortAcc nohead chrom temp stdin
      rm -f chrom/chr${c}_blatFugu.psl
      mv chrom/chr${c}.psl chrom/chr${c}_blatFugu.psl
    end

    # Load alignments
    ssh hgwdev
    cd /cluster/data/dm1/bed/blatFugu/chrom
    hgLoadPsl dm1 chr*_blatFugu.psl

    # Make fugu /gbdb/ symlink and load Fugu sequence data.
    mkdir /gbdb/dm1/fuguSeq
    ln -s /cluster/store3/fuguSeq/fugu_v3_mask.fasta /gbdb/dm1/fuguSeq
    # ! ! !  DO NOT RUN hgLoadSeq in /gbdb - it leaves .tab files
    cd /cluster/data/dm1/bed/blatFugu
    hgLoadSeq dm1 /gbdb/dm1/fuguSeq/fugu_v3_mask.fasta


# BLASTZ D.PSEUDOOBSCURA (DONE 11/14/03 angie)
    ssh kksilo
    mkdir /cluster/data/dm1/bed/blastz.dp1.2003-11-14
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14
    cat << '_EOF_' > DEF
# D.melanogaster vs. D.pseudoobscura
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/home/angie/schwartzbin:/cluster/bin/i386

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
#BLASTZ_ABRIDGE_REPEATS=1 if SMSK is specified
BLASTZ_ABRIDGE_REPEATS=0

# TARGET - D. melanogaster
SEQ1_DIR=/cluster/bluearc/drosophila/dm1/nib
# unused: SEQ1_RMSK=
SEQ1_SMSK=
SEQ1_FLAG=-drosophila
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY - D. pseudoobscura
SEQ2_DIR=/cluster/bluearc/drosophila/dp1/trfFa
# unused: SEQ2_RMSK=
SEQ2_SMSK=
SEQ2_FLAG=-drosophila
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/cluster/data/dm1/bed/blastz.dp1.2003-11-14

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len

#DEBUG=1
'_EOF_'
    # << this line keeps emacs coloring happy
    # run bash shell if you don't already:
    bash
    source DEF
    mkdir run
    ~angie/hummus/make-joblist $DEF > $BASE/run/j
    sh ./xdir.sh
    cd run
    sed -e 's#^#/cluster/home/angie/schwartzbin/#' j > j2
    wc -l j*
    head j2
    mv j2 j
    # cluster run
    ssh kk
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/run
    para create j
    para try, check, push, check, ....
#Completed: 15939 of 15939 jobs
#Average job time:                  16s       0.27m     0.00h    0.00d
#Longest job:                      287s       4.78m     0.08h    0.00d
#Submission to last job:           829s      13.82m     0.23h    0.01d

    # back in the bash shell on kksilo...
    mkdir /cluster/data/dm1/bed/blastz.dp1.2003-11-14/run.1
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/run.1
    ~angie/hummus/do.out2lav $DEF > j
    # small cluster run
    ssh kkr1u00
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/run.1
    para create j
    para try, check, push, check, ....
#Completed: 21 of 21 jobs
#Average job time:                  44s       0.73m     0.01h    0.00d
#Longest job:                       71s       1.18m     0.02h    0.00d
#Submission to last job:           111s       1.85m     0.03h    0.00d
    cd ..
    rm -r raw

    # Translate .lav to axt, with dp1 in scaffold coords for collaborators:
    ssh kksilo
    # oops, lavToAxt relies on nib (not fa), so make dp1 scaffoldNib first:
    mkdir /cluster/data/dp1/scaffoldsNib
    foreach f (/cluster/data/dp1/scaffolds/*.fa)
      faToNib -softMask $f /cluster/data/dp1/scaffoldsNib/$f:t:r.nib
    end
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14
    mkdir axtScaffold
    foreach c (lav/*)
      pushd $c
      set chr=$c:t
      set out=axtScaffold/$chr.axt
      echo "Translating $chr lav to $out"
      cat `ls -1 *.lav | sort -g` \
        | lavToAxt stdin /cluster/data/dm1/nib /cluster/data/dp1/scaffoldsNib \
            stdout \
        | axtSort stdin ../../$out
      popd
    end
    # Lift query coords to chrom-level for browser display
    mkdir axtChrom
    foreach f (axtScaffold/chr*.axt)
      liftUp -axtQ axtChrom/$f:t /cluster/data/dp1/jkStuff/liftAll.lft warn $f
    end


# CHAIN & NET DM1/DP1 BLASTZ ALIGNMENTS (DONE 11/24/03 angie)
    ssh kkr1u00
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14
    mkdir -p axtChainScaffoldQ/run1
    cd axtChainScaffoldQ/run1
    ls -1S ../../axtScaffold/*.axt > input.lst
    echo '#LOOP' > gsub
    echo 'doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out line+ out/$(root1).out}' >> gsub
    echo '#ENDLOOP' >> gsub
    gensub2 input.lst single gsub spec
    echo '#\!/bin/csh' > doChain
    echo 'axtChain $1 /cluster/data/dm1/nib /cluster/data/dp1/scaffoldsNib $2 > $3' \
    >> doChain
    chmod a+x doChain
    mkdir chain out
    para create spec
    para try, check, push, check, ...
#Completed: 11 of 11 jobs
#Average job time:                  27s       0.45m     0.01h    0.00d
#Longest job:                       49s       0.82m     0.01h    0.00d
#Submission to last job:            49s       0.82m     0.01h    0.00d

    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainScaffoldQ
    chainMergeSort run1/chain/*.chain > all.chain
    chainSplit chain all.chain
    rm run1/chain/*.chain

    mkdir -p ../axtChainChrom/chain
    foreach f (chain/chr*.chain)
      liftUp -chainQ ../axtChainChrom/chain/$f:t \
        /cluster/data/dp1/jkStuff/liftAll.lft warn $f
    end
    liftUp -chainQ ../axtChainChrom/all.chain \
      /cluster/data/dp1/jkStuff/liftAll.lft warn all.chain

    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainChrom/chain
    foreach i (*.chain)
      set c = $i:r
      hgLoadChain dm1 ${c}_chainDp1 $i
      echo done $c
    end

    # Create the nets.  You can do this while the database is loading
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainScaffoldQ
    # Oops, need a dp1/scaffold.sizes
    cp /dev/null /cluster/data/dp1/scaffold.sizes
    foreach f (/cluster/data/dp1/scaffolds/*.fa)
      set s = `faSize $f | awk '{print $1;}'`
      echo "$f:t:r\t$s" >> /cluster/data/dp1/scaffold.sizes
    end
    # First do a crude filter that eliminates many chains so the
    # real chainer has less work to do.
    mkdir preNet
    cd chain
    foreach i (*.chain)
      echo preNetting $i
      chainPreNet $i /cluster/data/dm1/chrom.sizes \
        /cluster/data/dp1/scaffold.sizes ../preNet/$i
    end
    cd ..
    # Run the main netter, putting the results in n1.
    mkdir n1 
    cd preNet
    foreach i (*.chain)
      set n = $i:r.net
      echo primary netting $i
      chainNet $i -minSpace=1 /cluster/data/dm1/chrom.sizes \
        /cluster/data/dp1/scaffold.sizes ../n1/$n /dev/null
    end
    cd ..
    # Classify parts of net as syntenic, nonsyntenic etc.
    cat n1/*.net | netSyntenic stdin noClass.net

    # The final step of net creation needs the database.
    # Best to wait for the database load to finish if it
    # hasn't already.
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainScaffoldQ
    netClass -liftQ=/cluster/data/dp1/jkStuff/liftAll.lft -noAr \
      noClass.net dm1 dp1 fruitfly.net
    rm -r n1 noClass.net
    netFilter -minGap=10 fruitfly.net > fruitflyFilt.net
    liftUp -netQ ../axtChainChrom/fruitflyFiltChrom.net \
      /cluster/data/dp1/jkStuff/liftAll.lft warn fruitflyFilt.net
    hgLoadNet dm1 netDp1 ../axtChainChrom/fruitflyFiltChrom.net
    # Move back to the file server to create axt files corresponding
    # to the net.
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainChrom
    mkdir ../axtNetChrom
    netSplit fruitflyFiltChrom.net fruitflyNet
    cd fruitflyNet
    foreach i (*.net)
        set c = $i:r
        netToAxt -maxGap=300 $i ../chain/$c.chain /cluster/data/dm1/nib \
          /cluster/data/dp1/nib ../../axtNetChrom/$c.axt
        echo done ../axt/$c.axt
    end
    cd ..
    rm -r fruitflyNet
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainScaffoldQ
    mkdir ../axtNetScaffoldQ
    netSplit fruitflyFilt.net fruitflyNet
    cd fruitflyNet
    foreach i (*.net)
        set c = $i:r
        netToAxt -maxGap=300 $i ../chain/$c.chain /cluster/data/dm1/nib \
          /cluster/data/dp1/scaffoldsNib ../../axtNetScaffoldQ/$c.axt
        echo done ../axtNetScaffoldQ/$c.axt
    end
    cd ..
    rm -r fruitflyNet
    # Load up the axtNet (alignment score wiggle) track:
    ssh hgwdev
    mkdir /gbdb/dm1/axtNetDp1
    foreach f (/cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtNetChrom/chr*.axt)
      ln -s $f /gbdb/dm1/axtNetDp1
    end
    hgLoadAxt dm1 axtNetDp1


PRODUCING GENSCAN PREDICTIONS (TODO 11/14/03 angie)
    # Run on small cluster -- genscan needs big mem.
    ssh kkr1u00
    mkdir /cluster/data/dm1/bed/genscan
    cd /cluster/data/dm1/bed/genscan
    # Make 3 subdirectories for genscan to put their output files in
    mkdir gtf pep subopt
    # Make hard-masked contigs
    foreach f (/cluster/data/dm1/?{,?}/chr*/chr?{,?}_?{,?}.fa)
      maskOutFa $f hard $f.masked
    end
    # Generate a list file, contigs.list, of all the hard-masked contigs that 
    # *do not* consist of all-N's (which would cause genscan to blow up)
    rm -f contigs.list
    touch contigs.list
    foreach f ( `ls -1S /cluster/data/dm1/?{,?}/chr*/chr?{,?}{,_random}_?{,?}.fa.masked` )
      egrep '[ACGT]' $f > /dev/null
      if ($status == 0) echo $f >> contigs.list
    end
    cat << '_EOF_' > gsub
#LOOP
/cluster/bin/i386/gsBig {check in line+ $(path1)} {check out line gtf/$(root1).gtf} -trans={check out line pep/$(root1).pep} -subopt={check out line subopt/$(root1).bed} -exe=/cluster/home/fanhsu/projects/compbio/bin/genscan-linux/genscan -par=/cluster/home/fanhsu/projects/compbio/bin/genscan-linux/HumanIso.smat -tmp=/tmp -window=2400000
#ENDLOOP
'_EOF_'
    # << this line keeps emacs coloring happy
    gensub2 contigs.list single gsub jobList
    para create jobList
    para try
    para check
    para push
#Completed: 79 of 79 jobs
#Average job time:                 136s       2.26m     0.04h    0.00d
#Longest job:                      229s       3.82m     0.06h    0.00d
#Submission to last job:          1718s      28.63m     0.48h    0.02d

    # If there are crashes, diagnose with "para problems".  
    # If a job crashes due to genscan running out of memory, re-run it 
    # manually with "-window=1200000" instead of "-window=2400000".
    # chr14_21, chr16_4
    
    # Convert these to chromosome level files as so:
    ssh kksilo
    cd /cluster/data/dm1/bed/genscan
    liftUp genscan.gtf ../../jkStuff/liftAll.lft warn gtf/*.gtf
    liftUp genscanSubopt.bed ../../jkStuff/liftAll.lft warn subopt/*.bed
    cat pep/*.pep > genscan.pep

    # Load into the database as so:
    ssh hgwdev
    cd /cluster/data/dm1/bed/genscan
    ldHgGene dm1 genscan genscan.gtf
    hgPepPred dm1 generic genscanPep genscan.pep
    hgLoadBed dm1 genscanSubopt genscanSubopt.bed


# EXTEND BDGPGENE AND CREATE BDGPNEAR FOR HGNEAR (DONE 10/15/03 angie)
    ssh hgwdev
    cd /cluster/data/dm1/bed/bdgpAnnotations
    cp ~/kent/src/hg/lib/bdgpSwissProt.sql .
    perl -we '%bName2all = (); \
              %bName2fb = (); \
              open(P, "echo \"select * from bdgpGeneInfo\" | hgsql -N dm1|") \
                || die; \
              while (<P>) { \
                chop; my @words = split("\t"); \
                $bName2fb{$words[0]} = \@words; \
              } \
              close(P); \
              open(P, "echo \"select bdgpGeneInfo.*,flyBaseSwissProt.* from bdgpGeneInfo,flyBaseSwissProt where bdgpGeneInfo.flyBaseId = flyBaseSwissProt.flyBaseId\" | hgsql -N dm1|") || die; \
              while (<P>) { \
                chop; my @words = split("\t"); \
                $bName2all{$words[0]} = \@words; \
              } \
              close(P); \
              open(P, "echo select name from bdgpGene | hgsql -N dm1|") ||die; \
              while (<P>) { \
                chop; $name = $_; \
                $bName = $name;  $bName =~ s/-R.*//; \
                if (exists($bName2all{$bName})) { \
                  ($bName, $fbID, $go, $symbol, $cyto, undef, \
                   undef, $spID, $spDesc, $spSymb) = @{$bName2all{$bName}}; \
                  print "INSERT INTO bdgpSwissProt VALUES ( \"$name\", \"$fbID\", \"$go\", \"$symbol\", \"$cyto\", \"$spID\", \"$spDesc\", \"$spSymb\");\n"; \
                } elsif (exists($bName2fb{$bName})) { \
                  ($bName, $fbID, $go, $symbol, $cyto, undef) = @{$bName2fb{$bName}}; \
                  print "INSERT INTO bdgpSwissProt VALUES ( \"$name\", \"$fbID\", \"$go\", \"$symbol\", \"$cyto\", \"n/a\", \"n/a\", \"n/a\");\n"; \
                } else { die "No info for $name."; } \
              } \
              close(P); ' \
    >> bdgpSwissProt.sql
    hgsql dm1 < bdgpSwissProt.sql

    # Next time around -- use the above table to add a proteinID field 
    # to bdgpGene.  Here's what I did this time, before doing the above:

    # Create a table bdgpGene2, which is just like bdgpGene but with 
    # a proteinID field.  The create statement is from 
    # mysqldump -u hguser -phguserstuff dm1 bdgpGene 
    # but edited to change the table name to bdgpGene2 and 
    # to include the proteinId field.  The insert statement copies 
    # the bdgpGene data into bdgpGene2.  
    cat << '_EOF_' > bdgpGene2.sql
CREATE TABLE bdgpGene2 (
  name varchar(255) NOT NULL default '',
  chrom varchar(255) NOT NULL default '',
  strand char(1) NOT NULL default '',
  txStart int(10) unsigned NOT NULL default '0',
  txEnd int(10) unsigned NOT NULL default '0',
  cdsStart int(10) unsigned NOT NULL default '0',
  cdsEnd int(10) unsigned NOT NULL default '0',
  exonCount int(10) unsigned NOT NULL default '0',
  exonStarts longblob NOT NULL,
  exonEnds longblob NOT NULL,
  proteinID varchar(255) NOT NULL default 'n/a',
  KEY name (name(16)),
  KEY chrom (chrom(8),txStart),
  KEY chrom_2 (chrom(8),txEnd)
) TYPE=MyISAM;
insert into bdgpGene2 select *,'n/a' from bdgpGene;
'_EOF_'
    # << this line keeps emacs coloring happy
    # The update statements populate the proteinID fields with SwissProt IDs 
    # where possible:
    perl -we '%bName2id = (); \
              open(P, "echo select bdgpGeneInfo.bdgpName,flyBaseSwissProt.swissProtId from bdgpGeneInfo,flyBaseSwissProt where bdgpGeneInfo.flyBaseId = flyBaseSwissProt.flyBaseId | hgsql -N dm1|") || die; \
              while (<P>) { \
                chop; @words = split("\t"); \
                $bName2id{$words[0]} = $words[1]; \
              } \
              close(P); \
              open(P, "echo select name from bdgpGene | hgsql -N dm1|") ||die; \
              while (<P>) { \
                chop; $name = $_; \
                $bName = $name;  $bName =~ s/-R.*//; \
                $id = $bName2id{$bName}; \
                print "update bdgpGene2 set proteinID = \"$id\" where name = \"$name\";\n" if (defined $id); \
                 \
              } \
              close(P); ' \
      >> bdgpGene2.sql
    # Create bdgpGene2:
    hgsql dm1 < bdgpGene2.sql
    # Now examine bdgpGene2 vs. bdgpGene manually, carefully.  
    # Do they have the same # rows?  
    #   select count(*) from bdgpGene;
    #   select count(*) from bdgpGene2;
    # Are most proteinID fields non-"n/a"? 
    #   select count(*) from bdgpGene2 where proteinID = "n/a";
    # Spot-check some genes... are the fields of bdgpGene2 identical to 
    # the fields of bdgpGene, except for the new proteinIDs?  
    #  select * from bdgpGene limit 3;
    #  select * from bdgpGene2 limit 3;
    # If so, then go ahead:
    echo "drop table bdgpGene; rename table bdgpGene2 to bdgpGene;" | hgsql dm1
    # and check the new bdgpGene manually.


#############################################################################
# MAKE HGNEAR
# adapted from makeHgNear.doc; split into sections.  See makeHgFixed.doc 
# for how Arbeitman et al's fly lifecycle expression data were loaded into 
# hgFixed.  

# BLASTP SELF, CLUSTER GENES, MAP TO EXP.DATA FOR HGNEAR (DONE 10/17/03 angie)
    ssh hgwdev
    # Now that bdgpGene has proteinID, use hgClusterGenes to cluster
    # together various alt-splicing isoforms, creating the tables
    # bdgpIsoforms and bdgpCanonical.  
    hgClusterGenes dm1 bdgpGene bdgpIsoforms bdgpCanonical
    # Extract peptides from bdgpGenes into fasta file
    # and create a blast database out of them.
    mkdir -p /cluster/data/dm1/bed/blastp
    cd /cluster/data/dm1/bed/blastp
    pepPredToFa dm1 bdgpGenePep bdgp.faa
    formatdb -i bdgp.faa -t bdgp -n bdgp
    cd ..

    # Copy over database to iscratch/i
    ssh kkr1u00
    if (-e /iscratch/i/dm1/blastp) then
      rm -r /iscratch/i/dm1/blastp
    endif
    mkdir -p /iscratch/i/dm1/blastp
    if (-e /iscratch/i/dm1/blastp) then
      rm -r /iscratch/i/dm1/blastp
    endif
    mkdir -p /iscratch/i/dm1/blastp
    cp /cluster/data/dm1/bed/blastp/bdgp.* /iscratch/i/dm1/blastp

    # Load up iscratch/i with blastp and related files
    # if necessary
    if (! -e /iscratch/i/blast/blastall) then
      mkdir -p /iscratch/i/blast
      cp /projects/compbio/bin/i686/blastall /iscratch/i/blast
      mkdir -p /iscratch/i/blast/data
      cp /projects/compbio/bin/i686/data/* /iscratch/i/blast/data
    endif
    iSync

    # Split up fasta file into bite sized chunks for cluster
    ssh kksilo
    cd /cluster/data/dm1/bed/blastp
    mkdir split
    faSplit sequence bdgp.faa 6000 split/bg

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/self/run/out
    cd /cluster/data/dm1/bed/blastp/self/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/dm1/blastp/bdgp -i \$1 -o \$2 -e 0.01 -m 8 -b 1000
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5817 of 5817 jobs
#Average job time:                   9s       0.15m     0.00h    0.00d
#Longest job:                      312s       5.20m     0.09h    0.00d
#Submission to last job:           312s       5.20m     0.09h    0.00d

    # Load into database.  This took only ~3 minutes for dm1.
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/self/run/out
    time hgLoadBlastTab dm1 bdgpBlastTab *.tab
    # Create table that maps between bdgp genes and RefSeq
    hgMapToGene dm1 refGene bdgpGene bdgpToRefSeq
    # Create table that maps between bdgp genes and LocusLink
    echo "select mrnaAcc,locusLinkId from refLink" | hgsql -N dm1 > refToLl.txt
    hgMapToGene dm1 refGene bdgpGene bdgpToLocusLink -lookup=refToLl.txt
    # Create table that maps between known genes and Pfam domains
    hgMapViaSwissProt dm1 bdgpGene name proteinID Pfam bdgpToPfam

    # Create a table that maps BDGP root names to canonical transcripts:
    cd /cluster/data/dm1/bed/blastp
    cat > bdgpToCanonical.sql <<end
CREATE TABLE bdgpToCanonical (
  name varchar(255) NOT NULL default '',
  value varchar(255) NOT NULL default '',
  KEY name (name(16)),
  KEY value (value(16))
) TYPE=MyISAM;
end
    perl -we 'open(P, "echo select transcript from bdgpCanonical | hgsql dm1 -N |") || die; \
              while (<P>) { \
                chop; $name = $value = $_; $value =~ s/-R.$//; \
                print "INSERT INTO bdgpToCanonical VALUES (\"$name\", \"$value\");\n"; \
              } \
              close(P);' \
      >> bdgpToCanonical.sql
    hgsql dm1 < bdgpToCanonical.sql
    # Create a table that maps between bdgp genes and the 
    # Stanford Microarray Project expression data. (see makeHgFixed.doc)
    hgExpDistance -lookup=bdgpToCanonical \
      dm1 hgFixed.arbFlyLifeMedianRatio dummyArg arbExpDistance

    # Make sure that GO database is up to date.
    See README in /cluster/store1/geneOntology.


# C.ELEGANS BLASTP FOR HGNEAR (DONE 10/15/03 angie)
    # Make C. elegans ortholog column using blastp on wormpep.
    # First make C. elegans protein database and copy it to iscratch/i
    # if it doesn't exist already:
    cd /cluster/data/ce1/bed
    mkdir blastp
    cd blastp
    wget ftp://ftp.sanger.ac.uk/pub/databases/wormpep/wormpep
    mv wormpep wormPep.faa
    formatdb -i wormPep.faa -t wormPep -n wormPep
    ssh kkr1u00
    if (-e /iscratch/i/ce1/blastp) then
    rm -r /iscratch/i/ce1/blastp
    endif
    mkdir -p /iscratch/i/ce1/blastp
    cp /cluster/data/ce1/bed/blastp/wormPep.p?? /iscratch/i/ce1/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/ce1/run/out
    cd /cluster/data/dm1/bed/blastp/ce1/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/ce1/blastp/wormPep -i \$1 -o \$2 -e 0.01 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5817 of 5817 jobs
#Average job time:                   9s       0.15m     0.00h    0.00d
#Longest job:                      152s       2.53m     0.04h    0.00d
#Submission to last job:           152s       2.53m     0.04h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/ce1/run/out
    hgLoadBlastTab dm1 ceBlastTab -maxPer=1 *.tab


# MOUSE BLASTP FOR HGNEAR (DONE 10/15/03 angie)
    # Make mouse ortholog column using blastp on mouse known genes.
    # First make mouse protein database and copy it to iscratch/i
    # if it doesn't exist already:
    cd /cluster/data/mm3/bed
    mkdir blastp
    cd blastp
    pepPredToFa mm3 knownGenePep known.faa
    formatdb -i known.faa -t known -n known
    ssh kkr1u00
    if (-e /iscratch/i/mm3/blastp) then
      rm -r /iscratch/i/mm3/blastp
    endif
    mkdir -p /iscratch/i/mm3/blastp
    cp /cluster/data/mm3/bed/blastp/known.p?? /iscratch/i/mm3/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/mm3/run/out
    cd /cluster/data/dm1/bed/blastp/mm3/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/mm3/blastp/known -i \$1 -o \$2 -e 0.001 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5817 of 5817 jobs
#Average job time:                  17s       0.28m     0.00h    0.00d
#Longest job:                      354s       5.90m     0.10h    0.00d
#Submission to last job:           354s       5.90m     0.10h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/mm3/run/out
    hgLoadBlastTab dm1 mmBlastTab -maxPer=1 *.tab


# HUMAN BLASTP FOR HGNEAR (IN DONE 10/15/03 angie)
    # Make human ortholog column using blastp on human known genes.
    # First make human protein database and copy it to iscratch/i
    # if it doesn't exist already:
    cd /cluster/data/hg16/bed
    mkdir blastp
    cd blastp
    pepPredToFa hg16 knownGenePep known.faa
    formatdb -i known.faa -t known -n known
    ssh kkr1u00
    if (-e /iscratch/i/hg16/blastp) then
      rm -r /iscratch/i/hg16/blastp
    endif
    mkdir -p /iscratch/i/hg16/blastp
    cp /cluster/data/hg16/bed/blastp/known.p?? /iscratch/i/hg16/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/hg16/run/out
    cd /cluster/data/dm1/bed/blastp/hg16/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/hg16/blastp/known -i \$1 -o \$2 -e 0.001 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5817 of 5817 jobs
#Average job time:                  14s       0.23m     0.00h    0.00d
#Longest job:                      305s       5.08m     0.08h    0.00d
#Submission to last job:           305s       5.08m     0.08h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/hg16/run/out
    hgLoadBlastTab dm1 hgBlastTab -maxPer=1 *.tab


# ZEBRAFISH BLASTP FOR HGNEAR (DONE 10/15/03 angie)
    # Make Danio rerio (zebrafish) ortholog column using blastp on Ensembl.
    # First make protein database and copy it to iscratch/i
    # if it doesn't exist already:
    cd /cluster/data/dm1/bed
    mkdir blastp
    cd blastp
    wget ftp://ftp.ensembl.org/pub/current_zebrafish/data/fasta/pep/Danio_rerio.ZFISH2.pep.fa.gz 
    zcat Dan*.pep.fa.gz > ensembl.faa
    echo "Translation:" > subs.in
    subs -e ensembl.faa > /dev/null
    formatdb -i ensembl.faa -t ensembl -n ensembl
    ssh kkr1u00
    if (-e /iscratch/i/dr1/blastp) then
    rm -r /iscratch/i/dr1/blastp
    endif
    mkdir -p /iscratch/i/dr1/blastp
    cp /cluster/data/dr1/bed/blastp/ensembl.p?? /iscratch/i/dr1/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/dr1/run/out
    cd /cluster/data/dm1/bed/blastp/dr1/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/dr1/blastp/ensembl -i \$1 -o \$2 -e 0.005 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5817 of 5817 jobs
#Average job time:                  10s       0.16m     0.00h    0.00d
#Longest job:                      225s       3.75m     0.06h    0.00d
#Submission to last job:           353s       5.88m     0.10h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/dr1/run/out
    hgLoadBlastTab dm1 drBlastTab -maxPer=1 *.tab


# YEAST BLASTP FOR HGNEAR (DONE 10/15/03 angie)
    # Make Saccharomyces cerevisiae (yeast) ortholog column using blastp on 
    # RefSeq.  First make protein database and copy it to iscratch/i
    # if it doesn't exist already:
    cd /cluster/data/sc1/bed
    mkdir blastp
    cd blastp
    wget ftp://genome-ftp.stanford.edu/pub/yeast/data_download/sequence/genomic_sequence/orf_protein/orf_trans.fasta.gz
    zcat orf_trans.fasta.gz > sgd.faa
    echo "ORFP:|" > subs.in
    subs -e sgd.faa > /dev/null
    formatdb -i sgd.faa -t sgd -n sgd
    ssh kkr1u00
    # Note: sc1 is a name conflict with SARS coronavirus... oh well, 
    # fortunately we won't be looking for homologs there.  :)
    if (-e /iscratch/i/sc1/blastp) then
      rm -r /iscratch/i/sc1/blastp
    endif
    mkdir -p /iscratch/i/sc1/blastp
    cp /cluster/data/sc1/bed/blastp/sgd.p?? /iscratch/i/sc1/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/sc1/run/out
    cd /cluster/data/dm1/bed/blastp/sc1/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/sc1/blastp/sgd -i \$1 -o \$2 -e 0.01 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5817 of 5817 jobs
#Average job time:                   4s       0.07m     0.00h    0.00d
#Longest job:                       41s       0.68m     0.01h    0.00d
#Submission to last job:            77s       1.28m     0.02h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/sc1/run/out
    hgLoadBlastTab dm1 scBlastTab -maxPer=1 *.tab


# MAKE ORGANISM-SPECIFIC HGNEARDATA FILES (DONE 10/17/03 angie)
    cd ~/kent/src/hg/near/hgNear/hgNearData
    # The directory name is the dbDb.genome field, processed by 
    # hdb.c's hgDirForOrg():
    mkdir Fruitfly
    cp C_elegans/*.{html,ra} Fruitfly/
    cd Fruitfly
    mv kimLifeCycleFull.html arbLifeCycleFull.html
    mv kimLifeCycleMedian.html arbLifeCycleMedian.html
    # edit all .ra and .html files as appropriate for D. melanogaster/dm1...
    # cvs add and check in Fruitfly/ and all .ra and .html files.
    # The "representatives" lines in columnDb.ra are tricky.  
    # For the median reps, I used this, edited to include an extra "-1," 
    # at each boundary between evelopmental stages (embryo -> larva, etc):
    # 29->30, 41->42, 59->60
    echo "select id from arbFlyLifeMedianExps;" | hgsql -N hgFixed \
    | perl -we '$i=0; \
                while (<>) { \
                  chop; print "$_,"; \
                  $i++; if (($i % 5) == 0) { print "-1," }; \
                } \
                print "\n";'
    # For the full reps, I used the output of this command minus the 
    # initial ",-1,".  -1 separators are inserted between each group 
    # that was lumped together in arbMed.ra:
    awk '-F      ' '{print $3;}' \
      ~/kent/src/hg/makeDb/hgMedianMicroarray/arbMed.ra \
    | perl -we 'while (<>) { \
                  @w=split(" "); \
                   print ",-1," . join(",", @w); \
                } \
                print "\n";'
    # Figure out what values to use for expMulti defn's absoluteMax.
    # Report on the max and 99.5%ile absolute score:
    echo select expScores from arbFlyLifeAll | hgsql hgFixed -N \
    | perl -we '$max = 0.0; \
                @all = (); \
                while (<>) { \
                  chop; @nums = split(","); \
                  foreach $num (@nums) { \
                    $max = $num if ($num > $max); \
                    push @all, $num if (defined $num && $num ne ""); \
                  } \
                } \
                $n = scalar(@all); \
                print "max is $max, N is $n\n"; \
                @all = sort { $a <=> $b } @all; \
                $useMax = $all[$n * 0.995 + 1]; \
                print "99.5%ile is $useMax -- use this for max\n";'
# max is 65535.000, N is 797364
# 99.5%ile is 15433.000 -- use this for max
    # N is the product of count(*) of arbFlyLifeAll{,Exps} .  
    # Now repeat the above perl in-liner, but on arbFlyLifeAllRatio, 
    # to determine ratioMax:  
    echo select expScores from arbFlyLifeAllRatio | hgsql hgFixed -N \
    # <paste in perl from above>
# max is 9.862, N is 797364
# 99.5%ile is 2.915 -- use this for max


# ENABLE HGNEAR FOR DM1 IN HGCENTRALTEST (DONE 10/17/03 angie)
    echo "update dbDb set hgNearOk = 1 where name = 'dm1';" \
      | hgsql -h genome-testdb hgcentraltest


# END OF HGNEAR STUFF
#############################################################################


# reload refseqs to pickup a change that uses locus_tag if gene name isn't
# available.  2003/10/16 markd

# First remove refseqs from databases:
    drop table refSeqStatus;
    drop table refLink;
    drop table refSeqSummary;
    drop table refGene;
    drop table refSeqAli;
    drop table refFlat;

    delete from gbSeq where srcDb = 'RefSeq';
    delete from gbStatus where srcDb = 'RefSeq';
    delete from gbExtFile where path like '%/refseq%';
    delete from gbLoaded where srcDb = 'RefSeq';
    delete from mrna where acc like 'NM\_%';
    delete from imageClone where acc like 'NM\_%';
    delete from mrna where acc like 'NR\_%';
    delete from imageClone where acc like 'NR\_%';

# now reload:
    cd /cluster/data/genbank
    ./bin/gbDbLoadStep -type=mrna dm1


# MAKE DOWNLOADABLE FILES (DONE 10/29/03 angie)
    ssh kksilo
    cd /cluster/data/dm1
    mkdir zips
    zip -j zips/chromOut.zip ?{,?}/chr?{,?}.fa.out
    zip -j zips/chromFa.zip ?{,?}/chr?{,?}.fa
    foreach f (?{,?}/chr?{,?}.fa)
      maskOutFa $f hard $f.masked
    end
    zip -j zips/chromFaMasked.zip ?{,?}/chr?{,?}.fa.masked
    cd bed/simpleRepeat
    zip ../../zips/chromTrf.zip trfMaskChrom/chr*.bed
    zip ../../zips/contigTrf.zip trfMask/N{T,G}*.bed
    cd ../..
    # Make a starter mrna.zip -- it will get updated regularly on the RR. 
    /cluster/data/genbank/bin/i386/gbGetSeqs -gbRoot=/cluster/data/genbank \
      -db=dm1 -native genbank mrna mrna.fa
    zip zips/mrna.zip mrna.fa
    rm mrna.fa
    foreach f (zips/*.zip)
      echo $f
      unzip -t $f | tail -1
    end
    ssh hgwdev
    mkdir /usr/local/apache/htdocs/goldenPath/dm1
    cd /usr/local/apache/htdocs/goldenPath/dm1
    mkdir bigZips database
    # Create README.txt files in bigZips/ and database/ to explain the files.
    cp -p /cluster/data/dm1/zips/*.zip bigZips


# ADD BLASTZ/CHAIN/NET DOWNLOADABLE FILES (DONE 12/1/03 angie)
    ssh hgwdev
    mkdir /usr/local/apache/htdocs/goldenPath/dm1/vsDp1
    cd /usr/local/apache/htdocs/goldenPath/dm1/vsDp1
    gzip -c \
      /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainScaffoldQ/all.chain \
      > chain.gz
    gzip -c \
      /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainScaffoldQ/fruitflyFilt.net \
      > net.gz
    mkdir axtNet
    foreach f (/cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtNetScaffoldQ/chr*axt)
      gzip -c $f > axtNet/$f:t.gz
    end
    # Make a README.txt which explains the files & formats.


# ANOPHELES ECORES FROM GENOSCOPE (DONE 11/10/03 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/anophelesEcores
    cd /cluster/data/dm1/bed/anophelesEcores
    # save attachment from Olivier Jaillon's email 11/10/03 to 
    # ecotig.6.4ucsc
    perl -wpe 'if (/^(\w+)\:\d+ (\d+) (\d+) (\w+)\:\d+ (\d+) (\d+)$/) { \
                 if ($1 ne $4) { die "diff chr: $1 $4"; } \
                 $name = "chr$1:$2-$6";  $start = $2 - 1; \
                 $sz1 = $3 - $start;  $sz2 = $6 - ($5 - 1);   $st2 = $5 - $2; \
                 $_ = "chr$1\t$start\t$6\t$name\t0\t+\t$start\t$6\t0\t2\t$sz1,$sz2,\t0,$st2,\n"; \
               } elsif (/^(\w+)\:\d+ (\d+) (\d+)\s*$/) { \
                 $name = "chr$1:$2-$3";  $start = $2 - 1;  $sz1 = $3 - $start; \
                 $_ = "chr$1\t$start\t$3\t$name\t0\t+\t$start\t$3\t0\t1\t$sz1,\t0,\n"; \
               } else { chop; die "cant parse line $.:\n|$_|"; }' \
    < ecotig.6.4ucsc > anophelesEcores.bed
    hgLoadBed -tab dm1 anophelesEcores anophelesEcores.bed


# MASKED-QUERY XENO RNA ALIGNMENTS (DONE 10/22/03 angie)
    # Experimental track... if this TRF-masking works out, then find out 
    # what it would take to add it as an option to Mark's genbank stuff.
    ssh kksilo
    mkdir /cluster/data/dm1/bed/xenoMrnaMasked
    cd /cluster/data/dm1/bed/xenoMrnaMasked
    # Grab the latest full-release (137) genbank mRNA's.  Strip the 
    # .version suffixes so we can use accessions from mrna.gbidx to 
    # pick out the non-D.mel. mrnas.  
    perl -wpe 's/^>(\w+).\d+/>$1/' \
      /cluster/data/genbank/data/processed/genbank.137.0/full/mrna.fa \
      > allMrna.fa
    grep -v "Drosophila melanogaster" \
      /cluster/data/genbank/data/processed/genbank.137.0/full/mrna.gbidx \
        | awk '{print $1;}' | grep -v "^#" \
    > xenoAcc.lst
    faSomeRecords allMrna.fa xenoAcc.lst flyXenoRna.fa
    # Split up the sequences into manageably sized files.  
    mkdir flyXenoRnaSplit
    faSplit about flyXenoRna.fa 10000000 flyXenoRnaSplit/xenoRna
    # Now TRF-mask the sequences... use repeats with period <= 9 to mask.
    mkdir trf
    rm -f trf.log; touch trf.log
    foreach f (flyXenoRnaSplit/*.fa)
      set b = trf/$f:t:r.bed
      echo $f to $b...
      /cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $f /dev/null \
        -bedAt=$b -tempDir=/tmp -maxPeriod=9 >& trf.log
      maskOutFa -soft $f $b $f
    end
    # distribute masked xenoRna sequence on i-servers
    mkdir /cluster/bluearc/dm1/mrna.137
    cp -p /cluster/data/dm1/bed/xenoMrnaMasked/flyXenoRnaSplit/*.fa \
      /cluster/bluearc/dm1/mrna.137/

    ssh kk
    cd /cluster/data/dm1/bed/xenoMrnaMasked
    mkdir psl
    ls -1S /iscratch/i/dm1/nib/chr*.nib > fly.lst
    ls -1S /cluster/bluearc/dm1/mrna.137/xenoRna*.fa > mrna.lst
    echo '#LOOP \
/cluster/bin/i386/blat -maxIntron=50000 $(path1) {check in line+ $(path2)} -q=rnax -t=dnax -mask=lower {check out line+ psl/$(root1)_$(root2).psl} \
#ENDLOOP' > gsub
    gensub2 fly.lst mrna.lst gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 814 of 814 jobs
#Average job time:                 459s       7.65m     0.13h    0.01d
#Longest job:                     1756s      29.27m     0.49h    0.02d
#Submission to last job:          2145s      35.75m     0.60h    0.02d
    ssh kksilo
    cd /cluster/data/dm1/bed/xenoMrnaMasked
    pslSort dirs raw.psl /cluster/store2/temp psl
    pslReps raw.psl cooked.psl /dev/null -minAli=0.25
    # pslFilter -minMatch=60 -gapSizeLogMod=2 -minScore=30 cooked.psl filt.psl
    pslFilter -minAli=250 -minUniqueMatch=15 cooked.psl filt.psl
    pslSortAcc nohead chrom /cluster/store2/temp filt.psl
    pslCat -dir chrom > xenoMrnaMasked.psl
    rm -r chrom raw.psl cooked.psl filt.psl
    # Load into database as so:
    ssh hgwdev
    cd /cluster/data/dm1/bed/xenoMrnaMasked
    hgLoadPsl dm1 xenoMrnaMasked.psl -tNameIx
    # Looks like no need to hgLoadSeq -- seqs are already loaded by Mark's 
    # stuff.


# BLAT HONEYBEE (apiMel0) (DONE 1/13/04 angie)
    ssh kk
    mkdir /cluster/data/dm1/bed/blatApiMel0
    cd /cluster/data/dm1/bed/blatApiMel0
    mkdir psl
    ls -1S /iscratch/i/dm1/nib/*.nib > fly.lst
    ls -1S /iscratch/i/apiMel0/chunks/*.fa > bee.lst
cat << 'EOF' > gsub
#LOOP
/cluster/bin/i386/blat -mask=lower -qMask=lower -q=dnax -t=dnax {check in exists+ $(path1)} {check in line+ $(path2)} {check out line+ /cluster/data/dm1/bed/blatApiMel0/psl/$(root1)_$(root2).psl}
#ENDLOOP
'EOF'
    # << this line makes emacs coloring happy
    gensub2 fly.lst bee.lst gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 495 of 495 jobs
#Average job time:                 802s      13.37m     0.22h    0.01d
#Longest job:                    26590s     443.17m     7.39h    0.31d
#Submission to last job:         27028s     450.47m     7.51h    0.31d
    # postprocess
    pslCat -dir psl > blatApiMel0.psl
    # load
    ssh hgwdev
    cd /cluster/data/dm1/bed/blatApiMel0
    hgLoadPsl dm1 blatApiMel0.psl
    mkdir /gbdb/dm1/apiMel0
    foreach f (/cluster/data/apiMel0/groups/*.fa)
      ln -s $f /gbdb/dm1/apiMel0/$f:t
    end
    hgLoadSeq dm1 /gbdb/dm1/apiMel0/*.fa


#  miRNA track (DONE - 2004-05-04 - Hiram)
    #	data from: Sam Griffiths-Jones <sgj@sanger.ac.uk>
    #	and Michel.Weber@ibcg.biotoul.fr
    #	notify them if this assembly updates to renew this track
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/miRNA
    cd /cluster/data/dm1/bed/miRNA
    wget --timestamping \
    "ftp://ftp.sanger.ac.uk/pub/databases/Rfam/miRNA/genomes/dme_bdgp3.*"
    grep -v "^track " dme_bdgp3.bed | sed -e "s/ /\t/g" > dm1.bed
    hgLoadBed dm1 miRNA dm1.bed
    # entry in trackDb/trackDb.ra already there
    #	featureBits dm1 miRNA
    #	6845 bases of 126527731 (0.005%) in intersection


# BLASTZ D.YAKUBA (DONE 5/22/04 angie)
    ssh kksilo
    mkdir /cluster/data/dm1/bed/blastz.droYak1.2004-05-22
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22
    cat << '_EOF_' > DEF
# D.melanogaster vs. D.yakuba
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
BLASTZ_ABRIDGE_REPEATS=0

# TARGET - D. melanogaster
SEQ1_DIR=/cluster/bluearc/drosophila/dm1/nib
# unused: SEQ1_RMSK=
SEQ1_SMSK=
SEQ1_FLAG=-drosophila
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY - D. yakuba
SEQ2_DIR=/iscratch/i/droYak1/nib
# unused: SEQ2_RMSK=
SEQ2_SMSK=
SEQ2_FLAG=-drosophila
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/cluster/data/dm1/bed/blastz.droYak1.2004-05-22

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len

#DEBUG=1
'_EOF_'
    # << this line keeps emacs coloring happy
    # run bash shell if you don't already:
    bash
    source DEF
    mkdir run
    ~angie/hummus/make-joblist $DEF > $BASE/run/j
    sh ./xdir.sh
    cd run
    sed -e 's#^#/cluster/home/angie/schwartzbin/#' j > j2
    wc -l j*
    head j2
    mv j2 j
    # cluster run
    ssh kk
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/run
    para create j
    para try, check, push, check, ....
#Completed: 672 of 672 jobs
#Average job time:                 164s       2.74m     0.05h    0.00d
#Longest job:                     1836s      30.60m     0.51h    0.02d
#Submission to last job:          2433s      40.55m     0.68h    0.03d

    # back on kksilo...
    mkdir /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/run.1
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/run.1
    ~angie/hummus/do.out2lav ../DEF > j
    # small cluster run
    ssh kki
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/run.1
    para create j
    para try, check, push, check, ....
#Completed: 21 of 21 jobs
#Average job time:                 453s       7.54m     0.13h    0.01d
#Longest job:                      650s      10.83m     0.18h    0.01d
#Submission to last job:           685s      11.42m     0.19h    0.01d
    cd ..
    rm -r raw

    # third run: lav -> axt
    ssh kki
    cd /cluster/data/hg16/bed/blastz.droYak1.2004-05-22
    mkdir axtChrom pslChrom run.2
    cd run.2
    cat << '_EOF_' > do.csh
#!/bin/csh -ef
cd $1
set chr = $1:t
cat `ls -1 *.lav | sort -g` \
| $HOME/bin/x86_64/lavToAxt stdin \
    /cluster/bluearc/drosophila/dm1/nib /iscratch/i/droYak1/nib stdout \
| $HOME/bin/x86_64/axtSort stdin ../../axtChrom/$chr.axt 
$HOME/bin/x86_64/axtToPsl ../../axtChrom/$chr.axt ../../S1.len ../../S2.len \
  ../../pslChrom/$chr.psl
'_EOF_'
    # << this line keeps emacs coloring happy
    chmod a+x do.csh
    cp /dev/null jobList
    foreach d (../lav/chr*)
      echo "do.csh $d" >> jobList
    end
    para create jobList
    para try, check, push, check
#Completed: 11 of 11 jobs
#Average job time:                1516s      25.27m     0.42h    0.02d
#Longest job:                     2903s      48.38m     0.81h    0.03d
#Submission to last job:          2903s      48.38m     0.81h    0.03d

# CHAIN YAKUBA BLASTZ (DONE 5/27/04 angie)
    # Run axtChain on little cluster
    ssh kki
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22
    mkdir -p axtChain/run1
    cd axtChain/run1
    mkdir out chain
    ls -1S /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChrom/*.axt \
      > input.lst
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out exists out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    cat << '_EOF_' > doChain
#!/bin/csh
axtFilter -notQ=chrUn_random $1 \
| axtChain -verbose=0 stdin \
  /cluster/bluearc/drosophila/dm1/nib \
  /iscratch/i/droYak1/nib $2 > $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doChain
    gensub2 input.lst single gsub jobList
    para create jobList
    para try, check, push, check...
#Completed: 11 of 11 jobs
#Average job time:                  18s       0.30m     0.01h    0.00d
#Longest job:                       35s       0.58m     0.01h    0.00d
#Submission to last job:            35s       0.58m     0.01h    0.00d

    # now on the cluster server, sort chains
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    chainMergeSort run1/chain/*.chain > all.chain
    chainSplit chain all.chain
    rm run1/chain/*.chain

    # take a look at score distr's
    foreach f (chain/*.chain)
      grep chain $f | awk '{print $2;}' | sort -nr > /tmp/score.$f:t:r
      echo $f:t:r
      textHistogram -binSize=10000 /tmp/score.$f:t:r
      echo ""
    end

    # Load chains into database
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain/chain
    foreach i (*.chain)
        set c = $i:r
        echo loading $c
        hgLoadChain dm1 ${c}_chainDroYak1 $i
    end


# NET YAKUBA BLASTZ (DONE 5/27/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    chainPreNet all.chain ../S1.len ../S2.len stdout \
    | chainNet stdin -minSpace=1 ../S1.len ../S2.len stdout /dev/null \
    | netSyntenic stdin noClass.net

    # Add classification info using db tables:
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    netClass -noAr noClass.net dm1 droYak1 yakuba.net

    # Make a 'syntenic' subset:
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    rm noClass.net
    netFilter -syn yakuba.net > yakubaSyn.net

    # Load the nets into database 
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    netFilter -minGap=10 yakuba.net |  hgLoadNet dm1 netDroYak1 stdin
    netFilter -minGap=10 yakubaSyn.net | hgLoadNet dm1 netSyntenyDroYak1 stdin


# MAKE VSDROYAK1 DOWNLOADABLES (DONE 5/27/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    cp all.chain yakuba.chain
    zip /cluster/data/dm1/zips/yakuba.chain.zip yakuba.chain
    rm yakuba.chain
    zip /cluster/data/dm1/zips/yakuba.net.zip yakuba.net

    ssh hgwdev
    mkdir /usr/local/apache/htdocs/goldenPath/dm1/vsDroYak1
    cd /usr/local/apache/htdocs/goldenPath/dm1/vsDroYak1
    mv /cluster/data/dm1/zips/yakuba*.zip .
    md5sum *.zip > md5sum.txt
    # Copy over & edit README.txt w/pointers to chain, net formats.


# GENERATE DROYAK1 MAF FOR MULTIZ FROM NET (DONE 5/27/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    netSplit yakuba.net net
    ssh kolossus
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22
    mkdir axtNet
    foreach f (axtChain/net/*)
      set chr = $f:t:r
      netToAxt $f axtChain/chain/$chr.chain /cluster/data/dm1/nib \
        /cluster/data/droYak1/nib stdout \
      | axtSort stdin axtNet/$chr.axt
    end

    mkdir mafNet
    foreach f (axtNet/chr*.axt)
      set maf = mafNet/$f:t:r.my.maf
      axtToMaf $f \
            /cluster/data/dm1/chrom.sizes /cluster/data/droYak1/chrom.sizes \
            $maf -tPrefix=dm1. -qPrefix=droYak1.
    end


# GENERATE DP1 MAF FOR MULTIZ FROM NET (DONE 5/27/04 angie)
    ssh kolossus
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14
    mkdir mafNet
    foreach f (axtNetChrom/chr*.axt)
      set maf = mafNet/$f:t:r.mp.maf
      axtSort $f stdout \
      | axtToMaf stdin \
            /cluster/data/dm1/chrom.sizes /cluster/data/dp1/chrom.sizes \
            $maf -tPrefix=dm1. -qPrefix=dp1.
    end


# MULTIZ MELANOGASTER/YAKUBA/PSEUDOOBSCURA (DONE 5/27/04 angie)
    # put the MAFs on bluearc
    ssh kksilo
    mkdir -p /cluster/bluearc/multiz.fly/my
    cp /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/mafNet/*.maf \
      /cluster/bluearc/multiz.fly/my
    mkdir -p /cluster/bluearc/multiz.fly/mp
    cp /cluster/data/dm1/bed/blastz.dp1.2003-11-14/mafNet/*.maf \
      /cluster/bluearc/multiz.fly/mp


    ssh kki
    mkdir /cluster/data/dm1/bed/multiz.dm1droYak1dp1
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp1
    mkdir myp
    # Wrapper script required because of stdout redirect:
    cat << '_EOF_' > doMultiz
#!/bin/csh
/cluster/bin/penn/multiz $1 $2 - > $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doMultiz
    rm -f jobList
    foreach file (/cluster/bluearc/multiz.fly/my/*.maf) 
      set root=$file:t:r:r
      echo "doMultiz /cluster/bluearc/multiz.fly/mp/${root}.mp.maf $file /cluster/data/dm1/bed/multiz.dm1droYak1dp1/myp/${root}.maf" >> jobList
    end
    para create jobList
    para try, check, push, check
#Completed: 11 of 11 jobs
#Average job time:                  53s       0.88m     0.01h    0.00d
#Longest job:                      141s       2.35m     0.04h    0.00d
#Submission to last job:           141s       2.35m     0.04h    0.00d

    # clean up bluearc (these are big files!)
    rm -r /cluster/bluearc/multiz.fly

    # zip it for download
    zip -j /cluster/data/dm1/zips/multizDm1DroYak1Dp1.zip myp/*.maf

    # setup external files for database reference
    ssh hgwdev
    mkdir -p /gbdb/dm1/multizDroYak1Dp1
    ln -s /cluster/data/dm1/bed/multiz.dm1droYak1dp1/myp/*.maf \
      /gbdb/dm1/multizDroYak1Dp1
    # load into database
    hgLoadMaf -warn dm1 multizDroYak1Dp1
    # put it out for download
    mkdir /usr/local/apache/htdocs/goldenPath/dm1/multizDroYak1Dp1
    cd /usr/local/apache/htdocs/goldenPath/dm1/multizDroYak1Dp1
    mv /cluster/data/dm1/zips/multizDm1DroYak1Dp1.zip .
    md5sum *.zip > md5sum.txt


