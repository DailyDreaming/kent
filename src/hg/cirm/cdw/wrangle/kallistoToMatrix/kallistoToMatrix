#!/usr/bin/env python2.7

# a converter for kallisto output files
# parses ~800 cells in 1-2 minutes
# and merges everything into a single matrix that can be read with one line in R
# also outputs a binary hash files that can be read from python with one line

import logging, sys, optparse
from collections import defaultdict
from os.path import join, basename, dirname, isfile, isdir, splitext
import os, marshal, glob
from multiprocessing.dummy import Pool as ThreadPool 

# === command line interface, options and help ===
parser = optparse.OptionParser("""usage: %prog [options] dirName outFname - collect expression counts from the files abundance.tsv, given a directory containing kallisto output directories.

Example:
kallistoToMatrix /hive/groups/cirm/submit/quake/quakeBrainGeo1/kallistoOut quakeBrainGeo1.tab -t 20
""")

parser.add_option("-d", "--debug", dest="debug", action="store_true", help="show debug messages") 
parser.add_option("-t", "--threads", dest="threads", action="store", type="int", help="number of threads to use, default %default", default=10) 
(options, args) = parser.parse_args()

if options.debug:
    logging.basicConfig(level=logging.DEBUG)
else:
    logging.basicConfig(level=logging.INFO)
# ==== functions =====
    
def iterAbundPaths(dirname):
    " yield all paths of files abundance.tsv in subdirs under dirname "
    for subDir in glob.glob(join(dirname, "*")):
        if not isdir(subDir):
            continue
        abdPath = join(subDir, "abundance.tsv")
        if not isfile(abdPath):
            continue
        yield abdPath

def parseKallisto(fname):
    """ parse a kallisto abundance.tsv file, return dict transcriptId -> est_tpm 
    Does not return a value for transcripts where est_tpm is 0
    """
    logging.debug("parsing %s" % fname)
    d = {}
    ifh = open(fname)
    ifh.readline()
    for line in ifh:
        fs = line.rstrip("\n").split("\t")
        if fs[3]!="0":
            d[fs[0]] = float(fs[3])
    return d
        
def outputBigMatrix(names, results, outFname):
    """
    given a list of names and a list of transcript -> count dictionaries,
    write out a matrix with transcript -> counts in columns
    """
    logging.info("Writing data to file %s" % outFname)
    ofh = open(outFname, "w")
    # write header
    ofh.write("#transcript\t%s\n" % "\t".join(names))
    
    # create set of all transcript names
    allTrans = set()
    for res in results:
        allTrans.update(res)

    # write out matrix
    for trans in allTrans:
        ofh.write("%s\t" % trans)
        row = []
        for countDict in results:
            row.append(str(countDict.get(trans, 0)))
        ofh.write("\t".join(row))
        ofh.write("\n")
    ofh.close()
        
# this was a try to write sparse matrices. 
# it results in smaller data files, but they are harder to compress with gzip
# overall the non-sparse matrices were smaller on the quake dataset

#def outputBigMatrixSparse(names, results, outFname):
    #logging.info("Writing data to file %s" % outFname)
    #ofh = open(outFname, "w")
    
    # create set of all transcript names
    #allTrans = set()
    #for res in results:
        #allTrans.update(res)

    # create dict with transcript -> number
    # and write out this mapping
    #ofh.write("#")
    #transToId = {}
    #for i, trans in enumerate(allTrans):
        #transToId[trans] = i
        #ofh.write("#%s=%d\n" % (trans, i))

    # write out matrix
    #for cellId, transDict in zip(names, results):
        #ofh.write("%s\t" % cellId)
        #strList = ["%d=%f" % (transToId[trans], val) for trans, val in transDict.iteritems()]
        #ofh.write(",".join(strList))
        #ofh.write("\n")
    #ofh.close()
    
# ----------- main --------------
if args==[]:
    parser.print_help()
    exit(1)

inDir = args[0]
outFname = args[1]

inFnames = list(iterAbundPaths(inDir))
logging.info("Found %d input files" % len(inFnames))

threadCount = options.threads
logging.info("Using %d threads to parse input files" % threadCount)

# multithreading in only 4 lines
pool = ThreadPool(threadCount)
results = pool.map(parseKallisto,inFnames)
pool.close()
pool.join()

# reduce the filenames to only their parent directory names
cellNames = [splitext(basename(dirname(f)))[0] for f in inFnames]

#outputBigMatrixSparse(cellNames, results, outFname)
outputBigMatrix(cellNames, results, outFname)

# also output as a binary file for now
# it's a lot easier and faster to parse, at least for python scripts
# can be read from python with a single line:
# matrix = marshal.load(open("data.tab.marshal"))
# matrix is then a nested hash: cellName -> transcript -> count
binPath = outFname+".marshal"
logging.info("Writing %s" % binPath)
allData = {}
for name, transDict in zip(cellNames, results):
    allData[name] = transDict
marshal.dump(allData, open(binPath, "wb"))
